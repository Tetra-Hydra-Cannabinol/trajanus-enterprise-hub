Title: Conversational Memory in LangChain for 2025
Channel: James Briggs
Video ID: EtldFS3JbGs
URL: https://www.youtube.com/watch?v=EtldFS3JbGs
Duration: 44:20
Level: INTERMEDIATE
Application: LangChain
Topics: Conversational Memory, ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory, RunnableWithMessageHistory, LCEL, LangSmith, Token Usage
Ingested: 2025-12-21
Source: Playwright Browser Extraction
==============================================================================

0:00 In this chapter, we're going to be taking a look at conversational memory in Langchain. We're going to be taking a\n0:06 look at the core like chat memory components that have already been in\n0:12 line chain since the start, but are essentially no longer in the library.\n0:18 And we'll be seeing how we actually implement those historic conversational\n0:24 memory utilities in the new versions of line chain. So 0.3. Now, as a\n0:31 pre-warning, this chapter is fairly long, but that is because conversational memory is just such a critical part of\n0:40 chat bots and agents. Conversational memory is what allows them to remember previous interactions. And without it,\n0:47 our chat bots and agents would just be responding to the most recent message\n0:53 without any understanding of previous interactions within a conversations. So they would just not be conversational.\n1:00 And depending on the type of conversation, we might want to go with\n1:05 various approaches to how we remember those interactions within a\n1:11 conversation. Now, throughout this chapter, we're going to be focusing on these four memory types. We'll be\n1:18 referring to these, and I'll be showing you actually how each one of these works. But what we're really focusing on\n1:24 is rewriting these for the latest version of lang chain using the what's\n1:29 called the runnable with message\n1:36 history. So we're going to be essentially taking a look at the original implementations for each of\n1:42 these four original memory types and then we'll be rewriting them with the the runnable memory history class. So\n1:49 just taking a look at each of these four very quickly. Consential buffer memory\n1:55 is I think the simplest and most intuitive of these memory types. It is\n2:01 literally just you have your messages. They come into this object. They are\n2:08 stored in this object as essentially a list and when you need them again it\n2:13 will return them to you. There's nothing nothing else to it. It's super simple. the conversation buffet window memory.\n2:19 Okay, so new word in the middle there, window. This works in pretty much the\n2:25 same way, but those messages that it has stored, it's not going to return all of them for you. Instead, it's just going\n2:31 to return the most recent, let's say, the most recent three, for example.\n2:37 Okay? And that is defined by a parameter K. conversational summary memory. Rather\n2:42 than keeping track of the entire uh interaction memory directly, what it's\n2:48 doing is as those interactions come in, it's actually going to take them and it's going to compress them into a\n2:54 smaller little summary of what has been within that conversation. And as every a\n3:00 new interaction is coming in, it's going to do that and keep iterating on that summary. And then that is going to be\n3:06 returned to us when we need it. And finally we have the conversational summary buffer memory. So this is it's\n3:14 taking so the buffer part of this is actually referring to very similar thing\n3:19 to the buffer window memory but rather than it being a you know most K messages\n3:24 it's looking at the number of tokens within your memory and it's returning the most recent K tokens. That's what\n3:33 the buffer part is there. And then it's also merging that with the summary\n3:39 memory here. So essentially what you're getting is almost like a list of the most recent messages based on the token\n3:46 length rather than the number of interactions plus a summary which would\n3:51 you know come at the the top here. So you get kind of both. The idea is that obviously this summary here would\n3:58 maintain all of your interactions in a very compressed form. So you're you're\n4:04 losing less information and you're still maintaining you know maybe the very first interaction the user might have\n4:10 introduced themselves given you their name hopefully that would be maintained within the summary and it would not be\n4:17 lost and then you have almost like a higher resolution on the most recent um K or K tokens from your memory. Okay, so\n4:26 let's jump over to the code. We're going into the 04 chat memory notebook. Open that in Collab. Okay, now here we are.\n4:33 Let's go ahead and install the prerequisites. Run all.\n4:39 We again can or cannot use alignmith. It is up to you. Enter that. And let's come\n4:46 down and start. So firstly just initialize our LM using 40 mini in this\n4:53 example again low temperature and we're going to start with conversation buffer\n4:58 memory. Okay. So this is the original version of this uh memory type. So let\n5:07 me uh where are we? We're here. So memory conversation buffer memory and we're returning messages that needs to\n5:14 be set to true. So the reason that we set return messages to true it mentions\n5:19 up here is if you do not do this, it's going to be returning your chat history\n5:25 as a string to an LLM. whereas well chat lens nowadays would expect\n5:33 message objects. So yeah, you just want to be returning these as messages rather\n5:38 than as strings. Okay, otherwise yeah, you're going to get some kind of strange behavior out from your LLMs if you\n5:45 return them strings. So you do want to make sure that it's true. I think by default it might not be true, but this\n5:51 is coming this is deprecated, right? It does tell you here as deprecation warning. This is coming from older blank\n5:58 chain but it's a good place to start just to understand this and then we're going to rewrite this with the runnables which is the recommended way of doing so\n6:05 nowadays. Okay. So adding messages to our memory. We're going to write this.\n6:12 Okay. So it's just a it's just a conversation user AI user AI so on random chat. Main things to note here is\n6:19 I do provide my name. We have the the model's name right towards the start of those interactions. Okay. Okay. So, I'm\n6:25 just going to add all of those. We'll do it like this. Okay. Then we can just see\n6:33 we can load our history like so. So, let's just see what we have there. Okay.\n6:39 So, we have a human message, AI message, human message. Right? This is it exactly what we I showed you just here. It's\n6:46 just in that message format from Langchain. Okay. So, we can do that.\n6:51 Alternatively, we can actually do this. So we can get our memory. We initialize the constational buffer memory as we did\n6:58 before and we can actually add it directly the message into our memory\n7:03 like that. So we can use this add user message add AI message so on and so on. Load again and it's going to give us the\n7:09 exact same thing again. There's multiple ways to do uh the same thing. Cool. So we have that to pass all of this into\n7:16 our LLM. Again this is all deprecated stuff. We're going to learn how to use properly in a moment. But this is how\n7:23 long chain was doing it in the past. So to pass all of this into our lm, we'd be\n7:28 using this conversation chain, right? Again, this is deprecated. Nowadays, we\n7:33 would be using ll for this. So I I just want to show you, okay, how this would\n7:39 all go together. And then we would invoke, okay, what is my name again? Let's run that and we'll see what we\n7:45 get. It's remembering everything, remember? So this conversation buffer memory, it doesn't drop messages. It\n7:52 just remembers everything, right? And honestly with the sort of high context\n7:57 windows of many LMs, that might be what you do. It depends on how long you expect a conversation to go on for, but\n8:03 you could you probably in most cases would get away with this. Okay, so what\n8:09 let's see what we get. Um I say, \"What is my name again?\" Okay, let's see what it gives me. Says, \"Your name is James.\"\n8:15 Great. Thank you. That works. Now, as I mentioned, all of this that I just showed you is actually\n8:21 deprecated. That's the old way of doing things. Let's see how we actually do this in modern or up to-ate blank chain.\n8:28 So, we're going to be using this runnable with message history. To implement that, we will need to use LSL.\n8:35 And for that, we will need to just define prompt templates, our LM as we usually would. Okay. So, we're going to\n8:41 set up our system prompt, which is just your helpful assistant called Zeta. Okay, we're going to put in this\n8:48 messages placeholder. Okay, so that's important. Essentially, that is where our messages that are\n8:56 coming from our conversational buffer memory is going to be inserted. Right,\n9:01 so it's going to be that chat history is going to be inserted after our system prompt but before our most recent query\n9:08 which is going to be inserted last here. Okay, so messages placeholder item\n9:13 that's important and we use that throughout the course as well. So we use it both for chat history and we'll see\n9:19 later on. We also use it for the intermediate thoughts that a agent would go through as well. So important to\n9:26 remember that little thing. We'll link our prompt template to our LM again if\n9:32 we would like. We could also add in the I think we only have the query here. Oh,\n9:39 we would probably also want our history as well. Uh, but I'm not going to do that right now. Okay, so we have our\n9:46 pipeline and we can go ahead and actually define our runnable with message history. Now, this class or\n9:53 object when we are initializing it does require a few items. We can see them here. Okay, so we see that we have our\n10:00 pipeline with history. So, it's basically going to be uh you can you can see here, right? We have that history\n10:05 messages key, right? This here has to align with what we provided as the\n10:10 messages placeholder in our pipeline. Right? So we have our pipeline prompt\n10:17 template here and here. Right? So that's where it's coming from. It's coming from messages placeholder. The variable name\n10:23 is history. Right? That's important. That links to this. Then for the input\n10:29 messages key here we have query that again links to this. Okay. are both\n10:37 important to have there. The other thing that is important is obviously we're passing in that pipeline\n10:43 from before. But then we also have this get session history. Basically what this is doing is it's saying okay I need to\n10:49 get uh the list of messages that make up my chat history that are going to be inserted into this variable. So that is\n10:55 a function that we define. Okay. And within within this function, what we're trying to do here is actually replicate\n11:03 what we have with the previous conversation buffer memory. Okay, so\n11:09 that's what we're doing here. So it's very simple, right? So we have uh this\n11:15 inmemory chat message history. Okay, so that's just the object that we're going to be returning. What this will do is it\n11:22 will set up a session ID. The session I is essentially like a unique identifier so that each conversation or interaction\n11:30 within a single conversation is being mapped to a specific conversation. So you don't have overlapping let's say of\n11:35 multiple users using the same system you want to have a unique session ID for each one of those. Okay. And what it's\n11:41 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here. We are going\n11:48 to initialize that session with an inmemory chat message history. Okay, that's it.\n11:57 And we return. Okay, and all that's going to do is it's going to basically append our messages. They will be\n12:03 appended within this chat map session ID and they're going to get returned.\n12:08 There's nothing really there's nothing else to it to be honest. So, we invoke\n12:14 our runnable. Let's see what we get. Oh, I need to run this.\n12:20 Okay, note that we do have this config. So, we have the session ID. That's to again, as I mentioned, keep different\n12:27 conversations separate. Okay, so we've run that. Now, let's run a few more. So,\n12:32 what is my name again? Let's see if it remembers. Your name is James. How can I help you today, James? Okay, so it's\n12:41 what we've just done there is literally conversation buffer memory but for up\n12:47 to-ate lang chain with L cell with runnables. So you know the recommended way of doing\n12:54 it nowadays. So that's a very simple example. Okay, there's really not that\n13:00 much to it. It gets a little more complicated as we start thinking about the different types of memory. Although\n13:06 with that being said, it's not massively complicated. We're only rarely going to be changing the way that we're getting\n13:12 our interactions. So let's uh let's dive into that and see how we would do\n13:18 something similar with the conversation buffer window memory. But first, let's actually just understand okay what is\n13:24 the conversation buffer window memory. So as I mentioned near the start, it's going to keep track of the last k\n13:30 messages. So there's a few things to keep in mind here. More messages does\n13:35 mean more tokens sent with each request. And if we have more tokens in each request, it means that we're increasing\n13:41 the latency of our responses and also the cost. So with the previous memory\n13:46 type, we're just sending everything. And because we're sending everything, that is going to be increasing our cost. It's\n13:52 going to be increasing our latency for every message, especially as a conversation gets longer and longer. And we don't we might not necessarily want\n13:58 to do that. So with this conversation buffer window memory, we're going to\n14:03 just say, okay, just return me the most recent messages. Okay, so let's or let's\n14:10 see how that would work. Here we're going to return the most recent four messages. Okay, we are again make sure\n14:17 we've turned messages is set to true. Again, this is deprecated. This is just the old way of doing it. In a moment,\n14:23 we'll see the updated way of doing this. We'll add all of our messages.\n14:30 Okay, so we have this and just see here, right? So we've added in all these\n14:36 messages. There's more than four messages here and we can actually see that here. So we have human message AI\n14:42 human AI human AI human AI. Right? So we've got four pairs of human AI\n14:49 interactions there. But here we don't have as more than four pairs. So four pairs would take us back all the way to\n14:57 here. I'm researching different types of conversational uh memory. Okay. And if\n15:03 we take a look here, the most the first message we have is I'm researching different types of conversational memory. So it's cut off these two here\n15:11 which will be a bit problematic when we ask you what our name is. Okay. So let's just see going to be using conversation\n15:17 chain object again. Again just remember that is deprecated. And I want to say what is my name again?\n15:23 Let's see. Let's see what it says. Uh I'm sorry, but I don't have access to\n15:29 your name or any personal information. If you like, you can tell me your name, right? So it doesn't actually remember. Uh so that's kind of like a negative of\n15:38 the conversation buffer window memory. Of course, the uh to fix that in this\n15:43 scenario, we might just want to increase K. Maybe we say remember previous eight interaction pairs and it will actually\n15:50 remember. So where's my name again? Your name is James. So now it remembers. We've just modified how much it is\n15:57 remembering. But of course, you know, there's pros and cons to this. It really depends on what you're trying to build.\n16:02 So let's take a look at how we would actually implement this with the\n16:07 runnable with message history. Okay. So you getting a little more\n16:13 complicated here. Although it it's it's not it's not complicated but well we'll\n16:19 see. Okay, so we have a buffer window message history. We're creating a class here. This class is going to inherit\n16:26 from the base chat message history object from lang chain. Okay. And and\n16:31 all of our other message history objects are going to do the same thing before with the in-memory message object that\n16:39 was basically replicating the buffer memory. So we didn't actually need to do\n16:44 anything. We didn't need to define our own class here. So in this case we do.\n16:50 So we follow the same pattern that lang chain follows with this base chat\n16:56 message history. And you can see a few of the functions here that are important. So add messages and clear are\n17:01 the ones that we're going to be focusing on. We also need to have messages which this object attribute here. Okay. So\n17:07 we're just implementing the synchronous methods here. If we want this to be\n17:14 async, if we want to support async, we would have to add a add messages, um, a\n17:19 get messages and a clear as well. So, let's go ahead and do that. We have\n17:24 messages. We have K. Again, we're looking at remembering the top K messages or most recent K messages only.\n17:30 So, it's important that we have that variable. We are adding messages through this class. This is going to be used by\n17:37 line chain within our runnable. So, we need to make sure that we do have this method. And all we're going to be doing\n17:42 is extending the self messages uh list here. And then we're actually just going to be trimming that down so that we're\n17:49 not remembering anything beyond those, you know, most recent K messages\n17:55 that we have set from here. And then we also have the clear method\n18:00 as well. So we need to include that. That's just going to clear the history. Okay. So it's not this isn't\n18:05 complicated, right? it just gives us this nice default sandal interface for\n18:11 message history and we just need to make sure we're following that pattern. Okay, I've included the uh this print here\n18:17 just so we can see what's happening. Okay, so we have that and now for that\n18:23 get chat history function that we defined earlier rather than using the built-in method we're going to be using\n18:30 our own object which is a buffer window message history which we defined just here. Okay, so if session ID is not in\n18:39 the chat map as we did before, we're going to be initializing our buffer window message history. We're setting K\n18:45 up here with a default value of four. And then we just return it. Okay. And and and that is it. So uh let's run\n18:51 this. We have our runnable with message history. We have all of these variables\n18:56 which are exactly the same as before. But then we also have these variables here with this history factory config.\n19:03 And this is where if we have um new variables that we've added to our\n19:11 message history, in this case K that we have down here, we need to provide that\n19:17 to line chain and tell it this is a new configurable field. Okay. And we've also added it for the session ID here as\n19:23 well. So we're just being explicit and have everything in there. So we have that and we run. Okay. Now let's go\n19:32 ahead and invoke and see what we get. Okay, so important here this history\n19:38 factory config that is kind of being fed through into our invoke so that we can actually modify those variables from\n19:45 here. Okay, so we have config configurable session ID. Okay, we just put whatever we want in here and then we\n19:51 also have the number K. Okay, so remember the previous four interactions.\n19:58 I think in this one we're doing something slightly different. I think we're remembering the four interactions\n20:03 rather than the previous four interaction pairs. Okay, so my name is James. Uh we're going to go through I'm\n20:09 just going to actually clear this and then I'm going to start again and we're going to use the exact same add user\n20:15 message add AI message that we used before. We're just manually inserting all that into our history so that we can\n20:21 then just see okay what is the result and you can see that k equals 4 is actually unlike before where we were\n20:28 having the uh saving the top four interaction pairs we're now saving the\n20:35 most recent four interactions not pairs just interactions. And honestly I just\n20:41 think that's clearer. I think it's weird that the number four for K would actually save the most recent eight\n20:48 messages, right? I I think that's odd. So, I'm just not replicating that weirdness. We could if we wanted to. I\n20:56 just don't like it. So, I'm not doing that. And anyway, we can see from\n21:01 messages that we're returning just the most four recent messages. Okay, which\n21:06 would be these four. Okay, cool. So we've just using the runnable we've\n21:11 replicated the old way of having a window memory and okay I'm going to say\n21:18 what is my name again as before it's not going to remember so we can come to here I'm sorry but I don't say personal\n21:24 information no so on and so on if you like tell me your name it doesn't know now let's try a new one where we\n21:31 initialize a new session okay so we're going with ID K14 so that's going to create a new\n21:37 conversation in there and we're going to say we're going to set K to 14.\n21:43 Okay, great. I'm going to manually insert the other uh messages as we did before. Okay, and we can see all of\n21:50 those. You can see at the top here, we are still maintaining that hi, my name is James message. Now, let's see if it\n21:57 remembers my name. Your name is James. Okay, there we go. Cool. So, that is\n22:03 working. We can also see. So, we just added this. What is my name again? Let's just see if did that get added to our\n22:10 list of messages, right? What is my name again? Nice. And then we also have the response your name is James. So just by\n22:17 invoking this because we're using the the runnable with message history, it's\n22:22 just automatically adding all of that into our message history which is nice.\n22:28 Cool. All right. So that is the buffer window memory. Now we are going to take a look\n22:34 at how we might do something a little more complicated which is uh the the summaries. Okay. So when you think about\n22:41 the summary you know what are we doing? We're actually taking the messages we're\n22:46 using that lm call to summarize them to compress them and then we're storing\n22:52 them within messages. So let's see how we would actually uh do that. So to\n22:57 start with let's just see how uh it was done in old line chain. So we have conversation summary memory go through\n23:06 that and let's just see what we get. So again same interactions\n23:13 right I'm just invoking invoking invoking I'm not adding these directly to the messages because it actually\n23:18 needs to go through a um like that summarization process and if we have a\n23:25 look we can see it happening. Okay, current conversation. So, sorry, current\n23:30 conversation. Hello there, my name is James. AI is generating current conversation. The human introduces\n23:36 himself as James. AI greets James warmly and expresses its readiness to chat and assists inquiring about how his day is\n23:43 going, right? So, it's summarizing the the previous interactions. And then we\n23:49 have, you know, after that summary, we have the most recent human message and then the AI is going to generate its\n23:55 response. Okay? And that continues going continues going and you can see that the the final summary here is going to be a\n24:01 lot longer. Okay. And it's different that first summary of course as day he mentions stage of researching different\n24:07 types of conversational memory. The AI responds enthusiastically explaining that conversational memory includes\n24:12 short-term memory, long-term memory, contextual memory, personalized memory and then inquires if James is focused on the specific type of memory. Okay, cool.\n24:21 So we get essentially the summary is just getting uh longer and longer as we go. But at some point the idea is that\n24:28 it's not going to keep just growing and it should actually be shorter than if you were saving every single interaction\n24:33 whilst maintaining as much of the information as possible. But of course\n24:39 you are going to maintain all of the information that you would with for example the the buffer memory. Right?\n24:46 With the summary, you are going to lose information, but hopefully less information than if you're just cutting\n24:54 interactions. So, you're trying to reduce your token count whilst maintaining as much information as\n25:00 possible. Now, let's go and ask uh what is my name again? It should be able to answer\n25:06 because we can see in the summary here that I introduce myself as James.\n25:12 Okay, response. Your name is James. How is your research going? Okay. So, has that cool? Let's see how we'd implement\n25:19 that. So, again, as before, we're going to go with that conversation summary\n25:25 message history. We're going to be importing a system message. Uh we're going to be using that not for the LM\n25:30 that we're chatting with, but for the LM that will be generating our summary. So,\n25:37 actually that is not quite correct. There's create a summary. Not that it matters. It's just the dock string. So,\n25:43 we have our messages and we also have the llm. So different different attribute here to what we had before.\n25:48 When we initialize a conversation summary message history, we need to passing in our LM. We have the same\n25:55 methods as before. We have add messages and clear. And what we're doing is as messages coming, we extend with our\n26:02 current messages, but then we're modifying those. Okay. So we construct\n26:08 our like instructions to make a summary. Okay. So that is here we have the system\n26:14 prompt uh given the existing conversation summary and the new messages generate a new summary of the conversation ensuring to maintain as\n26:21 much relevant information as possible. Okay. Then we have a human message here through that we're passing the existing\n26:28 summary. Okay. And then we're passing in the new messages.\n26:33 Okay. Cool. So we format those and invoke the lm\n26:41 here. And then what we're doing is in the messages we're actually replacing the existing history that we had before\n26:48 with a new history which is just a single system summary message. Okay,\n26:55 let's see what we get. As before, we have that get chat history exactly the same as before. The only real difference\n27:01 is that we're passing in the LM parameter here. And of course, as we're passing in the lm parameter in here, it\n27:07 does also mean that we're going to have to include that in the configurable field spec and that we're going to need\n27:14 to include that when we're invoking our pipeline. Okay, so we run that pass in\n27:21 the lm. Now, of course, one side effect of uh\n27:26 generating summaries for everything is that we're actually, you know, we're generating more. So you are actually\n27:32 using quite a lot of tokens. Whether or not you are saving tokens or not actually depends on the length of a\n27:38 conversation. As a conversation gets longer. If you're storing everything after a little while that the token\n27:45 usage is actually going to increase. So if in your use case you expect to have\n27:50 shorter conversations, you would be saving money and tokens by just using this standard buffer memory. Whereas if\n27:59 you're expecting very long conversations, you would be saving tokens and money by using the summary\n28:05 history. Okay, so let's see what we got from there. We have a summary of the conversation. James introduced himself\n28:11 by saying, \"Hi, my name responded warmly, asking me, \"Hi, James.\" Introduction included details about\n28:16 token usage. Okay, so we actually uh included everything here, which we\n28:22 probably should not have done. Why did we do that? Uh so in here we're including all of the\n28:35 oh in here so using we're including all of the content from the messages. So I think maybe if we just do\n28:42 hcontent for x in messages that should resolve\n28:49 that. Okay there we go. So we quickly fix\n28:55 that. So yeah, before we're passing in the entire message object, which obviously includes all of this information, whereas actually we just\n29:01 want to be passing in the content. So we modified that and now we're getting what\n29:08 we'd expect. Okay, cool. And then we can keep going, right? So as we as we keep going, the\n29:13 summary should get more like abstract like as we just saw here, it's literally\n29:19 just giving us the messages directly almost. Okay, so we're getting bit summary there and we can keep going.\n29:25 We're going to add just more messages to that. We'll see the, you know, as we'll\n29:31 get send those, we'll get a response, send it again, get response. We're just adding all of that, invoking all of\n29:38 that, and that will be, of course, adding everything into our message history. Okay, cool. So, we've run that.\n29:44 Let's see what the uh latest summary is.\n29:49 Okay, and then we have this. So this is a summary that we have inside of our our chat history.\n29:55 Okay, cool. Now finally, let's see what is my name again. We can just double\n30:01 check, you know, has my name in there, so it should be able to tell us.\n30:09 Okay, cool. So your name is James. Pretty interesting. So let's have a\n30:14 quick look over at Langsmith. So the reason I want to do this is just to point out okay the different essentially\n30:22 token usage that we're getting with each one of these. Okay. So we can see that we have these runnable message history\n30:27 which probably improved in naming there. But we can see okay how long is each one\n30:33 of these taken? How many tokens are they also using? Come back to here we have\n30:39 this runnable message history. This is we'll go through a few of these maybe to\n30:45 here I think. And we can see here this is that first interaction where we're using the buffer memory and we can see\n30:52 how many tokens we used here. So 112 tokens when we're asking what is my name again. Okay then we modified this to\n31:01 include I think it was like 14 interactions or something along those lines and obviously increases the number\n31:06 of tokens that we're using. Right? So we can could see that actually happening all in Lang stuff which is quite nice\n31:11 and we can compare okay how many tokens is each one of these using. Now this is\n31:16 looking at the buffer window and then if we come down to here and look at this\n31:22 one. So this is using our summary. Okay so our summary with what is my name again actually use more tokens in this\n31:28 scenario right which is interesting because we're trying to compress information. The reason there's more is\n31:34 because there's not there hasn't been that many interactions. As the conversation length increases\n31:41 with the summary, this total number of tokens, especially if we prompt it correctly to keep that low, that should\n31:48 remain relatively small. Whereas with the buffer memory, that will just keep\n31:54 increasing and increasing as a as a conversation gets longer. So useful little way of using lang there\n32:02 to just kind of figure out okay in terms of tokens and costs what we looking at for each of these memory types. Okay so\n32:09 our final memory type acts as a mix of the summary memory and the buffer\n32:16 memory. So, what it's going to do is keep the buffer up until an N number of\n32:23 tokens and then once a message exceeds the number of token limit for the\n32:28 buffer, it is actually going to be added into our summary. So this memory has the\n32:34 benefit of remembering in detail the most recent interactions whilst also not\n32:42 having the limitation of using too many tokens as a conversation gets longer and\n32:49 even potentially exceeding context windows if you try super hard. So this is a very interesting approach. Now as\n32:56 before let's try the original way of implementing this then we will go ahead\n33:03 and use our update method for implementing this. So we come down to here and we're going to line chain\n33:09 memory import conversation summary buffer memory. Okay, a few things here.\n33:15 lm for summary. We have the n number of tokens that we can keep before they get\n33:22 added to the summary. and then return messages of course. Okay, you can see again this is deprecated. We use the\n33:28 conversation chain and then we just pass in our memory there and then we can chat. Okay, so super straightforward.\n33:36 First message, we'll add a few more here. Again, we have to invoke because our\n33:43 memory type here is using MLM to create those summaries as it goes. And let's\n33:49 see what they look like. Okay. So we can see for the first message here we have a human message and then an AI message.\n33:57 Then we come a little bit lower down again. Same thing human message is the first thing in our history here. Then\n34:04 it's a system message. So this is at the point where we've exceeded that 300 token limit and the memory type here is\n34:11 generating those summaries. So that summary comes in as a message and we can see okay the human named James\n34:18 introduces himself and mentions he's researching different types of conversational memory and so on and so on right okay cool so we have that then\n34:27 let's come down a little bit further we can see okay so the summary there okay\n34:34 so that's what we that's what we have that is the implementation for the old\n34:40 version of this memory again we can see is deprecated. So, how do we implement this for our more recent versions of\n34:49 lang chain and specifically 0.3? Well, again, we're using that runnable with message history and it looks a little\n34:57 more complicated than we were getting before, but it's actually just, you know, it's nothing too complex. We're\n35:05 just creating a summary as we did with the previous memory type. But the\n35:10 decision for adding to that summary is based on in this case actually the number of messages. So I didn't go with\n35:16 the the lang chain version where it's a number of tokens. I don't like that. I\n35:22 prefer to go with messages. So what I'm doing is saying okay last k messages.\n35:27 Okay. Once we exceed k messages, the messages beyond that are going to be\n35:33 added to the memory. Okay, cool. So let's see. We first initialize our\n35:41 conversation summary buffer message history class with lm and k. Okay, so\n35:48 these two here. So lm of course to create summaries and k is just the the limit of the number of messages that we\n35:53 want to keep before adding them to the summary or dropping them from our messages and adding them to the summary.\n36:00 Okay. So we will begin with okay do we have an existing summary. So the reason\n36:07 we set this to none is we can't extract the summary the existing summary unless\n36:14 it already exists. And the only way we can do that is by checking okay do we\n36:19 have any messages. If yes we want to check if within those messages we have a system message because we're we're doing\n36:25 the same structure as what we have up here where the system message that first system message is actually our summary.\n36:32 So that's what we're doing here. We're checking if there is a summary message already stored within our messages.\n36:38 Okay. So we're checking for that. If we find it, we just do we have this little print\n36:45 statement so we can see that we found something and then we just make our existing summary. I should actually move\n36:53 this to the first instance here. Okay, so that existing summary will be set to\n37:02 the first message. Okay, and this would be a system message\n37:08 rather than a string. Cool. So we have that. Then we want to\n37:14 add any new messages to our history. Okay. So we're extending the history\n37:20 there. And then we're saying, okay, if the length of our history is exceeds the\n37:25 K value that we set, we're going to say, okay, we found that many messages. We're going to be dropping the latest. It's\n37:30 going to be the latest two messages. this I will say here one thing or one\n37:36 problem with this is that we're not going to be saving that many tokens if we're summarizing every two messages.\n37:43 So, what I would probably do is in in an actual like production setting, I would\n37:50 probably say let's go up to 20 messages. And once we hit 20\n37:56 messages, let's take the previous 10, we're going to summarize them and put them into our summary alongside any, you\n38:02 know, previous summary that already existed. But in in, you know, this is also fine as well. Okay. So, we say we\n38:11 found those measures. We're going to drop the latest two messages. Okay. So, we pull the the oldest messages out. I\n38:20 should say not the latest. It's the oldest. Not the latest. Want to keep the latest\n38:26 and drop the oldest. So, we pull out the oldest messages and keep only the most\n38:32 recent messages. Okay. Then I'm saying, okay, if we if we don't\n38:38 have any old messages to summarize, we don't do anything. and we just return. Okay, so let's indicates that this has\n38:44 not been triggered, we would hit this. But in the case this has been triggered\n38:51 and we do have old messages, we're going to come to here. Okay, so this is we can\n38:59 see we have a system message prompt template saying given the existing conversation summary and the new messages generate a new summary of the\n39:06 conversation ensuring to maintain as much relevant information as possible. So if you want to be more conservative\n39:12 with tokens, we could modify this prompt here to say keep the summary to within\n39:17 the length of a single paragraph for example. And then we have our human\n39:23 message prompt template which can say okay here's the existing conversation summary and here are new messages. Now\n39:28 new messages here is actually the old messages but the way that we're framing it to the LM here is that we want to\n39:35 summarize the whole conversation, right? It doesn't need to have the most recent measures that we're storing within our\n39:42 buffer. It doesn't need to know about those. That's irrelevant to the summary. So, we just tell it that we have these\n39:47 new measures. And as far as this LM is concerned, this is like the full set of interactions. Okay. So, then we would\n39:54 format those and invoke our LM. And then we'll print out our new summary so we\n40:00 can see what's going on there. And we would prepend that new summary to our\n40:06 conversation history. Okay. And and this will work. So we can just prepend it\n40:13 like this because we've already popped\n40:18 where was it up here. If we have an existing summary, we already popped that from the list. So\n40:24 it's already been pulled out of that list. So it's okay for us to just we don't need to say like we don't need to\n40:31 do this because we've already dropped that initial system message if it existed. Okay. And then we have the\n40:37 clear method as before. So that's all of the logic for our conversational summary\n40:45 buffer memory. We redefine our get chat history function with the lm and k\n40:53 parameters there. And then we'll also want to set the configurable fields again. So that is just going to be of\n40:59 course session id lm and k. Okay. So now we can invoke the k value\n41:07 to begin with is going to be four. Okay. So we can see no old messages to\n41:13 update summary with. That's good. Let's invoke this a few times and let's see\n41:18 what we get. Okay. So no old messages to update summary with.\n41:26 found six matches dropping the oldest two. And then we have new summary in the conversation. James and Bruce himself is\n41:33 interested researching different types of conversational memory. Right? So you can see there's quite a lot in here at\n41:38 the moment. So we would definitely want to prompt the LLM, the summary LLM to keep\n41:45 that short. Otherwise, we're just getting a ton of stuff, right? But we can see that that is you\n41:52 know it's it's working it's functional. So let's go back and see if we can prompt it to be a little more concise.\n41:59 So we come to here ensuring to maintain as much relevant information as possible. However we need to keep\n42:09 our summary concise. The limit\n42:15 is a single short paragraph. Okay. something like this. Let's try and let's\n42:22 see what we get with that. Okay, so message one again, nothing to\n42:28 update. See this? So, new summary. You can see it's a bit shorter. It doesn't have all those bullet points.\n42:37 Okay, so that seems better. Let's see. So you can see the first summary is a\n42:44 bit shorter, but then as soon as we get to the second and third summaries, the second summary\n42:50 is actually slightly longer than the third one. Okay, so we're going to be we're going to be losing a bit of\n42:56 information in this case, more than we were before, but we're saving a ton of tokens. So that's of course a good\n43:03 thing. And of course, we could keep going and adding many interactions here. And we should see that this conversation\n43:09 summary will be it should maintain that sort of length of around one short\n43:14 paragraph. So that is it for this chapter on conversational memory. We've\n43:21 seen a few different memory types. We've implemented their old deprecated versions so we can see what they were\n43:28 like and then we've reimplemented them for the latest versions of lang chain.\n43:33 And to be honest, using logic where we are getting much more into the weeds and\n43:39 that is in some ways okay it complicates things that is true but in other ways it\n43:45 gives us a ton of control. So we can modify those memory types as we did with that final summary buffer memory type.\n43:52 We can modify those to our liking which is incredibly useful when you're\n43:58 actually building applications for the real world. So that is it for this chapter. We'll move on to the next one.\n0:00 In this chapter, we're going to be taking a look at conversational memory in Langchain. We're going to be taking a\n0:06 look at the core like chat memory components that have already been in\n0:12 line chain since the start, but are essentially no longer in the library.\n0:18 And we'll be seeing how we actually implement those historic conversational\n0:24 memory utilities in the new versions of line chain. So 0.3. Now, as a\n0:31 pre-warning, this chapter is fairly long, but that is because conversational memory is just such a critical part of\n0:40 chat bots and agents. Conversational memory is what allows them to remember previous interactions. And without it,\n0:47 our chat bots and agents would just be responding to the most recent message\n0:53 without any understanding of previous interactions within a conversations. So they would just not be conversational.\n1:00 And depending on the type of conversation, we might want to go with\n1:05 various approaches to how we remember those interactions within a\n1:11 conversation. Now, throughout this chapter, we're going to be focusing on these four memory types. We'll be\n1:18 referring to these, and I'll be showing you actually how each one of these works. But what we're really focusing on\n1:24 is rewriting these for the latest version of lang chain using the what's\n1:29 called the runnable with message\n1:36 history. So we're going to be essentially taking a look at the original implementations for each of\n1:42 these four original memory types and then we'll be rewriting them with the the runnable memory history class. So\n1:49 just taking a look at each of these four very quickly. Consential buffer memory\n1:55 is I think the simplest and most intuitive of these memory types. It is\n2:01 literally just you have your messages. They come into this object. They are\n2:08 stored in this object as essentially a list and when you need them again it\n2:13 will return them to you. There's nothing nothing else to it. It's super simple. the conversation buffet window memory.\n2:19 Okay, so new word in the middle there, window. This works in pretty much the\n2:25 same way, but those messages that it has stored, it's not going to return all of them for you. Instead, it's just going\n2:31 to return the most recent, let's say, the most recent three, for example.\n2:37 Okay? And that is defined by a parameter K. conversational summary memory. Rather\n2:42 than keeping track of the entire uh interaction memory directly, what it's\n2:48 doing is as those interactions come in, it's actually going to take them and it's going to compress them into a\n2:54 smaller little summary of what has been within that conversation. And as every a\n3:00 new interaction is coming in, it's going to do that and keep iterating on that summary. And then that is going to be\n3:06 returned to us when we need it. And finally we have the conversational summary buffer memory. So this is it's\n3:14 taking so the buffer part of this is actually referring to very similar thing\n3:19 to the buffer window memory but rather than it being a you know most K messages\n3:24 it's looking at the number of tokens within your memory and it's returning the most recent K tokens. That's what\n3:33 the buffer part is there. And then it's also merging that with the summary\n3:39 memory here. So essentially what you're getting is almost like a list of the most recent messages based on the token\n3:46 length rather than the number of interactions plus a summary which would\n3:51 you know come at the the top here. So you get kind of both. The idea is that obviously this summary here would\n3:58 maintain all of your interactions in a very compressed form. So you're you're\n4:04 losing less information and you're still maintaining you know maybe the very first interaction the user might have\n4:10 introduced themselves given you their name hopefully that would be maintained within the summary and it would not be\n4:17 lost and then you have almost like a higher resolution on the most recent um K or K tokens from your memory. Okay, so\n4:26 let's jump over to the code. We're going into the 04 chat memory notebook. Open that in Collab. Okay, now here we are.\n4:33 Let's go ahead and install the prerequisites. Run all.\n4:39 We again can or cannot use alignmith. It is up to you. Enter that. And let's come\n4:46 down and start. So firstly just initialize our LM using 40 mini in this\n4:53 example again low temperature and we're going to start with conversation buffer\n4:58 memory. Okay. So this is the original version of this uh memory type. So let\n5:07 me uh where are we? We're here. So memory conversation buffer memory and we're returning messages that needs to\n5:14 be set to true. So the reason that we set return messages to true it mentions\n5:19 up here is if you do not do this, it's going to be returning your chat history\n5:25 as a string to an LLM. whereas well chat lens nowadays would expect\n5:33 message objects. So yeah, you just want to be returning these as messages rather\n5:38 than as strings. Okay, otherwise yeah, you're going to get some kind of strange behavior out from your LLMs if you\n5:45 return them strings. So you do want to make sure that it's true. I think by default it might not be true, but this\n5:51 is coming this is deprecated, right? It does tell you here as deprecation warning. This is coming from older blank\n5:58 chain but it's a good place to start just to understand this and then we're going to rewrite this with the runnables which is the recommended way of doing so\n6:05 nowadays. Okay. So adding messages to our memory. We're going to write this.\n6:12 Okay. So it's just a it's just a conversation user AI user AI so on random chat. Main things to note here is\n6:19 I do provide my name. We have the the model's name right towards the start of those interactions. Okay. Okay. So, I'm\n6:25 just going to add all of those. We'll do it like this. Okay. Then we can just see\n6:33 we can load our history like so. So, let's just see what we have there. Okay.\n6:39 So, we have a human message, AI message, human message. Right? This is it exactly what we I showed you just here. It's\n6:46 just in that message format from Langchain. Okay. So, we can do that.\n6:51 Alternatively, we can actually do this. So we can get our memory. We initialize the constational buffer memory as we did\n6:58 before and we can actually add it directly the message into our memory\n7:03 like that. So we can use this add user message add AI message so on and so on. Load again and it's going to give us the\n7:09 exact same thing again. There's multiple ways to do uh the same thing. Cool. So we have that to pass all of this into\n7:16 our LLM. Again this is all deprecated stuff. We're going to learn how to use properly in a moment. But this is how\n7:23 long chain was doing it in the past. So to pass all of this into our lm, we'd be\n7:28 using this conversation chain, right? Again, this is deprecated. Nowadays, we\n7:33 would be using ll for this. So I I just want to show you, okay, how this would\n7:39 all go together. And then we would invoke, okay, what is my name again? Let's run that and we'll see what we\n7:45 get. It's remembering everything, remember? So this conversation buffer memory, it doesn't drop messages. It\n7:52 just remembers everything, right? And honestly with the sort of high context\n7:57 windows of many LMs, that might be what you do. It depends on how long you expect a conversation to go on for, but\n8:03 you could you probably in most cases would get away with this. Okay, so what\n8:09 let's see what we get. Um I say, \"What is my name again?\" Okay, let's see what it gives me. Says, \"Your name is James.\"\n8:15 Great. Thank you. That works. Now, as I mentioned, all of this that I just showed you is actually\n8:21 deprecated. That's the old way of doing things. Let's see how we actually do this in modern or up to-ate blank chain.\n8:28 So, we're going to be using this runnable with message history. To implement that, we will need to use LSL.\n8:35 And for that, we will need to just define prompt templates, our LM as we usually would. Okay. So, we're going to\n8:41 set up our system prompt, which is just your helpful assistant called Zeta. Okay, we're going to put in this\n8:48 messages placeholder. Okay, so that's important. Essentially, that is where our messages that are\n8:56 coming from our conversational buffer memory is going to be inserted. Right,\n9:01 so it's going to be that chat history is going to be inserted after our system prompt but before our most recent query\n9:08 which is going to be inserted last here. Okay, so messages placeholder item\n9:13 that's important and we use that throughout the course as well. So we use it both for chat history and we'll see\n9:19 later on. We also use it for the intermediate thoughts that a agent would go through as well. So important to\n9:26 remember that little thing. We'll link our prompt template to our LM again if\n9:32 we would like. We could also add in the I think we only have the query here. Oh,\n9:39 we would probably also want our history as well. Uh, but I'm not going to do that right now. Okay, so we have our\n9:46 pipeline and we can go ahead and actually define our runnable with message history. Now, this class or\n9:53 object when we are initializing it does require a few items. We can see them here. Okay, so we see that we have our\n10:00 pipeline with history. So, it's basically going to be uh you can you can see here, right? We have that history\n10:05 messages key, right? This here has to align with what we provided as the\n10:10 messages placeholder in our pipeline. Right? So we have our pipeline prompt\n10:17 template here and here. Right? So that's where it's coming from. It's coming from messages placeholder. The variable name\n10:23 is history. Right? That's important. That links to this. Then for the input\n10:29 messages key here we have query that again links to this. Okay. are both\n10:37 important to have there. The other thing that is important is obviously we're passing in that pipeline\n10:43 from before. But then we also have this get session history. Basically what this is doing is it's saying okay I need to\n10:49 get uh the list of messages that make up my chat history that are going to be inserted into this variable. So that is\n10:55 a function that we define. Okay. And within within this function, what we're trying to do here is actually replicate\n11:03 what we have with the previous conversation buffer memory. Okay, so\n11:09 that's what we're doing here. So it's very simple, right? So we have uh this\n11:15 inmemory chat message history. Okay, so that's just the object that we're going to be returning. What this will do is it\n11:22 will set up a session ID. The session I is essentially like a unique identifier so that each conversation or interaction\n11:30 within a single conversation is being mapped to a specific conversation. So you don't have overlapping let's say of\n11:35 multiple users using the same system you want to have a unique session ID for each one of those. Okay. And what it's\n11:41 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here. We are going\n11:48 to initialize that session with an inmemory chat message history. Okay, that's it.\n11:57 And we return. Okay, and all that's going to do is it's going to basically append our messages. They will be\n12:03 appended within this chat map session ID and they're going to get returned.\n12:08 There's nothing really there's nothing else to it to be honest. So, we invoke\n12:14 our runnable. Let's see what we get. Oh, I need to run this.\n12:20 Okay, note that we do have this config. So, we have the session ID. That's to again, as I mentioned, keep different\n12:27 conversations separate. Okay, so we've run that. Now, let's run a few more. So,\n12:32 what is my name again? Let's see if it remembers. Your name is James. How can I help you today, James? Okay, so it's\n12:41 what we've just done there is literally conversation buffer memory but for up\n12:47 to-ate lang chain with L cell with runnables. So you know the recommended way of doing\n12:54 it nowadays. So that's a very simple example. Okay, there's really not that\n13:00 much to it. It gets a little more complicated as we start thinking about the different types of memory. Although\n13:06 with that being said, it's not massively complicated. We're only rarely going to be changing the way that we're getting\n13:12 our interactions. So let's uh let's dive into that and see how we would do\n13:18 something similar with the conversation buffer window memory. But first, let's actually just understand okay what is\n13:24 the conversation buffer window memory. So as I mentioned near the start, it's going to keep track of the last k\n13:30 messages. So there's a few things to keep in mind here. More messages does\n13:35 mean more tokens sent with each request. And if we have more tokens in each request, it means that we're increasing\n13:41 the latency of our responses and also the cost. So with the previous memory\n13:46 type, we're just sending everything. And because we're sending everything, that is going to be increasing our cost. It's\n13:52 going to be increasing our latency for every message, especially as a conversation gets longer and longer. And we don't we might not necessarily want\n13:58 to do that. So with this conversation buffer window memory, we're going to\n14:03 just say, okay, just return me the most recent messages. Okay, so let's or let's\n14:10 see how that would work. Here we're going to return the most recent four messages. Okay, we are again make sure\n14:17 we've turned messages is set to true. Again, this is deprecated. This is just the old way of doing it. In a moment,\n14:23 we'll see the updated way of doing this. We'll add all of our messages.\n14:30 Okay, so we have this and just see here, right? So we've added in all these\n14:36 messages. There's more than four messages here and we can actually see that here. So we have human message AI\n14:42 human AI human AI human AI. Right? So we've got four pairs of human AI\n14:49 interactions there. But here we don't have as more than four pairs. So four pairs would take us back all the way to\n14:57 here. I'm researching different types of conversational uh memory. Okay. And if\n15:03 we take a look here, the most the first message we have is I'm researching different types of conversational memory. So it's cut off these two here\n15:11 which will be a bit problematic when we ask you what our name is. Okay. So let's just see going to be using conversation\n15:17 chain object again. Again just remember that is deprecated. And I want to say what is my name again?\n15:23 Let's see. Let's see what it says. Uh I'm sorry, but I don't have access to\n15:29 your name or any personal information. If you like, you can tell me your name, right? So it doesn't actually remember. Uh so that's kind of like a negative of\n15:38 the conversation buffer window memory. Of course, the uh to fix that in this\n15:43 scenario, we might just want to increase K. Maybe we say remember previous eight interaction pairs and it will actually\n15:50 remember. So where's my name again? Your name is James. So now it remembers. We've just modified how much it is\n15:57 remembering. But of course, you know, there's pros and cons to this. It really depends on what you're trying to build.\n16:02 So let's take a look at how we would actually implement this with the\n16:07 runnable with message history. Okay. So you getting a little more\n16:13 complicated here. Although it it's it's not it's not complicated but well we'll\n16:19 see. Okay, so we have a buffer window message history. We're creating a class here. This class is going to inherit\n16:26 from the base chat message history object from lang chain. Okay. And and\n16:31 all of our other message history objects are going to do the same thing before with the in-memory message object that\n16:39 was basically replicating the buffer memory. So we didn't actually need to do\n16:44 anything. We didn't need to define our own class here. So in this case we do.\n16:50 So we follow the same pattern that lang chain follows with this base chat\n16:56 message history. And you can see a few of the functions here that are important. So add messages and clear are\n17:01 the ones that we're going to be focusing on. We also need to have messages which this object attribute here. Okay. So\n17:07 we're just implementing the synchronous methods here. If we want this to be\n17:14 async, if we want to support async, we would have to add a add messages, um, a\n17:19 get messages and a clear as well. So, let's go ahead and do that. We have\n17:24 messages. We have K. Again, we're looking at remembering the top K messages or most recent K messages only.\n17:30 So, it's important that we have that variable. We are adding messages through this class. This is going to be used by\n17:37 line chain within our runnable. So, we need to make sure that we do have this method. And all we're going to be doing\n17:42 is extending the self messages uh list here. And then we're actually just going to be trimming that down so that we're\n17:49 not remembering anything beyond those, you know, most recent K messages\n17:55 that we have set from here. And then we also have the clear method\n18:00 as well. So we need to include that. That's just going to clear the history. Okay. So it's not this isn't\n18:05 complicated, right? it just gives us this nice default sandal interface for\n18:11 message history and we just need to make sure we're following that pattern. Okay, I've included the uh this print here\n18:17 just so we can see what's happening. Okay, so we have that and now for that\n18:23 get chat history function that we defined earlier rather than using the built-in method we're going to be using\n18:30 our own object which is a buffer window message history which we defined just here. Okay, so if session ID is not in\n18:39 the chat map as we did before, we're going to be initializing our buffer window message history. We're setting K\n18:45 up here with a default value of four. And then we just return it. Okay. And and and that is it. So uh let's run\n18:51 this. We have our runnable with message history. We have all of these variables\n18:56 which are exactly the same as before. But then we also have these variables here with this history factory config.\n19:03 And this is where if we have um new variables that we've added to our\n19:11 message history, in this case K that we have down here, we need to provide that\n19:17 to line chain and tell it this is a new configurable field. Okay. And we've also added it for the session ID here as\n19:23 well. So we're just being explicit and have everything in there. So we have that and we run. Okay. Now let's go\n19:32 ahead and invoke and see what we get. Okay, so important here this history\n19:38 factory config that is kind of being fed through into our invoke so that we can actually modify those variables from\n19:45 here. Okay, so we have config configurable session ID. Okay, we just put whatever we want in here and then we\n19:51 also have the number K. Okay, so remember the previous four interactions.\n19:58 I think in this one we're doing something slightly different. I think we're remembering the four interactions\n20:03 rather than the previous four interaction pairs. Okay, so my name is James. Uh we're going to go through I'm\n20:09 just going to actually clear this and then I'm going to start again and we're going to use the exact same add user\n20:15 message add AI message that we used before. We're just manually inserting all that into our history so that we can\n20:21 then just see okay what is the result and you can see that k equals 4 is actually unlike before where we were\n20:28 having the uh saving the top four interaction pairs we're now saving the\n20:35 most recent four interactions not pairs just interactions. And honestly I just\n20:41 think that's clearer. I think it's weird that the number four for K would actually save the most recent eight\n20:48 messages, right? I I think that's odd. So, I'm just not replicating that weirdness. We could if we wanted to. I\n20:56 just don't like it. So, I'm not doing that. And anyway, we can see from\n21:01 messages that we're returning just the most four recent messages. Okay, which\n21:06 would be these four. Okay, cool. So we've just using the runnable we've\n21:11 replicated the old way of having a window memory and okay I'm going to say\n21:18 what is my name again as before it's not going to remember so we can come to here I'm sorry but I don't say personal\n21:24 information no so on and so on if you like tell me your name it doesn't know now let's try a new one where we\n21:31 initialize a new session okay so we're going with ID K14 so that's going to create a new\n21:37 conversation in there and we're going to say we're going to set K to 14.\n21:43 Okay, great. I'm going to manually insert the other uh messages as we did before. Okay, and we can see all of\n21:50 those. You can see at the top here, we are still maintaining that hi, my name is James message. Now, let's see if it\n21:57 remembers my name. Your name is James. Okay, there we go. Cool. So, that is\n22:03 working. We can also see. So, we just added this. What is my name again? Let's just see if did that get added to our\n22:10 list of messages, right? What is my name again? Nice. And then we also have the response your name is James. So just by\n22:17 invoking this because we're using the the runnable with message history, it's\n22:22 just automatically adding all of that into our message history which is nice.\n22:28 Cool. All right. So that is the buffer window memory. Now we are going to take a look\n22:34 at how we might do something a little more complicated which is uh the the summaries. Okay. So when you think about\n22:41 the summary you know what are we doing? We're actually taking the messages we're\n22:46 using that lm call to summarize them to compress them and then we're storing\n22:52 them within messages. So let's see how we would actually uh do that. So to\n22:57 start with let's just see how uh it was done in old line chain. So we have conversation summary memory go through\n23:06 that and let's just see what we get. So again same interactions\n23:13 right I'm just invoking invoking invoking I'm not adding these directly to the messages because it actually\n23:18 needs to go through a um like that summarization process and if we have a\n23:25 look we can see it happening. Okay, current conversation. So, sorry, current\n23:30 conversation. Hello there, my name is James. AI is generating current conversation. The human introduces\n23:36 himself as James. AI greets James warmly and expresses its readiness to chat and assists inquiring about how his day is\n23:43 going, right? So, it's summarizing the the previous interactions. And then we\n23:49 have, you know, after that summary, we have the most recent human message and then the AI is going to generate its\n23:55 response. Okay? And that continues going continues going and you can see that the the final summary here is going to be a\n24:01 lot longer. Okay. And it's different that first summary of course as day he mentions stage of researching different\n24:07 types of conversational memory. The AI responds enthusiastically explaining that conversational memory includes\n24:12 short-term memory, long-term memory, contextual memory, personalized memory and then inquires if James is focused on the specific type of memory. Okay, cool.\n24:21 So we get essentially the summary is just getting uh longer and longer as we go. But at some point the idea is that\n24:28 it's not going to keep just growing and it should actually be shorter than if you were saving every single interaction\n24:33 whilst maintaining as much of the information as possible. But of course\n24:39 you are going to maintain all of the information that you would with for example the the buffer memory. Right?\n24:46 With the summary, you are going to lose information, but hopefully less information than if you're just cutting\n24:54 interactions. So, you're trying to reduce your token count whilst maintaining as much information as\n25:00 possible. Now, let's go and ask uh what is my name again? It should be able to answer\n25:06 because we can see in the summary here that I introduce myself as James.\n25:12 Okay, response. Your name is James. How is your research going? Okay. So, has that cool? Let's see how we'd implement\n25:19 that. So, again, as before, we're going to go with that conversation summary\n25:25 message history. We're going to be importing a system message. Uh we're going to be using that not for the LM\n25:30 that we're chatting with, but for the LM that will be generating our summary. So,\n25:37 actually that is not quite correct. There's create a summary. Not that it matters. It's just the dock string. So,\n25:43 we have our messages and we also have the llm. So different different attribute here to what we had before.\n25:48 When we initialize a conversation summary message history, we need to passing in our LM. We have the same\n25:55 methods as before. We have add messages and clear. And what we're doing is as messages coming, we extend with our\n26:02 current messages, but then we're modifying those. Okay. So we construct\n26:08 our like instructions to make a summary. Okay. So that is here we have the system\n26:14 prompt uh given the existing conversation summary and the new messages generate a new summary of the conversation ensuring to maintain as\n26:21 much relevant information as possible. Okay. Then we have a human message here through that we're passing the existing\n26:28 summary. Okay. And then we're passing in the new messages.\n26:33 Okay. Cool. So we format those and invoke the lm\n26:41 here. And then what we're doing is in the messages we're actually replacing the existing history that we had before\n26:48 with a new history which is just a single system summary message. Okay,\n26:55 let's see what we get. As before, we have that get chat history exactly the same as before. The only real difference\n27:01 is that we're passing in the LM parameter here. And of course, as we're passing in the lm parameter in here, it\n27:07 does also mean that we're going to have to include that in the configurable field spec and that we're going to need\n27:14 to include that when we're invoking our pipeline. Okay, so we run that pass in\n27:21 the lm. Now, of course, one side effect of uh\n27:26 generating summaries for everything is that we're actually, you know, we're generating more. So you are actually\n27:32 using quite a lot of tokens. Whether or not you are saving tokens or not actually depends on the length of a\n27:38 conversation. As a conversation gets longer. If you're storing everything after a little while that the token\n27:45 usage is actually going to increase. So if in your use case you expect to have\n27:50 shorter conversations, you would be saving money and tokens by just using this standard buffer memory. Whereas if\n27:59 you're expecting very long conversations, you would be saving tokens and money by using the summary\n28:05 history. Okay, so let's see what we got from there. We have a summary of the conversation. James introduced himself\n28:11 by saying, \"Hi, my name responded warmly, asking me, \"Hi, James.\" Introduction included details about\n28:16 token usage. Okay, so we actually uh included everything here, which we\n28:22 probably should not have done. Why did we do that? Uh so in here we're including all of the\n28:35 oh in here so using we're including all of the content from the messages. So I think maybe if we just do\n28:42 hcontent for x in messages that should resolve\n28:49 that. Okay there we go. So we quickly fix\n28:55 that. So yeah, before we're passing in the entire message object, which obviously includes all of this information, whereas actually we just\n29:01 want to be passing in the content. So we modified that and now we're getting what\n29:08 we'd expect. Okay, cool. And then we can keep going, right? So as we as we keep going, the\n29:13 summary should get more like abstract like as we just saw here, it's literally\n29:19 just giving us the messages directly almost. Okay, so we're getting bit summary there and we can keep going.\n29:25 We're going to add just more messages to that. We'll see the, you know, as we'll\n29:31 get send those, we'll get a response, send it again, get response. We're just adding all of that, invoking all of\n29:38 that, and that will be, of course, adding everything into our message history. Okay, cool. So, we've run that.\n29:44 Let's see what the uh latest summary is.\n29:49 Okay, and then we have this. So this is a summary that we have inside of our our chat history.\n29:55 Okay, cool. Now finally, let's see what is my name again. We can just double\n30:01 check, you know, has my name in there, so it should be able to tell us.\n30:09 Okay, cool. So your name is James. Pretty interesting. So let's have a\n30:14 quick look over at Langsmith. So the reason I want to do this is just to point out okay the different essentially\n30:22 token usage that we're getting with each one of these. Okay. So we can see that we have these runnable message history\n30:27 which probably improved in naming there. But we can see okay how long is each one\n30:33 of these taken? How many tokens are they also using? Come back to here we have\n30:39 this runnable message history. This is we'll go through a few of these maybe to\n30:45 here I think. And we can see here this is that first interaction where we're using the buffer memory and we can see\n30:52 how many tokens we used here. So 112 tokens when we're asking what is my name again. Okay then we modified this to\n31:01 include I think it was like 14 interactions or something along those lines and obviously increases the number\n31:06 of tokens that we're using. Right? So we can could see that actually happening all in Lang stuff which is quite nice\n31:11 and we can compare okay how many tokens is each one of these using. Now this is\n31:16 looking at the buffer window and then if we come down to here and look at this\n31:22 one. So this is using our summary. Okay so our summary with what is my name again actually use more tokens in this\n31:28 scenario right which is interesting because we're trying to compress information. The reason there's more is\n31:34 because there's not there hasn't been that many interactions. As the conversation length increases\n31:41 with the summary, this total number of tokens, especially if we prompt it correctly to keep that low, that should\n31:48 remain relatively small. Whereas with the buffer memory, that will just keep\n31:54 increasing and increasing as a as a conversation gets longer. So useful little way of using lang there\n32:02 to just kind of figure out okay in terms of tokens and costs what we looking at for each of these memory types. Okay so\n32:09 our final memory type acts as a mix of the summary memory and the buffer\n32:16 memory. So, what it's going to do is keep the buffer up until an N number of\n32:23 tokens and then once a message exceeds the number of token limit for the\n32:28 buffer, it is actually going to be added into our summary. So this memory has the\n32:34 benefit of remembering in detail the most recent interactions whilst also not\n32:42 having the limitation of using too many tokens as a conversation gets longer and\n32:49 even potentially exceeding context windows if you try super hard. So this is a very interesting approach. Now as\n32:56 before let's try the original way of implementing this then we will go ahead\n33:03 and use our update method for implementing this. So we come down to here and we're going to line chain\n33:09 memory import conversation summary buffer memory. Okay, a few things here.\n33:15 lm for summary. We have the n number of tokens that we can keep before they get\n33:22 added to the summary. and then return messages of course. Okay, you can see again this is deprecated. We use the\n33:28 conversation chain and then we just pass in our memory there and then we can chat. Okay, so super straightforward.\n33:36 First message, we'll add a few more here. Again, we have to invoke because our\n33:43 memory type here is using MLM to create those summaries as it goes. And let's\n33:49 see what they look like. Okay. So we can see for the first message here we have a human message and then an AI message.\n33:57 Then we come a little bit lower down again. Same thing human message is the first thing in our history here. Then\n34:04 it's a system message. So this is at the point where we've exceeded that 300 token limit and the memory type here is\n34:11 generating those summaries. So that summary comes in as a message and we can see okay the human named James\n34:18 introduces himself and mentions he's researching different types of conversational memory and so on and so on right okay cool so we have that then\n34:27 let's come down a little bit further we can see okay so the summary there okay\n34:34 so that's what we that's what we have that is the implementation for the old\n34:40 version of this memory again we can see is deprecated. So, how do we implement this for our more recent versions of\n34:49 lang chain and specifically 0.3? Well, again, we're using that runnable with message history and it looks a little\n34:57 more complicated than we were getting before, but it's actually just, you know, it's nothing too complex. We're\n35:05 just creating a summary as we did with the previous memory type. But the\n35:10 decision for adding to that summary is based on in this case actually the number of messages. So I didn't go with\n35:16 the the lang chain version where it's a number of tokens. I don't like that. I\n35:22 prefer to go with messages. So what I'm doing is saying okay last k messages.\n35:27 Okay. Once we exceed k messages, the messages beyond that are going to be\n35:33 added to the memory. Okay, cool. So let's see. We first initialize our\n35:41 conversation summary buffer message history class with lm and k. Okay, so\n35:48 these two here. So lm of course to create summaries and k is just the the limit of the number of messages that we\n35:53 want to keep before adding them to the summary or dropping them from our messages and adding them to the summary.\n36:00 Okay. So we will begin with okay do we have an existing summary. So the reason\n36:07 we set this to none is we can't extract the summary the existing summary unless\n36:14 it already exists. And the only way we can do that is by checking okay do we\n36:19 have any messages. If yes we want to check if within those messages we have a system message because we're we're doing\n36:25 the same structure as what we have up here where the system message that first system message is actually our summary.\n36:32 So that's what we're doing here. We're checking if there is a summary message already stored within our messages.\n36:38 Okay. So we're checking for that. If we find it, we just do we have this little print\n36:45 statement so we can see that we found something and then we just make our existing summary. I should actually move\n36:53 this to the first instance here. Okay, so that existing summary will be set to\n37:02 the first message. Okay, and this would be a system message\n37:08 rather than a string. Cool. So we have that. Then we want to\n37:14 add any new messages to our history. Okay. So we're extending the history\n37:20 there. And then we're saying, okay, if the length of our history is exceeds the\n37:25 K value that we set, we're going to say, okay, we found that many messages. We're going to be dropping the latest. It's\n37:30 going to be the latest two messages. this I will say here one thing or one\n37:36 problem with this is that we're not going to be saving that many tokens if we're summarizing every two messages.\n37:43 So, what I would probably do is in in an actual like production setting, I would\n37:50 probably say let's go up to 20 messages. And once we hit 20\n37:56 messages, let's take the previous 10, we're going to summarize them and put them into our summary alongside any, you\n38:02 know, previous summary that already existed. But in in, you know, this is also fine as well. Okay. So, we say we\n38:11 found those measures. We're going to drop the latest two messages. Okay. So, we pull the the oldest messages out. I\n38:20 should say not the latest. It's the oldest. Not the latest. Want to keep the latest\n38:26 and drop the oldest. So, we pull out the oldest messages and keep only the most\n38:32 recent messages. Okay. Then I'm saying, okay, if we if we don't\n38:38 have any old messages to summarize, we don't do anything. and we just return. Okay, so let's indicates that this has\n38:44 not been triggered, we would hit this. But in the case this has been triggered\n38:51 and we do have old messages, we're going to come to here. Okay, so this is we can\n38:59 see we have a system message prompt template saying given the existing conversation summary and the new messages generate a new summary of the\n39:06 conversation ensuring to maintain as much relevant information as possible. So if you want to be more conservative\n39:12 with tokens, we could modify this prompt here to say keep the summary to within\n39:17 the length of a single paragraph for example. And then we have our human\n39:23 message prompt template which can say okay here's the existing conversation summary and here are new messages. Now\n39:28 new messages here is actually the old messages but the way that we're framing it to the LM here is that we want to\n39:35 summarize the whole conversation, right? It doesn't need to have the most recent measures that we're storing within our\n39:42 buffer. It doesn't need to know about those. That's irrelevant to the summary. So, we just tell it that we have these\n39:47 new measures. And as far as this LM is concerned, this is like the full set of interactions. Okay. So, then we would\n39:54 format those and invoke our LM. And then we'll print out our new summary so we\n40:00 can see what's going on there. And we would prepend that new summary to our\n40:06 conversation history. Okay. And and this will work. So we can just prepend it\n40:13 like this because we've already popped\n40:18 where was it up here. If we have an existing summary, we already popped that from the list. So\n40:24 it's already been pulled out of that list. So it's okay for us to just we don't need to say like we don't need to\n40:31 do this because we've already dropped that initial system message if it existed. Okay. And then we have the\n40:37 clear method as before. So that's all of the logic for our conversational summary\n40:45 buffer memory. We redefine our get chat history function with the lm and k\n40:53 parameters there. And then we'll also want to set the configurable fields again. So that is just going to be of\n40:59 course session id lm and k. Okay. So now we can invoke the k value\n41:07 to begin with is going to be four. Okay. So we can see no old messages to\n41:13 update summary with. That's good. Let's invoke this a few times and let's see\n41:18 what we get. Okay. So no old messages to update summary with.\n41:26 found six matches dropping the oldest two. And then we have new summary in the conversation. James and Bruce himself is\n41:33 interested researching different types of conversational memory. Right? So you can see there's quite a lot in here at\n41:38 the moment. So we would definitely want to prompt the LLM, the summary LLM to keep\n41:45 that short. Otherwise, we're just getting a ton of stuff, right? But we can see that that is you\n41:52 know it's it's working it's functional. So let's go back and see if we can prompt it to be a little more concise.\n41:59 So we come to here ensuring to maintain as much relevant information as possible. However we need to keep\n42:09 our summary concise. The limit\n42:15 is a single short paragraph. Okay. something like this. Let's try and let's\n42:22 see what we get with that. Okay, so message one again, nothing to\n42:28 update. See this? So, new summary. You can see it's a bit shorter. It doesn't have all those bullet points.\n42:37 Okay, so that seems better. Let's see. So you can see the first summary is a\n42:44 bit shorter, but then as soon as we get to the second and third summaries, the second summary\n42:50 is actually slightly longer than the third one. Okay, so we're going to be we're going to be losing a bit of\n42:56 information in this case, more than we were before, but we're saving a ton of tokens. So that's of course a good\n43:03 thing. And of course, we could keep going and adding many interactions here. And we should see that this conversation\n43:09 summary will be it should maintain that sort of length of around one short\n43:14 paragraph. So that is it for this chapter on conversational memory. We've\n43:21 seen a few different memory types. We've implemented their old deprecated versions so we can see what they were\n43:28 like and then we've reimplemented them for the latest versions of lang chain.\n43:33 And to be honest, using logic where we are getting much more into the weeds and\n43:39 that is in some ways okay it complicates things that is true but in other ways it\n43:45 gives us a ton of control. So we can modify those memory types as we did with that final summary buffer memory type.\n43:52 We can modify those to our liking which is incredibly useful when you're\n43:58 actually building applications for the real world. So that is it for this chapter. We'll move on to the next one.