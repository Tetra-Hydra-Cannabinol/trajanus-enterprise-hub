Title: LangChain Master Class For Beginners 2024 [+20 Examples, LangChain V0.2]
Channel: aiwithbrandon
Video ID: yF9kGESAi3M
URL: https://www.youtube.com/watch?v=yF9kGESAi3M
Duration: 3:17:50
Level: BEGINNER
Application: LangChain
Topics: Chat Models, Prompt Templates, Chains, RAG, Vector Databases, Embeddings, Retrievers, Agents, Tools, LangChain V0.2, OpenAI, Claude, Gemini
Ingested: 2025-12-21
Source: Playwright Browser Extraction
==============================================================================

0:00 hey guys welcome to this Lang chain master class for beginners in this video you're going to learn everything you\n0:06 need to know about Lang chain so that you can start your own AI development journey and by the end of this master\n0:12 class you're going to learn everything that you need to know about Lang chain so that you can go off and create your own rag chat Bots create your own agents\n0:19 and tools and use Lang chain to automate task and even though there's a ton of information in this tutorial I've done\n0:26 my best to make it as beginner friendly as possible you'll see as we go through this tutorial that I've structured\n0:31 everything to start out with the absolute Basics so that we can build up a strong foundation and from there we're\n0:37 going to add in more advanced features and complexities so that you can see what Lane chain is fully capable of and\n0:43 because I want you to get started building your own lane chain projects as fast as possible I've actually included over 20 different examples in this video\n0:50 so that you can just copy my code and start using it in your own projects and to make things even easier I have a link\n0:57 down the description below where you can download all the source code in this video completely for free and it would\n1:02 mean a lot to me if you could hit that like And subscribe button while you're down there too also if you get stuck at all during this tutorial you're in luck\n1:09 because I created a free school Community for AI developers just like you in the community you can go over there and ask questions get support join\n1:15 our weekly free coaching calls and we have over, 1500 different active members in the community so it's a great place\n1:22 for you to meet like-minded AI developers on your development Journey so I definitely recommend checking out it's completely for free and I have a\n1:29 link down the description below below so come over and join the party but enough of that let's go ahead and dive into the rest of the\n1:35 video all right so let's go ahead and cover the outline for this tutorial so that you can understand what you're\n1:40 getting into and so that you can get the most out of this tutorial so to start off the first thing we're going to do is\n1:45 set up the environment on your local computer so that you can run the over 20 different Lang chain examples that I've\n1:51 built for you guys from there we're going to start diving deep into each of the core components of Lang chain so to\n1:56 start off we're going to start working with chat models and this is basically how we're going to start interfacing with you know open AI chat gbt we're\n2:03 going to start working with CLA that's all going to happen in the chat model section and what we're going to do from there is we're going to start working\n2:08 with prompt templates next and this is how we're going to be able to format the inputs that we pass over to our chat\n2:14 models and this is really just helping us build up a good strong foundation so that eventually when we start trying to\n2:19 automate task using our chains which is my favorite part of this whole tutorial we're going to be able to really\n2:25 understand how we can start you know automating task by putting together chat models or prompts and other tasks and\n2:31 really run them all together to automate your workflows from there we're going to start working with rag now this is a\n2:36 huge section of this entire tutorial as you can see we have a ton of different examples in here and if you've heard of\n2:42 people chatting with their PDFs or documents this is what they were doing rag retrieval augmented generation so\n2:47 we're going to do a huge Deep dive into this one and then finally what we're going to do to wrap up this tutorial is we're going to do a deep dive into\n2:54 agents and tools and as you can see we're going to start off with the basics and then we're going to do a deep dive into agents which are are basically just\n3:00 you know chat models however they can make decisions on their own and act it's super cool how it works and then we're\n3:05 going to do a deep dive into tools and tools are how we're going to supercharge our agents to provide them more\n3:11 capabilities so as you can see we have a ton of information in this tutorial so let's go ahead and start diving into the\n3:17 first section which is going to be setting up your local environment so that you can run all the different examples inside of this code base oh\n3:23 real quick I want to mention if you want to get the most out of this video to learn linkchain as quickly as possible\n3:28 here's my recommendation first I recommend watching the video the whole way through on two times speed just so\n3:33 that you can understand all the highlevel Core Concepts of blank chain and then I recommend coming back through\n3:40 this video a second time and just skip to the part that you want to learn about for whatever project you're building for\n3:45 example if you're learning about rag I would definitely recommend just skipping down through the time sense below to\n3:50 watch rewatch the rag section and do a deep dive into the exact part that you want to learn about this is how I go\n3:56 about learning new Concepts so I just want to throw it out to you guys too so you can speed up your development journey and start gilding projects so\n4:02 all right enough of that let's actually go in and start setting up your environment all right guys so it's time\n4:08 for us to start working on setting up our local environment so that we can run all the different examples inside of\n4:13 this project now inside of the read me I have outlined all the different steps that you need to take in order to start\n4:19 running these and it's actually super straightforward so let me just quickly walk you through it to start off we need to install Python and we need to install\n4:25 poetry python I'm sure you know what that is poetry basically if you haven't heard of it before or it's a dependency\n4:30 management tool for python so as you can see we have something called a p project. tommo and this includes all of\n4:37 the different dependencies that we need to install in order to run all the different examples that I've set up for\n4:43 you and poetry makes this super simple to do so what you'll do go over to click this link right here and it will walk\n4:50 you through all the installation steps to install poetry on your local computer and once You' have installed poetry\n4:55 you'll be able to run commands like this so poetry to confirm that it's working and you can see yep poetry is working\n5:01 it's giving me back information and then from there what we can do is once you're inside of your code base and remember\n5:07 all the source code for this project is completely for free just click the link down the description below and you can download it but once you've done that\n5:13 what you can do is type in poetry install D- noout and what it'll do is\n5:18 it'll go through and install all the different dependencies that we saw back over in our py project. Tomo file over\n5:26 here it'll install it and what's awesome is then we can eventually start to run commands like this poetry shell and what\n5:33 this will do is it'll actually spin an interactive shell up that we can see right here it actually has the name of\n5:39 basically our project that we're working on you can see right here it has our name of our project that we're working on and we've created and we can actually\n5:45 start you know running Python and actually start calling all the different example code bases that we have all the\n5:51 different projects so you can see we can do something like python one. chat models and then we can start running you\n5:56 know all of our different code examples so if you got this far you are good to go go for the rest of the course when it comes to python now we just have a few\n6:03 more cleanup things that we need to do first off uh coming back to our read me is I mentioned that you need to update\n6:10 your environment variables so uh when you download the source code you will only see a EnV example file and this is\n6:18 where you're going to instore your environment variables eventually what we're going to do is you are going to\n6:24 rename this file to justv and what this will do is is it'll\n6:29 become your environment variable folder so that whenever you go to run your you know start using open Ai and some of\n6:36 your other different projects they require an open AI key or a Google key or you know all these keys and that's\n6:42 going to get stored to your environment variables I've already have mine set up and we'll be walking through it later but that's the second thing you need to\n6:48 do and actually in addition to that go over to open AI Google fir craw and\n6:53 actually start adding in those open ad keys but we'll talk more about that later on okay all right enough of talking about environment if you've\n7:00 gotten this far everything should be working and we can now move on to actually start playing with the code and Diving deep into L Jing so we're going\n7:06 to go over next and start working with chat models hey guys I meant to show you this quick tip that's going to make coding inside a visual studio code and\n7:13 cursor with python so much easier and solve a big headache that you'll probably have so if you head over to one\n7:18 of your python files you'll notice that you have a bunch of squigglies inside of your project and the reason why is cuz\n7:25 Visual Studio code is not properly hooked up to the new python environment that you just created with all the\n7:31 proper dependencies so here's how we fix that first you're just going to open up a terminal down here and what you'll do\n7:37 is you'll type in poetry shell like we did earlier and this will actually access and you can see yep we're in the\n7:42 right shell but the important part is is it gives us the location of where this basically all of our dependencies were\n7:49 installed so here's where the magic happens you'll come down here and click that you know interpreter path and\n7:54 you'll say enter interpreter path and then you'll just paste in the path you just copied\n7:59 and whenever you do that it'll actually get rid of all the squigglies because now Visual Studio code or cursor is\n8:05 hooked up to your development environment with python so now every time you add a new dependency with poetry ad or whatever you do it will\n8:12 actually you know sync up and you won't have all those random squiggly marks even though you have the Right Packages installed and just as a final note if\n8:18 you ever like well I want to not use this poetry shell environment anymore all you have to do is just type in the word exit and it'll put you back to your\n8:25 base environment so yeah that's a quick crash course on poetry and setting up visual studio code and after that let's dive back into the\n8:32 video all right so it's time for us to dive into our first core component of Lang chain which is going to be chat\n8:39 models now what the heck are chat models and what do they do well a chat model is\n8:44 basically Lang Chain's way of making it super easy for us developers to talk to all the different large language models\n8:51 out there like chat BT claw Gemini and a bunch more they abstract away all the complexity and allow us to basically\n8:58 have conversation ations with these models hints chat and chat models it's all about conversations so what is super\n9:05 nice about Lang chain if we go over to their documentation you can see they have a list of all the models that they\n9:12 currently support now it is very important to mention that for this whole tutorial we're going to be using version\n9:18 0.2 of Lang chain this is the most upto-date one and a lot of the features and version 0.1 will soon be deprecated\n9:26 uh whenever they upgrade you know probably the next you know five six months from now but enough of that let's dive back into chat models and talk\n9:32 about how we can use them so as you can see looking at chat models here's a huge list of all the different models that we\n9:39 have access to some of these models are better at other things and what's very interesting is chat models inside of\n9:45 Lang chain they provide different functionality such as tool calling outputs you know do they support\n9:51 outputting content in Json are they multimodal such as accepting images and audio so that's what you can see at a\n9:58 high level going over here and what's nice is if you want to work with any of these different models you can just\n10:03 click uh you know use poetry ad and you can type in the name of the package and you'll add it to the environment that\n10:09 you just set up a few seconds ago so let's go ahead and do a deeper dive and look at chat open AI because that's the\n10:15 one we're going to be using mostly inside of this tutorial so if we go over here to chat open aai you can do a\n10:21 deeper dive and look at some of the examples that they already have set up for you so you can see once again they\n10:26 recap what it's capable of doing and then they walk walk you through Yep this is how you can start setting up your\n10:33 files to start using this new package to start chatting with open Ai and eventually down here they dive into\n10:39 showing you y this is how you can start actually using it updating the models and so forth but of there documentation\n10:45 let's go ahead and actually head over to the example that I created for you guys where we're going to start at the absolute Basics and work our way up so\n10:51 you can see what these chat models are capable of so let's come over here start looking at the code and uh take it from\n10:57 here and before we dive into the code on this specific file I just want to give you a\n11:03 quick overview of what you can expect from each of these files so what I'll do in each one of the examples that we're\n11:08 going to run through I will try to provide documentation for you guys up top so you on your own can do a deeper\n11:14 dive into whatever concept we're just learning for example we just talked about chat models and I showed you a\n11:19 link and I also did open AI chat models and we walked through another link so in all the files wherever I point out\n11:24 something you'll be able to go ahead and click those links and a deeper dive on your own if you ever want to learn more about those topics but enough of that\n11:30 let's actually go ahead and start talking about chat models on a basic\n11:35 level all right guys so let's walk through the Three core steps that we need to take to start interacting with\n11:41 our chat models in this case our open AI chat model at a super basic level so the first thing that we're going to do is\n11:47 load our environment variables and if you remember from the beginning we set up aemv file which stored all of our\n11:53 keys to all of the different platforms we were going to access in this case we're trying to work with open AI so it's important that we have an open AI\n12:00 key feel free if you haven't set that up just head over to the open a website go create an account and you'll actually be\n12:06 able to access your open AI key and bring it back and copy and paste it over here now once you have done that what's\n12:12 nice is this load. EnV is going to add all those environment variables so that we can start accessing them in this file\n12:19 and you might be wondering like Brandon what the heck I don't see us accessing the open AI key anywhere well if we\n12:25 actually Peak under the hood inside of the chat model which actually gets imported from linkchain open Ai and\n12:31 that's actually if you head back over to those packages that we had set up earlier that's where it was stored but if you actually hit command on your\n12:37 keyboard if you're on Mac or control if you're on Windows and actually click on it you can actually start looking at the\n12:43 source code under the hood and you can see hey in order to start using this chat model you need to have this API key\n12:50 and we're actually going to start using it if you scroll down a little bit you can actually see under the hood it's\n12:56 actually automatically grabbing this environment key here and unless you pass it in manually and just to like give you\n13:01 guys a full tool you could actually manually pass in your API key here it's just not the safest manner because if\n13:07 you accidentally save your open a key to the public other people could actually grab your API key from GitHub and it's\n13:14 not the most secure so that's why you're going to store everything in your environment variables file okay enough of that let's keep chugging along to\n13:20 actually you know walk all the way through this example the next thing that we're going to do is once we have created our chat model which in this\n13:26 case we're going to say we're using jet gbt 4.0 but we easily could have done something like chat gbt 4 we could have\n13:32 done something like chat gbt for uh you know 3.5 turbo we could have easily changed things up but once we've created\n13:38 that model we can actually St uh start now interacting with it and using it and the key lesson here is we're going to\n13:45 interact with our models in pretty much everything in Lane chain using the do invoke property this is a function that\n13:52 really just whatever we're using it triggers its core functionality so in this case we're working with chat models so it's going to go trigger off like hey\n13:59 open AI or Claud start processing the request I give you but when we're using chains or rag or agents later on\n14:07 everything uses invoke so that's a very key thing to keep in mind as you're working with Lang chain all right so\n14:12 let's just keep walking through what's going on well in our case we're telling our model hey go perform this query for\n14:18 me and we're going to get back a result so let's actually dive into these results so we can see what's happening under the hood and the way we're going\n14:23 to do that is we're going to open up our terminal and let's close it out and give you guys some more space but once again\n14:29 we're going to use poetry shell and this is going to open up that interactive shell that we created earlier which uses\n14:35 you know our new Lang chain crash course environment what we can do is we can actually start calling this function so\n14:41 we're just going to call it Python and this is our chat model so one Tab and it'll go ahead and autocomplete for me\n14:47 and this is the first example so I'll hit one and tab again and it'll autocomplete now I can start running it\n14:52 so let's actually go ahead and press enter and start looking what's happening under the hood so here's what's actually\n14:59 super interesting so you can see we had two print statements one was for the full result and one was content only so\n15:04 for full results you can see that open AI under the hood gives us back a ton of information they give us back the\n15:11 content which is the exact answer we wanted 81 divid by 9 is 9 but then they give us this metadata which is like how\n15:17 many tokens did we use which you know basically why did we finish there's a ton of information which run number was\n15:23 this basically there's a ton of information that they give back to us 99% time you don't care about it however\n15:29 I just want to show you that it is accessible if you ever need to use it so most of the time when you're using you know these chat models the main thing\n15:36 that you want to grab is the content because the content is the example so whenever we grab the result and we\n15:42 access the content property we'll get back the exact string that we usually want to you know show to our users or\n15:48 pull out and pass over to the next prompt in our you know our chat model so that's under the hood how our first\n15:55 basic chat models are going to work so what I would like to do next is we're going to go ahead and move over to the\n16:00 next example where we're going to start actually showing you guys how to do a basic conversation using our chat models\n16:06 to where we can actually like you know pass in more information and actually having a full-on conversation let's go and start working on this example\n16:13 now all right guys so welcome to the second example where we're going to start focusing more on creating a\n16:18 conversation with our chat models now the three new important Concepts to know when going into this example are there\n16:25 are three different types of messages in our case that's going to be a system message and a system message just sets\n16:31 like the broad context for the conversation so these types of messages are usually something like hey you are a\n16:37 professional accountant or hey you are a professional python software engineer help me write this code so that's like\n16:43 just the broad what's going on inside the conversation just the context and then from there there's two different\n16:48 types of messages there are human messages which is us talking to the AI and then there's AI messages which are\n16:54 the AI responding back to us so those are the three types of messages that you can have in a conversation with the AI\n17:01 okay cool now let's dive into the rest of this code so you can kind of see what's happening and this is just once again we're building on our foundation\n17:07 so we're just copying a lot of the code from the previous example and now we're going to start building on top in this case we're going to start creating a\n17:14 conversation so in our case you can see a conversation is nothing more than we have our messages list and our list is\n17:21 just going to store a combination and series of messages in our case we're just GNA um start off with a system\n17:27 message and it's important system messages best practice and I think it's actually enforced a system message must\n17:32 come first and then from there you can alternate human AI human AI but system always comes first because remember this\n17:39 is the context for the conversation as a whole so in our case we're going to say hey solve the math problem and then from\n17:45 there we can pass in a human message and what we would expect to get back is obviously an AI message so let's\n17:51 actually kind of see how we can actually trigger this conversation so we can get a result so in this case you'll see that\n17:57 we have a once again again we have our model our chat gbt model and we're going to call that important function that we\n18:02 talked about in the last one which is invoke but this time we're not going to pass in a you know a hardcoded string\n18:09 like we did over here where you can see we passed in a hardcoded string this time we're actually going to pass in our entire message history and it'll\n18:16 actually read through the entire conversation and then spit out a result if you worked with cat GPT it's exactly like that if you've ever typed into like\n18:22 you know the chbt website it's just like that okay cool so what we're going to do is we're going to go ahead I'm going to comment this out we're going to run the\n18:29 code so we can actually see what answer we get so this time we're going to do python this is you know still working\n18:34 with chat models and this is example two so you can actually see whenever we get an example back with working with chbt\n18:40 it says you know 81 divided by 9 is 9 so you know that's exactly what we would expect to see okay cool but what's nice\n18:46 is we can continue this example and actually have a full-blown conversation so I'll show you what that looks like\n18:52 now so you can see in this second part we actually have continued on the conversation by adding in AI responses\n18:58 and then messages so this case we've added now like the response from the previous one and now we're going to you\n19:04 know just continue adding human messages so I'm just going to run this so you can actually see what's going to happen and this time it's going to come back and\n19:10 give us you know you know 10times 5 so this is completely basic however here's why chat conversations are super\n19:16 important in the real world when you're building your conversations you know it's very common for you to provide an\n19:22 example like hey chat gbt or AI model build me an email it returns a response\n19:28 and then you provide provide feedback cuz remember this is all about conversations and what's going to happen is as you provide those you know\n19:34 feedback of like hey no make it less formal make it a bulleted list you know as you provide that feedback what's nice\n19:39 is going forward in your conversation you can say okay great now do exactly what you did for that last email but now\n19:45 do it for this email and you know storing this message history like you have right here is how you're going to\n19:50 be able to basically make your chat models have awareness in context of what's good and what's bad and what's wrong so this is a very powerful tool\n19:57 and you actually use this a lot more whenever you know you're working on bigger and larger projects but okay cool\n20:03 well now that we have that under the hood and we actually understand like just like basic conversations let's keep it going and actually start working on\n20:10 actually exploring other alternatives for different chat models that we can use because right now we've been focusing on only open AI but let's look\n20:16 at a few different examples of how we can use Lang chain chat models but with different you know llms so let's go\n20:22 ahead and start working on that now all right guys so welcome to this third example where we're going to start\n20:28 exploring different Alternatives of working with other models outside of open AI so in this case we're going to\n20:35 explore Google's Gemini models we're going to look at anthropic or CLA and look at Open Eye because I just want to\n20:42 show you guys how easy Lang chain makes it to work with these different models so let's scroll down so we can look and\n20:48 compare and contrast all the new code so what you can see up top this is exactly\n20:53 what we've been doing so far in all our examples we create our chat model once we have the model we we go off and\n20:59 invoke it and then we get back some sort of result well Lang chain abstracts all the complexity away and makes it super\n21:05 easy to do the same thing with our anthropic models and our Google Gemini models all we do is we instead of using\n21:12 the chat open AI model like we've been doing in the past we now just use chat anthropic and then we can pass in\n21:18 whatever specific model within Cloud that we want to use just like we did with chat gbt up here and then we'll do\n21:24 the normal part where we just you know go off and invoke it with our messages and get back a result and is the exact\n21:29 same thing for Google's giz models down here at the bottom so link chain makes it super easy to work with these\n21:35 different models and this is very important as you build larger projects because certain models are much better\n21:40 at you know at performing certain tasks some are cheaper some are faster so for different situations you need to use\n21:45 different models Lang chain makes it super easy to do and if you want to explore all the different models that\n21:50 each you know anthropic provides and Google provides I have links for you guys and just as a important reminder if\n21:56 you want to go off and explore all all the different chat models back over here in our first example back when we were\n22:02 working with a chat model documentation you could start searching through here so you can explore all the different models and all the different\n22:09 functionalities that these different models provide but okay cool enough of that let's go ahead and start exploring our next example where we're going to be\n22:16 diving in and actually having a conversation with our user through the terminal so let's go and start working on this example\n22:22 now all right guys welcome to the fourth chat model example where we're going to start actually having a realtime\n22:27 conversation with with our AI models this is going to feel just like the chat gbt website except it's running locally\n22:33 on our computer so let's walk through how we're going to set this up so the important part per usual we're going to create our chat model by loading all our\n22:39 environment variables and creating an instance of it now here's where all the interesting part happens because we're having a conversation we're going to\n22:46 create a chat history list and this is going to store all our messages so as we ask questions we're going to add\n22:52 messages to the chat history as the AI responds we're going to add those messages to the chat history and we're just going to continually keep add add\n22:58 in all of our messages to this list so here's how that works in this code example so first off the first message\n23:04 we're going to add is our system message because if you remember system messages are just general context for the\n23:09 conversation that's about to happen so we're going to create that system message and then we're going to add it\n23:15 to our chat history by calling append on the chat history list cool so now here's\n23:20 where all the core logic happens so this is where our chat Loop happens and basically here's the core Loop we first\n23:27 ask our users hey what is the question or prompt that you have for the chat model we're going to get back a query so\n23:33 this is whatever the user passed to us if the user gave us the keyword exit we're just going to stop the\n23:38 conversation right then and there and we're just going to print out the whole chat history however if they didn't give us the keyword exit we're just going to\n23:44 have a full-on conversation so what we're going to do is we're going to add the person's query as a human message\n23:50 and we're going to add it to our chat history then what we're going to do is pass over that entire chat history list\n23:56 over to our model like you can see right here and then we're going to invoke it so that model is going to then read all\n24:02 the messages in our chat history get a response and then we're going to print back that response to our user and we're\n24:09 also going to most importantly track that response by once again updating our chat history so we're going to update\n24:15 our chat history with an AI message cuz you know this is the AI response so enough talk let's go ahead and look at a\n24:21 code example so you guys can see this in action so I'm going to once again open up our terminal and I'm just going to\n24:26 call Python and then this is our chat model project and this is the fourth example so now we can actually start\n24:32 running it and per usual it's now going to ask for query so I'm just going to ask it who are you so then what we'll do\n24:39 is we'll pass that question over to open AI open AI will generate a response and it actually prints it back to us so this\n24:45 is exactly you know if we're talking to chat gbt on their website this is exactly what it feels like and we can\n24:50 actually ask questions about it because as a conversation we can refer to previous messages can you expand on that\n24:57 and what it'll do is it'll you know cuz it can refer back to the previous messages and actually provide additional\n25:02 context so this is super powerful and you'll definitely be using this a lot more and finally we can pass in the word\n25:08 exit it'll quit the conversation and what's cool is you can see our entire message history here so you could save\n25:14 this off somewhere you could you know save it locally save it to the cloud do whatever you want so that whenever the user comes back in the future you can\n25:20 continue the conversation and we'll actually do a much more deeper example later on where we actually save this to\n25:25 the cloud so you'll learn about that in just a little bit all right cool enough of that let's go ahead and start working on our fifth example which is exactly\n25:32 what I was talking about where we're going to start saving our messages to the cloud so let's go ahead and start working on this where we're going to save all of our messages over to\n25:38 Firebase I think you're really going to love this one all right let's go ahead all right guys welcome to the\n25:44 fifth chat model example so this one is by far my favorite because you're going to learn how to save all the messages\n25:49 you type locally over to the cloud and this case we're going to be saving everything to Firebase so what I've done\n25:55 is I've actually typed out all the installation in steps for you guys because there's there's you know a little bit more background setup because\n26:02 we're working with the cloud but once we knock out all the development setup in the cloud what we'll do is just like run through the code so you can see how\n26:08 everything works but I think you'll see that like all in all this is actually pretty straightforward the hard part is\n26:13 just setting everything up in the cloud so let's go ahead and do that now and the first thing I do want to mention about the cloud is this is actually\n26:18 based on this Google fire store codebase example so you can see like this is exactly what we're doing however uh I\n26:25 feel like they didn't do the best job walking through how to get it set up and I struggled a ton the first time I did this so I just want to share all the\n26:31 lessons learned that I have done and and I've copied all my lessons learned here okay so the first thing that you need to\n26:37 do is create a Firebase account so just head over to you know console firebase.com and you're going to create\n26:44 an account and once you've created an account you're going to then go over here and create a project to so I'm not going to like walk through creating a\n26:50 project cuz it's it's super simple but just come over here click the drop- down menu and click create a project so\n26:55 that's how you create a project once you've done that you'll then need to create when you go over to the build tab\n27:01 there's something called the firestore database and this is where we're going to be storing all of our messages inside of our chat history except our chat\n27:08 history is now in the cloud so whenever you click fir store database if it's the first time you're using the project you're going to want to turn this on so\n27:15 there'll just be a turn on button here for you and you'll want to click that and once you've done that and you run the code eventually all of your messages\n27:21 and user sessions will get saved up to the cloud and more on this later but I just want to go ahead and like paint a picture of where we're going fantastic\n27:27 so once you've done that let's heading back to our examples you'll need to start copying some information about your Firebase project so what I mean by\n27:34 that is you need to start copying information such as the project ID so in our case what you'll do come back over\n27:40 here you're going to click the gear icon you'll hit project settings and this is where you'll find your project ID\n27:46 project number and everything that's you know related to what you're what you're doing with the project you just created\n27:51 over here in Firebase Okay cool so that's the easy part now this is where things get a little bit more complicated\n27:57 because we're trying to work with the cloud so Google cloud and we're trying to have our local computer communicate\n28:04 with the cloud so this is where we're going to have to go a little bit deeper so the first thing we're going to have to do is install the Google Cloud\n28:10 command line interface on our computer because what we're trying to do is authenticate our local computer to make\n28:17 requests to the back end that's all we're trying to do so what you'll do is first thing you'll click this link over\n28:22 here and it will take you to this and this will basically just walk you through exactly what you need need to\n28:28 type to install the Google Cloud CLI it's super straightforward you'll just click download once you've downloaded it\n28:34 literally walks you through step by-step how to install it so that's super super easy then once you've gone through and\n28:40 installed everything and initialized it the final part that we're going to do is you need to authenticate your local\n28:46 Google Cloud CLI to your account so once again click this link it'll take you over here and it will just walk you\n28:52 through how you can actually authenticate your local computer to Google Cloud cuz we're just trying to create some default credal\n28:58 that way whenever our codee's running it just can you know seamlessly make a call to the back end and the main things that\n29:04 you need to do first off just run Google Cloud off application default login a\n29:09 signin like normal Google Cloud signin screen will pop up you'll just log in it'll make a service account for you\n29:14 life fantastic and then you'll just run you'll run this basically this script as well to finally get everything working\n29:20 so I hope that's enough I don't want to go too deep just because I want to focus more on Lang chain but run through those\n29:25 steps you guys will be good to go and then you'll find finally have everything working inside your code so all right\n29:31 enough of that here's how we're going to get things working over here so you're now going to come in here uh CU you have your project ID that you just copied\n29:37 you're then going to have a session ID so we're just going to make a new one so we're going to call this user session\n29:43 new on this is where all of our messages are going to be saved and then finally there's a thing called a collection name\n29:48 this is heading back over to Firebase just the way our fir store saves data\n29:53 it's in a collection document basically a database type so you can see collection document collection so it\n30:00 just keeps alternating so we're going to store all of our messages in the chat history collection and each session is\n30:07 going to be a document and that document is just going to contain a list of messages so that's what's about to happen so what we need to do first off\n30:14 is we need to initialize our firestore client and what that's going to do is allow us with our client to go off and\n30:21 make requests so that's why we're doing it and then what we're going to do next is have something called firestore chat\n30:27 message history this is basically we'll just click in here so you can see what's going on but basically what we're trying to do is in the past you and I were\n30:34 storing all of our messages in a list well Lang chain has provided a bunch of different options for us to store all of\n30:41 the different ways that we can have and store our messages so in this case we're using fir store to save our messages\n30:48 there's a few different examples inside of Lang chain where we can actually save our messages using I think the like file\n30:53 message history class where we can actually save all the messages locally on our computer so there's a bunch of different options that I want you guys\n31:00 to be aware of and feel free to explore through the L chain documentation to see other options but just know at its core\n31:06 all it's doing is just saving a list of messages you know human messages AI messages back and forth except now they're just being saved off to\n31:12 somewhere else so that's what's happening under the hood but let's keep going so in our case now that we have\n31:17 basically we've created our new chat history except our new chat history is up in the cloud so what we can do now is\n31:22 actually start running through the exact same Loop that we did last time which is where the human's going to ask a question question and now what we're\n31:29 going to do is just keep adding with our chat history we're just going to add user messages or we're going to add AI messages and we're just going to go back\n31:34 and forth so enough talking let's actually dive into the demo because this is super cool in my opinion so we're going to go step by step because there's\n31:40 a lot of cool ways to show this off so the first thing that we're going to do let's clear this out and we're just going to run python so we're in the chat\n31:47 models example where this is the fifth example now what this will do is it'll start you know initializing firestore\n31:52 client it'll start the chat message history and usually this takes a few seconds to get started and then now we\n31:58 can actually start chatting so it's cool though uh and you'll see this get populated in second our current chat history well because this is the first\n32:04 time I created that new session ID there's no history of this chat in the cloud you know there's no user session d\n32:11 new so there's no messages to load but now I can actually start talking about it so I can say who is Sam Alman so\n32:20 it'll go off and answer questions and you know so here's here's who Sam is but what's super cool is if now if I come\n32:26 back over to the class we can actually see our messages being stored in real time you'll notice they're stored as\n32:31 bite strings and they're not actual just like plain text messages that's just totally fine that's how Firebase and fire store are storing messages but\n32:38 let's keep going does he have a brother so we can keep asking and then\n32:44 it'll say yes he has a brother here's his name and then what we can do is exit it so we'll exit Okay fantastic all of\n32:51 our messages are cleared you can see over over here we actually still have additional messages but what's nice is\n32:56 whenever we restart the same file with the same you know chat session it'll\n33:02 actually say the current chat history and it'll pull all of the different you know messages that we have previously\n33:08 sent to the AI so this is a super cool way and I hope you guys are like this is awesome this is a super cool way for us\n33:14 to continue conversations at a later date without having to completely refresh from start so I hope you guys\n33:20 thought this was awesome because we can still you know do additional stuff like who was I just talking about let me fix\n33:26 this oh sorry wrong button I can say who we just talking about and then because\n33:32 this is a chat history it can refer to the previous messages so yeah this is awesome so I hope you guys are pumped\n33:37 and that concludes the first module where we're just run through all the different capabilities with working with\n33:43 chat models and from here we're going to move on to prompt templates next so I do just want to point out before we keep going if you have any questions\n33:49 definitely hit that link down the description below for school go over there and ask any questions you have or hop on our weekly coaching calls would\n33:55 love to help you guys out but let's keep chugging along and start moving over to prompt\n34:00 templates all right guys welcome to the second module in this Lang chain crash course this whole module is completely\n34:06 dedicated to prompt templates and I think you'll find this is a super simple concept but it's going to make our lives\n34:12 much easier as developers as we work with link chain so what the heck is a prompt template why does it matter how\n34:17 does it work with chat models those are the main things we're going to be tackling in this module so the first thing is prompt templates the best way\n34:23 to think of them is exactly what the name says we are building up a prompt that's our whole goal we're trying to\n34:29 create a prompt but we've created some sort of template like this and it's up to us to pass in values into that\n34:36 template so you know kind of think of like Fillin the blank is the best way to describe it so you know you could do\n34:41 generate three jokes about dogs or something like that is kind of what we're what we're shooting for here so\n34:47 enough like high level let's actually dive into like what this actually looks like in action as like a quick overview\n34:54 and then after this we'll dive into the code okay so like I said our whole goal is we're striving to generate a prompt\n35:00 but we're going to be passing in some variables to make this work so you know we're going to be taking like a user\n35:05 response to help us really fill out this prompt and this is important because eventually these prompts are going to\n35:11 get passed over to our chat models to you know to to perform some action so in our case like this is a joke template So\n35:17 eventually this template once it gets populated it's going to get passed over to you know chbt or Claude one of those\n35:23 models and it's actually going to spit out some jokes for us so that's just this prompt is going to help us structure The Prompt that we pass over\n35:29 so it's super helpful we'll dive into some more examples here in just a second but this is exactly how it works it's up to you as a you know whenever you're\n35:35 creating these prompts to basically create a string and inside that string it's up to you to use these curly\n35:41 brackets to create variables these variables are later going to be populated with these values and what's\n35:48 important to notice here is once we've created our prompt template we're going to Define an input dictionary and this\n35:54 input dictionary is going to Define for each one of these variables the values that are going to replace them because\n35:59 eventually whenever we have our template pass in these inputs and call invoke because remember invoke is the super\n36:06 magic word for all of Lang chain what it's going to do is it's going to basically do some string interpolation and replace all these values so that we\n36:13 end up with an output that looks just like this obviously this was a super simple example but let's just think like\n36:18 higher level like for a real world setting you could eventually be creating some sort of tool that helps developer\n36:24 automatically debug their code well in your case you would create a prompt template that's like here is the user's\n36:30 code and then you know you would have user's code here's the error they're getting that would be the next variable\n36:37 please help them debug this and then basically you would pass in that entire prompt filled with the user's code and\n36:42 error over to open Ai and then open AI would tell you oh it looks like here's a bug here's how it fix it so this is just\n36:48 a really nice way for us to create structured prompts that make it easy you know where we can do some prompt engineering and easily add in our users\n36:55 input to The Prompt templates that we're so uh enough jargon let's actually dive into an example and start walking\n37:01 through our first example working with prompt templates and I think this will make a ton of sense and it'll be super easy to\n37:08 understand all right guys so let's go ahead and dive into the code and this one's going to be a super simple Code\n37:13 walkthrough there's three parts to this and then I have one little extra part so you guys can see you know when not to\n37:19 use prompt templates but let's walk through this part by part just so you understand everything at a basic level and then we'll work our way up from\n37:24 there okay so if you remember our whole goal is we're trying to create prompts that we could eventually pass over to\n37:30 our chat models so we're focusing in on creating prompt templates that's all we're trying to do so let's go part by\n37:36 part in this code and what we're going to be trying to do is trying to relate it back to this outline that we have over here so the first thing we need to\n37:41 do is create a prompt template that stores our variables of things we want to replace so that's exactly what we're\n37:47 doing right here in this code we're creating a template which is just a string and what we're going to do is\n37:52 we're going to call chat prompt template and like well what the heck is this well if you remember at high level we're\n37:58 eventually trying to our end goal is we just talked about chat models that's all we were just talking about in module one\n38:04 well what we're trying to do is create prompts that we could eventually pass over to those chat models so that you\n38:09 know AI can basically answer whatever questions or ask that we're putting together in our prompts so that's our\n38:14 end goal so what this chat prompt template does what it's trying to do is it converts our string into a template\n38:20 that makes it easy to work with to actually replace you know basically replace all these variables with these\n38:26 values so basically what's happening under the hood so once we create this prompt template so once we have our\n38:31 template what we're trying to do is we want to pass in an input dictionary that has all of our keys and values so that's\n38:39 basically exactly what was happening right here and what we're trying to do is once again we're calling that magical\n38:44 word invoke so in this case what's going to happen under the hood is we have this prompt template we're going to call\n38:49 invoke with these specific values and all it's going to do is it's going to go through each variable and replace it\n38:55 with the appropriate value so let's go ahead and run this code so you can see an action so in our case we're going to\n39:00 do python now we're in prompt templates and we're going to run the first example so two one and what this will do is now\n39:07 when I run it what you'll notice is it actually doesn't spit out a string it actually spits out a human message so\n39:13 tell me a joke about cats so that's pretty cool it actually filled in the values that we told it to and if you\n39:19 remember what it did is it spit out basically a messages array that we could eventually pass over to our chat models\n39:25 cuz that's where we're trying to go so that was super simple example one of the other things I want to show you in this is we can actually instead of just\n39:31 replacing one value let's go ahead and look at part two what you'll notice in this one is we can actually replace\n39:37 multiple values in a prompt template so this is the exact same exact same thing\n39:42 except this time what we're doing is replacing multiple values so let's go ahead and yeah we're still calling the same chat prompt template from template\n39:49 so let's go ahead and run the same example again just so you can see it in action so yeah running with multiple placeholders this time tell me a funny\n39:56 story about a panda that's exactly what happens okay enough of the simple examples let's actually get on to some\n40:01 more of the cooler parts which is part three so if you can uh if you've seen so far every time it's created something it\n40:08 has basically been from a human message point of view but sometimes you actually want to do a little bit more you want to\n40:14 have a little bit more control so what you'll notice in part three is we can actually start defining basically we can\n40:22 specify the types of messages we want to create and not only what type of message we want to create we can actually still\n40:28 you know replace all the variables in here and what you might notice and which I thought was weird the first time I used it I would have expected that you\n40:34 know what the heck is this like Tuple where the first part's a simple uh you know the system and then the message\n40:41 then we have another human then message like why aren't we just using like human messages and system messages well it all\n40:47 comes back to the way the chat prop template works is it just expects this tupal format where you define the\n40:53 message type first and then the content so just know if you want to basically use prompt templates and also Define the\n40:59 message type you have to go with this Tuple approach but yeah let's go ahead and see an action so you can see exactly what it looks like so we're running part\n41:06 three so we're just going to run the code again so you can see now we have a list of messages and the first message\n41:12 in this array is going to be a system message and you know you're a comedian who tells jokes about lawyers so that's\n41:17 setting the context and then you can say tell me three jokes so we actually just replace the variables there too so yeah\n41:23 so once again we've created these messages and we could actually pass this message list over to our chat models and\n41:28 it would spit out those jokes about lawyers okay now I just want to show you this is just a lesson learn that I thought was pretty interesting so if we\n41:35 come down here to the final part what you will notice I'm just going to comment this out so you can see it so\n41:41 this code does work so you know how we just talked about tupal well anywhere\n41:46 you want to do string interpolation you know where we're replacing values you have to use the tupal so you'll notice\n41:52 we want to replace Topic in this system's tupal so we have to put it in a tupal and then well for the human\n41:58 message we're not doing any interpolation there so we're just going to leave it how it is so this example would 100% work let's clear it out and\n42:05 I'm just going to show you it works so this one works yep it looks just like we talked about in the first example well\n42:11 if you come down here to the final example that I have in this file you'll notice I say this does not work and\n42:17 basically what we're doing here is we're trying to do interpolation on a human message so let me just show you what\n42:22 happens when you try and like if you don't use a tupal you'll basically get uh you'll basically notice here tell me\n42:28 a joke and it never did the replacing part I ran into a lot of issues when I first started using Lane chain and doing\n42:34 prompt template Replacements and I was like why isn't it replacing it and it's like oh it has to be in this tupal\n42:39 format okay I hopefully um that's enough prompt templates like Basics let's actually start moving on to the next\n42:45 part where we're going to start using actually start using these prompt templates with a chat model in our second example so let's go ahead and\n42:50 start working on that now all right so welcome to the second prompt template example this one we're\n42:57 actually going to tie together everything for module one which is creating our chat models and everything we just learned about prompt templates\n43:03 so let's go ahead and merge both these together in the example so you'll actually really understand the whole value of creating these different prompt\n43:09 templates okay so per usual like we did in all of module one we're going to load our environment variables and we're\n43:15 going to go ahead and create a chat model that's using open AI in this case now let's go part by part so you can\n43:20 actually see the exact same basically all the prompt templates that we created in demo part one we're actually now just\n43:26 going to like not only create those prompt templates but we're actually going to pass over those prompts to our\n43:32 model so let's look at example one in depth and then we'll just run it you know for everything else so per usual uh\n43:38 we're going to create our template with the variable we want to replace once we have that string template we're going to call chat prompt template and we're\n43:44 going to make an actual template from that string just so it's easy for Lang chain to actually start manipulating it\n43:51 and from there we're going to do exactly what we did last time which is call invoke cuz that's the magic word for our\n43:56 prompt templates to take in that string and actually start replacing all the values so once we call invoke we're\n44:02 going to get back that prompt and what's cool is we can now pass that prompt which is if you remember from the last\n44:08 one it's now just going to be a message array that has all these values in here so you know it'll say tummy joke about\n44:14 cat and that'll be one of the messages one of a messages in our message history that we're going to pass to our model I\n44:20 hope that makes sense um you whenever I run it it'll actually make more sense so um but what you can see is we're now going to pass that prompt over to our\n44:27 model and we're going to tell our model invoke because we want our model to actually like perform actions on these\n44:33 messages and we're going to print the result so you'll start to see you know invoke is going to get called a lot as\n44:39 you start to work with Lane chain more and more and I'm not going to go too deep in part two and part three because we just copied and pasted over all the\n44:45 examples from the last time and now we're just actually going to see let AI actually run basically run the request\n44:50 which is creating jokes about all these different prompts we've created so what we're going to do call python example two and this is the second example for\n44:57 prompt templates with a chat model let's go ahead and run that and this will take a few different examples seconds but you\n45:03 can see so prompt template prompt from a single template which was right here tell me a joke about cats so you know\n45:09 sure here's a joke about cats for you why was the cat sitting on the computer because I wanted to keep an eye on the\n45:15 mouse so obviously they're pretty corny and same with multiple placeholders so it actually told a short story about you\n45:21 know a short funny story about pandas so here's our funny short story and then\n45:27 finally when it comes to the end where we're doing you know prompts with system and human messages what we're doing is\n45:33 creating three jokes about lawyers and that's exactly what it did here are your three lawyer jokes so why don't sh\n45:38 sharks attack lawyers because of professional courtesy so once again pretty corny but um yeah so I wanted you guys to see prompts with prompt\n45:45 templates because right now we're calling invoke to get the prompt we're calling invoke again to get the result\n45:51 which leads us perfectly to chains because this is exactly what we're about to do next where we're actually like\n45:57 technically we're chaining prompts with chat models so this will make more sense so we set ourselves up perfectly to get\n46:02 to dive into section three which is all about change so let's go ahead and start diving into that model now and this will\n46:07 all make sense and it'll actually be a lot simpler so you'll see what I mean in just a\n46:13 second all right guys so welcome to the third module in this Lane chain crash course this chain section is by far my\n46:19 favorite part of the whole tutorial and I hope you guys love it just as much as I do so let's walk through what the heck is a chain why is it important and then\n46:27 just go through a few different examples real quick and then we'll hop over to the code okay so at a high level what the heck is a chain well inside a lane\n46:33 chain it's nothing more than tying together a series of task so if you\n46:38 remember so far we've been using invoke the invoke function a lot so we've been using it to create prompts we've been\n46:45 using it to basically have chat models respond to us so what what we could do instead is have the prompt be chained\n46:52 like the output of a prompt whenever we invoke it that output be fed to the model and then that model whenever we\n46:58 call it invoke again it will spit its outputs and it could just go to the next item so as you can see we're kind of just chaining together different task\n47:05 and we're passing the input from the first one over to the next one and we just keep going down and down and down\n47:10 so that's exactly what's happening right here have a prompt invoke it spit the outputs over to the next thing Tak in\n47:17 those inputs process it with invoke and then spit it out to the next thing so that's exactly what's happening with chains and I think you'll think when you\n47:23 see the code it's super simple and there's another common thing when you're using using chains inside of Lang chain\n47:28 and it's called Lang chain expression language it's a fancy way to say and describe like how you can create chains\n47:35 cuz you can go the hardcore code way which we're not going to get into we're going to stick to this which is just anytime you want to chain items together\n47:42 in L chain you're going to use the pipe operator so this is like right above your return key on your keyboard it's\n47:48 just the straight up and down basically pipe key so that's how you're going to create a chain you're just going to put\n47:53 you know a prompt a chat model and there's a few other things you could put in here too but this is how you're going to do it item or a task a pipe operator\n48:00 another task a pipe operator another task super easy to set up and then whenever you want to run the chain all\n48:06 you do is you call the chain and you call invoke on it and then you pass in your input dictionary of all your\n48:11 different keys and values this actually should be a quick uh my bad this should just be a quick dictionary just like\n48:17 this and that's how it works so this initial key dictionary gets passed in over here as the first into the first\n48:23 prompt then it gets passed along all the way through so that's just at a high level just a simple chain now let's just\n48:28 I just want to show you the realm of the possibilities when it comes to Lang chain first and then we'll hop into the code okay so when it comes to chain\n48:34 possibilities the first thing to know is like you don't have to do just prompt chat model you can actually keep going\n48:40 so you can just continually keep going as long as you want that's option one the other thing that's super cool is you\n48:45 can run task in parallel so you can have task you know just like oh kick off kick\n48:50 off and you can just start running your chains in parallel and then you can actually even at the end of them you can actually join them together and have\n48:57 your final results all come back and be fed into one so this is a cool way just to like if you need to do some parallel\n49:03 processing this is a cool way to do that the final thing is branching so branching is a way inside of your chains\n49:10 to have it to go like let's you know at this node let's kick off some actions cool well based on the results of those\n49:16 actions let's just say like oh it's um if we're doing a review of a movie well if the movie was good cool we're going\n49:23 to go down this branch and this chain path if the movie was awful we're going to go down this path and if it was so so\n49:28 we're going to go down path C so that's just kind of like the realm of the possibility when it comes to chains but enough like theoretical highle stuff\n49:35 let's actually dive into the code and start working on the simple model first and then we'll work our way up through all the different possibilities so let's\n49:41 go ahead and dive into the code all right everybody Welcome to the first code example when it comes to\n49:48 chains I think once you see how this all works you're going to be like oh my gosh this is so much easier than what we were doing beforehand and what's cool is\n49:55 we're going to use basically the same examples that we did from beforehand but just reformat them so they work with chains so let's go ahead and dive\n50:01 through this and walk through it step by step so the first thing that you notice we're in the chain Basics file what we're going to do is exactly what we've\n50:08 been doing so far with everything else we're going to create our chat model once we create our chap model we're\n50:13 going to create our prompt template our prompt template is going to contain our variables that we want to replace in\n50:19 this case it's going to be the topic and the joke count from there what we're going to do is create our chain now what\n50:25 you'll notice this is very similar to to the drawings I just created a few seconds ago the first thing we're going\n50:30 to do is have our prompt template which is going to contain all the messages that we want to basically replace with\n50:35 our inputs later on create our prompt from there we're going to pass that prompt to our model once our model then\n50:42 has basically is able to process that prompt what we're going to do eventually is spit everything out to a string\n50:48 parser at first I'm going to get rid of this string parser just so you can see why we need it but just know at a high\n50:53 level all it does is it takes you know how so far every time we get the result we do do content well that's exactly\n50:59 what the string parser does for us it just goes ahead and grab the content but we'll we'll add it back in in a little bit so comment it out just so you guys\n51:06 can see it how it works okay cool so what I'm going to do is go ahead and run this chain just so you guys can see how\n51:11 easy it works and how simple it is how much less code we're doing in this approach versus what we were doing last time so what I'm going to do ahead is go\n51:18 ahead and open up our terminal and then if you remember now we're in the third module cuz we're working with chains and\n51:23 this is the basic example so we're just going to run python three so we can go open our chain file and this is the\n51:29 first example and what I would expect to see is once we print the result we're going to get that long response that we\n51:35 normally get whenever we don't grab the content so you can see yep here are whenever we look at the content you can\n51:41 see yep here's the three lawyer jokes and then we're seeing all that extra information that we get back from our LM\n51:46 like tokens and so forth but if we undo our changes earlier and just use the\n51:51 string parser that you can see right here where we're just doing the string output parser this also comes from we'll\n51:57 talk about this more but the output parser Library that's where it's coming from but if we rerun it now what you'll\n52:02 notice is it actually just spits out the final text like this is what we would actually expect to return to our\n52:08 customers if this was a web app or you know this is what we would like most likely pass to the next model if we were\n52:14 doing this you know and continuing the chain so this is awesome I hope you guys see how much easier this is to use\n52:20 compared to what we were doing last time i' actually let's just do a side by side so you guys can kind of see exactly what\n52:25 this looks like so over here what we were doing yeah at the bottom we had our you know we\n52:31 created our prompt templates then we did our prompt then we invoked it then after we were done invoking it we saved the\n52:37 result then passed it to our model then did it again and again so all of this code right here basically uh just gets\n52:44 replaced with this single one line which is beautiful so I hope you guys like this so now that we understand the\n52:49 basics what I want to do is we're going to dive deep for a second in the next example where we're going to understand\n52:55 how chains work under the hood and after we do that we're going to start building off those examples you saw earlier where we're going to do you\n53:01 know pillow chains and then branching and much more so let's go ahead and dive into the next\n53:07 example all right so welcome to the second example in the chains module and\n53:12 in this example what we're going to do is a deep dive into how chains work under the hood now this isn't super\n53:18 important for you to like memorize or understand exactly how everything works under the hood it's just I want you to\n53:23 be aware of a few Concepts such as runnable sequences runnables and runnable lambdas those are the main\n53:29 three things that we're going to talk about in this section okay so let's dive into the code and we'll look around so\n53:34 per usual what we're doing is we're creating our models and loading our environment variables creating our prompts and then most of the new action\n53:42 when it comes to creating basically chains the manual way happens right here\n53:47 so what we're going to do is talk about this at a high level and then we're going to dive into each part so at a\n53:52 high level what we're doing is we are creating runnable lambdas so think of a\n53:58 runnable is a task whenever I was talking about task in the initial outline for chains under the hood the\n54:04 way it actually works in Lane chain is it's just called a runnable so what we're doing a runnable Lambda is nothing\n54:10 more than a task that's a Lambda function and if you're newer to python don't worry about this Lambda functions\n54:17 kind of get like a little bit more into weeds but they're just a quick way to make functions so like here's the input\n54:22 to a function and then here's the actions we want to take so that's just kind of what Lambda functions are so\n54:28 that's part one part two is this thing called a runnable sequence and a runnable sequence is nothing more than\n54:34 we're going to do this task then this task and this task then this task and put all those tasks together you have a\n54:40 sequence AKA a runable sequence AKA a chain so that's exactly what's happening\n54:46 so let's actually dive in deeper into these runnable lambdas and runnable sequence so you can actually see what the heck's going on so the first thing\n54:52 is what we're going to do is we're going to create the format prompt runnable Lambda and what this is going to do is\n55:00 all it's going to do is take in our prompt template that we've done in the past and then instead of calling invoke\n55:05 on it what we're going to do is just call the format prompt and the format prompt is basically just going to like\n55:11 yep replace all these you know values right here with these values down here\n55:17 so that's exactly what's happening under the hood and you can see we are passing all of our arguments and spreading them\n55:23 right here that's exactly what that's doing basically we're just passing this dictionary and spreading it so that we can replace all these key values with\n55:30 these values down here so that's what's happening don't have to understand how it works exactly I just I just want you to be aware of like what's happening\n55:36 from there once we have our prompt and we've created that task we're going to create an invoke model task and the\n55:41 whole point of this runnable Lambda once again just a task that takes in a Lambda function all this is going to do is it's\n55:47 going to say yep we have a model we're going to invoke the model and what we're going to do is pass in a you know pass\n55:54 in the input from the previous function into this one and we're going to convert whatever the heck we just received to\n56:00 messages so that's what's exactly happening there finally we're going to parse the output so if you remember the string output parser well under the hood\n56:07 all it's doing was grabbing the content from the output and that's what we're going to do right here so here's how now\n56:14 that we have all these individual runnables here's how it works with a sequence and let me just show you under\n56:19 the hood what a sequence looks like so um just in case you haven't seen this if you have a Mac you can hit command and\n56:25 click basically any any class and it'll take you to the definition if you have a Windows just hit control and then click\n56:32 the the class and it'll take you to the basically the the definition but basically this is recapping everything\n56:37 that we have just talked about you know a runnable is nothing more than just like it takes the output from one passes\n56:44 it to the next as an input so just we really just creating a sequence of task that need to occur they give us a cool\n56:49 little example just to show us like yes whenever we use Lang chain expression language to where we just like pipe task\n56:56 and runnables together it's actually the same thing as you know calling runnable sequence first and last and then once\n57:02 you have a sequence you can invoke it and pass items into it so that's what's happening under the hood so let's\n57:07 actually hop back to our code example and finish this one up real quick so we can actually keep keep going down the chain what you'll notice is whenever I\n57:14 created my runnable sequence you'll have three different properties that you can call and this is because if you scroll\n57:19 down to the actual code part you can see there's three different things there's first middle and last first is a single\n57:25 run last is a single runable and middle is a list so that's just how you have to pass\n57:31 in your sequence so if you have two items you'll call first and last if you have five items 10 items 100 you know\n57:38 you'll have to call first for the first item last for the 100th and everything in between will just get passed in as a\n57:44 list so that's exactly what's happening so in our case the format prompt where we convert our text over to a prompt\n57:50 happens as the first task our middle task where we have our formatted prompt and pass it to to our model so we can\n57:57 actually process it that gets passed in the middle list and then finally for the last thing once we you know have our\n58:03 prompt pass it to our model process it in Ai and we just want to get that final text output well that's why we're going\n58:09 to call the last runnable task which is just going to be Parts output and when you put all that together you end up\n58:14 getting a chain and that chain is exactly we'll call it the exact same way that we were doing earlier so what I'm\n58:19 going to do I'm going to run the code real fast just so you can see how it works and then I'm going to compare them side by side just so you can see like\n58:25 that's crazy that they do the same thing but this just takes so much more work cuz we're not using the L chain\n58:31 expression language so this is chains under the hood so we're going to run it and as you can see whenever it finally\n58:36 gets done it's just going to spit out a response which is you know just three jokes about lawyers so yeah that's just\n58:43 that's three different jokes and just to show you a quick side by side let's go open up three one chain Basics and zoom\n58:51 out yeah so instead of doing all this nonsense where we're creating runnable lambdas and passing stuff over and over\n58:56 and creating runable sequence just stick to L chain expression language and it just works so I hope you guys Now\n59:02 understand why L chain expression language is so nice and that's probably what you'll use 99% of the time whenever\n59:08 you're working with building chains but enough of that going deep let's actually hop back up and start working on some of\n59:14 the additional ways we can use chains so we're going to broaden them out in the extended section and then we'll keep moving on from there so let's go ahead\n59:20 and hop over to part number three all right guys so welcome to the\n59:26 third example in the chain module and in this example we're going to Showcase how you can continually extend your chains\n59:33 and add on additional runnables or probably the easier way to think of it is additional task to continually happen\n59:39 and you can just keep growing that chain forever so let's just go ahead and look at the code in this example so you can see how it actually works so what we're\n59:45 going to do is the per usual where we're going to load our environment variables then we're going to create our model and\n59:50 then we're going to create our prompt that's all the usual stuff all of the new chain extending has happens right\n59:57 here where we're going to uppercase the output and then we're going to count Words so let's actually dive into what's\n1:00:02 happening well what we're going to notice is we're creating our usual chain and then we've piped in an uppercase\n1:00:09 output well under the hood when we actually look at what's going on in this function because it is important\n1:00:14 whenever you extend or add new items to your chain by doing the pipe operator you need to add runnables that was one\n1:00:21 of the key things we talked about in the under the hood section so in this case we're going to add a run Lambda and\n1:00:26 runnable lambdas just make it super easy to like basically process any type of function so in our case we're just going\n1:00:32 to do some like string manipulation where we're going to Upper C like sorry I I'll slow down for you guys but like\n1:00:37 we're going to take in an input and what we're going to do is call uper on it which is going to capitalize the entire output so that's what's happening under\n1:00:43 the hood but what's cool with runnable Lambda functions is you could also use these to go off and make like an API call you could you know you could really\n1:00:50 do anything that you normally could do with python code you could just throw it in a runnable Lambda function so that's the part that makes this really cool but\n1:00:56 so now that we have uppercased the output what we're going to do is continue building on our chain and we're going to pipe the output of this over to\n1:01:03 count words so in this case when we come look at our count Words runnable Lambda you can see we're going to taking an\n1:01:08 input this input is going to return basically a string that we're going to do some interpolation on which basically\n1:01:15 just means some like formatting and passing in values so the first thing we're going to do is count the word count so what you can see is we're going\n1:01:21 to split the input that's given to us and then once we're done with that we're going to count count how long the actual\n1:01:28 input was and then once we're done with that we're just going to spit out like right here it just says you know curly\n1:01:33 brackets X that's where we're just going to print out the input that was given to us enough talk let's actually dive in so\n1:01:38 you can look at what actually happens under the hood when we actually run this so this is the third module for chains this is the third example and when we\n1:01:45 spit it out we would expect to see the word count yep just like that and then jokes so as you can see this code ran\n1:01:51 perfectly we spit out the output and you can actually see that this code ran too because everything is capitalized where\n1:01:57 it wasn't originally so this is how you can continually build chains and as you're going off and becoming you know a\n1:02:03 real world professional L chain developer some of the other practical things you would do is just like I said you would use runnable lambdas to go off\n1:02:09 and like create API calls to go do something you would then there's just a ton of stuff that you could do and\n1:02:14 chains make it super easy to you know pipe an input from the previous result over to your current function and just\n1:02:20 continue going down the chain so this is awesome so now that we've covered extending and continually growing your chain let's hop over to talk about how\n1:02:27 you can actually run chains in parallel I think you're really going to like this section too all right guys so welcome to the\n1:02:33 fourth example in the chain module in this code what we're going to be doing is showing off how you can run chains in\n1:02:40 parallel so I think this one's super cool cuz if you've ever thought about creating social media posts where you\n1:02:45 have like an idea and then you want to like Branch off to write something on LinkedIn and then Twitter and then elsewhere this is exactly how you're\n1:02:52 going to go about doing it so let's go ahead and dive into the code at a high level and then I have a visual for you guys just cuz it is it is a little bit\n1:02:58 weirder to see how it all works and then we'll come back to the code to like really dive in okay so at a high level\n1:03:04 this file is a little bit longer and I'm just going to hop down to the chain part and then from there we're going to go over to the visual I was just talking\n1:03:10 about so at a high level what you can do is you can see we are creating a chain this chain has the usual prompt has a\n1:03:16 model we do some string outputting and then we start doing this funny thing called runnable parallel under the hood\n1:03:22 basically what this is doing is it's creating different branches to run you know if we're looking at Pros cuz this\n1:03:28 is going to be a pro con list and we're just going to run run our Pros in one chain and we're going to run our cons in\n1:03:34 another chain and eventually merge those Pro cons into a final list so that's why we have two branches one for pro a pro\n1:03:40 branch and then a con branch and that's where they're going to go off and generate pros and cons and we're going to come back and put into a final list\n1:03:46 so that was high level just talking through the code so let's hop back over to what this looks like under the hood\n1:03:52 when we're actually like you know visually what does this look like so that chain that we were just looking at\n1:03:57 what we're going to do is it's going to take in a prompt this prompt is going to take in a product name from there it's going to construct a prompt using in our\n1:04:04 case I think we're going to do the MacBook Pro and then from there it's up to the model to produce a list of features about that product from there\n1:04:11 we're going to grab the content like we normally do from the result from the model and just pull out the content string and what's cool is we're going to\n1:04:18 pass that string to our two separate chains so I don't know if you saw back over a second ago but we had a pro chain\n1:04:24 and a con chain that's exactly what we're doing here so the pros it's going to take in you know the list of features\n1:04:30 and in our case we're going to pull out all the pros from those features and then what we're going to do on the other side in parallel is find all the cons\n1:04:37 for all those features and then finally once we're done with both of those different chains that are running in\n1:04:42 parallel we're going to grab both of the outputs and then put them together in a final Lambda runnable so we're going to\n1:04:50 get both of those outputs and then put them in a basically a nice print statement for our users to read pretty easily so that's exactly what we're\n1:04:56 doing now let's hop back to the code and do a deeper dive now you have a good understanding of like what's happening\n1:05:01 at a super high level visually all right let's hop back okay let's dive into this part by part and I think the best way to\n1:05:07 go about it is just start you know at the first item in each chain and then walk our way through so when it comes to\n1:05:13 the prompt template this is where we're going to be doing our normal prompt template stuff meaning you know we're just going to say like hey you're an\n1:05:18 expert on doing product reviews and I need you to basically list the main features for this product we're going to\n1:05:24 create a prompt for it in our case like I said this is going to be a MacBook Pro and then we're going to pass that prompt over to our model who's going to go off\n1:05:30 and actually list those features out then we're going to do our string output parser none of this should be new so far\n1:05:36 but then we're going to hop into this runnable parallel basically you know runnables are how chains work under the\n1:05:42 hood so in this case we're doing parallel meaning we have a bunch of branches that we can spin off our\n1:05:47 different chains so in in this case we're going to have a pro branch and a cons branch and you can actually dive in\n1:05:54 and see what each one of these these are so this Pros Branch under the hood it's a runnable Lambda and what we're doing\n1:06:00 is we're actually just creating in our case we're creating a new chain right here that's exactly what it is I can\n1:06:06 actually rename it for you guys so you can actually see exactly what it is we'll call this one a chain that one a\n1:06:12 chain perfect just so it makes a lot more sense and it's even more clear what's actually happening so we're creating a chain for our prob branch in\n1:06:18 this case we're saying like hey take in the input which in this case is a feature list I want you to analyze those\n1:06:25 probes so analyze Pros all it does is it creates a new prompt template just like we normally do and once we have that\n1:06:31 prompt template what we do is we pass that prompt template over to our model for our model to go off and generate\n1:06:36 those pros and then finally per usual we use the string output parser to convert that output into a nice string and the\n1:06:43 con branch is the exact same thing we take in that list of features about our product we pass that list of features\n1:06:49 over to this prompt template so you know given these features list the cons of each one of those features from there\n1:06:56 pass it over to our model string output so all this is completely normal we've done this a ton of times now here's\n1:07:01 what's different after we run our parallel branches what we end up with is actually one big dictionary and that\n1:07:08 dictionary if we'll print it out I'll print it out for you guys so you can actually see what I'm talking about right here so we'll say final output\n1:07:14 what you can see is it'll actually include something called branches and this branches will under the hood\n1:07:21 actually list out our pros and cons cuz that's how we're going to save them and you'll see it in a second whenever I print it out but that's the final part\n1:07:27 what we're going to do is pass in our Pro output and our con output into this combined Pros con function and what this\n1:07:34 is going to do is just print out a nice list string final output for us so we can compare like yep should we get the\n1:07:40 Mac should we not get the Mac so let's run it and actually dive into the outputs so let's come down here let's\n1:07:45 clear all old outputs up and start running it now what's going to do under the hood like I said it's going to start running and the first print output we\n1:07:52 should see is this final output right here where we're going to have our Pros chain and our cons chain we're going to\n1:07:58 be able to view both of them that's what's going to happen in here and then finally afterwards we should expect another final output I just want to show\n1:08:04 you guys under the hood how the inputs are getting passed into this and how we can actually view like yeah it does make\n1:08:10 sense that we need to grab the Branch's object and then within the Branch's object we need to grab the pros and cons\n1:08:15 so let's scroll up so you guys can see exactly what I'm talking about takes a few seconds Okay cool so here's where\n1:08:21 everything's in a dictionary that's why it's all cluttered to see but you can see final output what it does you can\n1:08:27 see it's a dictionary inside the dictionary you can see the first object inside of it is called branches and then\n1:08:34 within branches we can grab pros and then if you scroll down a little bit later you can also see cons if I just\n1:08:40 search for it you'll be able to see it cons let's see yeah so you can see here's the cons object so how does this\n1:08:46 actually work under the hood well it's because everything in this runnable parallel is equal to we basically\n1:08:52 created a dictionary right here because you can see we have branches is equal to and then we're creating our object so\n1:08:58 Pros are going to be saved as the result of this Pro chain and cons are going to come from this Con chain so that's\n1:09:05 exactly how it works so I hope that makes sense once again this is getting a little bit deeper into the weeds when it\n1:09:10 comes to working with Lane chain and we're starting to do a little bit more like fanciness with python so if you have any questions on any of this please\n1:09:16 definitely go over to the school community that I've created for you guys drop a comment you know take a screenshot of what's con confusing you\n1:09:22 and I'd be happy to help or hop on our weekly coaching calls but I hope you guys see the value in this because in\n1:09:28 the past you would have had to run your Pros chain that would have taken 30 seconds then you had to run your cons\n1:09:33 chain that would take another 30 seconds but now when we can run stuff in parallel we can cut it down and make things a lot more efficient if you're\n1:09:39 going to be building applications for your users where they're expecting a speedy output this is exactly what you're going to want to do to help\n1:09:45 automate and speed up some of your task all right so enough of talking about running chains in parallel let's go over to learning how we can start branching\n1:09:52 our chains based on different conditions let's go ahead and dive into task numberb five next see\n1:09:58 y all right guys so welcome to the fifth example in the chain module and this\n1:10:03 final example what we're going to do is cover branching and I think the best way to cover this topic before diving into\n1:10:09 the code is to actually look at what this code looks like visually so let's hop back over here to excal draw and let\n1:10:16 me run you through what's actually happening under the hood so there are two parts to this chain the first part\n1:10:22 is what we're going to call the classification chain and what we're trying to do is we're trying to take in feedback from our customers so they're\n1:10:29 going to say like man that was awesome or like oh that was the worst product I ever bought and depending on what type of feedback they give us we're going to\n1:10:36 have different prompts to generate different types of messages so we're going to have a positive Branch we're\n1:10:41 going to have a negative Branch a neutral branch and then if something goes wrong we're going to have an escalation Branch so here's a quick run\n1:10:47 through of what's Happening visually and then we'll hop back over to the code so visually what we're going to do is do the normal chain for our classification\n1:10:54 where we're going to have have a feedback prompt what we're going to do is in this prompt we're obviously going\n1:11:00 to populate the prompt with the user's input in this case it's going to be their feedback about what we're doing\n1:11:06 then we're going to categorize that feedback into one of you know one of these four keywords positive neutral\n1:11:11 negative or escalate so that's how we're going to basically classify everything then we're going to pull out the\n1:11:17 category with a string parser like we've done a ton of times by now and here's where the New Concept comes in we're\n1:11:23 going to it's going to be called a runnable brand and basically the way this works is it's think of it as like an if statement so\n1:11:29 if the you know if a conditions met we're going to run a different chain so this is a really good way to like really\n1:11:36 you know make your chains more intelligent and in different scenarios run different chains so in this case\n1:11:41 just you know going deeper we're going to check if the category was positive if it was fantastic we're going to run the\n1:11:47 positive chain was it negative cool we'll run the negative chain so forth and so forth so I hope this all makes\n1:11:52 sense now let's hop back over to the code so you can see what this actually look looks like inside of Lang chain all\n1:11:57 right so welcome back to the code and I think what you'll notice is this is going to be super straightforward so\n1:12:03 what we have to start is we have a bunch of different prompt templates each one of these prompt templates for positive\n1:12:09 feedback negative feedback neutral feedback and escalate feedback all these different prop templates come from right\n1:12:15 here so as you can see across the board we have all these different prompts that's what we're doing we're building up each one of these chains for the\n1:12:21 different types of feedback all right let's keep going then eventually what we're going to do is we also need to create the prompts for our\n1:12:27 classification part so this is you know how you know your helpful assistant help us classify the sentiment of this\n1:12:34 feedback is it positive negative neutral or do we need to escalate it and this is how we're you know using our prompts to\n1:12:39 pass in our users feedback cool so here's where we're going to get into the new part which is the runnable branch so\n1:12:46 this is the part that is different than anything you've seen so far so what you're going to do is you're going to create a runnable branch and the\n1:12:52 synopsis of how this works is you're going to have a input so in this case we're just going to use a Lambda\n1:12:58 function and then you have a conditional right here so in this case we're going to say like hey is the word positive in\n1:13:05 the input that you just gave me if so fantastic I'm going to run and this is basically oops sorry this is basically\n1:13:11 the positive chain I'll just write right here back chain fantastic so this is\n1:13:17 actually what's happening under the hood we didn't full out you know call each one of these their own chains but each one of these is a neutral chain a\n1:13:24 positive chain and a negative chain so here's you know this because this is once again we're using Lane chain\n1:13:30 expression language to create a positive chain so as you can see we're just going to take in the promp template that we saw above we're going to pass that to a\n1:13:37 model and then we're going to get back the output so this is a message that we could respond to our users saying like oh you loved our product fantastic I\n1:13:44 really appreciate it and we hope you you know continue to buy further products and services from us okay so now you've\n1:13:50 kind of seen how we're going to use runnable branches to conditionally run different chain such as positive chains\n1:13:56 check to see if it's negative to run the negative chain see if it's neutral to run the negative chain the other part that I haven't mentioned yet is there's\n1:14:02 a default so when you're using runnable branches if any of these messages didn't\n1:14:08 trigger off like if we didn't run you know the positive negative or neutral you basically can create a default\n1:14:13 branch and in this case we're going to call it the escalate branch and what this one's going to do is it's actually\n1:14:19 just going to make a nice message for us that we can see where it's going to like hey generate a message to escalate this to a human agent so that's just kind of\n1:14:25 how it works and this could definitely be super helpful in a customer service type of situation so I just wanted to show you guys this cuz it's super\n1:14:31 helpful and helps you build more complex tools all right so now that we got the runnable branches out of the way let's go back to see how we can make this work\n1:14:37 so basically in the end we're going to have our classification chain and then you know which is just going to be this\n1:14:42 whole part right here this is the entire classification chain right here and then the entire bottom part all of this huge\n1:14:49 square right here is going to be this second chain so we're chaining chains together to make super long complicated\n1:14:55 ones I hope you guys like this all right so what we're going to do next is I'm just going to go ahead and run it so you guys can see the results and if you want\n1:15:01 to play around with this code for yourself what you'll notice is I went ahead and typed up a bunch of different types of reviews so you can experiment\n1:15:07 with each of the different branch pths so let's go ahead and run it for a positive review fast just so you can see you know how it works so we're just\n1:15:14 going to go open up our terminal and this time what we're going to do we're going to do Python and this is the third\n1:15:19 module so chains five to run this branching program so we're going to run it and what we would expect to happen is\n1:15:25 it to ex to go down the positive branching path and generate a nice positive message so you can see you know\n1:15:31 I hope this finds you well I just want to personally thank you for your positive feedback so you can see it's actually it's actually working and what\n1:15:37 we could do too if you want to try out something different let's try out a bad review just so you can kind of see how\n1:15:42 it would go down the in this case go down the negative branching path so I replaced it with the negative comment\n1:15:48 it's terrible I broke it after just one use and then you can run it again so let's see you know thank you for your\n1:15:54 feedback I'm sorry that your experience was not up to your expectations we take all this feedback so as you can see this\n1:15:59 is a way we can handle a bunch of different scenarios using GL chain expression so yeah that concludes the\n1:16:05 third module where we're covering chains and what we're going to do next is move over to rag which is going to be what\n1:16:11 we're learning about retrieval augmented generation this is going to be a huge module I hope you guys are pumped for it\n1:16:16 let's go ahead and dive into it now all right guys welcome to module number four where we're going to start\n1:16:22 learning about retrieval augmented Generation all also known as rag so in this quick overview section what I want\n1:16:28 to do is I want to talk about what is rag why do we need it and then I want to do a high Lev overview of like just\n1:16:34 running through a simple example just so everything makes sense whenever we start to dive into these code examples that\n1:16:40 I've set up over here for you guys okay so let's just go ahead and just talk about what is rag at a super high level\n1:16:45 and kind of why do we need it well these large language models that we have set up like chat gbt Gemini and llama 3 all\n1:16:52 of these models have a con straint on how much knowledge they already have and that can be a problem whenever we want\n1:16:59 to start answering questions or whenever we have questions about additional things such as like hey what's happening\n1:17:05 today in the news well these llms have a cut off date of you know a few months or even a year ago so it has no idea what's\n1:17:12 happening right now also when it comes to maybe just like inside of your company you have some specialized\n1:17:19 documents for y's processes or you have some other information about a product You're Building well the LMS obviously\n1:17:25 don't know anything about it so it's up to us to use rag to feed in additional\n1:17:31 new information to these lolms so under the hood all we're trying to do is just give these lolms additional information\n1:17:37 so they can answer our question that's all that's going on at a high level so as you can see we can feed in things\n1:17:42 like PDFs so whenever you hear people talking to PDFs or chatting with them that's exactly what's happening they're using rag so we can feed in websites we\n1:17:49 can feed in source code and video transcripts and the list goes on and on but okay that's enough at a high level\n1:17:55 so what I want to quickly do next is let's dive over to a big example that I set up for you guys where we're going to\n1:18:01 walk through exactly how rag works at a super high level so when we hop over to the code and we go through all these\n1:18:06 different examples you'll have a high level understanding of like oh that's why we're doing what we're doing so let's go ahead and dive into that high\n1:18:12 level overview right now all right guys so welcome to this quick overview of how rag works now I've\n1:18:19 put together this flowchart for you guys so that we can understand two critical components of working with Rag and I'm\n1:18:26 going to walk through both of them at a high level and kind of go super quick because we're going to go through this a bunch more times as we actually dive\n1:18:32 into the code but the main thing I want us to take away from this demo is for you understand the Core highle Concepts\n1:18:39 such as terms like vector store embeddings llms chunks tokens we're\n1:18:44 going to learn we're going to just quickly say all these words and it'll mean a lot more when we dive into the code and that's where we're going to be covering in part one and then part two\n1:18:50 of this quick demo I want to walk through like great well once we have actually saved all of the information\n1:18:56 from our PDF over to this special database called a vector store well how can we actually start asking questions\n1:19:03 and pulling out information so that we can get a AI generated response that's\n1:19:09 informed meaning if we're you know chatting with a document about how to like cook a specific food well we want\n1:19:16 to pull out the specific instructions for that food along with our question to generate an AI response so that's what\n1:19:22 we're going to be talking about at a high level and we're going to go pip part and this will hopefully all make sense and I do think one of the core\n1:19:29 parts to help this make sense of like why we're doing what we're doing is to actually start at the vector store and\n1:19:36 work our way back up so you understand why we're doing it because and this will make more sense so hang on for two\n1:19:42 minutes and it'll make all sense okay so at the end of the day everything that we're trying to do comes to this part\n1:19:47 right here it's the bridge and connection between our database which stores all the information about our\n1:19:53 Tech Source plus this Retriever and this retriever is where we're going to like\n1:19:59 hey man please give me all the information about you know let's just say we're working with Harry Potter and\n1:20:05 we ask like hey which Professor also is a werewolf well what we want to do in this example is we want to be able to\n1:20:12 look up things such as like professor and werewolf how well if we were manually doing this what we would do\n1:20:18 just like command F in a PDF and just search for those words a thousand times but there's going to be a ton of entries\n1:20:24 and what we're trying to do is really find a werewolf and a professor that's the information we're looking for okay\n1:20:29 so what we're trying to do and this is a keyword here we're trying to look for similarity inside of the original teex\n1:20:37 source that we're looking for that's all we're trying to do we want to look for similar pieces of information inside\n1:20:43 this original PDF that talk about professors and werewolves okay so that's all we're trying to do here now well how\n1:20:49 does this actually come into play when it comes to rag well what's going to happen under the hood is this PDF of\n1:20:55 Harry Potter is going to be broken up into a thousand different chunks because you can see this PDF is has about 10\n1:21:01 million tokens and that's way too big for us to answer any questions or like analyze what's going on because chat gbt\n1:21:09 has a window of about 8,000 tokens and 10 million is obviously way too big to pass into it so what we need to do\n1:21:15 because we're just trying to pull out the similar pieces of information and pass it over to jbt so it can generate\n1:21:20 an AI response about it so what we need to do is we need to split this up into a bunch of smaller chunks that are about a\n1:21:27 th000 each what's great about this is because like I said chbt can take in 8,000 prompts 8,000 tokens so if we pass\n1:21:35 in one chunk up to three or four chunks there's still plenty of room for us to also pass in a question about those\n1:21:42 chunks okay hang on we're getting there all right well each one of these chunks is just plain text so you can see like\n1:21:48 chunk one is just like you know Harry you're a wizard and you know and basically how this is working under the hood is we're just starting at the\n1:21:55 beginning of the book taking out a thousand words putting it a chunk putting it getting the next few thousand words putting in another chunk well the\n1:22:01 problem is we said our original goal is we went to look up similar phrases well how the heck does AI compare and search\n1:22:09 for similar phrases well this is where the term embedding comes into play an embedding is nothing more than the\n1:22:17 numerical representation of text so that was a mouthful what does that actually mean well if we were to like in this\n1:22:23 super simple example if we were to embed the word dog cat in house you can see\n1:22:28 that a dog has this embedding a cat has this embedding and a house has this\n1:22:34 embedding and you'll notice they're nothing more than just a list of numbers but they actually under the hood have\n1:22:39 some meaning and relationship to to one another so for example a dog and a cat\n1:22:44 are super similar the only thing that's different are these last two numbers but even then they're not that different cuz\n1:22:50 they got four legs they're furry they live in a house but a house on the other hand it's got a roof it's got walls\n1:22:56 brick they're completely different subjects so if we were to ever like search about dogs or animals with four\n1:23:03 legs well these two things would come together these are super similar so we would get responses about dogs and cats\n1:23:10 so with the point I'm just trying to get across is embeddings allow us to easily in search for similar items and the way\n1:23:18 we're able to do this cuz like machines love numbers and numbers are easy to compare and see how different and\n1:23:24 similar things are okay so what we're going to do is we're going to convert these chunks of text over to embeddings\n1:23:31 so that we can easily compare them and contrast them and once we have all these different chunks of text embedded what\n1:23:38 we're going to do is start saving the embedding plus the original text to a vector store and the reason why this is\n1:23:44 important is because going back to our original problem what we want to do is we want to be able to ask questions and\n1:23:50 retrieve related documents from this Vector store all right so now let's walk through this flow down here now that\n1:23:56 we're in part two and you'll see exactly why everything that we just did above is super important so going back to us with\n1:24:02 our Harry Potter fans if we ask a question which Professor is also a werewolf well that question will be\n1:24:09 embedded itself because what we're trying to do is see we're trying to look up similar documents to our question and\n1:24:17 the way we're able to do that is because our embedding we now have a numerical representation of Professor and werewolf\n1:24:24 and and then eventually cuz now it's embedded now whenever we go to our use our it's going to be called a retriever\n1:24:30 whenever we use our retriever to look up similar embeddings in the vector store it'll go like oh well chapter 4\n1:24:36 paragraph 1 2 and 3 all talk about this professor that's a werewolf let me give you back those chunks of data or those\n1:24:44 Blobs of text so that you can then ask questions about them so that's all that's happening under the hood we go\n1:24:50 step one here's my question here's my embedding that has my question great let's return this chunk and this chunk\n1:24:57 because those were the most similar and once we have those chunks returned what we can do is pass them over to chat gbt\n1:25:04 and what's going to happen under the hood that you'll see is all that's doing is is just adding those pieces of text\n1:25:09 at the very beginning of our prompt and then at the very bottom we'll have our question and then we can now ask\n1:25:15 questions about those pieces of information so what it would really be is like which Professor is a werewolf\n1:25:21 and then these three chapters or these three PA paragraphs will have all the information we need to generate an AI\n1:25:27 informed response now that was a ton of information don't worry we're going to go part by part when we look at the code\n1:25:33 it's just at a high level the main thing I want you to understand is we have huge Tech sources that have way too much\n1:25:38 information for any AI to consume we need to break it up into smaller chunks those chunks need to be converted over\n1:25:45 to eddings the reason we need to do that is because we need to see how similar each different chunk is to one another\n1:25:51 so that later whenever we go to ask questions about those different pieces\n1:25:56 of text and embeddings we can go oh yep these two are super similar here's the information you requested back so that\n1:26:02 you can ask a question so that was a mouthful let's actually go ahead and start diving over to the code now so we\n1:26:08 can see all of this in action and I promise you after you see this a few times it'll all make sense and you'll be\n1:26:13 able to start building your own rag use cases so let's go ahead and hop back over to the code all right guys so welcome to the\n1:26:20 first code example inside of the rag module and I'm super excited you guys to see how everything ties together that we\n1:26:27 just walked through in the highle drawing so this is exactly where you're going to connect the dots you're just to paint a road map of what we're about to\n1:26:33 do is I have broken up the first code example into two parts as you can see there's a 1 a and a 1B 1 a directly ties\n1:26:41 to the initial breakdown of what we were setting up top in our initial drawing\n1:26:46 drawing where we were first converting some Tech Source over to our Vector store so that's going to be in 1 a and\n1:26:52 then 1B is going to be all about actually asking questions and retrieving documents from that Vector store so\n1:26:59 everything we talked about when it come to like embedding chunking saving stuff through the vector store that's all\n1:27:04 going to happen in 1 a and then building our retrievers once again embedding you're going to see that all in 1 B okay\n1:27:10 so we're going to go ahead now and start walking through this code part by part and some of this code is a little bit\n1:27:15 more advanced just because there's a lot more moving Parts but don't worry I'm going to cover everything and once again\n1:27:21 if you have questions you can always head over to the fre School Community I created for you guys just take a screenshot or a Tim stamp of the video\n1:27:27 and say Here's Where I'm stuck I don't understand this and myself or someone else in the community will be sure to help get you unstuck so you can keep\n1:27:33 going and building out your awesome projects all right so let's go through this part by part and also walk through some of the folder structure just so it\n1:27:40 all makes sense okay so the first thing that we're going to be doing is inside of 1A is we need to set up some file\n1:27:47 paths so that we can connect to different parts of our project so the main things that we need to connect to\n1:27:53 is our initial text source so this is the document we're going to be asking questions about in this case we're going to be asking questions about the Odyssey\n1:27:59 and you can see how we access that book is by looking inside of our current directory go to the books folder and\n1:28:06 then look for the Odyssey text so if right now U my current directory is inside of the rag folder if I open up\n1:28:13 books you can see that I actually have the Odyssey text file right in here so that's how we're going to access that and then next thing is you can see I\n1:28:19 have this what I'm calling a persistent directory so that's actually just where I'm going to be storing our chroma\n1:28:25 database and same thing this one we just look at the current database open up the DB folder whenever you start to save\n1:28:32 files this will actually Auto get created so you might not see DB out the gate and then next we're going to save\n1:28:38 everything to the chroma DB folder so that's what you can see now this is since I've already ran this code before\n1:28:44 I already have this but I'm going to delete it so I'm on the same page as you guys Okay cool so let's keep chugg along\n1:28:50 in our code and so you can understand what's going on so if you remember the first part of what we're trying to do in\n1:28:55 this highle code is go from a Tech Source over to our Vector storm so if we've already went ahead and chunked our\n1:29:03 document converted everything to embeddings and saved it to our Vector store there's no reason to rerun 1A so\n1:29:10 this is why I have this code right here it's just going to say hey does a persistent directory already exist in\n1:29:16 this case the chroma DB already exists if it does just completely skip all of this because there's no reason for us to\n1:29:23 go off and you know there's no reason for us to go through and pay because embeddings do cost money to convert from\n1:29:28 text to embeddings so we're just going to completely skip it but if we do not have that persistent directory what we're going to do is go off and actually\n1:29:35 set everything up so the first thing that we're going to do is we're going to start at the top and actually load in\n1:29:42 the file path and what I'm going to do to help this make sense is we're just going to quickly hop back and forth from part to part so it all makes sense so\n1:29:48 first thing we are going to load that file path which is connected to our Odus text and we're going to use a text\n1:29:54 loader a text loader is just a great way to actually like Point like yep here's the file path to my document that I want\n1:30:01 to load later on we'll talk about things like web loaders to where you can actually pull information from the websites but just I'm just want to like\n1:30:07 expose you to the realm of the possible but right now we're just going to load a text document once we have that file loaded what we're going to do is we want\n1:30:14 to actually start splitting up this document into small chunks exactly like\n1:30:20 we talked about here so so far we've loaded it and now what we're going to do is start chunking it and there's a bunch\n1:30:25 of different chunking strategies that we will talk about later on in the text splitting Deep dive but for right now\n1:30:31 just know we're going to try and chunk our basically each section of the book\n1:30:37 every 1,000 characters and just to go like a little you know help explain a little bit more just so we're on the\n1:30:42 same page you can also set this thing called chunk overlap and all that does\n1:30:47 is let's just say this is a chunk of text and let's just say in the middle of\n1:30:52 a paragraph Was A characters what it'll do is it'll cut off so you kind of get cut off in the middle of a sentence and\n1:30:59 then when you load the next chunk it's kind of like well I have a query about the end of the first chunk but it's\n1:31:05 about the second chunk too so it just I don't know it's it's just difficult for us to understand semantically what's happening in a chunk so what you'll\n1:31:12 notice a lot of times people do is they'll set some overlap so what that's what that'll mean is just like you'll\n1:31:18 always grab a little bit of the next section too that way there's there's just always overlap and a lot of times\n1:31:23 you'll get better performance results so a lot of times just like give you numbers a lot of people will do things such as like 200 or 400 or common values\n1:31:31 so yeah but right now for example we're just going to keep everything lean and mean and set the chunk size to 1,000 tokens fantastic so once we have set up\n1:31:38 our text splitter what we're going to do is actually split up our original documents into a ton of those chunks\n1:31:43 that you see right here each one's going to be a th000 tokens now once we have all of our chunks also docs whatever you\n1:31:50 want to call them what we can do next is we need to start moving over to in embedding them now in this casee we're\n1:31:55 just going to use the open Ai embeddings and we'll actually go through later on in an embedding Deep dive to show you\n1:32:01 other options that you have but for right now just know that we're going to use open AI to convert our text chunks\n1:32:07 over to embeddings and we're going to be using the text embedding three small model and there's actually a few\n1:32:14 different models that you can use there's one that's three the three small which is the cheapest one and I that's\n1:32:20 one I primarily use and there's also a three large just just know as we talked about earlier there's this things called\n1:32:27 numerical representations where just which were just long arrays of like a dog is equal to like an array of like 1\n1:32:33 two three well the large model just has a lot more values so we can get a lot more precise with our embeddings so just\n1:32:39 just know three small is probably completely fine for the projects that you're working on okay let's keep\n1:32:44 chugging along so once we have set up our embeddings what we're going to do is we're going to use our chroma Vector\n1:32:50 store to actually save all of our documents with our beddings and we're going to say you are going to save the\n1:32:57 results over here into this persistent directory so what I'm going to do is I'm going to run this just so you guys can\n1:33:02 see what it looks like under the hood so we're going to run python this is the fourth module and this is 1A and we're\n1:33:08 going to run R rag Basics I'm going to zoom out so we can actually see it so you can see we are creating our embeddings we're finished creating our\n1:33:14 embeddings and then now we're going to create our Vector store if you scroll up actually a little bit you can see we\n1:33:20 saved a little bit of our first chunk cuz in our code we said you you know how many chunks do we create in this case\n1:33:26 826 where each chunk has a th000 tokens and you can see the first sample chunk and this first sample chunk is actually\n1:33:32 the very first part of the book so if you go open the book you can see at like line one this is the exact same so yeah\n1:33:39 we've actually just you know this is working hope you guys are hype to see it's all all coming together all right so let's go back down to the bottom so\n1:33:46 you can see our Vector stor is created and we can confirm that by going over to our database folder and checking for our\n1:33:53 chroma DB and you can see at the beginning DB chroma DB yep it made it\n1:33:59 and just to show you guys if I actually try running it again the way we've set up the code is it'll give us an alert saying Yep this Vector store is already\n1:34:05 created no need to redo it cool so now that we have that set up let's go ahead and move on to Part B where we can\n1:34:11 actually start asking questions and pulling information from the vector store so let's walk over here to 1B and\n1:34:17 start going through this once again part by part so per usual what we need to do is we need to have we need to point\n1:34:23 point to our Vector store so that we can actually start you know referencing it so in this case copying the same thing\n1:34:29 look inside our current directory then actually access the database folder and look for chrom ADB once we have that set\n1:34:36 up we need to specify which embedding we're going to be using and we're just it's important to use the same one just\n1:34:42 so that you know you wouldn't want to change languages when you're asking questions so like we just need to be very specific and use the same thing\n1:34:48 consistent or else you're just going to get some weird results fantastic next what we're going to do is we're going to use chroma DB once again we're just\n1:34:55 going to spin it up saying yep here is the persistent directory where you've already saved all of your edings in\n1:35:01 those text chunks and here's the embedding function so that whenever you get asked queries you know how to go off\n1:35:07 and do a similarity check and see which relevant documents are the most similar all right so in this case all we're\n1:35:12 going to do is for this question we're just going to ask the Odyssey so the main character is Odus we just want to know who is his wife so this is the\n1:35:19 query that we're going to be asking to our Vector store and I do think it is important for you guys if you haven't\n1:35:24 seen what Vector stores look like I actually think they did a great job this is the chroma DB website and I think\n1:35:32 they did a great job of helping you guys visualize what's going on so under the hood you can see we're going to ask a\n1:35:37 query so who's ODS is's wife we're going to convert that query over to an\n1:35:42 embedding once we have that embedding what we're going to do is access chroma DB and it will pull out the documents\n1:35:49 and they're connected embeddings cuz like that's how we can do the similarity score door and we can see like oh this\n1:35:55 one's the most similar and here's the related document let me pass that back and once you get passed back you know it\n1:36:00 eventually gets passed back to your whatever llm you're working with and generate that AI response so I just thought this was a neat little graphic I\n1:36:06 wanted to show you guys let's go back to ours though so we can keep cranking this out so what we need to do next is work\n1:36:12 on that Retriever and the retriever is what's going to take in our query and\n1:36:18 eventually return back those related documents but when we're setting up our retriever we have a few different\n1:36:23 different options for what we can do and per usual what we're going to do is I have a deep dive later on so you can see\n1:36:29 all the possibilities when it comes to working with Achievers but for right now just know we have a few different search\n1:36:35 types that give us different results and for this example we're going to be doing a similarity score threshold all that\n1:36:41 does is pretty much exactly what it says it's going to do a similarity check to find the most related documents and it's\n1:36:47 only going to return documents that you know up to us how similar so this number you can see we have something called a\n1:36:53 score threshold well the higher this number is the higher the text document has to be to our query and we'll play\n1:37:00 around with this just so you can see it in action and the other argument that you'll see is this random letter K with\n1:37:05 a number so what the heck is that well what it's going to do is it's going to return the K closest documents so if\n1:37:12 there was 100 documents that were you know that were Above This threshold what this will do is it will return the top\n1:37:18 three closest results so that's how you have to kind of set up your retriever if you want to uh to do this and feel free\n1:37:24 to bump this number up or down just remember depending on which llm you're working with you're kind of capped at you know right around usually 8,000\n1:37:31 tokens so in this example we're going to be pulling out 3,000 tokens whenever we're all said and done and passing it\n1:37:36 to our llm okay so now that we have set up our retriever set up the parameters what we can do is we're going to call\n1:37:43 that invoke function remember we talked about that a lot in the chat model section invoke is the magic word inside of link chain for actually taking action\n1:37:50 on the models that you set up so in this case on our we're going to pass in the query invoke it and it's going to give\n1:37:56 us back the relevant documents so I'm going to go ahead and run this and then we're going to play around with some of these parameters just so you can see\n1:38:02 what's happening under the hood and I do want to point out all we're doing in this example is just showing the\n1:38:08 relevant documents later on we'll actually pass these documents over to a chat model so we can start interacting\n1:38:14 with them but for right now I just want to show you guys that yes our Vector store is returning related documents and\n1:38:20 we'll just look through them okay so let's come down here clear this up we're going to run inside of our fourth module\n1:38:27 we're going to run 1B to actually start grabbing those documents and let's run it okay so what I would expect is to\n1:38:33 eventually get back three different documents that relate to OD deus's wife so you can see we have document one\n1:38:40 document two document three that's cool it also includes a source we'll talk more about that later but the main thing\n1:38:46 right now that you can see is his wife is Penelope so you can actually like kind of see like we're talking about in\n1:38:52 this document M we'll actually just do a quick search so you can look up like the term wife it gets mentioned in a few of\n1:38:58 these documents so yeah you are indeed blessed in the possession of a wife endowed with such rare excellent of\n1:39:04 understanding and so faithful as Penelope so that's like you can see this document perfectly describes you know\n1:39:10 like this is his wife so in the other documents too kind of talk about you know some of the other they they refer\n1:39:16 to wife so they're not as similar but you can see this was the closest one that's why it's document number one all\n1:39:22 right now that you've seen that work what I want to do is show you how we can play around with some of these parameters to get different results so I\n1:39:28 can actually bump this up to k equal 10 and I'll bump this down so we basically\n1:39:33 we're we're lowering the threshold and saying we want more results so what we're going to do is I would expect to get back a bunch more documents this\n1:39:39 time that are related to a wife so you can see yeah we got back 10 more documents this time let's see how many\n1:39:45 times his wife's mentioned penelopy yeah so these These are getting less relevant\n1:39:51 seven out of the 10 documents referred to her okay so what I want to do next is I want to show you guys what happens if\n1:39:57 you get a little too stringent so let's just say we only want to have documents that are like super super close to our\n1:40:04 initial question so if we were to run this updated code where we've updated the threshold to 0.09 meaning like very\n1:40:10 very similar what we'll do is we'll probably run into the problem of like hey you were too strict with your search\n1:40:16 results I couldn't find anything so we're going to give it a second and whenever it gets back to us it'll\n1:40:21 probably tell us hey man cannot find that oops it looks like there's a quick bug let me just close this out clear it\n1:40:27 out let me try this one more time and now it should hopefully give us back an\n1:40:32 answer yeah so you can see this time for whatever reason it just hit a bug yeah there are no relevant documents that retrieve using the relevant score 09 so\n1:40:39 we know we got too strict so that's uh just you know as you're building your own rag applications if you're not\n1:40:44 getting back results you might just be a little too strict with your similarity score okay cool well that's it for\n1:40:50 module number one now what we're going to do from here is just to paint a road map for you guys we're going to head into part two where we're going to start\n1:40:56 adding metadata to our document so we can actually kind of see like oh this is\n1:41:02 where this Source came from so we're going to go ahead and learn how to add metadata and view it in our request\n1:41:07 let's go ahead and start working on that now hey guys so welcome to the second example inside of the rag tutorial and\n1:41:14 what we're going to work on in this one is start setting up metadata now why is metadata important in context of rag\n1:41:21 well it all has to come back to the fact that lolms hallucinate and what I mean by that is Hallucination is a really\n1:41:27 nice way of saying they just get stuff wrong but what's cool is whenever we start adding in metadata to our rag\n1:41:32 queries what we're basically doing is allowing our llms to respond with like here is the source of information for\n1:41:39 where I'm generating this information I'm passing back to you so if we were asking a question about a meeting that\n1:41:46 happened in the past at our company it would tell us like oh John said this and Bob responded with this and it would say\n1:41:53 in here's the source where I got this information from so you could always just click the source go check it out on\n1:41:58 your own and be like oh yeah that is exactly what happened and dive into more of the details that went on in that meeting so that's metadata at a high\n1:42:04 level now let's actually dive into the code example so you can actually see how we set this up in practice and you're going to notice this is super similar to\n1:42:11 the rag Basics tutorial we're just adding in a little bit more complexity so you can understand all the capabilities of working with rag okay so\n1:42:19 per usual what we're going to do is set up our current directories but this time what you're going to notice is instead\n1:42:24 of loading a specific book we're loading a bunch of books and that's because what we're trying to do in the end of this\n1:42:29 tutorial is ask about a certain character and a certain book and we would expect the documents to we would\n1:42:35 expect our retriever eventually to only respond with documents related to that character so you'll see exactly what I'm\n1:42:42 talking about we're going to ask about and Romeo and Juliet so we would expect to only get back information about Romeo and Juliet and and the metadata related\n1:42:48 to it but you'll see that set up in just a second okay so what we're going to do this time is we're going to start using a different database so this time we're\n1:42:55 going to be using chroma DB with metadata I've already set mine up and you'll set Yours up in just a second but\n1:43:00 the main thing this code is identical to the other one we're just going to check to see hey does this persistent directory not exist if not let's just go\n1:43:08 through that main process we talked about last time which is going to be you know setting up our Vector store if we\n1:43:13 already have the database completely skip it so let's walk through what's happening under the hood when we are setting up this Vector store because\n1:43:18 there are a few differences in this one the main one is to start off is we are going going and searching through each\n1:43:24 one of the files in our books directory and we're going to grab all the files that end in a.txt so we're going to be\n1:43:30 grabbing all of these different books right here and what we're going to do with these book files is we're going to\n1:43:35 iterate through each of them and start copying the same process we did last time except for one tweak where we're\n1:43:41 going to add metadata here's what I mean first we're going to actually set up a new file path by joining in you know the\n1:43:46 name of the book file with our local books directory then we're going to load that book just like we did in the last\n1:43:53 example but what we're going to do now is once we have that document loaded we're going to add some metadata to it\n1:43:58 in this case we're going to set up this source for this document so whenever we're loading for example the\n1:44:04 Frankenstein book we're going to say the source of this book is Frankenstein or whenever we're looking at the ilad what\n1:44:10 we're going to do is we and you know retrieve a document it's going to say the source of the retrieved information\n1:44:15 comes from the ilad and you could definitely get a lot fancier with this you could break up your documents and set it up to be per chapter or per you\n1:44:23 know if you had a long meeting you could set it up to intro you know speaker one speaker two you could do it however you\n1:44:29 wanted to just so whenever you're setting up your metadata you could have like chapter and so forth and so forth\n1:44:35 but in this example we're just going to stick to a book Source cool once we have set up our metadata we're just going to add it to our documents array or list\n1:44:41 and then from there we're going to copy the same process first we're going to iterate through each one of those documents that we have and we're going\n1:44:47 to use our character splitter to chunk everything up into 1,000 tokens once we have our Docs we're going to do the\n1:44:53 exact same thing where we're going to pass in our docs plus our embeddings over to our Vector store down here so\n1:45:01 that we can go ahead and save everything okay so I hope this is making sense because now we're going to hop to 2B so we can actually start asking questions\n1:45:07 to our Vector store and pull out documents so what we're going to do in this example is once again set up our\n1:45:13 file directories and make sure we point to that new Vector store that we just set up the one that's going to be having\n1:45:19 metadata so this one right here and what we're going to do is we're going to use our our new embeddings plus our Vector\n1:45:26 store so that we can instantiate our new chroma instance so we can start passing questions to it now this Vector store\n1:45:33 retriever is the exact same as last time all we're going to do is just use similarity score I want the top three\n1:45:38 result and here's my threshold I set it super low for this one just cuz for whatever reason it was being a little\n1:45:43 weird however let's go ahead and run this code so you can see what it's doing under the hood so we will do python we\n1:45:50 are in the rag module this is the second exle to be so we're going to go ahead and run it and what I would expect to\n1:45:56 get back is information on how did Juliet die plus the metadata okay so you\n1:46:01 can actually kind of see it right here like in document number two Juliet's\n1:46:06 talking she obviously thinks Romeo killed himself and out of true love down\n1:46:12 here she stabs herself so like you can like it actually went ahead and just gave us the exact piece of information\n1:46:18 we were looking for which is crazy that it did that so efficiently and the part that's EXT cool is you can see this is\n1:46:24 the source where this information came from so you can go back and double check the AI to make sure it didn't just come up and hallucinate with some some weird\n1:46:31 facts yeah that's how adding metadata to your rag queries works at a basic level\n1:46:36 I hope you guys appreciated that and what we're going to do next is actually start doing some deep dives into each of\n1:46:41 the elements that we have been using to basically do our rag queries so we're going to head over next and do a deep\n1:46:48 dive into text splitting then we'll you know keep going down the chain from there so you can see all As specs and\n1:46:53 how we can add in some variety to our rag queries so let's go ahead and move over to part number\n1:46:59 three so welcome to example number three inside the rag module this example is\n1:47:04 all about going deep into different options when it comes to using the text splitter so what I what I mean by that\n1:47:11 is there are a bunch of different ways you can actually go about splitting up your large documents and you're going to\n1:47:16 in this example explore all of them so how to split it up based on characters sentences you know paragraphs and more\n1:47:22 we're going to be doing all that in this example and we're actually going to be spinning up a bunch of different Vector stores so that you can see how all of\n1:47:29 the different text splitting methods compare contrast when we actually make a query so you'll see exactly what I'm\n1:47:35 talking about here in just a second and per usual we're just going to start at the top of the code work our way down so that you understand everything that's\n1:47:41 going on all right so let's go ahead and dive into the code so the first thing that we're going to be doing is setting up our basically our normal file\n1:47:48 directories so we're going to just say like Yep this is the book we want to analyze and here is the path to our\n1:47:54 databases and you'll notice that we don't have a specific database called out yet and that's because we're going\n1:48:00 to be dynamically spinning up about four or five Vector stores down below okay so\n1:48:05 what we're going to do just make sure everything exists and we're going to do the normal you know kind of set up our\n1:48:10 normal loaders this time we're just going to load our book Romeo and Juliet grab the document and stand up and\n1:48:16 specify our Vector store or sorry our embeddings now here's where things get interesting what we're going to do as\n1:48:23 you can see I have I think five different examples to show you guys and each one of these examples kind of\n1:48:29 specifies which text splitter we're going to be doing and when it's useful to use this text splitter just so you\n1:48:35 guys always have a reference to come back to of like oh yeah I think it was text splitter number three that was good for this scenario and you can always\n1:48:41 just come back and actually explore but here's what's happening at a high level and then we'll dive into this Vector store function that's a little bit kind\n1:48:47 of it's it's different so what you'll notice is we're going to specify a specific type of text splitter along\n1:48:54 with the parameters we want to use and after that we're then going to actually go ahead and split up the document and\n1:49:00 once we have all those split up chunks what we're going to do is then call this create Vector store with all of our\n1:49:06 chunks and then we're going to give this database a name and once we do that and pass over that information we're going\n1:49:12 to call this create Vector store and what you can see in here is we're going to finally specify our persistent\n1:49:19 directory here and then actually go ahead and Save save everything to our new chroma database so that's exactly\n1:49:26 what's happening under the hood okay so let's walk through each one of these and I'll tell you when you should use each\n1:49:32 method and then at the very end we're actually going to go off and query the vector store okay so to start off the\n1:49:38 first type of splitting we're going to do is just the normal character-based splitting in the past what we've been\n1:49:44 doing I'm just going to show you what we've been doing for 2A in the past so you can see what we've nor been doing is\n1:49:50 character based splitting so this is nothing new this time we're just adding some overlap and you can see this is a\n1:49:56 great method to use when the type of your content doesn't really like there's no syntactical kind of meaning for\n1:50:02 example like if you're reading a book we talked about maybe keeping paragraphs together because we kind of want to like keep chunks of information together so\n1:50:09 this is just great when we just need to Chunk Up and split a bunch of random information the next example I want to show you guys is sentence-based text\n1:50:16 splitting so this one is when we actually want to split Things based on you know a sentence so this way you know\n1:50:21 maybe pair graphs are too long so we're just going to split everything based up on sentences so we're going to look for\n1:50:27 you know periods explanation marks question marks so and that's how we're going to split things up and once again\n1:50:32 we're going to be splitting things into chunk sizes of a thousand we're going to call this our chroma sentence then keep\n1:50:39 chugging along the next one is going to be our token text splitter and so this actually splits based on a token so in\n1:50:46 this one what's going to happen is we'll actually maybe split in the middle of a word so if let's say a word word was\n1:50:53 super long and you know maybe the first part of that sentence was right over the\n1:50:58 the chunk limit well half of the words going to get cut out so I would not recommend this for a lot of you know a\n1:51:05 lot of like documents based but I just wanted to show you guys that this does exist all right the next one that I just\n1:51:10 want to show you guys is going to be the recursive character splitter this is the one that I would actually most people use I've seen people use this the most\n1:51:17 and I would definitely recommend using number four the most cuz like it tries to naturally split around sentences and\n1:51:22 paragraphs within the character limit so it does a really good job of like making sure the information that gets pulled\n1:51:27 out just kind of it makes sense it would make sense for this cluster of information to be stored together okay\n1:51:33 cool and the final one I just want to show you guys is going to be custom teex splitting and all we're doing with just to show you guys we can make our own\n1:51:39 Splitters and these Splitters you can specify like oh I want to split chunks based on at the end of a paragraph so\n1:51:45 this is what this you know double backspace it's just a it's a double Escape or two new lines so in our case\n1:51:50 we're assuming that's a new paragraph okay cool and we're going to save that custom Tech splitter to our chroma DB\n1:51:56 custom fantastic so what we're going to do next I'm just going to show you is for each one of our databases that we just sped up Vector stores we're going\n1:52:03 to go off and query those Vector stores with this question how did Juliet die\n1:52:08 and the way we're going to do that is we're going to pass in the name of the vector store along with our query to this function right here and all it's\n1:52:15 going to do is look up our persistent directory that's going to be our Vector store and we're just going to do the\n1:52:20 normal thing that we've done so far to where we have have our Vector store we set up our retriever set up you know our\n1:52:26 search parameters for similarity score and then we're just going to grab the relevant documents and spit them out so\n1:52:31 that's exactly what we're going to do so let's go ahead and start running this query to embed everything first off using the different text spits and start\n1:52:37 asking questions so let's go ahead and run this and this is our third example so let's go ahead and do this is in the\n1:52:43 rag module and this is example number three so let's go ahead and run this super fast and this one will take longer\n1:52:49 because we're doing five six different types of settings but you can see that it's working we're creating everything\n1:52:55 so far we're based doing it for characters next we're going to do it based on sentences tokens and then from\n1:53:00 there our recursive and then finally we're going to do our custom text splitter and once this is all done we're going to then finally ask you know hey\n1:53:06 so how did how did she die and you can see let's just go result by result real fast just so you can kind of compare\n1:53:13 each one so the first one for character this is what we've been using so far and this does look super similar to what\n1:53:19 we've been doing it's just a lot of text from there what you can see when it comes over to the sentence one this one\n1:53:25 actually looks a lot more structured and then finally let's just keep going along when it comes to our token what I would\n1:53:31 expect to see at the very end of this one is like it get cut off at a random word so in this case it got cut off\n1:53:37 right at my but if this had been a longer word it would have got cut off and we wouldn't have been able to like you know we could have potentially lost\n1:53:42 some information when it comes to our recursive text butter you can see that this has done a really good job you know\n1:53:49 cutting at the very end of a of a sentence so that did a really good job or verse I can't remember what happens\n1:53:55 inside of a how Shakespeare wrote and then finally when it comes to doing our custom DB it didn't it you know it kind\n1:54:02 of just the way it chunked it it just didn't do a good job and that's because the way we sped up to split based on two\n1:54:08 new returns it just didn't do a good job of grabbing information but the main thing I just want you to learn is there's a ton of different ways that you\n1:54:14 can actually go about using different text Splitters to grab information my recommendation for you is to always use\n1:54:20 the recursive text splitter whenever for you're starting out and then you can get a little bit fancy using the token and\n1:54:26 text splitter and some of the sentence ones but just main thing always use the recursive but enough of that let's go\n1:54:32 ahead and dive into the next module where we're going to start talking about the different types of embeddings that you guys can use inside of your your rag\n1:54:38 setup so let's go ahead and dive into example number four right now so welcome to example number four\n1:54:46 inside the rag module and in this example we're going to do a deep dive into embedding just so you can see\n1:54:52 what's possible okay and just to give a little bit of background so far we have strictly used open AIS embeddings and in\n1:54:59 this example I'm going to show you how you can start to use custom embeddings why you'd want to do that and show you some other models inside of the open AI\n1:55:06 embeddings that you might want to use okay so let's quickly run through the code so you can see what's going on and\n1:55:11 then at the end we'll run it so you can actually compare and contrast two different results using two different embeddings so you can see which one\n1:55:18 works better and which one doesn't okay so what we're going to do is our per usual we're going to load in a book that\n1:55:25 we want to ask questions about and what we're going to do in this one uh per usual you'll see we have a database\n1:55:30 directory because we're going to end up creating two different Vector stores in this example and after that what we're\n1:55:35 going to do is per usual go ahead and load our document and we're going to split it up in this case we're just\n1:55:41 going to use the character splitter and once we have that set up what we're going to do is we're going to end up\n1:55:46 using this function called create Vector store more than that in a second the important part is I have two different\n1:55:53 embeddings down here so let's walk through each one super fast so the first one is once again like I said we're just\n1:55:59 going to be using the open AI embedding and so far we've been using the let me\n1:56:04 go ahead and just show you the exact one we've been using we've been using it's called text let's see text embedding\n1:56:10 three small this is the embedding that we've been using the entire tutorial and now we're branching out and we're going\n1:56:16 to be using the Ada embedding okay so let's actually go ahead and look how these compare price wise and actually\n1:56:23 look at what the different options you have so over here on the open AI website you can actually look up embedding and\n1:56:30 you can see there's a few different examples so this one is going to be the Ada example version two this one is one\n1:56:36 of the more pricey ones and you can actually go under the hood and and look at the different options they have so\n1:56:43 Ada so you can see if you scroll deeper down they have different qualities so the adaa example has it performs a\n1:56:51 little it it doesn't perform as well as some of the other options but you're able to add more data compared to the\n1:56:59 large model so it's all a procon but personally I just go with a small model for most of the stuff I work on just\n1:57:05 because it's the cheapest and you get really good results but the are other options and I def want you to be aware of that okay all right let's hop back to\n1:57:11 it so what we're going to do in this case is we're going to create a new open AI embedding using this new model and\n1:57:18 that's option one and like I said these open a Bings they're best just for general purpose with really good\n1:57:24 accuracy and then but remember you have to pay because this is all happening on\n1:57:29 open AI servers now option two what this embedding is going to be is we're actually going to download an embedding\n1:57:36 model from hugging face hugging face is just a like I'll just go ahead and show you guys what it is so you can see it in\n1:57:43 action so let me pull up these models for yall real fast so as you can see whenever we head over to hugging face\n1:57:48 which is just a great place to go ahead and download models that other people have created ated locally on your machine so you can run them completely\n1:57:54 for free so you can see there's a bunch of different models here and we can actually search for embeddings we\n1:58:00 actually already searching for beddings and you can see there's just a ton of them yeah dozens but in our example what\n1:58:05 we're going to do is use one of the more common ones which is just going to be a sentence Transformer and all it's going to do is its main thing is it's going to\n1:58:13 like I said it's basically just going to run locally and perform in beddings personally I don't really use these that much I just want to show you what's\n1:58:18 possible the main Pro and the reason why you would want to use one of these models from hugging face is because you\n1:58:24 can run it locally for free which is one of the biggest benefits it's usually not as performative so you're kind of\n1:58:30 trading performance for cost so it really just depends where you fall on that Spectrum okay so what we're going\n1:58:35 to do next is now that we have gone off and specify the two different embedding models that we want to use and the\n1:58:41 different names we want to call these different models for our Vector stores we're going to go ahead and create those databases now you've seen this before\n1:58:48 what we're going to do just pass in the the embedding and this is going to just passing right here so we're going to set up the vector store name and then we're\n1:58:54 going to say which embedding to use and that's how we're going to go off and create our two Vector stores one for open Ai and one for hugging face now it\n1:59:01 is important to mention whenever you do use this embedding because it is running locally on your computer you're going to download a pretty big file I think it's\n1:59:08 half a gigabyte but that's how big the file is to perform your embeddings locally so it just beware if you're\n1:59:14 going to download this code and run it it does uh take up a good bit of space and might be even more okay so what\n1:59:20 we're going to do next is we're going to do our normal comparison where we're just going to ask a question to our\n1:59:25 Vector stores and we're just going to compare the results of open AI to our hugging face example so let's go ahead\n1:59:31 and go ahead and open this up and we're going to run our fourth example so four\n1:59:37 because we're in the rag module fourth example going to go off and run it now what you'll notice is I've already gone\n1:59:43 ahead and actually created these models in the past so it's not going to generate a new hugging face Transformer\n1:59:50 cuz I've already done it in the past but you will see that we are going to be able to query these documents so in our\n1:59:57 open AI example output let's see what words we get back so we didn't actually\n2:00:03 get a good example in either one of these so what we can do is update our Vector store and we can actually update\n2:00:10 our K to get back more results clear it run it again and this time we can actually hopefully search to see if we\n2:00:17 got back any information about OD deus's wife who in this case is penelopy and it's so funny because sometimes you'll\n2:00:22 get back an example that does include the right information so you can see now it just changes per run so you can see\n2:00:28 now in our hugging face example we got some information about Penelope we got one of the documents contained\n2:00:34 information about her and then if we go back up you can see that our open AI\n2:00:40 example came back with a few more examples talking about Penelope so overall they both got towards the right\n2:00:45 answer it just open a returned back a lot more relevant information so kind of what we expected it's more performative\n2:00:51 at the cost of spinning a little bit more M on it okay cool enough with doing our embedding Deep dive what we're going\n2:00:57 to do next is we're going to move over to working with retrievers and learning about different search types and\n2:01:02 different arguments we can pass in to our retrievers and this is really going to help up our game when we're working with different rag applications all\n2:01:08 right let's go ahead and head over to example number five so welcome to this fifth example in\n2:01:15 the rag module in this example we're going to be doing a deep dive into retrievers and understanding all the\n2:01:22 different ways we can actually update how we search for different documents inside of our Vector stores so that's\n2:01:27 the main goal of this quick Deep dive okay so what we're going to do as a high level just to also understand we're going to by the end of this example\n2:01:34 showcase how by fine-tuning different search parameters and our retrievers how we're going to get different results so\n2:01:40 you're going to learn more about different types of search queries and different ways to basically you know pass in custom parameters for those\n2:01:46 different types of searches and we're going to be able to at the end of this compare and contrast the different results okay cool so let's quickly run\n2:01:52 through this one so the main thing we're going to do our normal part of setting up a accessing our Vector store in this\n2:01:59 case we're going to reuse our old Vector store which included all of our metadata so this is one that had all of our books\n2:02:05 with the different sources outside that we're going to use our usual embedding model we're going to spin up our chroma\n2:02:11 DB instance and then from here what we're going to do I'm going to close that out and we're going to come back to\n2:02:16 this query Vector store but what you can see is we're trying out three or four\n2:02:22 different examples of searching inside of our Vector store so what you'll notice is let me just show you like a a\n2:02:28 compare and contrast so the first thing is we pass in the name of the vector store that we want to search the query\n2:02:35 so this is like hey what question do you have what type of embeddings are we going to use and then here comes the\n2:02:41 important part we're going to pass in what type of search basically what type\n2:02:46 of search model we want to use and then any parameters so let's actually hop into this query Vector stor you can see\n2:02:52 all this in action under the hood so like I said you can see the search type and the search type parameters right\n2:02:57 here so we're dynamically spinning up and creating our retriever because that's the main area that we're focusing\n2:03:03 on in this tutorial we want to compare and contrast all the different options and not only do we want to like access\n2:03:09 these retrievers we want to go off eventually and fetch information with these different types of retrievers so\n2:03:15 that we can compare and contrast the relevant documents just so we can see like oh yeah this search type performs better than this search type okay so\n2:03:22 let's hop back down to our three examples below so that you can actually see how they work so so far what we've\n2:03:28 been using is and the rest of our code has been let's scroll down so I can show you we've been using similarity score\n2:03:34 with threshold basically so this is how we say yep I want to get the top three results and I only want to get back\n2:03:40 information that's over this score threshold that way we only get back relevant data however sometimes you\n2:03:46 don't want to actually do that threshold scoring if you know for a fact like yes every question I'm going to be asking is\n2:03:53 relevant to that database you really don't need the threshold so this is a very quick way to say yep here is my\n2:04:01 data store I want to just grab all the similar results don't care how relevant they are I but I do want to grab the top\n2:04:08 three results that's exactly what's going to happen with this similarity score and there's no way of really filtering out the like oh yeah this\n2:04:14 one's not that relevant okay the next one that we want to do is working with\n2:04:19 the max marginal relev an search query so what does this one do why is it\n2:04:25 important well what it does is it tries to not only get the most relevant information but it tries to kind of\n2:04:31 spread out meaning if for example how we're going to ask about questions about\n2:04:37 Juliet's death well what this type of search score would do instead of sorry I'm going compare and contrast with this\n2:04:43 similarity score let's just say all the top three documents about her death were all right next to each other so it's\n2:04:50 just going to grab all of that information and maybe leave out some additional context and what's cool about\n2:04:56 this Max margin result is not only is it looking for super relevant information but it's kind of also looking for\n2:05:02 adjacent information around her death so it's not just going to be like yeah she died because of blank it's going to\n2:05:08 maybe skip around to like a few paragraphs later that's also talking about her death but it might not be the\n2:05:14 most relevant information to help generate a more well-rounded response so very cool search method and you can\n2:05:20 actually see Let's uh scroll down here so you can kind of see so yeah so K is how many different responses we want to\n2:05:27 get fetch K specify the number of documents to initially fetch so this is you know how we can actually kind of\n2:05:33 specify like we're going to grab a ton of documents and then inside of that we're going to return the top three and\n2:05:38 we're going to like space out those results so you're going to get a well Diversified set of results so this is\n2:05:44 what you can see kind of right here so feel free to play with this Lambda multiplier like I said the main thing it\n2:05:50 does is control the diversity so if you want a ton of spread out information to get like as much wide range of context\n2:05:56 around the topic as possible you're going to want to bump that number to zero to get the maximum diversity and if\n2:06:01 you want super similar information keep it to one okay cool so enough of that I'm excited to show you guys this one\n2:06:07 actually running so you can see how it performs and the final one this is the one that we've been using the entire time which is just our similarity score\n2:06:15 with a threshold okay so let's go ahead and actually run this so you can see how it performs and we're going to actually\n2:06:20 this time because there's going to be so much information um over here in my terminal and we'll look at the results over here so what we're going to do is\n2:06:27 run python this is in the rag section and we are on the fifth example so we're\n2:06:33 going to go ahead and run this code and it's going to go off and actually oh my bad I need to actually load in my\n2:06:38 environment variables I didn't do that real fast we'll just quickly fix that on the fly so we'll do load oops\n2:06:46 load. EnV and then once we have that we'll come over here and import it fantastic that was my bad save it now\n2:06:53 when we run it again it'll work and it'll actually go ahead and show all the different results for the different Vector stores okay so let's run through\n2:07:00 this part by part just so you can kind of see how it works so the first example that we're going to look at we have to\n2:07:06 come all the way up just because there are a ton of different examples so the first one is just the regular similarity score so you can see it is going to\n2:07:13 return three different documents that are the most similar to our initial quest which was how did Juliet die so\n2:07:20 we're going to get back a ton of information and then hopefully one of these will actually include the way that she died\n2:07:25 yeah so she stabs her health so yeah you can see document two include the exact pieces of information that we needed so\n2:07:31 the next one that we were wanting to do was the max marginal relevant score and this is the one that allows us to get\n2:07:36 not only the piece of information we want but some of more of the contextual information around her death so let's\n2:07:42 actually see if this result includes anything around her dagger so this one does talk about you know laying her\n2:07:48 dagger down and it also yeah this one didn't perform as well as the other one\n2:07:54 so this would be a good way for us to go off and actually potentially like oh maybe let's just get some less diverse\n2:08:00 information so we really hone in so like I said everything's a game and you have to optimize and tune those parameters to\n2:08:06 get the results that you're looking for and the third one which is our usual one which is hey let's go off and actually\n2:08:12 get everything that's within a similarity score this one is going to return per usual the one where she she\n2:08:19 snatches a dagger and stabs herself so like I said there's a few different ways you can work with these different\n2:08:24 retrieval messages retrieval methods to experiment with grabbing information from your vector store so I just want\n2:08:30 you guys to be aware of a few of the most common approaches but enough of that what we're going to do next is Hop on to the next example where we're\n2:08:37 actually going to not only be retrieving information now but we're going to actually ask a one-off question of like\n2:08:43 hey what happened and we're going to get an AI generated response let's go ahead and dive into this example\n2:08:50 next so welcome to example number six inside of the rag module I'm super\n2:08:55 excited for this example because you're finally going to learn how to tie all the information that we've been storing\n2:09:00 in our Vector store we're going to be grabbing that information and passing it over to an llm so we can actually\n2:09:06 finally generate an AI generated response so I'm pumped for you guys to see it so now that you know what we're\n2:09:11 going to be doing let's actually dive into the code so you can see how you can start chatting with your data okay so\n2:09:17 per usual what we're going to do is we are going to go off and grab our Vector store location per usual we're going to\n2:09:24 be chatting with our Vector store that has all the different options from all the different books what we're going to\n2:09:29 be doing is use our usual text embedding from there we're going to spin up an instant of our Vector store from there\n2:09:35 we're going to be passing in a new question and this one is just going to be hey how can I learn more about Lang\n2:09:40 chain and now that we have our query we're going to set up a new retriever in this one we're just going to use the\n2:09:47 similarity search method which is going to like I said just how we just learned about it's just going to grab the most\n2:09:53 relevant document and it's going to return only one result and from there what we're going to do is use the\n2:09:58 Retriever and go ahead and search our Vector store for that query and return relevant documents so it's important to\n2:10:05 realize that this is a list of documents now in our example we're actually going to print it so you can see the\n2:10:10 underlying document but here's where the magic happens what we're doing under the\n2:10:15 hood is we are generating a r query and we're not only going to just use our\n2:10:21 query we're going to pass in the content from our Vector store that we just pulled out combine them and we're going\n2:10:27 to use our chat models to generate a response so this is going to be awesome and let me just walk you through the\n2:10:33 prompt so you can understand what's happening at a high level so the first thing is we are creating our prompt and we're going to say you know here's some\n2:10:39 documents that might help you answer this question here's the question now here's the relevant documents we could\n2:10:44 have probably used a prompt template to do this and you'll see we're going to do some of that later on but we could have even done it up here and then what\n2:10:51 you're going to see is we're going to join basically we're just going to do some string manipulation here to where we're going to grab all the content from\n2:10:57 our relevant document and really just spit it out in a nice text format and if you remember each one of these documents\n2:11:04 because we set this up earlier when we were doing our embeddings each one of these is going to be a th tokens long so\n2:11:09 we're going to have plenty of space to pass in this information into our query because hey we're only getting one result so 1K tokens and we have up to\n2:11:17 eight so 8,000 tokens so what we're going to do is once we have this combined input we are going to spin up\n2:11:23 our new chat model and we're going to use the latest version of open AI so ct40 and we are going to go ahead and\n2:11:30 generate our messages and what what we're going to do is pass in our messages with our combined input\n2:11:36 containing our query and all the information from our Vector store into our model and we're going to invoke it\n2:11:42 and what this is going to do is generate a result for us which is going to contain the AI response and for this\n2:11:48 example let's just only generate the content only so you can just see the actual like response that it's going to\n2:11:53 say like well if you want to learn more about langing chain here is my recommendation so what we're going to do\n2:11:59 is go ahead and run this is module number four for rag this is example number six for a one-off question so\n2:12:06 let's go ahead and run it so what you can see is it'll hopefully spit back some relevant documents and then at the end it'll do the AI generated response\n2:12:13 with information from our documents so this is so cool so let me just show you the AI generated response first and\n2:12:20 we'll hop back up to the relevant doc so I was a little bit uh a little cheeky and what I did is I put in a document\n2:12:26 about myself inside of the book section and you can see I have something oh\n2:12:31 where did it go for you guys yeah Lang chain demo and what you can see in here is you know hey if you\n2:12:37 want to learn more about Lang chain you can go over here to the official documentation or if you want more\n2:12:42 in-depth tutorials and insights on Lang chain check out my YouTube channel and uh here's a link to it don't forget to\n2:12:49 like And subscribe especially if youve made it this far this video don't forget to uh to like And subscribe if you want to learn more about AI yeah as you can\n2:12:55 see it's generating an AI response that officially responds to our query in a conversational way and you can see this\n2:13:02 is the exact document text that we were able to like manipulate and turn and use for our AI response and if we actually\n2:13:10 look inside this text demo you can see I put a bunch of information and we got back the part in our response for\n2:13:17 further exploration yeah so in our the way just the way the chunking worked is we got this piece of text and yeah so I\n2:13:23 hope you guys think that is super cool for our oneoff example what we're going to do next is I'm going to show you how\n2:13:29 you can have a full-on conversation with your rag data so let's go off and start working on example number\n2:13:36 seven so welcome to example number seven inside the rag module this is definitely\n2:13:42 the most complicated example in this section however it is the most useful\n2:13:47 one and probably the example you'll use the most often when ever actually building out a rag solution inside of\n2:13:53 your applications for your users okay so what we're going to do in this one is we're going to quickly speed through all\n2:13:59 the parts that are similar and then we're going to focus on the parts of this code example that are different that allow us to actually have a\n2:14:05 conversation with our rag application so that we can you know ask a question follow up with it get additional\n2:14:11 information from our Vector store and just keep going so that's what we're striving to do here in this example so\n2:14:16 let's go ahead and dive in so in part one what we're doing is per usual we're just spinning up a Vector store that\n2:14:21 points to all the books we've defined what we're going to do is we're just going to use our usual retriever which\n2:14:27 is just going to be a similarity one to get the top results for this example we're just going to use CH gbt 40 as our\n2:14:32 llm okay so now let's dive into this part of the code that's all new so the main thing that we're trying to do here\n2:14:38 I think it's best if we actually start at the bottom to understand what's happening so what we're trying to do is\n2:14:43 we are trying to set up this retrieval chain all this is going to do is we want to be able to retrieve information from\n2:14:50 our Vector store we want to have awareness of all the conversations we've had up to this point and based on the\n2:14:56 information from our Vector store and our conversation we want to generate a result that is all we're trying to do\n2:15:02 and that's what's happening under the hood okay so how is this happening well there's a few things that need to happen\n2:15:09 part one is we need to be able to grab information from our Vector store well\n2:15:14 how are we going to do that well we're going to be using this library and function called create stuff documents\n2:15:21 chain what does that mean I know it's a weird term but basically what it's doing under the hood is it makes a chain for\n2:15:28 us that will take in a list of documents and pass it to a model so that's what it's doing under the hood okay so we\n2:15:35 have documents and we need to feed those over to an llm that's exactly what this Chain's going to do for us so cuz we\n2:15:42 have open Ai and now you'll be like okay well where do these documents you know where does this conversation and\n2:15:47 documents come from well let's keep working back up so next is to create this document chain we also need to\n2:15:54 provide some information about like what the heck's going on so that our llm is aware of what it needs to do well this\n2:15:59 is where our we are going to make a prompt for us we're just going to call this the QA prompt so you know question\n2:16:06 answering prompt and we're just going to say hey you're an assistant who does question answering use the following pieces of retrieve context to answer the\n2:16:12 question if you don't know the answer just say you don't know use three sentences maximum and keep the answer concise so we're really just going to\n2:16:18 say hey here's the Act exact piece of information you need to know from this rag query Okay cool so this prompt is\n2:16:25 then you know we're going to use our prompt templates back from module number two to pass in our basically just pass\n2:16:32 in our QA system prompt here to say this is your system context this is what you should be doing and then we're going to\n2:16:38 pass in our chat history our chat history will happen later on and that's as we chat with the our llm we're just\n2:16:44 going to slowly build out a list of messages and then finally we're just going to continually add in the human inputs every time the human has a\n2:16:50 question it's just going to get passed in as an input right here okay cool so now you understand how we have this\n2:16:56 question answering chain so this is going to kind of set up the whole part where we're like responding to questions\n2:17:02 now we need to have what we're going to do and call the history aware retriever so what is this well the history aware\n2:17:10 retriever this is where we're actually going to start passing in and working with our Vector store retriever remember\n2:17:17 retriever is how we interface and pull information from our Vector store and we're also you'll see here in just a\n2:17:22 second but we're going to do something very similar to where we're going to have our our AI our llm in this case who's going to be doing the thinking and\n2:17:28 generating some responses we're going to have a retriever which is pulling down the information from our Vector store and then we're going to have a prompt\n2:17:34 which is kind of sets the scene for what's going to happen next so let's go part by part so this all makes sense\n2:17:40 okay so for this history aware retriever we'll go start with a prompt so this prompt is saying like hey you're going\n2:17:45 to have some chat history and it's up to you to basically interpret what's being\n2:17:51 said and not answer the question but just kind of provide context for what's going on so this is all you're doing\n2:17:58 it's just you it's up to you to reformulate the questions so that we can properly search for information inside\n2:18:05 of the vector store so that's all we're doing you're going to get a question just rephrase it for the vector store so\n2:18:10 that we can retrieve the proper information so I hope that makes sense and then let's just go back and look at the other parts uh the retriever we've\n2:18:17 already set up we're just going to get the similarity score and we're just going to grab the top three result results and then when it comes to our llm we're just using chat GPT 40 okay so\n2:18:25 now that we have all of these different things set up we can actually go ahead now that we have our full retrieval\n2:18:31 chain that has two parts our history aware retriever which is pulling information from the vector store and we\n2:18:37 have our qu question answer chain which is actually like taking in user input and actually responding and generating\n2:18:44 answers to our users now that we have this entire retrieval chain set up we're going to call it our rag chain what we\n2:18:50 can do is start having a conversation with our llm and our Vector store so\n2:18:55 this is the exciting part so I hope you guys like it so what we're going to do is start asking uh we're going to go\n2:19:01 ahead and run the program so you can see it in action but I just want to show you at a quick level what's happening first\n2:19:06 we have a chat history and that chat history was important because we were using it up here inside of our\n2:19:12 contextualized prompt just so we can have a historical reference to what's being said okay and we're also using it\n2:19:18 in our question a prompt Okay so so that we're constantly adding messages to this\n2:19:23 so you can see here whenever we invoke our our rag chain we're passing in the users query and we're also passing in\n2:19:30 that chat history very similar to what we did when we were setting up our first conversations with our chat models in\n2:19:35 section one second what we're doing next is once we get the result what we're going to do is just print it out for the user to see and then we are going to\n2:19:42 append and update our chat history so that we have our initial query and we get back the result from the AI and we\n2:19:49 continually just add it in a pin it to chat history so that it's aware and we can have a full-on conversation all right enough talking let's go ahead and\n2:19:56 dive into the example so you can see that it's working so what I'm going to do is just go ahead and open up the terminal clear everything out and start\n2:20:01 our conversation so this is in module 4 and this is example number seven so what\n2:20:07 we can do is go ahead and run it and then you know it's going to start our\n2:20:13 chat conversation for us so we'll ask the same question we did last time how can I learn more about Lang\n2:20:21 chain what this is going to do is go go off and search through our Vector store and actually spit out a you know a\n2:20:27 response to us so yeah hey to learn more you're responding to us concisely like we asked using the information from the\n2:20:32 vector store it's basically consolidating to 3 CES as maximum answering the exact question that's\n2:20:38 awesome I also just finished Romeo and\n2:20:45 Juliet how did uh she die so let's just this is kind of going out out on a limb\n2:20:51 not sure if this one will work I just want to show you guys that we can actually have a conversation with it and we'll come back to my initial question\n2:20:56 just a second so that's awesome hope fingers crossed it'll come back and actually respond like yeah she kills\n2:21:02 herself with Romeo's dagger so the cool part is we have specified I just want to show you something we specified do\n2:21:09 not basically yeah if you don't know the answer just say you don't know and what's cool about this is the fact that\n2:21:15 it's not making up information it's pulling information strictly from our rag documents and going from there so we\n2:21:21 can try and ask something else so who is Brandon again and this is where we're\n2:21:26 actually going to start using some more of the conversational awareness because it's going to have to go Brandon oh yeah\n2:21:32 this is his YouTube channel so we would expect uh we would expect it to refer up to two messages ago yeah Brandon is the\n2:21:38 creator you can find his videos here so yeah so it's kind of doing the the full-on conversation part as well as the\n2:21:44 additional retrieval part where it's actually accessing our Vector store so I hope this makes sense to you guys this\n2:21:49 is like I said by far one of the most complicated examples in this whole module and really this whole course so\n2:21:55 if you have any questions please head over to school it's a preschool community and pop a question take a\n2:22:01 screenshot and say I'm stuck here don't understand it or I've been building this on my own and I keep getting stuck here\n2:22:06 please help and myself or one of the other developers in the community will try and help you get you unstuck but\n2:22:12 enough of this one let's go ahead and hop over to the last module we have which is all about web scraping and\n2:22:18 actually using information we get from the the web in our Vector stores so let's go ahead and dive over to example\n2:22:24 number eight right now hey guys so welcome to the final\n2:22:29 example in the rag module what we're going to be doing in this video is going over two methods for you to go off and\n2:22:36 scrape information from the web and add it to your vector store so that you can start asking questions and interacting\n2:22:41 with the data and what we're going to do is I'm going to show you a basic example first so you can see it's working and show you some of the cons and then we're\n2:22:47 going to go over to using a tool called fire crawl which is is awesome and want to show you how it's going to perform a\n2:22:53 lot better and provide much more better results really okay let's go ahead and dive in and go part byart so the main\n2:22:59 thing that we're going to see for the beginning is same thing per usual we're just going to set up our directories and\n2:23:05 set up our file paths and this one what we're going to do next is we need to start specifying a place where we want\n2:23:12 to start scraping the web in this case we're just going to go on apple.com and what we're going to do is instead of using the text loader like we've been\n2:23:18 using for everything else we're just going to use the web based loader which is going to go off and scrape the\n2:23:25 basically scrape a website for us so it's you know instead of looking through a book and loading it it's going to just\n2:23:30 go over to a website and load the information so that's what's happening under the hood and then everything else\n2:23:35 is going to be identical we're just going to split the text once we have split up the text into all the documents\n2:23:41 we're going to start passing over basically those split up chunks plus our embeddings over to a vector store that\n2:23:47 you can see right here so yeah we're just going to create a new Vector store from scratch and that way we can start accessing it once we have set up our\n2:23:53 Vector store what we can do is start asking questions to it and we're just going to use a similarity score and this\n2:23:58 one we're just going to grab the top three results and you know we're not going to chat with this data at all we're just in this example we're just\n2:24:04 going to get back results of like oh yeah these these documents have the relevant information okay so I'm going\n2:24:09 to run this one we're going to look through the results together and then we're going to hop over to fir crawl after we kind of go over the pros and cons so let's go ahead and clear this up\n2:24:17 and then we're running the you know we're in the module this is the eth example and this is the basic one so\n2:24:24 what we're going to do I'm going to zoom out so we can actually go through the results together so it's going to spit\n2:24:29 out a ton of information so this is just like the first one so you can see that it's going like okay well U we got five\n2:24:37 chunks from the Apple website here's the first one where you can kind of see like oh yeah we're talking about the Apple\n2:24:42 iPhone 15 Pro we're looking at you know just some of the main key phrases and\n2:24:48 we're adding all this information to our database so that's just an example one and then when it comes to actually\n2:24:54 asking questions to our Vector store we said what products are announced it's just spitting back this information so\n2:24:59 it's just like it's saying Apple intelligence it's saying iPhone 15 iPad Pro so it's it's really not like saying\n2:25:05 what's new just kind of spitting out what's on the website and uh yeah just going to keep doing that and just return\n2:25:11 a bunch of a bunch of information not super helpful so but it did work we can actually interact and it did a good job\n2:25:17 of web scraping so plus plus on both of those but the important part is I want to show you example number two which is\n2:25:23 using fir crawl just so you guys can see and compare and contrast the different results now fir crawl it is a free tool\n2:25:30 that you can use however they do have like you know they have some free usage for you however you do have like a paid\n2:25:37 version so let me just show you yeah so they do have like a free plan and a hobby plan but once you see how cool\n2:25:42 they are like like some of the results I think you like dang if you're doing a lot of web based scraping this is the tool for you make sure like super easy\n2:25:49 their key you know key slogan is like hey we turn websites into llm ready data so like you know here's a website that's\n2:25:55 gross oh we'll actually pull like a markdown version of this data for you so that it's easy to use and interact with\n2:26:02 inside of your llms so enough talking about let's actually go ahead and and use it and the one thing I do want to\n2:26:07 point out is once you've signed up a major free account you will have to copy and paste this API key over to your\n2:26:13 environment variables folder that we have or file that we've set up and you'll just go ahead and paste it in there so you'll just update the fir\n2:26:19 crawl and Pi just put your key in your you'll just put yours right here okay cool enough of that let's go ahead and\n2:26:25 actually start running this so you can see how this fire crawl example compares to the other one and the only thing that\n2:26:31 I'm going to just say different uh cuz some of this code is like well this is weird what's what's happening I will run\n2:26:36 through it real fast you know set up your create your new persistent Vector database for your vector store we're\n2:26:42 going to call this one fir crawl and this time because we can't just do web loader which goes off and does everything for us we actually have to uh\n2:26:48 be a little bit more specific using the fir craw loader tool so this is a package that you can install and if you\n2:26:55 actually go back to our py project. tommo you will see that we have where is it down here yeah you will see that we\n2:27:01 have a fir crawl package so this is how we're accessing this tool but hey we're going to use the fir craw loader here's\n2:27:06 our API key so you can validate that we are allowed to do this here's the website I want you to go scrape and they\n2:27:11 have a few different modes ones I think called crawl and one's called scrape crawl just go to a whole website and\n2:27:17 mode goes through like a single page so I would defin I'd use scrape to start off with CU you can easily blow all your\n2:27:23 tokens if you crawl so once you have scraped that website we're going to load all that information like we normally do\n2:27:30 and what we're going to do in this one is we're actually going to chunk through that document that we have pulled out\n2:27:36 and we're going to add a lot of metadata that you'll see later on so it's actually pretty cool this the setup they\n2:27:41 have and then what you're going to do next split everything up into different chunks pass over those chunks plus the\n2:27:48 embeddings over to to a new Vector store and once we have that Vector store set up we can start asking and making\n2:27:55 queries to it okay so enough of that let's go ahead and start you know let's go ahead and start testing it out so\n2:28:01 what we're going to do in this one we're going to do Python and the rag module this is the eighth example and this is\n2:28:06 fir crawl so we're going to go ahead and run it and it'll take a few seconds to go off and scrape the website beginning\n2:28:13 to crawl the website and I might need to update my question just because uh wwc 24 was a little bit ago and uh I'm sure\n2:28:21 a lot has happened since okay so the main thing is we grabbed 14 chunks of information compared to you know\n2:28:28 whenever we were running the other example we only grabbed you know three or five so we're grabbing a lot more information because the fir crawl does a\n2:28:35 much better job of actually you know pulling out information and it actually I don't know how it works under the hood\n2:28:41 but they do a much better job of like getting around like a lot of websites only load HTML whenever you try to use a\n2:28:47 tool like web loader they only give you h HML so you're missing out a lot of the JavaScript code that gets rendered so\n2:28:53 that's how we're grabbing so much more information when it comes to fir craw so under the hood you can see now whenever\n2:28:58 we ask you so you can see in a sample chunk that like hey we grabbed well for whatever reason it chunked it too much\n2:29:04 but you can see whenever we look at like relevant documents we can start to see some of the updated questions that are\n2:29:11 related now it didn't do a fantastic job but let's update it to talk about Apple\n2:29:16 intelligence tell yeah whenever I wrote this first query I made it for uh basically I did it back\n2:29:23 in the day whenever wwc 24 was about to happen so now we're going to do a second question and this time what it's going\n2:29:29 to do is provide information strictly about Apple intelligence so yeah document two all about Apple\n2:29:35 intelligence so like I said much cooler you can actually see a lot more of like\n2:29:41 the links now if you want to go off and actually like interact and actually pull this information and using a sales page\n2:29:47 isn't the best example using things like like Reddit or places like that where you want to actually like pull actually\n2:29:53 a lot of content from like a lot of users conversation that's a great place to try out fir crawl so but enough of\n2:29:58 that I hope you guys enjoyed seeing how you can actually start pulling information from the web you can go\n2:30:04 really deep into a rabbit hole when it comes to this but just for now when it comes to like scraping Basics this is\n2:30:09 plenty to get you guys started but yeah enough for module number four where we learned about Rag and the next step\n2:30:16 we're going to head over to start working with agents and tools and this is going to blow your mind so let's go ahead and\n2:30:22 head over to module number five hey guys so welcome to this fifth\n2:30:27 module in this Lang chain master class in this module you're going to learn everything you need to know about using\n2:30:33 agents and tools and before we dive into actually going through all the awesome code examples that I've set up for you\n2:30:38 guys what we're going to do at a super high level is go through what the heck are agents what are tools how do they\n2:30:44 relate and I will say before I like actually dove in to work with agents for the first time I thought they were going\n2:30:49 to be this super magical thing that I would never understand it was going to be super overwhelming but it really\n2:30:54 wasn't once you U now that you've gotten this far inside the master class and you have a really good understanding of prompts and you have a good\n2:31:01 understanding of chat models and so forth this isn't going to be too complex so stick around I think you guys are\n2:31:06 going to be like oh that's actually pretty easy to understand so let's dive into it what is an agent well under the\n2:31:11 hood an agent is nothing more than an llm that has been provided a specific prompt to guide Its Behavior so the best\n2:31:19 way I like to think of it is a state machine and if you've never heard of a state machine it's basically just a machine that has certain States and it\n2:31:25 can perform different actions at each state so let's just say it's at State one well at State one it can do\n2:31:31 something else and once it's done with that it goes to the next state and basically it will just perform a certain Loop of task and that's basically what a\n2:31:39 state machine is in the same way that's exactly how agents work we are creating a prompt that defines certain States and\n2:31:45 behaviors for our agent to do and what it's going to do is just flow through all of those different states and at\n2:31:51 each state perform a different action so let's walk through it in this actual diagram and we're going through it part by part so the first thing that most\n2:31:57 prompts have inside of an when they Define our agents is they usually have an action State and this action state\n2:32:02 says well this is where you can actually perform an action so think of it if we're creating a writer agent well the\n2:32:09 whole point of that agent is to write so usually we'll say our end goal is to write a bullet list about what's going\n2:32:15 on in the news today well this agent will know it needs to write a bullet list so let's just say it starts off\n2:32:21 with our input and just talks about the recent Apple event and it writes a bullet list well then what it'll go is\n2:32:26 go to the next state where it can make observations about its actions and from here what it'll do is go that was a\n2:32:33 little weird you only produced four bullet points then we'll come back down to the thought and when we are in the\n2:32:39 thought stage this is usually we'll plan out plan is the key word here we usually plan out what upcoming actions or\n2:32:46 behaviors we need to do next so in this case we'll go oh for whatever reason we only had four bullet points I need to\n2:32:52 rewrite my article or my bullet list and make sure I have five bullet points this time so let's add one more and then\n2:32:59 we'll just go back up action we'll update our bullet list and have one more so we'll hit a total of five observation\n2:33:04 phase yep we have all five bullets things look good and then now that we get back to our thought stage you know\n2:33:10 we're going to say everything looks great we look like we're done and our thought phase will go okay cool I no longer need to work as an agent here's\n2:33:16 my final answer and that's a super simplified version but really under the hood that's exactly what's happening an\n2:33:22 agent gets a specific prompt with an llm that tells it how to behave we pass in a\n2:33:27 goal and it'll just work towards that goal and just cycle through these stages and there's a bunch of different types\n2:33:32 of Agents but this is the core Loop that you'll hear about the most right here okay now what the heck is a tool why do\n2:33:38 we care about uh care about it when it comes to agents and I when doing this course at first I thought about breaking\n2:33:43 them up but I didn't just because they're pretty much useless without each other and you'll understand why in just\n2:33:48 a second so a tool is nothing more than a basically some usually some code and\n2:33:54 that provides our agents with additional functionality so basically these tools provide our agents with superpowers is\n2:34:01 the best way I like to think about them we give them upgrades is the best way I like to like kind of think of it my in my head so a few common tools that\n2:34:08 you'll hear of are tools that will allow you to search the internet so think tavali serper duck ducko there's a ton\n2:34:15 of these tools and they allow our agent to interact with the outside world cuz if you remember these llms they have\n2:34:21 constraints they're just you know U they have cut off dates and they basically you know can only think thoughts and\n2:34:28 write text and this is how we allow them to interact with the outside world so that's what the search internet tool can\n2:34:33 do from there we could also set up our agents to actually execute code so we\n2:34:38 can hook them up to an interpreter to like oh here's the python code let's go over here oh yeah you can actually now\n2:34:45 write and draw some you can you know run some code you can generate some graphs with plot Le like you can do a lot of\n2:34:51 stuff when it comes to having your agents interact with code the final one you could also have tools that allow\n2:34:56 your agents to go interact with databases so obviously R so far we've kind of like hardcoded and wired up R in\n2:35:03 the past with in our rag section to work with a vector store but you can actually set up tools to allow your agent to\n2:35:09 interact with a vector store a SQL database like there's a ton that you can do so it's just up to you to create\n2:35:15 those tools and then give those uh tools to your agents and so just give you an example if we were trying to you know\n2:35:22 find today's news find today's like top five most talked about articles and then\n2:35:28 plot them inside of a a python plotly chart for like how correlated they were\n2:35:33 something silly but basically you know we could do that with tools because going through the same Loop we would\n2:35:38 understand first I need to go off and find all the news articles and find the most talked about ones well whenever I\n2:35:45 take that action I'll search the internet come back I now have information about the outside world\n2:35:50 thought hm okay well now that I have information about the outside world I need to start mapping the information I\n2:35:56 found to a to some sort of chart where we can visualize how correlated everything is just you know Silly\n2:36:01 example but when now we come back to the action step and goes okay well I now know the five most talked about articles\n2:36:08 now I can start actually generating code to make a visualization out of this\n2:36:13 information so then I'll start executing code oh NOP it didn't work it didn't properly run go through it a few times\n2:36:20 okay great now everything's working here's all the python code so you can generate some graphics so that's exactly how it works under the hood and you're\n2:36:27 just going to keep going through this core Loop and we've now supercharged our agent with tools to go off and interact\n2:36:32 with the outside world and actually start taking some action so that's enough of everything at a high level what we're going to do in the rest of\n2:36:38 this example is I've provided about five to six different code examples that you'll see where we're actually like\n2:36:43 code everything up that we just talked about so enough talking let's go ahead and dive into our first example so you can see all this in\n2:36:51 action all right guys so welcome to the first example inside of the agent and Tool module now what we're going to do\n2:36:58 inside of this example is just walk through everything at a super high level because this is going to be the first\n2:37:04 time we're going to be introducing agents and tools inside of actual code so we're going to go through it super slowly and then after that we go through\n2:37:11 this example we're going to later do a deep dive into agents so you can understand all the different ways we can\n2:37:16 use them and then we're going to do a deeper dive into tools so you can learn learn how to use existing tools and learn how to create your own so that's\n2:37:22 what we're about to do let's go ahead and dive into this part by part so you guys can Master agents and tools all\n2:37:28 right so some of this is going to be super similar to start off we're going to be loading our environment variables because we need to use open AI now the\n2:37:36 first thing that we're going to do in here is go off and create a tool and for this one we're just going to create a\n2:37:41 super simple tool that allows us to access the current date and time for our local computer CU if you think about it\n2:37:48 our llms you they were generated in the past they have constraints of and cut off dates and they actually have no way\n2:37:55 of knowing what time it is currently so whenever we're actually using these llms in Agent form we can add a tool time so\n2:38:03 that our agents can access our current time this is going to be super helpful as you go off to build larger and larger\n2:38:09 agents where they need to interact so this is just a super basic one so let's walk through what's happening so the\n2:38:14 first thing is we're going to create just a regular function that all it does is it reports date time because date\n2:38:20 time is just a standard library and we're just going to grab this the current time and then we're going to\n2:38:26 return the current time in this format to where it's hour minute minute so that's all we're trying to do and that's\n2:38:31 the whole purpose on this function we'll dive later into why had to set up our parameters like this for the function\n2:38:37 but we'll come back to that later all right cool well now once we have defined our specific function that we want to do\n2:38:43 we have to wrap it inside of a tool and later we'll get into actually like using existing tools but for right now this is\n2:38:49 just like how you can make your own simple tool with your own code but basically all you do is you call the tool class which comes from up here\n2:38:56 blank chain core tools that's how we can create a tool and it's just up to you to give your tool a name a descriptive name\n2:39:02 about like you know what does this tool do that way whenever the agent's executing and it's like oh I need to\n2:39:08 solve a Time problem oh it makes sense for me to use the tool that talks about time so that's why it's super important\n2:39:15 for us to give the name and description to be very representative of what what going to happen under the hood from\n2:39:21 there we pass in the function that we want the tool to perform whenever it gets called on so in this case whenever\n2:39:27 we say like hey give me the time under the hood it's going to trigger this function which is going to return our\n2:39:33 string representation of the current time okay so that's how we make a tool that's how we add it to our tools list\n2:39:39 and once we eventually have our tools list we can actually pass this over to our agents and you'll see that here in just a little bit okay so now we've got\n2:39:46 tools out of the way at a super basic level now we're going to work on creating our agents so if you look at\n2:39:52 this you might be like what the heck are we doing what are we what are we pulling what is this word react well uh react\n2:39:59 stands for reason and action and all we're doing here is we are pulling out a\n2:40:04 prompt because you remember from earlier earlier from prompt templates all we're doing is pulling out a prompt template\n2:40:10 that tells our llm how to act so if you actually go and read what this uh what's\n2:40:17 going to happen at this prompt you'll head over to to a website like this I'll actually copy the link for you guys so\n2:40:22 you can read it yourselves but you can see kind of how we talked about earlier an agent's nothing more than an lolm\n2:40:29 with a super specific prompt telling it how to behave so this is exactly what's happening under the hood this prompt\n2:40:35 template that we're going to create an agent around you know it's just like hey answer the following the best you can\n2:40:40 here's the tools you have access to and then that Loop that we kind of talked about earlier where you're taking action\n2:40:46 you're making observations planning out thoughts and providing a fin answer this is exactly what you know what this\n2:40:52 prompt template is telling you to do so you know hey use the following format to basically perform actions so you know\n2:40:59 taking the input think about it take action here's the you know like as we're\n2:41:04 working with tools sometimes when whenever we're taking an action like get time well sometimes we might have to\n2:41:11 pass it in parameters to our functions so like I want to get the time in Tokyo\n2:41:16 I want to get the weather in San Francisco so that's what actions like you know use the tool and then action\n2:41:21 input is like whatever parameters we want to pass over to it so just hope hopefully that makes sense observation\n2:41:27 hey look at the results of the action thought you know go go off and here's what I need to do next basically plan\n2:41:33 yeah so and you can kind of see we're telling our llm to repeat this over and over and over until we get a final\n2:41:38 answer okay so that's what's happening under the hood and we're actually able to like the part that's nice is it's\n2:41:45 kind of like GitHub where we're able to just pull down and reuse other people's code that's working but in this case\n2:41:51 we're just pulling down other people's prompts which is pretty cool if you think about it okay so now that we have our prompt which is going to tell our\n2:41:57 llm how to act we need to keep chugging along and we actually need to specify which llm we want to create our agent\n2:42:04 around and this time we're just going to use cat gbt 40 that's the latest open AI\n2:42:09 model that's out at the time of this recording and then from there what we're going to do is go off and actually use\n2:42:15 the agent we've defined we're going to pass in the prompt that we fine and we're also going to pass in all the\n2:42:21 tools we've set up to actually go off and create our agent so if you look under the hood what's actually happening\n2:42:28 is this create react agent it's coming from the the Lang chain agents repo so\n2:42:33 I'll just scroll up to the top so you guys can see it yeah this is coming all from Lang chain agents and we're really\n2:42:39 just importing these two classes to create our agents and run them but the main thing for our agents is we're\n2:42:45 sticking to that react part to where we are going to you know reason about what we want to do take action and just\n2:42:51 continue that cycle over and over in order to actually achieve our goals so this is kind of what what it looks like\n2:42:57 under the hood if you want to actually dive in here and read more about what's going on but really we're just combining\n2:43:02 all the ingredients of everything that's needed to make an agent and just putting it into one one class so that we can\n2:43:09 actually start passing information to it and running our agent and that's exactly what we're going to do next once we have\n2:43:14 our agent it's up to us to then pass in everything over to an agent executor so\n2:43:21 an agent executor is just basically going to help manage the Run of an agent as it goes off to solve problems so if\n2:43:28 you come over here you can see an agent executor we're just going to say like hey this is allowing us to use tools and\n2:43:34 and if you actually dive into the code here too you can see this is where we're going to be like actually performing The\n2:43:40 Run and accessing the tools putting information back and forth and and continuing to go from there but really\n2:43:45 don't need to like dive into it the main thing is just understand that like yes whenever I want to run my agent I need\n2:43:51 to use an agent executor Okay cool so once we have our agent executor set up what we can do is once again use our\n2:43:57 magical word when it comes to Lang chain and we need to invoke our agent executor to start actually spinning everything up\n2:44:04 and what you'll notice is instead of just passing in a string we're passing in a dictionary and the keyword in our\n2:44:11 dictionary is input this is the main thing that our agents expect to read is\n2:44:16 an input so in our case what time is it is the question that our agents are going to be answering so enough of that\n2:44:22 what we're going to do is let's go ahead and run it so you can see all of this happen in real time so let's open up our terminal and we're going to run python\n2:44:30 this is the fifth module because we're learning about agents and tools and this is the first example so let's go ahead and run it and see what happens so what\n2:44:36 it's going to do as you go off and actually use agents you'll see a lot of colorcoded text usually coming out of\n2:44:42 the output as it's going through that reasoning taking action and making observations so you'll usually see things like purple green and white and\n2:44:48 this is is uh anytime the agents's performing an action you can see like all right what's the action I want to\n2:44:54 perform Let Me zoom in for you guys so what's the action I want to perform I went to get the time so that's pretty\n2:44:59 cool that it was able to look through all the available tools that it had and pick the correct one and then what it\n2:45:04 did next is it passed in an action input and because we didn't specify any\n2:45:10 parameters for AR tools it just gave in none but you'll notice sometimes when you're creating tools and we'll dive\n2:45:15 into this here in a little bit if you do not have any parameters for your function sometimes they'll mess up so we just\n2:45:21 accept all parameters for arguments and keyword arguments but we just don't do anything with them so that's just a quick work around but what's cool is\n2:45:28 once we you know perform the action pass in the action input you can see here in blue we get the local time back for when\n2:45:35 I'm recording right now and then now that we have that answer from our tool the agent goes I now know the final\n2:45:42 answer and once it knows it's the final answer because if you actually come back over here the thought is I know I now know the final answer it then gives us\n2:45:49 back the final answer so then that's super cool cuz you can see right here it said the current time is 8:31 p.m. so\n2:45:55 that's the agent thinking and then finally under the hood I've said that 100 times right now but what's happening\n2:46:01 is it finally now that it has the answer it generates a object that it returns\n2:46:06 back to us and you can see this object contains nothing more than the original input plus the output of the final\n2:46:12 answer from this agent so that's what's happening and what we're going to do next is um and I hope you guys first off\n2:46:17 I hope you think that is aome awesome because it's very cool that we can have agents reason and take action in the\n2:46:24 world and uh you know everything we're going to do from here we're only going to add in more complexity and show off\n2:46:29 cooler features just so that you guys can match uh Master agents and tools and just to dive in what we're going to do\n2:46:34 next like I said we have two different folders for you guys to go over all the different ways that we can use agents\n2:46:40 and tools and the first one we're going to do is we're going to do a deeper dive into agents so that you can understand\n2:46:47 different ways that we can like figure these agents to work and eventually we're going to hop over to showing you\n2:46:52 how to use more custom tools so let's go ahead and head over to our first agent Deep dive example and start looking at\n2:46:58 react chat hey guys so welcome to the second example in this agent and tools module\n2:47:06 and in this example we're going to go a little bit deeper into working with different types of agents and really\n2:47:12 expanding what they're capable of doing so in this example we're actually going to focus on swapping up the underlying\n2:47:19 prompt that we're using in the agent sprinkle a few more tools and then kind of add some chat capabilities to it so\n2:47:26 that our agents can now go off and do some internet searching for us and give us answers so let's go ahead and dive\n2:47:31 into this example so you can U understand what's going on okay so to start we're going to do the exact same\n2:47:38 thing we did last time but we're going to add in some more tools so this time we're going to do another get current time tool exact same thing and then next\n2:47:45 we're going to add in the search Wikipedia tool and this this is really just a function to go off and use the\n2:47:50 Wikipedia library and what it's going to do is basically get a summary about whatever topic we're interested in so we\n2:47:57 get ask a questions about famous people events times and Wikipedia is going to give us back a quick summary you know\n2:48:03 two sentences of information it has on that topic okay so now that we've defined those functions we now need to\n2:48:09 package them into our tools list so and we need to format everything to be in the proper tool class so that's where\n2:48:16 we're going to set up the name a description so that our agent can go oh yeah that's the tool I want to use and\n2:48:22 then the actual underlying functionality we want to trigger cool so now that we have our tools defined for our agents we\n2:48:27 can now move over to actually creating our agent and what you'll notice in this example is we're using a different type\n2:48:35 of agent this time for our prompt excuse me so last time we were just using the react agent but this time we're using\n2:48:41 the structur chat agent so what this prompt is focused on it's actually having a chat so if we head over to uh\n2:48:49 over here where we actually can see the underly prompt template it looks a lot different than the last one so you can\n2:48:55 see like hey you're responding to a human with helpful information you have access to the tools in our tools list\n2:49:01 and I need you to basically create a a Json blob for these tools to go off and perform actions that's enough of that\n2:49:08 and then what's going to happen next is like all right cool here's you're going to search for a question go off and\n2:49:15 basically have thoughts about it take action so this is where we're going to use that Json blob and this is basically\n2:49:20 going to be like you know information from the past that we've taken like so whenever we get a response back we're\n2:49:26 going to save everything as a Jason blob and I'll keep it at that but the main thing is it's going to allow us to have\n2:49:32 a conversation with our agents and it's going to go off and you know use tools to perform actions on our behalf okay so\n2:49:38 that's what's happening under the hood now what are we going to do next per usual we're going to uh spin up a chat\n2:49:44 model I'll actually go ahead and make sure we're using the right model so we're going to GPT 40 make sure using\n2:49:51 the latest one and then from there what we're going to do is we're going to use this new we haven't used it before but\n2:49:57 it's called a conversation buffer memory all it does is it allows us to store our\n2:50:03 chat history in in memory that's all that's happening so in the past we've done things such as you know we'll just\n2:50:10 do like chat history and we'll save it to a list well this is just a more and\n2:50:15 you know we'll always add in our system messages and human messages well this conversation buffer memory it\n2:50:21 just it does a better job of doing it so uh we're just going to use it for this one it makes things simpler to set up\n2:50:27 for us okay well now that we have everything initialized let's start combining things to go off and create\n2:50:33 our agents so that we can start running them so per usual we're going to create our agent and we're going to pass in the\n2:50:39 main ingredients which are going to be our prompt our tools that we defined in the specific llm we want to use in the\n2:50:45 underlying agent and then from there what we can do is create our agent executor and this is going to be you can\n2:50:52 see we're adding in more tools to our agent executor this time these are the same as last time our agents and tools\n2:50:59 but now we're going to add in memory to our agent executor and this is how we're going to keep track of our previous\n2:51:05 responses and messages with our agent because we're going to be talking to this one and what you can see from here\n2:51:11 is we're now going to now that we have our agent executor been up and it's ready to run what we're going to do is\n2:51:17 start go ahead and start our chat conversation with the user so in our case we're going to start chatting with\n2:51:22 our agent so this is going to look exactly like our initial chat example we did except now we've supercharged it\n2:51:28 with an agent that's not just talking to a vector store it's going off and searching the internet for us to get\n2:51:33 responses so this is super cool and I hope you guys can see the value of it so let's go ahead and hop down here and\n2:51:39 start running our agent and actually seeing how it works so I'm going to make this a lot bigger because you'll see how\n2:51:47 cool it is cuz it's going to actually show us behind the scene what the agent is thinking and before I trigger it I do want to show you guys one thing let's go\n2:51:54 ahead and get the code ready so agent Deep dive and this is example number one the thing that per usual we're going to use the magic word when it comes to\n2:52:01 running Lang chain tools so the agent executor and per usual we do have to pass in the input and you'll notice as\n2:52:08 you use these agents tools a lot more everything's kind of structured as an input and an output so that's how we can\n2:52:13 show the response from the AI we're just going to say yep grab the output and per usual when the user submits a message\n2:52:20 we're going to add a human message and whenever the AI generates a message we're going to add an AI message okay let's go ahead and run it so you can see\n2:52:26 what's happening under the hood so we're going to start off asking a question we're just going to say who is George\n2:52:32 Washington and this will go off and use Wikipedia so that's cool it found the proper tool to use and as you can see it\n2:52:38 passed in the action input of like well who are we trying to query about and it's so cool that the AI went from a\n2:52:46 question like who is George Washington to picking out the core topic of that question and then passed it as the\n2:52:53 action input so it's amazing that this AI can figure out based on what parameters we need to pass in yeah just\n2:52:58 grab the core part of it so that's super cool now in the yellow we're actually getting back a response from our tool so\n2:53:05 our tool from Wikipedia says you know like hey here's everything you need to know about him and so forth and so forth\n2:53:12 and then once we have the final answer it generates a response to us so you can see the bot came back and said yep give\n2:53:19 us everything that Wikipedia said so uh let's see we can actually ask because this is a conversation we can add to our\n2:53:26 original question so how old was he when he died so because this is a\n2:53:32 conversation we can actually refer to our previous messages so in this case it didn't even have to go off to Wikipedia\n2:53:38 this time because it was like oh wait already know the information so I can just use use that to respond and we can\n2:53:44 ask other things such as who is Elon mus and how old is he right now so this\n2:53:51 question is a little bit more interesting because it's a two-part question first you have to go off and figure out who is the main person that\n2:53:58 we're talking about so in this case it's Elon Musk for whatever reason it's kind of struggling right now to to find out\n2:54:04 who he is but usually this is just the part of Agents where it's just going to go in a loop until it finally gets an answer for whatever reason it couldn't\n2:54:10 figure out that one so we'll just ask a different question because I do want to show you guys the key key underlying part who is Steve Jobs\n2:54:19 and how old was he when he died and the main reason I'm doing these two-part questions is because I want to show you\n2:54:25 guys how these agents can understand what we're trying to do yeah and for some reason I think Wikipedia is just\n2:54:31 crapping out on me but the main thing to know is what would happen under the hood normally is these agents would go off\n2:54:38 and find the result for the first part which is who is Steve Jobs and once it knows the answer it would then go off\n2:54:43 and actually find a you know using the response it would then trigger the next part and the question so you can see\n2:54:49 like yep he's an American entrepreneur obviously known for founding apple and he died at the age of 56 so that's\n2:54:56 that's very cool that I can do a two-part question and you can even go deeper to where it triggers off a second\n2:55:01 Wikipedia call but I'm not going to go into that for this example so I hope you guys are like wow these agents are\n2:55:07 powerful they can act on my behalf and go you know find information inform for me and I can just talk to them so I hope\n2:55:13 you find that super cool and what we're going to do next is dive into the next agent example that I've set up for you\n2:55:19 guys where you can actually talk to a document store so we're going to work on this one now hey guys so welcome to the third\n2:55:26 example in this agents and tools module in this example we're going to be diving into how we can connect our agents up to\n2:55:34 a vector store so that they can kind of work together to answer questions about our data so that's exactly what we're\n2:55:39 going to be setting up in this example so let's go ahead and dive in and what you'll notice is a lot of the information that I'm going to be showing\n2:55:45 in this module was built off of everything that we've kind of done in module 4 with Rag and all the previous\n2:55:51 modules before that so the log this is going to look super familiar so I'm not going to dive super deep into it but to\n2:55:56 start off we are going to be setting up all of our file path directories so we can point to our Vector store so this is\n2:56:03 the same Vector store that we did earlier that read all those different books for us so we're going to go off\n2:56:08 from there set up our embedding so that we can actually you know uh whenever we ask a question it we can embed it and\n2:56:16 compare it to all of our other documents to you know grab the most similar answers so we're going to then spin up our Vector store with our Vector\n2:56:22 database and the specific embedding function we want to use and then we're going to use the exact same retriever\n2:56:28 that we used the entire time in the past we're just going to use the similarity one this time which is just going to\n2:56:33 grab the most similar results not worry about a threshold and in this case we're going to grab the three top results and\n2:56:39 if you remember we used a 1,000 tokens per result uh per document so this is\n2:56:44 going to give us 3,000 tokens worth of information for our agent to use all right let's keep chugging along from\n2:56:50 here we're going to start working on creating our agents or sorry llm so in this case we're just going to use Chad\n2:56:56 gbt 4.0 and then now we're going to be kind of combining two different examples so in the past we had an example where\n2:57:04 you could ask specific questions about the doc store to the vector store that we've set up and but now we're going to\n2:57:09 be combining it with our agents so this is the exact same demo we did before where we kind of first set up like you\n2:57:16 know just like contextual what we're doing here in the first place so like hey you are working with chat history to\n2:57:23 solve an answer that's all you're doing and the second part was the history aware retriever so this is like hey go\n2:57:29 look at the previous questions that we've worked with plus you can look at the uh you can use the retriever to go\n2:57:34 off and answer or grab information from our Vector store so this is definitely going to go a little in the weeds but we\n2:57:39 already covered this in detail in our previous rag example right here where we did rag conversation so you've already\n2:57:46 done this in the past we're just now building on top of it okay cool per usual we we're just going off and\n2:57:52 creating stuff from our document chain this is how we can actually if you go under the hood this is how you can go\n2:57:58 about passing a list of documents over to a model so that they can be processed once we' set up all that we can set up\n2:58:04 our retrieval chain which will basically be able to interact with our Vector store okay enough of talking about\n2:58:10 setting up our Vector store now it's time for us to go off and set up our agent so we can have our agent\n2:58:16 communicate with our Vector store on on our behalf and perform actions and and lookups information so in this one what\n2:58:22 we're going to do is we are going to use the same react agent that we did in the first time which is just going to it's\n2:58:27 going to think about stuff take action make observations and just keep performing that in a loop and per usual when we're working with agents we have\n2:58:34 to set up a tool well this time we're going to set up a Custom Tool and what's interesting is under the hood this tool\n2:58:40 is going to do is it's actually going to invoke our rag chain so what's very cool is now anytime our agent has a problem\n2:58:48 where it needs to answer a question what it's going to do is it's going to go well I don't know the answer to that question but you know what I bet this\n2:58:55 tool does because it's useful for whenever you need to answer questions about the context of whatever the\n2:59:01 question is so what's cool is all we're going to do here is this we're going to invoke this rag chain and what we're\n2:59:07 going to do to it is we're going to pass in the input so it's going to be the person's question but we're also going\n2:59:12 to pass in the chat history so that we can have some uh contextual awareness of\n2:59:18 previous messages so that's super cool so now that we have made a new tool for\n2:59:23 our agent we're going to go through the normal process of setting up and creating our agents so that we can\n2:59:28 actually start running them so now that we have all of our agents set up with a proper tooling we can now go into another while loop and in this while\n2:59:35 loop we're going to once again start chatting to our agent so I'm going to come down here we're going to clear\n2:59:40 things up and now we're going to actually start running this example so you can see it in action so we're going to do python this is the agents and\n2:59:48 tools module we're working inside of the agent deep dive and we're going to start using the react doc store so let's go\n2:59:55 ahead and run this one and you can see so we can start asking questions and what's nice already at the gate we are\n3:00:01 you know accessing the vector store so because there was that one file I set up where I talked about Lang chain that's\n3:00:07 the first question I'm going to ask so how can I learn more about Lang\n3:00:14 chain so what it's going to do is it's going to well this is a question I need to get an answer about and then what\n3:00:20 it's going to do is actually trigger the AI to go off and get that information\n3:00:25 and what's cool is like it responded with the exact part of like yeah this is you know to learn more about Lang chain\n3:00:31 that what the document said was like yeah go watch Brandon's YouTube channel here's a link to the YouTube channel so\n3:00:37 as you can see that's pretty freaking cool and then you can follow up because it is a conversation so you can say who\n3:00:42 is Brandon and then it'll it'll go off and actually you know say oh he has a\n3:00:48 YouTube channel where he talks about like AI does not mention the name Brandon oh I guess I should have\n3:00:54 capitalized it so so yeah that's how it works at a super high level when it comes to working with the dock store\n3:01:01 this example was a little weird but I just wanted to show it to you guys just so you can understand like oh yeah you can use you can really start connect\n3:01:07 these agents with different basically different tools and use these agents in different ways and I would do want to\n3:01:12 show you guys real fast when it does come to the agent Deep dive I do want to show you whenever we set everything to\n3:01:18 verbose verbose normally yeah so here in the agents and tools I just want to show\n3:01:23 you guys when this is actually going off and grabbing the information so we'll do this here for both and I'm going to\n3:01:29 rerun this example just so you guys can see that it is it's actually grabbing from the doc store because I I like to\n3:01:34 see the agent think that's one of my favorite things so whenever we run this again we'll now do python fifth module\n3:01:40 fifth module agent Deep dive example number two we're going to run it I'm going to ask who is branded again so um\n3:01:46 or how do I learn more about L chain how do I learn more about Lang\n3:01:52 chain and then now you can see it's actually whenever it's running you can see that it's saying like Okay I need to\n3:01:58 answer a question about I need to to answer a question because I have no idea what is Lang chain in the context of\n3:02:04 what I'm learning about okay cool well I'm going to call the answer question tool the input I'm going to pass in is\n3:02:10 how do I learn more about Lang chain and then so you can see cuz we passed in an\n3:02:15 input coming back down here to our code yeah so here's our input the query is now the same as this query the chat\n3:02:21 history this was our first question so it's not updating it yet and then the context well this is all the information\n3:02:27 that we get back from our Vector store so this is some information from our Vector store some of it looks like it is\n3:02:35 yeah this is all documents from our Vector store and once it has those 3,000 tokens worth of information and then\n3:02:41 converts that into a final answer that it then spits back to us that we can talk to here so yeah I hope you guys I\n3:02:47 think that's super cool to see how the agent's thinking and operating you know whenever it's in verbos mode but enough\n3:02:53 of doing the agent Deep dive what we're going to do from here is we're going to head over and start going deeper into\n3:02:58 different ways that you can create your tools so that you guys can Master this and create your own tools and really\n3:03:04 supercharge your agents so let's go ahead and head into that next hey guys so welcome to the third\n3:03:11 example inside of the agents and tools module and in this example I'm going to show you guys the most basic way we can\n3:03:18 go about creating tools and that's going to be using the tool Constructor and you've already done this a few times but\n3:03:23 now we're just going to go into a deeper dive of understanding like oh that's actually what's going on and here's how\n3:03:29 I can start making tools so what I want to do is to start off I just want to show you the three functions that we're\n3:03:35 going to try and add to our tool set so the first one is just going to be greet User it's going to take in a name and\n3:03:41 then it's just going to come back and say like hello we're going to do reverse string all this is going to do is it's\n3:03:47 going to take in some text and reverse it and split it back and then finally we're going to set up a concatenate\n3:03:53 strings tool that just takes in two strings and concatenates them together and spits out a string so well obviously\n3:04:00 you know that's just regular python let's dive into the tool section where we're going to start using the tool\n3:04:05 Constructor so this is exactly what we've done so far whenever we've created tools in the past where we've kind of\n3:04:12 set up a name set up a description and then Define the specific function that we want to call but the main thing I\n3:04:17 want what to bring your focus to whenever we're using the tool Constructor is this is a great way to go\n3:04:23 about creating simple tools because all we're doing is just saying like here's the name here's the description and then\n3:04:28 we're just passing it a function we're not specifying anywhere the like oh yeah this takes in two parameters one's an\n3:04:35 one's a string one's one's a number like we're just relying 100% on the llm underneath to understand what the tool\n3:04:43 needs and provide it to it so and most of the time that actually works really well and that's whenever you just have a\n3:04:49 simple tool this is the best way to go about it it's super simple to do and the part that I did want to show you guys as well is if you're not using this\n3:04:57 complete basic way to create a tool you can go off and use something called the structure tool and the structure tool is\n3:05:04 great whenever you want to kind of set up more complex functions so like this one takes in two parameters so you know\n3:05:11 if you're doing anything that takes in more than one parameter I would recommend going with a structured tool and the part that makes this different\n3:05:17 different is it takes in all the same information as before except then now it takes in this schema and this schema\n3:05:24 just we're just going to use pedantic which is just a great way to like define basically like typed models um so we're\n3:05:30 just going to say like hey here are our arguments for concatenating string I expect to have two two properties A and\n3:05:38 B where a is the first string and B is the second string so we're clearly defining oh wow whenever I pass in I\n3:05:44 need to basically give two strings and that's how it's going to how it's going to work so now that you've kind of seen\n3:05:50 how this works I'm going to go ahead and actually trigger this to run and so you can actually see all these tools working\n3:05:56 in action and the only thing I do want to mention that we haven't shown so far is uh I am using a new prompt for this\n3:06:03 one it's called open aai tools agent and what this one is doing is it's going to\n3:06:09 it's basically just focused more on using tools so you can kind of see you're a helpful assistant here's your\n3:06:14 chat history here's your human inputs so this one all it does really is it's just\n3:06:19 yeah it kind of just it works really well whenever you're trying to use tools and so what we're going to do is go\n3:06:25 ahead and Trigger this and what we're expecting to see back is three different responses one we're going to tell the\n3:06:31 agent executor to greet Alice so greet is going to hopefully trigger our first\n3:06:36 tool then eventually we're going to say reverse the string hello and then we're going to say concatenate and we're\n3:06:42 expecting our agent to go up and use the appropriate tools to make this happen and we're going to log the entire process okay so what we're going to do\n3:06:48 come down here clear things up and we're going to run our example so we're going\n3:06:54 to do Python and this is the fifth module this is a tool Deep dive and we're going to look at our first example\n3:07:00 together so like I said under the hood this is going to go off and Trigger each one of these so let's look and see how\n3:07:06 it did so for the first one we said hey we're trying to greet Alice so you can\n3:07:11 see that it said I need to you know invoke greeting user with the keyword Alicia or Alice and what it's going to\n3:07:19 do is come back with an answer because if you come back to our original function all it was supposed to do is\n3:07:24 pass in hello and the person's name and that's exactly what it did so thumbs up for the first one the next one is\n3:07:30 reversing the string so you can see that like yep we're invoking reverse string with hello it did it it performed the\n3:07:36 code and now we print it back and then finally what we're going to do for the last one is concatenate strings so you\n3:07:43 can see I need to invoke concatenate strings and it did a really good job of creating a dictionary where it specified\n3:07:50 what is a and what is B and this all comes back to the fact that we set up our structur tool and we passed in an\n3:07:57 argument schema that was the magic that allowed this to happen and now you can see that it did a great job of splitting\n3:08:03 it uh concatenating it together okay cool so that was hopefully a quick Deep\n3:08:08 dive into creating tools and now we're going to go into the next module where\n3:08:13 we're going to start learning how you can use the tool decorator to kind of simplify some of this so let's go ahead and start working on example number two\n3:08:21 in the tool deep dive right now hey guys so welcome to the third and\n3:08:26 final example in the tool Deep dive and in this example you're going to learn how to create tools but you're going to learn how to do it where you have the\n3:08:33 maximum control over how these tools behave and what we're going to do in this module is we're going to set up two\n3:08:38 tools the first one is going to be a simple Search tool where we're going to go off and use Tavi to search the\n3:08:44 internet and the next one is just going to be a multiply numbers tool just a super super basic one so that's what we're going to be doing in this example\n3:08:50 and I just want to walk you through the major parts together real fast cuz some of this is pretty similar so the first thing that we're going to be doing is\n3:08:56 setting up our pedantic models and this is what we're going to be doing to define the specific inputs for both of\n3:09:02 our tools so this is nothing new you've seen this so far all right now let's actually dive into the part where we're\n3:09:08 going to create our tools and how we have maximum control over them so the main way we're going to be creating our\n3:09:13 tools is by using the base tool class now hop over to the L chain doc so you\n3:09:18 can kind of see it so the main way it works is we're going to be using the subass base tool to generate new tools\n3:09:25 so we're going to inherit from based tools to create new classes that are tools basically so what you can see as\n3:09:31 Lang chain says like hey this is the way you can have Maxum control over your tools but you know it takes a little bit\n3:09:37 more work and the part that's interesting is you can kind of see like we are basically using this example I\n3:09:43 just want to like walk you through the setup of it but the part that's nice is you can actually set up your tools to have different functionality so you can\n3:09:49 set up a tool to have a run so you can see this is a private run method and\n3:09:55 what you can see is we can actually say hey here's what you need to do here's your inputs don't worry about run\n3:10:01 manager I've never really found it helpful and then you can Define the output and what's super cool is you can actually if you wanted to go even harder\n3:10:08 you could actually Define the output as a pedantic model and if it fails during the run to generate the proper response\n3:10:14 it'll redo it so this is a really cool way if you plan on doing doing you know spitting out some Json or you know you\n3:10:20 want to make sure that you print out an object that for sure has a like a person's contact info so it for sure had\n3:10:26 to have the person's first name last name and email you could set up a pedantic model as the output and you can\n3:10:32 do that all here inside of your custom tools when you're inheriting from base tool and then finally you can set up\n3:10:37 things to run synchronously or you can set them up to run asynchronously we're just going to stick to synchronously for\n3:10:42 this example okay so let's hop back over to our example so the first thing that we're going to be doing is setting up a\n3:10:48 Search tool because we want our agent to go off and access the internet well what we're going to do is we are going to do\n3:10:54 exactly kind of what we did in the tool Constructor where we have to give our tool a name because we need to let our\n3:10:59 AI know like Yep this is the name of the tool and here's when I would need to use it whenever I need to answer questions\n3:11:05 about current events so you know anything that's not inside of the ai's knowledge base and then finally what is\n3:11:11 the argument schema well this is exactly what I want to be passed in the simple search input which just contains a query\n3:11:18 okay now whenever this tool gets triggered and calls underlying run method what's going to happen is we\n3:11:25 expect to get that query of a string of what we need to go search now here's where things get interesting this is\n3:11:30 where we've kind of defined what this function should do so in our case we're going to use Tav and if you head over to\n3:11:36 their documents this is how you can connect your llms to the web and it just makes it super simple to go off and\n3:11:42 search the internet uh and then just I'm sure some of you guys are curious how much it cost it's free every month for\n3:11:48 1,000 calls and then it goes from up from there but no I've I've used them and I've really liked them so far so definitely recommend checking them out\n3:11:54 and to get started it's super simple you're going to basically just make sure you have access to this class you're\n3:12:00 going to set up an account so you get API key and then once you have an API key you can just start using their client and making request and that's\n3:12:07 exactly what we're doing in rcode we're importing that client that we would have installed I've already added to the uh\n3:12:12 Tomo file so it's already going to work for you guys now we're going to and then once you have have your environment\n3:12:17 variable set up and grab it you're just going to make sure you paste it in here and once you've done that you can now actually start using the client to make\n3:12:24 requests to the Internet so let's go ahead and we're going to run through the second tool real fast and then we're going to run it so you can see that like\n3:12:30 wow we are actually communicating with the internet and it's super cool all right and the final one this is just a super simple example I just wanted to\n3:12:36 show you guys like yeah you can create a tool to multiply numbers this one's super simple I just wanted to to add it in here for you guys but once you\n3:12:42 created your two tools what you need to do is you need to go off and basically in your tools list we're going to create\n3:12:48 two new tools so as you can see each one of these is a class so to create a new instance of the class we're just kind of\n3:12:54 come down here and just you know create them down here so once we have two instances of these classes we're going to do the normal thing with our agents\n3:13:01 set up the llm we want to use give it the prompt to guide its instructions set up our tool calling agent um this one is\n3:13:09 a little bit different from what we've done in the past in the past we've been doing the the react agent so the tool\n3:13:14 calling agent it just specializes in using tools so that's what it's kind of doing and then finally what we're going\n3:13:20 to do is once we have our agent set up we're going to use the agent executor to actually start uh handling our runs all\n3:13:26 right so let's go ahead and run our agent with these two different examples the first example we're going to be\n3:13:32 using is search for Apple intelligence and I'm going to take off these quotes just so we have to make our uh AI learn\n3:13:40 a little bit more about Apple intelligence and then finally we're just going to pass in a sentence and we\n3:13:46 expect this sentence to get converted over to numbers that you can see specifically floats that we can then\n3:13:52 start to use to to to multiply so that's what we I expect to see happen whenever we run this so let's go ahead and run it\n3:13:58 real fast so this is in the agent module and we are in the tools section and this is the final one let's go ahead and run\n3:14:05 it so we can see the search results and look at just the math so you can see out the gate what it did is it said all\n3:14:11 right I need to do a simple surge because that was the name of our tool let's go back up here a simple surge and\n3:14:17 we just want to what is the query of what we want to look up oh we want to look up Apple intelligence so it pulled out the key topic and then Tav our\n3:14:24 Search tool came back to us and said well here's your query here's the what you want to look up and then it gave us\n3:14:29 back the results so it went off and searched the internet in a way to where we get quick titles URLs so if we wanted\n3:14:37 to we can go off and actually explore some of these URLs and dive deeper but you can see it came back to us with some\n3:14:43 more information about each each of these things and today is is the middle of June so you can see this is grabbing\n3:14:49 all recent information about what's happening with the new Apple intelligence release okay cool and then\n3:14:54 once it gave us back the actual like this is the raw score and actually you can see it here here's some of the\n3:15:00 relevant information so just putting it in a nice format for us um that's what's\n3:15:05 happening with the tool and once it has that information um it's just going to spit out the final response for us so\n3:15:12 you can kind of see it all right here this is the final response and the outputs right here it just going to look\n3:15:17 exactly this is basically the output if you printed it out that's what you would see uh um that you you were to pass it\n3:15:22 out to your user okay and enough of that the tool went super simple the reason why I kind of like this example is\n3:15:28 because usually tools struggle with like if you just to run this as a regular tool decorator for whatever reason it\n3:15:34 kind of struggles to recognize like oh yeah that's a float so I just thought this was a cool example of like oh yeah we have to convert a string to a\n3:15:40 numerical representation so yeah and we get back to the correct answer and it's a float which is what we want okay all\n3:15:46 right so that's it for the tool example and this is actually it for the entire uh agent and Tool Deep dive just to like\n3:15:53 plant some seeds in your brains of like what else is possible crew AI is by far one of my favorite pack uh products out\n3:15:59 there when it comes to working with agents because they make it super cool for us to actually set up agents to work\n3:16:05 with other agents so right now we have one agent you know performing a task but what happens if you had like a writer\n3:16:11 agent collaborating with a researcher agent who each specialized like the uh if the if the researcher agent\n3:16:17 specialized in exploring the web and the writer uh specialized in writing articles that were had a good Rhythm\n3:16:23 used proper like languaging to write interesting articles well these guys could work together to generate an\n3:16:29 awesome report so that's just scratching the service of what cre I can do but I just wanted to like plant some seeds for\n3:16:35 like what's next after you guys learn like how to make an individual agent well multi-agents tools are the next and\n3:16:42 I definitely recommend creai and I have a ton of videos on crei inside of my YouTube channel so I definitely recommend and checking that out next but\n3:16:48 yeah that's it for the fifth module so let's go ahead and wrap things up hey guys so I hope you guys have enjoyed\n3:16:54 this Lang chain master class we covered a ton of information in all five of the different modules hopefully you guys are\n3:17:00 now Pros when it comes to working with chat models prompt templates chains Rag\n3:17:06 and your agents and tools and just as a recap all the source code in this video is completely for free there's a link\n3:17:11 down the description below while you're down there it would mean a lot to me if y'all can hit that like And subscribe\n3:17:17 especially if you've made it this far in in this video also there's that free school Community where you can meet like-minded AI developers and we have\n3:17:24 those weekly free coaching calls so you'll definitely want to take advantage of that and then outside of that I have\n3:17:29 a ton of other AI related content on my YouTube channel everything from Full stack AI tutorials all the way to like\n3:17:36 crew AI Deep dive so you're definitely going want check out one of those after this video but enough of that I hope you\n3:17:42 guys have a great day and I can't wait to see youall around in the next one see you\n0:00 hey guys welcome to this Lang chain master class for beginners in this video you're going to learn everything you\n0:06 need to know about Lang chain so that you can start your own AI development journey and by the end of this master\n0:12 class you're going to learn everything that you need to know about Lang chain so that you can go off and create your own rag chat Bots create your own agents\n0:19 and tools and use Lang chain to automate task and even though there's a ton of information in this tutorial I've done\n0:26 my best to make it as beginner friendly as possible you'll see as we go through this tutorial that I've structured\n0:31 everything to start out with the absolute Basics so that we can build up a strong foundation and from there we're\n0:37 going to add in more advanced features and complexities so that you can see what Lane chain is fully capable of and\n0:43 because I want you to get started building your own lane chain projects as fast as possible I've actually included over 20 different examples in this video\n0:50 so that you can just copy my code and start using it in your own projects and to make things even easier I have a link\n0:57 down the description below where you can download all the source code in this video completely for free and it would\n1:02 mean a lot to me if you could hit that like And subscribe button while you're down there too also if you get stuck at all during this tutorial you're in luck\n1:09 because I created a free school Community for AI developers just like you in the community you can go over there and ask questions get support join\n1:15 our weekly free coaching calls and we have over, 1500 different active members in the community so it's a great place\n1:22 for you to meet like-minded AI developers on your development Journey so I definitely recommend checking out it's completely for free and I have a\n1:29 link down the description below below so come over and join the party but enough of that let's go ahead and dive into the rest of the\n1:35 video all right so let's go ahead and cover the outline for this tutorial so that you can understand what you're\n1:40 getting into and so that you can get the most out of this tutorial so to start off the first thing we're going to do is\n1:45 set up the environment on your local computer so that you can run the over 20 different Lang chain examples that I've\n1:51 built for you guys from there we're going to start diving deep into each of the core components of Lang chain so to\n1:56 start off we're going to start working with chat models and this is basically how we're going to start interfacing with you know open AI chat gbt we're\n2:03 going to start working with CLA that's all going to happen in the chat model section and what we're going to do from there is we're going to start working\n2:08 with prompt templates next and this is how we're going to be able to format the inputs that we pass over to our chat\n2:14 models and this is really just helping us build up a good strong foundation so that eventually when we start trying to\n2:19 automate task using our chains which is my favorite part of this whole tutorial we're going to be able to really\n2:25 understand how we can start you know automating task by putting together chat models or prompts and other tasks and\n2:31 really run them all together to automate your workflows from there we're going to start working with rag now this is a\n2:36 huge section of this entire tutorial as you can see we have a ton of different examples in here and if you've heard of\n2:42 people chatting with their PDFs or documents this is what they were doing rag retrieval augmented generation so\n2:47 we're going to do a huge Deep dive into this one and then finally what we're going to do to wrap up this tutorial is we're going to do a deep dive into\n2:54 agents and tools and as you can see we're going to start off with the basics and then we're going to do a deep dive into agents which are are basically just\n3:00 you know chat models however they can make decisions on their own and act it's super cool how it works and then we're\n3:05 going to do a deep dive into tools and tools are how we're going to supercharge our agents to provide them more\n3:11 capabilities so as you can see we have a ton of information in this tutorial so let's go ahead and start diving into the\n3:17 first section which is going to be setting up your local environment so that you can run all the different examples inside of this code base oh\n3:23 real quick I want to mention if you want to get the most out of this video to learn linkchain as quickly as possible\n3:28 here's my recommendation first I recommend watching the video the whole way through on two times speed just so\n3:33 that you can understand all the highlevel Core Concepts of blank chain and then I recommend coming back through\n3:40 this video a second time and just skip to the part that you want to learn about for whatever project you're building for\n3:45 example if you're learning about rag I would definitely recommend just skipping down through the time sense below to\n3:50 watch rewatch the rag section and do a deep dive into the exact part that you want to learn about this is how I go\n3:56 about learning new Concepts so I just want to throw it out to you guys too so you can speed up your development journey and start gilding projects so\n4:02 all right enough of that let's actually go in and start setting up your environment all right guys so it's time\n4:08 for us to start working on setting up our local environment so that we can run all the different examples inside of\n4:13 this project now inside of the read me I have outlined all the different steps that you need to take in order to start\n4:19 running these and it's actually super straightforward so let me just quickly walk you through it to start off we need to install Python and we need to install\n4:25 poetry python I'm sure you know what that is poetry basically if you haven't heard of it before or it's a dependency\n4:30 management tool for python so as you can see we have something called a p project. tommo and this includes all of\n4:37 the different dependencies that we need to install in order to run all the different examples that I've set up for\n4:43 you and poetry makes this super simple to do so what you'll do go over to click this link right here and it will walk\n4:50 you through all the installation steps to install poetry on your local computer and once You' have installed poetry\n4:55 you'll be able to run commands like this so poetry to confirm that it's working and you can see yep poetry is working\n5:01 it's giving me back information and then from there what we can do is once you're inside of your code base and remember\n5:07 all the source code for this project is completely for free just click the link down the description below and you can download it but once you've done that\n5:13 what you can do is type in poetry install D- noout and what it'll do is\n5:18 it'll go through and install all the different dependencies that we saw back over in our py project. Tomo file over\n5:26 here it'll install it and what's awesome is then we can eventually start to run commands like this poetry shell and what\n5:33 this will do is it'll actually spin an interactive shell up that we can see right here it actually has the name of\n5:39 basically our project that we're working on you can see right here it has our name of our project that we're working on and we've created and we can actually\n5:45 start you know running Python and actually start calling all the different example code bases that we have all the\n5:51 different projects so you can see we can do something like python one. chat models and then we can start running you\n5:56 know all of our different code examples so if you got this far you are good to go go for the rest of the course when it comes to python now we just have a few\n6:03 more cleanup things that we need to do first off uh coming back to our read me is I mentioned that you need to update\n6:10 your environment variables so uh when you download the source code you will only see a EnV example file and this is\n6:18 where you're going to instore your environment variables eventually what we're going to do is you are going to\n6:24 rename this file to justv and what this will do is is it'll\n6:29 become your environment variable folder so that whenever you go to run your you know start using open Ai and some of\n6:36 your other different projects they require an open AI key or a Google key or you know all these keys and that's\n6:42 going to get stored to your environment variables I've already have mine set up and we'll be walking through it later but that's the second thing you need to\n6:48 do and actually in addition to that go over to open AI Google fir craw and\n6:53 actually start adding in those open ad keys but we'll talk more about that later on okay all right enough of talking about environment if you've\n7:00 gotten this far everything should be working and we can now move on to actually start playing with the code and Diving deep into L Jing so we're going\n7:06 to go over next and start working with chat models hey guys I meant to show you this quick tip that's going to make coding inside a visual studio code and\n7:13 cursor with python so much easier and solve a big headache that you'll probably have so if you head over to one\n7:18 of your python files you'll notice that you have a bunch of squigglies inside of your project and the reason why is cuz\n7:25 Visual Studio code is not properly hooked up to the new python environment that you just created with all the\n7:31 proper dependencies so here's how we fix that first you're just going to open up a terminal down here and what you'll do\n7:37 is you'll type in poetry shell like we did earlier and this will actually access and you can see yep we're in the\n7:42 right shell but the important part is is it gives us the location of where this basically all of our dependencies were\n7:49 installed so here's where the magic happens you'll come down here and click that you know interpreter path and\n7:54 you'll say enter interpreter path and then you'll just paste in the path you just copied\n7:59 and whenever you do that it'll actually get rid of all the squigglies because now Visual Studio code or cursor is\n8:05 hooked up to your development environment with python so now every time you add a new dependency with poetry ad or whatever you do it will\n8:12 actually you know sync up and you won't have all those random squiggly marks even though you have the Right Packages installed and just as a final note if\n8:18 you ever like well I want to not use this poetry shell environment anymore all you have to do is just type in the word exit and it'll put you back to your\n8:25 base environment so yeah that's a quick crash course on poetry and setting up visual studio code and after that let's dive back into the\n8:32 video all right so it's time for us to dive into our first core component of Lang chain which is going to be chat\n8:39 models now what the heck are chat models and what do they do well a chat model is\n8:44 basically Lang Chain's way of making it super easy for us developers to talk to all the different large language models\n8:51 out there like chat BT claw Gemini and a bunch more they abstract away all the complexity and allow us to basically\n8:58 have conversation ations with these models hints chat and chat models it's all about conversations so what is super\n9:05 nice about Lang chain if we go over to their documentation you can see they have a list of all the models that they\n9:12 currently support now it is very important to mention that for this whole tutorial we're going to be using version\n9:18 0.2 of Lang chain this is the most upto-date one and a lot of the features and version 0.1 will soon be deprecated\n9:26 uh whenever they upgrade you know probably the next you know five six months from now but enough of that let's dive back into chat models and talk\n9:32 about how we can use them so as you can see looking at chat models here's a huge list of all the different models that we\n9:39 have access to some of these models are better at other things and what's very interesting is chat models inside of\n9:45 Lang chain they provide different functionality such as tool calling outputs you know do they support\n9:51 outputting content in Json are they multimodal such as accepting images and audio so that's what you can see at a\n9:58 high level going over here and what's nice is if you want to work with any of these different models you can just\n10:03 click uh you know use poetry ad and you can type in the name of the package and you'll add it to the environment that\n10:09 you just set up a few seconds ago so let's go ahead and do a deeper dive and look at chat open AI because that's the\n10:15 one we're going to be using mostly inside of this tutorial so if we go over here to chat open aai you can do a\n10:21 deeper dive and look at some of the examples that they already have set up for you so you can see once again they\n10:26 recap what it's capable of doing and then they walk walk you through Yep this is how you can start setting up your\n10:33 files to start using this new package to start chatting with open Ai and eventually down here they dive into\n10:39 showing you y this is how you can start actually using it updating the models and so forth but of there documentation\n10:45 let's go ahead and actually head over to the example that I created for you guys where we're going to start at the absolute Basics and work our way up so\n10:51 you can see what these chat models are capable of so let's come over here start looking at the code and uh take it from\n10:57 here and before we dive into the code on this specific file I just want to give you a\n11:03 quick overview of what you can expect from each of these files so what I'll do in each one of the examples that we're\n11:08 going to run through I will try to provide documentation for you guys up top so you on your own can do a deeper\n11:14 dive into whatever concept we're just learning for example we just talked about chat models and I showed you a\n11:19 link and I also did open AI chat models and we walked through another link so in all the files wherever I point out\n11:24 something you'll be able to go ahead and click those links and a deeper dive on your own if you ever want to learn more about those topics but enough of that\n11:30 let's actually go ahead and start talking about chat models on a basic\n11:35 level all right guys so let's walk through the Three core steps that we need to take to start interacting with\n11:41 our chat models in this case our open AI chat model at a super basic level so the first thing that we're going to do is\n11:47 load our environment variables and if you remember from the beginning we set up aemv file which stored all of our\n11:53 keys to all of the different platforms we were going to access in this case we're trying to work with open AI so it's important that we have an open AI\n12:00 key feel free if you haven't set that up just head over to the open a website go create an account and you'll actually be\n12:06 able to access your open AI key and bring it back and copy and paste it over here now once you have done that what's\n12:12 nice is this load. EnV is going to add all those environment variables so that we can start accessing them in this file\n12:19 and you might be wondering like Brandon what the heck I don't see us accessing the open AI key anywhere well if we\n12:25 actually Peak under the hood inside of the chat model which actually gets imported from linkchain open Ai and\n12:31 that's actually if you head back over to those packages that we had set up earlier that's where it was stored but if you actually hit command on your\n12:37 keyboard if you're on Mac or control if you're on Windows and actually click on it you can actually start looking at the\n12:43 source code under the hood and you can see hey in order to start using this chat model you need to have this API key\n12:50 and we're actually going to start using it if you scroll down a little bit you can actually see under the hood it's\n12:56 actually automatically grabbing this environment key here and unless you pass it in manually and just to like give you\n13:01 guys a full tool you could actually manually pass in your API key here it's just not the safest manner because if\n13:07 you accidentally save your open a key to the public other people could actually grab your API key from GitHub and it's\n13:14 not the most secure so that's why you're going to store everything in your environment variables file okay enough of that let's keep chugging along to\n13:20 actually you know walk all the way through this example the next thing that we're going to do is once we have created our chat model which in this\n13:26 case we're going to say we're using jet gbt 4.0 but we easily could have done something like chat gbt 4 we could have\n13:32 done something like chat gbt for uh you know 3.5 turbo we could have easily changed things up but once we've created\n13:38 that model we can actually St uh start now interacting with it and using it and the key lesson here is we're going to\n13:45 interact with our models in pretty much everything in Lane chain using the do invoke property this is a function that\n13:52 really just whatever we're using it triggers its core functionality so in this case we're working with chat models so it's going to go trigger off like hey\n13:59 open AI or Claud start processing the request I give you but when we're using chains or rag or agents later on\n14:07 everything uses invoke so that's a very key thing to keep in mind as you're working with Lang chain all right so\n14:12 let's just keep walking through what's going on well in our case we're telling our model hey go perform this query for\n14:18 me and we're going to get back a result so let's actually dive into these results so we can see what's happening under the hood and the way we're going\n14:23 to do that is we're going to open up our terminal and let's close it out and give you guys some more space but once again\n14:29 we're going to use poetry shell and this is going to open up that interactive shell that we created earlier which uses\n14:35 you know our new Lang chain crash course environment what we can do is we can actually start calling this function so\n14:41 we're just going to call it Python and this is our chat model so one Tab and it'll go ahead and autocomplete for me\n14:47 and this is the first example so I'll hit one and tab again and it'll autocomplete now I can start running it\n14:52 so let's actually go ahead and press enter and start looking what's happening under the hood so here's what's actually\n14:59 super interesting so you can see we had two print statements one was for the full result and one was content only so\n15:04 for full results you can see that open AI under the hood gives us back a ton of information they give us back the\n15:11 content which is the exact answer we wanted 81 divid by 9 is 9 but then they give us this metadata which is like how\n15:17 many tokens did we use which you know basically why did we finish there's a ton of information which run number was\n15:23 this basically there's a ton of information that they give back to us 99% time you don't care about it however\n15:29 I just want to show you that it is accessible if you ever need to use it so most of the time when you're using you know these chat models the main thing\n15:36 that you want to grab is the content because the content is the example so whenever we grab the result and we\n15:42 access the content property we'll get back the exact string that we usually want to you know show to our users or\n15:48 pull out and pass over to the next prompt in our you know our chat model so that's under the hood how our first\n15:55 basic chat models are going to work so what I would like to do next is we're going to go ahead and move over to the\n16:00 next example where we're going to start actually showing you guys how to do a basic conversation using our chat models\n16:06 to where we can actually like you know pass in more information and actually having a full-on conversation let's go and start working on this example\n16:13 now all right guys so welcome to the second example where we're going to start focusing more on creating a\n16:18 conversation with our chat models now the three new important Concepts to know when going into this example are there\n16:25 are three different types of messages in our case that's going to be a system message and a system message just sets\n16:31 like the broad context for the conversation so these types of messages are usually something like hey you are a\n16:37 professional accountant or hey you are a professional python software engineer help me write this code so that's like\n16:43 just the broad what's going on inside the conversation just the context and then from there there's two different\n16:48 types of messages there are human messages which is us talking to the AI and then there's AI messages which are\n16:54 the AI responding back to us so those are the three types of messages that you can have in a conversation with the AI\n17:01 okay cool now let's dive into the rest of this code so you can kind of see what's happening and this is just once again we're building on our foundation\n17:07 so we're just copying a lot of the code from the previous example and now we're going to start building on top in this case we're going to start creating a\n17:14 conversation so in our case you can see a conversation is nothing more than we have our messages list and our list is\n17:21 just going to store a combination and series of messages in our case we're just GNA um start off with a system\n17:27 message and it's important system messages best practice and I think it's actually enforced a system message must\n17:32 come first and then from there you can alternate human AI human AI but system always comes first because remember this\n17:39 is the context for the conversation as a whole so in our case we're going to say hey solve the math problem and then from\n17:45 there we can pass in a human message and what we would expect to get back is obviously an AI message so let's\n17:51 actually kind of see how we can actually trigger this conversation so we can get a result so in this case you'll see that\n17:57 we have a once again again we have our model our chat gbt model and we're going to call that important function that we\n18:02 talked about in the last one which is invoke but this time we're not going to pass in a you know a hardcoded string\n18:09 like we did over here where you can see we passed in a hardcoded string this time we're actually going to pass in our entire message history and it'll\n18:16 actually read through the entire conversation and then spit out a result if you worked with cat GPT it's exactly like that if you've ever typed into like\n18:22 you know the chbt website it's just like that okay cool so what we're going to do is we're going to go ahead I'm going to comment this out we're going to run the\n18:29 code so we can actually see what answer we get so this time we're going to do python this is you know still working\n18:34 with chat models and this is example two so you can actually see whenever we get an example back with working with chbt\n18:40 it says you know 81 divided by 9 is 9 so you know that's exactly what we would expect to see okay cool but what's nice\n18:46 is we can continue this example and actually have a full-blown conversation so I'll show you what that looks like\n18:52 now so you can see in this second part we actually have continued on the conversation by adding in AI responses\n18:58 and then messages so this case we've added now like the response from the previous one and now we're going to you\n19:04 know just continue adding human messages so I'm just going to run this so you can actually see what's going to happen and this time it's going to come back and\n19:10 give us you know you know 10times 5 so this is completely basic however here's why chat conversations are super\n19:16 important in the real world when you're building your conversations you know it's very common for you to provide an\n19:22 example like hey chat gbt or AI model build me an email it returns a response\n19:28 and then you provide provide feedback cuz remember this is all about conversations and what's going to happen is as you provide those you know\n19:34 feedback of like hey no make it less formal make it a bulleted list you know as you provide that feedback what's nice\n19:39 is going forward in your conversation you can say okay great now do exactly what you did for that last email but now\n19:45 do it for this email and you know storing this message history like you have right here is how you're going to\n19:50 be able to basically make your chat models have awareness in context of what's good and what's bad and what's wrong so this is a very powerful tool\n19:57 and you actually use this a lot more whenever you know you're working on bigger and larger projects but okay cool\n20:03 well now that we have that under the hood and we actually understand like just like basic conversations let's keep it going and actually start working on\n20:10 actually exploring other alternatives for different chat models that we can use because right now we've been focusing on only open AI but let's look\n20:16 at a few different examples of how we can use Lang chain chat models but with different you know llms so let's go\n20:22 ahead and start working on that now all right guys so welcome to this third example where we're going to start\n20:28 exploring different Alternatives of working with other models outside of open AI so in this case we're going to\n20:35 explore Google's Gemini models we're going to look at anthropic or CLA and look at Open Eye because I just want to\n20:42 show you guys how easy Lang chain makes it to work with these different models so let's scroll down so we can look and\n20:48 compare and contrast all the new code so what you can see up top this is exactly\n20:53 what we've been doing so far in all our examples we create our chat model once we have the model we we go off and\n20:59 invoke it and then we get back some sort of result well Lang chain abstracts all the complexity away and makes it super\n21:05 easy to do the same thing with our anthropic models and our Google Gemini models all we do is we instead of using\n21:12 the chat open AI model like we've been doing in the past we now just use chat anthropic and then we can pass in\n21:18 whatever specific model within Cloud that we want to use just like we did with chat gbt up here and then we'll do\n21:24 the normal part where we just you know go off and invoke it with our messages and get back a result and is the exact\n21:29 same thing for Google's giz models down here at the bottom so link chain makes it super easy to work with these\n21:35 different models and this is very important as you build larger projects because certain models are much better\n21:40 at you know at performing certain tasks some are cheaper some are faster so for different situations you need to use\n21:45 different models Lang chain makes it super easy to do and if you want to explore all the different models that\n21:50 each you know anthropic provides and Google provides I have links for you guys and just as a important reminder if\n21:56 you want to go off and explore all all the different chat models back over here in our first example back when we were\n22:02 working with a chat model documentation you could start searching through here so you can explore all the different models and all the different\n22:09 functionalities that these different models provide but okay cool enough of that let's go ahead and start exploring our next example where we're going to be\n22:16 diving in and actually having a conversation with our user through the terminal so let's go and start working on this example\n22:22 now all right guys welcome to the fourth chat model example where we're going to start actually having a realtime\n22:27 conversation with with our AI models this is going to feel just like the chat gbt website except it's running locally\n22:33 on our computer so let's walk through how we're going to set this up so the important part per usual we're going to create our chat model by loading all our\n22:39 environment variables and creating an instance of it now here's where all the interesting part happens because we're having a conversation we're going to\n22:46 create a chat history list and this is going to store all our messages so as we ask questions we're going to add\n22:52 messages to the chat history as the AI responds we're going to add those messages to the chat history and we're just going to continually keep add add\n22:58 in all of our messages to this list so here's how that works in this code example so first off the first message\n23:04 we're going to add is our system message because if you remember system messages are just general context for the\n23:09 conversation that's about to happen so we're going to create that system message and then we're going to add it\n23:15 to our chat history by calling append on the chat history list cool so now here's\n23:20 where all the core logic happens so this is where our chat Loop happens and basically here's the core Loop we first\n23:27 ask our users hey what is the question or prompt that you have for the chat model we're going to get back a query so\n23:33 this is whatever the user passed to us if the user gave us the keyword exit we're just going to stop the\n23:38 conversation right then and there and we're just going to print out the whole chat history however if they didn't give us the keyword exit we're just going to\n23:44 have a full-on conversation so what we're going to do is we're going to add the person's query as a human message\n23:50 and we're going to add it to our chat history then what we're going to do is pass over that entire chat history list\n23:56 over to our model like you can see right here and then we're going to invoke it so that model is going to then read all\n24:02 the messages in our chat history get a response and then we're going to print back that response to our user and we're\n24:09 also going to most importantly track that response by once again updating our chat history so we're going to update\n24:15 our chat history with an AI message cuz you know this is the AI response so enough talk let's go ahead and look at a\n24:21 code example so you guys can see this in action so I'm going to once again open up our terminal and I'm just going to\n24:26 call Python and then this is our chat model project and this is the fourth example so now we can actually start\n24:32 running it and per usual it's now going to ask for query so I'm just going to ask it who are you so then what we'll do\n24:39 is we'll pass that question over to open AI open AI will generate a response and it actually prints it back to us so this\n24:45 is exactly you know if we're talking to chat gbt on their website this is exactly what it feels like and we can\n24:50 actually ask questions about it because as a conversation we can refer to previous messages can you expand on that\n24:57 and what it'll do is it'll you know cuz it can refer back to the previous messages and actually provide additional\n25:02 context so this is super powerful and you'll definitely be using this a lot more and finally we can pass in the word\n25:08 exit it'll quit the conversation and what's cool is you can see our entire message history here so you could save\n25:14 this off somewhere you could you know save it locally save it to the cloud do whatever you want so that whenever the user comes back in the future you can\n25:20 continue the conversation and we'll actually do a much more deeper example later on where we actually save this to\n25:25 the cloud so you'll learn about that in just a little bit all right cool enough of that let's go ahead and start working on our fifth example which is exactly\n25:32 what I was talking about where we're going to start saving our messages to the cloud so let's go ahead and start working on this where we're going to save all of our messages over to\n25:38 Firebase I think you're really going to love this one all right let's go ahead all right guys welcome to the\n25:44 fifth chat model example so this one is by far my favorite because you're going to learn how to save all the messages\n25:49 you type locally over to the cloud and this case we're going to be saving everything to Firebase so what I've done\n25:55 is I've actually typed out all the installation in steps for you guys because there's there's you know a little bit more background setup because\n26:02 we're working with the cloud but once we knock out all the development setup in the cloud what we'll do is just like run through the code so you can see how\n26:08 everything works but I think you'll see that like all in all this is actually pretty straightforward the hard part is\n26:13 just setting everything up in the cloud so let's go ahead and do that now and the first thing I do want to mention about the cloud is this is actually\n26:18 based on this Google fire store codebase example so you can see like this is exactly what we're doing however uh I\n26:25 feel like they didn't do the best job walking through how to get it set up and I struggled a ton the first time I did this so I just want to share all the\n26:31 lessons learned that I have done and and I've copied all my lessons learned here okay so the first thing that you need to\n26:37 do is create a Firebase account so just head over to you know console firebase.com and you're going to create\n26:44 an account and once you've created an account you're going to then go over here and create a project to so I'm not going to like walk through creating a\n26:50 project cuz it's it's super simple but just come over here click the drop- down menu and click create a project so\n26:55 that's how you create a project once you've done that you'll then need to create when you go over to the build tab\n27:01 there's something called the firestore database and this is where we're going to be storing all of our messages inside of our chat history except our chat\n27:08 history is now in the cloud so whenever you click fir store database if it's the first time you're using the project you're going to want to turn this on so\n27:15 there'll just be a turn on button here for you and you'll want to click that and once you've done that and you run the code eventually all of your messages\n27:21 and user sessions will get saved up to the cloud and more on this later but I just want to go ahead and like paint a picture of where we're going fantastic\n27:27 so once you've done that let's heading back to our examples you'll need to start copying some information about your Firebase project so what I mean by\n27:34 that is you need to start copying information such as the project ID so in our case what you'll do come back over\n27:40 here you're going to click the gear icon you'll hit project settings and this is where you'll find your project ID\n27:46 project number and everything that's you know related to what you're what you're doing with the project you just created\n27:51 over here in Firebase Okay cool so that's the easy part now this is where things get a little bit more complicated\n27:57 because we're trying to work with the cloud so Google cloud and we're trying to have our local computer communicate\n28:04 with the cloud so this is where we're going to have to go a little bit deeper so the first thing we're going to have to do is install the Google Cloud\n28:10 command line interface on our computer because what we're trying to do is authenticate our local computer to make\n28:17 requests to the back end that's all we're trying to do so what you'll do is first thing you'll click this link over\n28:22 here and it will take you to this and this will basically just walk you through exactly what you need need to\n28:28 type to install the Google Cloud CLI it's super straightforward you'll just click download once you've downloaded it\n28:34 literally walks you through step by-step how to install it so that's super super easy then once you've gone through and\n28:40 installed everything and initialized it the final part that we're going to do is you need to authenticate your local\n28:46 Google Cloud CLI to your account so once again click this link it'll take you over here and it will just walk you\n28:52 through how you can actually authenticate your local computer to Google Cloud cuz we're just trying to create some default credal\n28:58 that way whenever our codee's running it just can you know seamlessly make a call to the back end and the main things that\n29:04 you need to do first off just run Google Cloud off application default login a\n29:09 signin like normal Google Cloud signin screen will pop up you'll just log in it'll make a service account for you\n29:14 life fantastic and then you'll just run you'll run this basically this script as well to finally get everything working\n29:20 so I hope that's enough I don't want to go too deep just because I want to focus more on Lang chain but run through those\n29:25 steps you guys will be good to go and then you'll find finally have everything working inside your code so all right\n29:31 enough of that here's how we're going to get things working over here so you're now going to come in here uh CU you have your project ID that you just copied\n29:37 you're then going to have a session ID so we're just going to make a new one so we're going to call this user session\n29:43 new on this is where all of our messages are going to be saved and then finally there's a thing called a collection name\n29:48 this is heading back over to Firebase just the way our fir store saves data\n29:53 it's in a collection document basically a database type so you can see collection document collection so it\n30:00 just keeps alternating so we're going to store all of our messages in the chat history collection and each session is\n30:07 going to be a document and that document is just going to contain a list of messages so that's what's about to happen so what we need to do first off\n30:14 is we need to initialize our firestore client and what that's going to do is allow us with our client to go off and\n30:21 make requests so that's why we're doing it and then what we're going to do next is have something called firestore chat\n30:27 message history this is basically we'll just click in here so you can see what's going on but basically what we're trying to do is in the past you and I were\n30:34 storing all of our messages in a list well Lang chain has provided a bunch of different options for us to store all of\n30:41 the different ways that we can have and store our messages so in this case we're using fir store to save our messages\n30:48 there's a few different examples inside of Lang chain where we can actually save our messages using I think the like file\n30:53 message history class where we can actually save all the messages locally on our computer so there's a bunch of different options that I want you guys\n31:00 to be aware of and feel free to explore through the L chain documentation to see other options but just know at its core\n31:06 all it's doing is just saving a list of messages you know human messages AI messages back and forth except now they're just being saved off to\n31:12 somewhere else so that's what's happening under the hood but let's keep going so in our case now that we have\n31:17 basically we've created our new chat history except our new chat history is up in the cloud so what we can do now is\n31:22 actually start running through the exact same Loop that we did last time which is where the human's going to ask a question question and now what we're\n31:29 going to do is just keep adding with our chat history we're just going to add user messages or we're going to add AI messages and we're just going to go back\n31:34 and forth so enough talking let's actually dive into the demo because this is super cool in my opinion so we're going to go step by step because there's\n31:40 a lot of cool ways to show this off so the first thing that we're going to do let's clear this out and we're just going to run python so we're in the chat\n31:47 models example where this is the fifth example now what this will do is it'll start you know initializing firestore\n31:52 client it'll start the chat message history and usually this takes a few seconds to get started and then now we\n31:58 can actually start chatting so it's cool though uh and you'll see this get populated in second our current chat history well because this is the first\n32:04 time I created that new session ID there's no history of this chat in the cloud you know there's no user session d\n32:11 new so there's no messages to load but now I can actually start talking about it so I can say who is Sam Alman so\n32:20 it'll go off and answer questions and you know so here's here's who Sam is but what's super cool is if now if I come\n32:26 back over to the class we can actually see our messages being stored in real time you'll notice they're stored as\n32:31 bite strings and they're not actual just like plain text messages that's just totally fine that's how Firebase and fire store are storing messages but\n32:38 let's keep going does he have a brother so we can keep asking and then\n32:44 it'll say yes he has a brother here's his name and then what we can do is exit it so we'll exit Okay fantastic all of\n32:51 our messages are cleared you can see over over here we actually still have additional messages but what's nice is\n32:56 whenever we restart the same file with the same you know chat session it'll\n33:02 actually say the current chat history and it'll pull all of the different you know messages that we have previously\n33:08 sent to the AI so this is a super cool way and I hope you guys are like this is awesome this is a super cool way for us\n33:14 to continue conversations at a later date without having to completely refresh from start so I hope you guys\n33:20 thought this was awesome because we can still you know do additional stuff like who was I just talking about let me fix\n33:26 this oh sorry wrong button I can say who we just talking about and then because\n33:32 this is a chat history it can refer to the previous messages so yeah this is awesome so I hope you guys are pumped\n33:37 and that concludes the first module where we're just run through all the different capabilities with working with\n33:43 chat models and from here we're going to move on to prompt templates next so I do just want to point out before we keep going if you have any questions\n33:49 definitely hit that link down the description below for school go over there and ask any questions you have or hop on our weekly coaching calls would\n33:55 love to help you guys out but let's keep chugging along and start moving over to prompt\n34:00 templates all right guys welcome to the second module in this Lang chain crash course this whole module is completely\n34:06 dedicated to prompt templates and I think you'll find this is a super simple concept but it's going to make our lives\n34:12 much easier as developers as we work with link chain so what the heck is a prompt template why does it matter how\n34:17 does it work with chat models those are the main things we're going to be tackling in this module so the first thing is prompt templates the best way\n34:23 to think of them is exactly what the name says we are building up a prompt that's our whole goal we're trying to\n34:29 create a prompt but we've created some sort of template like this and it's up to us to pass in values into that\n34:36 template so you know kind of think of like Fillin the blank is the best way to describe it so you know you could do\n34:41 generate three jokes about dogs or something like that is kind of what we're what we're shooting for here so\n34:47 enough like high level let's actually dive into like what this actually looks like in action as like a quick overview\n34:54 and then after this we'll dive into the code okay so like I said our whole goal is we're striving to generate a prompt\n35:00 but we're going to be passing in some variables to make this work so you know we're going to be taking like a user\n35:05 response to help us really fill out this prompt and this is important because eventually these prompts are going to\n35:11 get passed over to our chat models to you know to to perform some action so in our case like this is a joke template So\n35:17 eventually this template once it gets populated it's going to get passed over to you know chbt or Claude one of those\n35:23 models and it's actually going to spit out some jokes for us so that's just this prompt is going to help us structure The Prompt that we pass over\n35:29 so it's super helpful we'll dive into some more examples here in just a second but this is exactly how it works it's up to you as a you know whenever you're\n35:35 creating these prompts to basically create a string and inside that string it's up to you to use these curly\n35:41 brackets to create variables these variables are later going to be populated with these values and what's\n35:48 important to notice here is once we've created our prompt template we're going to Define an input dictionary and this\n35:54 input dictionary is going to Define for each one of these variables the values that are going to replace them because\n35:59 eventually whenever we have our template pass in these inputs and call invoke because remember invoke is the super\n36:06 magic word for all of Lang chain what it's going to do is it's going to basically do some string interpolation and replace all these values so that we\n36:13 end up with an output that looks just like this obviously this was a super simple example but let's just think like\n36:18 higher level like for a real world setting you could eventually be creating some sort of tool that helps developer\n36:24 automatically debug their code well in your case you would create a prompt template that's like here is the user's\n36:30 code and then you know you would have user's code here's the error they're getting that would be the next variable\n36:37 please help them debug this and then basically you would pass in that entire prompt filled with the user's code and\n36:42 error over to open Ai and then open AI would tell you oh it looks like here's a bug here's how it fix it so this is just\n36:48 a really nice way for us to create structured prompts that make it easy you know where we can do some prompt engineering and easily add in our users\n36:55 input to The Prompt templates that we're so uh enough jargon let's actually dive into an example and start walking\n37:01 through our first example working with prompt templates and I think this will make a ton of sense and it'll be super easy to\n37:08 understand all right guys so let's go ahead and dive into the code and this one's going to be a super simple Code\n37:13 walkthrough there's three parts to this and then I have one little extra part so you guys can see you know when not to\n37:19 use prompt templates but let's walk through this part by part just so you understand everything at a basic level and then we'll work our way up from\n37:24 there okay so if you remember our whole goal is we're trying to create prompts that we could eventually pass over to\n37:30 our chat models so we're focusing in on creating prompt templates that's all we're trying to do so let's go part by\n37:36 part in this code and what we're going to be trying to do is trying to relate it back to this outline that we have over here so the first thing we need to\n37:41 do is create a prompt template that stores our variables of things we want to replace so that's exactly what we're\n37:47 doing right here in this code we're creating a template which is just a string and what we're going to do is\n37:52 we're going to call chat prompt template and like well what the heck is this well if you remember at high level we're\n37:58 eventually trying to our end goal is we just talked about chat models that's all we were just talking about in module one\n38:04 well what we're trying to do is create prompts that we could eventually pass over to those chat models so that you\n38:09 know AI can basically answer whatever questions or ask that we're putting together in our prompts so that's our\n38:14 end goal so what this chat prompt template does what it's trying to do is it converts our string into a template\n38:20 that makes it easy to work with to actually replace you know basically replace all these variables with these\n38:26 values so basically what's happening under the hood so once we create this prompt template so once we have our\n38:31 template what we're trying to do is we want to pass in an input dictionary that has all of our keys and values so that's\n38:39 basically exactly what was happening right here and what we're trying to do is once again we're calling that magical\n38:44 word invoke so in this case what's going to happen under the hood is we have this prompt template we're going to call\n38:49 invoke with these specific values and all it's going to do is it's going to go through each variable and replace it\n38:55 with the appropriate value so let's go ahead and run this code so you can see an action so in our case we're going to\n39:00 do python now we're in prompt templates and we're going to run the first example so two one and what this will do is now\n39:07 when I run it what you'll notice is it actually doesn't spit out a string it actually spits out a human message so\n39:13 tell me a joke about cats so that's pretty cool it actually filled in the values that we told it to and if you\n39:19 remember what it did is it spit out basically a messages array that we could eventually pass over to our chat models\n39:25 cuz that's where we're trying to go so that was super simple example one of the other things I want to show you in this is we can actually instead of just\n39:31 replacing one value let's go ahead and look at part two what you'll notice in this one is we can actually replace\n39:37 multiple values in a prompt template so this is the exact same exact same thing\n39:42 except this time what we're doing is replacing multiple values so let's go ahead and yeah we're still calling the same chat prompt template from template\n39:49 so let's go ahead and run the same example again just so you can see it in action so yeah running with multiple placeholders this time tell me a funny\n39:56 story about a panda that's exactly what happens okay enough of the simple examples let's actually get on to some\n40:01 more of the cooler parts which is part three so if you can uh if you've seen so far every time it's created something it\n40:08 has basically been from a human message point of view but sometimes you actually want to do a little bit more you want to\n40:14 have a little bit more control so what you'll notice in part three is we can actually start defining basically we can\n40:22 specify the types of messages we want to create and not only what type of message we want to create we can actually still\n40:28 you know replace all the variables in here and what you might notice and which I thought was weird the first time I used it I would have expected that you\n40:34 know what the heck is this like Tuple where the first part's a simple uh you know the system and then the message\n40:41 then we have another human then message like why aren't we just using like human messages and system messages well it all\n40:47 comes back to the way the chat prop template works is it just expects this tupal format where you define the\n40:53 message type first and then the content so just know if you want to basically use prompt templates and also Define the\n40:59 message type you have to go with this Tuple approach but yeah let's go ahead and see an action so you can see exactly what it looks like so we're running part\n41:06 three so we're just going to run the code again so you can see now we have a list of messages and the first message\n41:12 in this array is going to be a system message and you know you're a comedian who tells jokes about lawyers so that's\n41:17 setting the context and then you can say tell me three jokes so we actually just replace the variables there too so yeah\n41:23 so once again we've created these messages and we could actually pass this message list over to our chat models and\n41:28 it would spit out those jokes about lawyers okay now I just want to show you this is just a lesson learn that I thought was pretty interesting so if we\n41:35 come down here to the final part what you will notice I'm just going to comment this out so you can see it so\n41:41 this code does work so you know how we just talked about tupal well anywhere\n41:46 you want to do string interpolation you know where we're replacing values you have to use the tupal so you'll notice\n41:52 we want to replace Topic in this system's tupal so we have to put it in a tupal and then well for the human\n41:58 message we're not doing any interpolation there so we're just going to leave it how it is so this example would 100% work let's clear it out and\n42:05 I'm just going to show you it works so this one works yep it looks just like we talked about in the first example well\n42:11 if you come down here to the final example that I have in this file you'll notice I say this does not work and\n42:17 basically what we're doing here is we're trying to do interpolation on a human message so let me just show you what\n42:22 happens when you try and like if you don't use a tupal you'll basically get uh you'll basically notice here tell me\n42:28 a joke and it never did the replacing part I ran into a lot of issues when I first started using Lane chain and doing\n42:34 prompt template Replacements and I was like why isn't it replacing it and it's like oh it has to be in this tupal\n42:39 format okay I hopefully um that's enough prompt templates like Basics let's actually start moving on to the next\n42:45 part where we're going to start using actually start using these prompt templates with a chat model in our second example so let's go ahead and\n42:50 start working on that now all right so welcome to the second prompt template example this one we're\n42:57 actually going to tie together everything for module one which is creating our chat models and everything we just learned about prompt templates\n43:03 so let's go ahead and merge both these together in the example so you'll actually really understand the whole value of creating these different prompt\n43:09 templates okay so per usual like we did in all of module one we're going to load our environment variables and we're\n43:15 going to go ahead and create a chat model that's using open AI in this case now let's go part by part so you can\n43:20 actually see the exact same basically all the prompt templates that we created in demo part one we're actually now just\n43:26 going to like not only create those prompt templates but we're actually going to pass over those prompts to our\n43:32 model so let's look at example one in depth and then we'll just run it you know for everything else so per usual uh\n43:38 we're going to create our template with the variable we want to replace once we have that string template we're going to call chat prompt template and we're\n43:44 going to make an actual template from that string just so it's easy for Lang chain to actually start manipulating it\n43:51 and from there we're going to do exactly what we did last time which is call invoke cuz that's the magic word for our\n43:56 prompt templates to take in that string and actually start replacing all the values so once we call invoke we're\n44:02 going to get back that prompt and what's cool is we can now pass that prompt which is if you remember from the last\n44:08 one it's now just going to be a message array that has all these values in here so you know it'll say tummy joke about\n44:14 cat and that'll be one of the messages one of a messages in our message history that we're going to pass to our model I\n44:20 hope that makes sense um you whenever I run it it'll actually make more sense so um but what you can see is we're now going to pass that prompt over to our\n44:27 model and we're going to tell our model invoke because we want our model to actually like perform actions on these\n44:33 messages and we're going to print the result so you'll start to see you know invoke is going to get called a lot as\n44:39 you start to work with Lane chain more and more and I'm not going to go too deep in part two and part three because we just copied and pasted over all the\n44:45 examples from the last time and now we're just actually going to see let AI actually run basically run the request\n44:50 which is creating jokes about all these different prompts we've created so what we're going to do call python example two and this is the second example for\n44:57 prompt templates with a chat model let's go ahead and run that and this will take a few different examples seconds but you\n45:03 can see so prompt template prompt from a single template which was right here tell me a joke about cats so you know\n45:09 sure here's a joke about cats for you why was the cat sitting on the computer because I wanted to keep an eye on the\n45:15 mouse so obviously they're pretty corny and same with multiple placeholders so it actually told a short story about you\n45:21 know a short funny story about pandas so here's our funny short story and then\n45:27 finally when it comes to the end where we're doing you know prompts with system and human messages what we're doing is\n45:33 creating three jokes about lawyers and that's exactly what it did here are your three lawyer jokes so why don't sh\n45:38 sharks attack lawyers because of professional courtesy so once again pretty corny but um yeah so I wanted you guys to see prompts with prompt\n45:45 templates because right now we're calling invoke to get the prompt we're calling invoke again to get the result\n45:51 which leads us perfectly to chains because this is exactly what we're about to do next where we're actually like\n45:57 technically we're chaining prompts with chat models so this will make more sense so we set ourselves up perfectly to get\n46:02 to dive into section three which is all about change so let's go ahead and start diving into that model now and this will\n46:07 all make sense and it'll actually be a lot simpler so you'll see what I mean in just a\n46:13 second all right guys so welcome to the third module in this Lane chain crash course this chain section is by far my\n46:19 favorite part of the whole tutorial and I hope you guys love it just as much as I do so let's walk through what the heck is a chain why is it important and then\n46:27 just go through a few different examples real quick and then we'll hop over to the code okay so at a high level what the heck is a chain well inside a lane\n46:33 chain it's nothing more than tying together a series of task so if you\n46:38 remember so far we've been using invoke the invoke function a lot so we've been using it to create prompts we've been\n46:45 using it to basically have chat models respond to us so what what we could do instead is have the prompt be chained\n46:52 like the output of a prompt whenever we invoke it that output be fed to the model and then that model whenever we\n46:58 call it invoke again it will spit its outputs and it could just go to the next item so as you can see we're kind of just chaining together different task\n47:05 and we're passing the input from the first one over to the next one and we just keep going down and down and down\n47:10 so that's exactly what's happening right here have a prompt invoke it spit the outputs over to the next thing Tak in\n47:17 those inputs process it with invoke and then spit it out to the next thing so that's exactly what's happening with chains and I think you'll think when you\n47:23 see the code it's super simple and there's another common thing when you're using using chains inside of Lang chain\n47:28 and it's called Lang chain expression language it's a fancy way to say and describe like how you can create chains\n47:35 cuz you can go the hardcore code way which we're not going to get into we're going to stick to this which is just anytime you want to chain items together\n47:42 in L chain you're going to use the pipe operator so this is like right above your return key on your keyboard it's\n47:48 just the straight up and down basically pipe key so that's how you're going to create a chain you're just going to put\n47:53 you know a prompt a chat model and there's a few other things you could put in here too but this is how you're going to do it item or a task a pipe operator\n48:00 another task a pipe operator another task super easy to set up and then whenever you want to run the chain all\n48:06 you do is you call the chain and you call invoke on it and then you pass in your input dictionary of all your\n48:11 different keys and values this actually should be a quick uh my bad this should just be a quick dictionary just like\n48:17 this and that's how it works so this initial key dictionary gets passed in over here as the first into the first\n48:23 prompt then it gets passed along all the way through so that's just at a high level just a simple chain now let's just\n48:28 I just want to show you the realm of the possibilities when it comes to Lang chain first and then we'll hop into the code okay so when it comes to chain\n48:34 possibilities the first thing to know is like you don't have to do just prompt chat model you can actually keep going\n48:40 so you can just continually keep going as long as you want that's option one the other thing that's super cool is you\n48:45 can run task in parallel so you can have task you know just like oh kick off kick\n48:50 off and you can just start running your chains in parallel and then you can actually even at the end of them you can actually join them together and have\n48:57 your final results all come back and be fed into one so this is a cool way just to like if you need to do some parallel\n49:03 processing this is a cool way to do that the final thing is branching so branching is a way inside of your chains\n49:10 to have it to go like let's you know at this node let's kick off some actions cool well based on the results of those\n49:16 actions let's just say like oh it's um if we're doing a review of a movie well if the movie was good cool we're going\n49:23 to go down this branch and this chain path if the movie was awful we're going to go down this path and if it was so so\n49:28 we're going to go down path C so that's just kind of like the realm of the possibility when it comes to chains but enough like theoretical highle stuff\n49:35 let's actually dive into the code and start working on the simple model first and then we'll work our way up through all the different possibilities so let's\n49:41 go ahead and dive into the code all right everybody Welcome to the first code example when it comes to\n49:48 chains I think once you see how this all works you're going to be like oh my gosh this is so much easier than what we were doing beforehand and what's cool is\n49:55 we're going to use basically the same examples that we did from beforehand but just reformat them so they work with chains so let's go ahead and dive\n50:01 through this and walk through it step by step so the first thing that you notice we're in the chain Basics file what we're going to do is exactly what we've\n50:08 been doing so far with everything else we're going to create our chat model once we create our chap model we're\n50:13 going to create our prompt template our prompt template is going to contain our variables that we want to replace in\n50:19 this case it's going to be the topic and the joke count from there what we're going to do is create our chain now what\n50:25 you'll notice this is very similar to to the drawings I just created a few seconds ago the first thing we're going\n50:30 to do is have our prompt template which is going to contain all the messages that we want to basically replace with\n50:35 our inputs later on create our prompt from there we're going to pass that prompt to our model once our model then\n50:42 has basically is able to process that prompt what we're going to do eventually is spit everything out to a string\n50:48 parser at first I'm going to get rid of this string parser just so you can see why we need it but just know at a high\n50:53 level all it does is it takes you know how so far every time we get the result we do do content well that's exactly\n50:59 what the string parser does for us it just goes ahead and grab the content but we'll we'll add it back in in a little bit so comment it out just so you guys\n51:06 can see it how it works okay cool so what I'm going to do is go ahead and run this chain just so you guys can see how\n51:11 easy it works and how simple it is how much less code we're doing in this approach versus what we were doing last time so what I'm going to do ahead is go\n51:18 ahead and open up our terminal and then if you remember now we're in the third module cuz we're working with chains and\n51:23 this is the basic example so we're just going to run python three so we can go open our chain file and this is the\n51:29 first example and what I would expect to see is once we print the result we're going to get that long response that we\n51:35 normally get whenever we don't grab the content so you can see yep here are whenever we look at the content you can\n51:41 see yep here's the three lawyer jokes and then we're seeing all that extra information that we get back from our LM\n51:46 like tokens and so forth but if we undo our changes earlier and just use the\n51:51 string parser that you can see right here where we're just doing the string output parser this also comes from we'll\n51:57 talk about this more but the output parser Library that's where it's coming from but if we rerun it now what you'll\n52:02 notice is it actually just spits out the final text like this is what we would actually expect to return to our\n52:08 customers if this was a web app or you know this is what we would like most likely pass to the next model if we were\n52:14 doing this you know and continuing the chain so this is awesome I hope you guys see how much easier this is to use\n52:20 compared to what we were doing last time i' actually let's just do a side by side so you guys can kind of see exactly what\n52:25 this looks like so over here what we were doing yeah at the bottom we had our you know we\n52:31 created our prompt templates then we did our prompt then we invoked it then after we were done invoking it we saved the\n52:37 result then passed it to our model then did it again and again so all of this code right here basically uh just gets\n52:44 replaced with this single one line which is beautiful so I hope you guys like this so now that we understand the\n52:49 basics what I want to do is we're going to dive deep for a second in the next example where we're going to understand\n52:55 how chains work under the hood and after we do that we're going to start building off those examples you saw earlier where we're going to do you\n53:01 know pillow chains and then branching and much more so let's go ahead and dive into the next\n53:07 example all right so welcome to the second example in the chains module and\n53:12 in this example what we're going to do is a deep dive into how chains work under the hood now this isn't super\n53:18 important for you to like memorize or understand exactly how everything works under the hood it's just I want you to\n53:23 be aware of a few Concepts such as runnable sequences runnables and runnable lambdas those are the main\n53:29 three things that we're going to talk about in this section okay so let's dive into the code and we'll look around so\n53:34 per usual what we're doing is we're creating our models and loading our environment variables creating our prompts and then most of the new action\n53:42 when it comes to creating basically chains the manual way happens right here\n53:47 so what we're going to do is talk about this at a high level and then we're going to dive into each part so at a\n53:52 high level what we're doing is we are creating runnable lambdas so think of a\n53:58 runnable is a task whenever I was talking about task in the initial outline for chains under the hood the\n54:04 way it actually works in Lane chain is it's just called a runnable so what we're doing a runnable Lambda is nothing\n54:10 more than a task that's a Lambda function and if you're newer to python don't worry about this Lambda functions\n54:17 kind of get like a little bit more into weeds but they're just a quick way to make functions so like here's the input\n54:22 to a function and then here's the actions we want to take so that's just kind of what Lambda functions are so\n54:28 that's part one part two is this thing called a runnable sequence and a runnable sequence is nothing more than\n54:34 we're going to do this task then this task and this task then this task and put all those tasks together you have a\n54:40 sequence AKA a runable sequence AKA a chain so that's exactly what's happening\n54:46 so let's actually dive in deeper into these runnable lambdas and runnable sequence so you can actually see what the heck's going on so the first thing\n54:52 is what we're going to do is we're going to create the format prompt runnable Lambda and what this is going to do is\n55:00 all it's going to do is take in our prompt template that we've done in the past and then instead of calling invoke\n55:05 on it what we're going to do is just call the format prompt and the format prompt is basically just going to like\n55:11 yep replace all these you know values right here with these values down here\n55:17 so that's exactly what's happening under the hood and you can see we are passing all of our arguments and spreading them\n55:23 right here that's exactly what that's doing basically we're just passing this dictionary and spreading it so that we can replace all these key values with\n55:30 these values down here so that's what's happening don't have to understand how it works exactly I just I just want you to be aware of like what's happening\n55:36 from there once we have our prompt and we've created that task we're going to create an invoke model task and the\n55:41 whole point of this runnable Lambda once again just a task that takes in a Lambda function all this is going to do is it's\n55:47 going to say yep we have a model we're going to invoke the model and what we're going to do is pass in a you know pass\n55:54 in the input from the previous function into this one and we're going to convert whatever the heck we just received to\n56:00 messages so that's what's exactly happening there finally we're going to parse the output so if you remember the string output parser well under the hood\n56:07 all it's doing was grabbing the content from the output and that's what we're going to do right here so here's how now\n56:14 that we have all these individual runnables here's how it works with a sequence and let me just show you under\n56:19 the hood what a sequence looks like so um just in case you haven't seen this if you have a Mac you can hit command and\n56:25 click basically any any class and it'll take you to the definition if you have a Windows just hit control and then click\n56:32 the the class and it'll take you to the basically the the definition but basically this is recapping everything\n56:37 that we have just talked about you know a runnable is nothing more than just like it takes the output from one passes\n56:44 it to the next as an input so just we really just creating a sequence of task that need to occur they give us a cool\n56:49 little example just to show us like yes whenever we use Lang chain expression language to where we just like pipe task\n56:56 and runnables together it's actually the same thing as you know calling runnable sequence first and last and then once\n57:02 you have a sequence you can invoke it and pass items into it so that's what's happening under the hood so let's\n57:07 actually hop back to our code example and finish this one up real quick so we can actually keep keep going down the chain what you'll notice is whenever I\n57:14 created my runnable sequence you'll have three different properties that you can call and this is because if you scroll\n57:19 down to the actual code part you can see there's three different things there's first middle and last first is a single\n57:25 run last is a single runable and middle is a list so that's just how you have to pass\n57:31 in your sequence so if you have two items you'll call first and last if you have five items 10 items 100 you know\n57:38 you'll have to call first for the first item last for the 100th and everything in between will just get passed in as a\n57:44 list so that's exactly what's happening so in our case the format prompt where we convert our text over to a prompt\n57:50 happens as the first task our middle task where we have our formatted prompt and pass it to to our model so we can\n57:57 actually process it that gets passed in the middle list and then finally for the last thing once we you know have our\n58:03 prompt pass it to our model process it in Ai and we just want to get that final text output well that's why we're going\n58:09 to call the last runnable task which is just going to be Parts output and when you put all that together you end up\n58:14 getting a chain and that chain is exactly we'll call it the exact same way that we were doing earlier so what I'm\n58:19 going to do I'm going to run the code real fast just so you can see how it works and then I'm going to compare them side by side just so you can see like\n58:25 that's crazy that they do the same thing but this just takes so much more work cuz we're not using the L chain\n58:31 expression language so this is chains under the hood so we're going to run it and as you can see whenever it finally\n58:36 gets done it's just going to spit out a response which is you know just three jokes about lawyers so yeah that's just\n58:43 that's three different jokes and just to show you a quick side by side let's go open up three one chain Basics and zoom\n58:51 out yeah so instead of doing all this nonsense where we're creating runnable lambdas and passing stuff over and over\n58:56 and creating runable sequence just stick to L chain expression language and it just works so I hope you guys Now\n59:02 understand why L chain expression language is so nice and that's probably what you'll use 99% of the time whenever\n59:08 you're working with building chains but enough of that going deep let's actually hop back up and start working on some of\n59:14 the additional ways we can use chains so we're going to broaden them out in the extended section and then we'll keep moving on from there so let's go ahead\n59:20 and hop over to part number three all right guys so welcome to the\n59:26 third example in the chain module and in this example we're going to Showcase how you can continually extend your chains\n59:33 and add on additional runnables or probably the easier way to think of it is additional task to continually happen\n59:39 and you can just keep growing that chain forever so let's just go ahead and look at the code in this example so you can see how it actually works so what we're\n59:45 going to do is the per usual where we're going to load our environment variables then we're going to create our model and\n59:50 then we're going to create our prompt that's all the usual stuff all of the new chain extending has happens right\n59:57 here where we're going to uppercase the output and then we're going to count Words so let's actually dive into what's\n1:00:02 happening well what we're going to notice is we're creating our usual chain and then we've piped in an uppercase\n1:00:09 output well under the hood when we actually look at what's going on in this function because it is important\n1:00:14 whenever you extend or add new items to your chain by doing the pipe operator you need to add runnables that was one\n1:00:21 of the key things we talked about in the under the hood section so in this case we're going to add a run Lambda and\n1:00:26 runnable lambdas just make it super easy to like basically process any type of function so in our case we're just going\n1:00:32 to do some like string manipulation where we're going to Upper C like sorry I I'll slow down for you guys but like\n1:00:37 we're going to take in an input and what we're going to do is call uper on it which is going to capitalize the entire output so that's what's happening under\n1:00:43 the hood but what's cool with runnable Lambda functions is you could also use these to go off and make like an API call you could you know you could really\n1:00:50 do anything that you normally could do with python code you could just throw it in a runnable Lambda function so that's the part that makes this really cool but\n1:00:56 so now that we have uppercased the output what we're going to do is continue building on our chain and we're going to pipe the output of this over to\n1:01:03 count words so in this case when we come look at our count Words runnable Lambda you can see we're going to taking an\n1:01:08 input this input is going to return basically a string that we're going to do some interpolation on which basically\n1:01:15 just means some like formatting and passing in values so the first thing we're going to do is count the word count so what you can see is we're going\n1:01:21 to split the input that's given to us and then once we're done with that we're going to count count how long the actual\n1:01:28 input was and then once we're done with that we're just going to spit out like right here it just says you know curly\n1:01:33 brackets X that's where we're just going to print out the input that was given to us enough talk let's actually dive in so\n1:01:38 you can look at what actually happens under the hood when we actually run this so this is the third module for chains this is the third example and when we\n1:01:45 spit it out we would expect to see the word count yep just like that and then jokes so as you can see this code ran\n1:01:51 perfectly we spit out the output and you can actually see that this code ran too because everything is capitalized where\n1:01:57 it wasn't originally so this is how you can continually build chains and as you're going off and becoming you know a\n1:02:03 real world professional L chain developer some of the other practical things you would do is just like I said you would use runnable lambdas to go off\n1:02:09 and like create API calls to go do something you would then there's just a ton of stuff that you could do and\n1:02:14 chains make it super easy to you know pipe an input from the previous result over to your current function and just\n1:02:20 continue going down the chain so this is awesome so now that we've covered extending and continually growing your chain let's hop over to talk about how\n1:02:27 you can actually run chains in parallel I think you're really going to like this section too all right guys so welcome to the\n1:02:33 fourth example in the chain module in this code what we're going to be doing is showing off how you can run chains in\n1:02:40 parallel so I think this one's super cool cuz if you've ever thought about creating social media posts where you\n1:02:45 have like an idea and then you want to like Branch off to write something on LinkedIn and then Twitter and then elsewhere this is exactly how you're\n1:02:52 going to go about doing it so let's go ahead and dive into the code at a high level and then I have a visual for you guys just cuz it is it is a little bit\n1:02:58 weirder to see how it all works and then we'll come back to the code to like really dive in okay so at a high level\n1:03:04 this file is a little bit longer and I'm just going to hop down to the chain part and then from there we're going to go over to the visual I was just talking\n1:03:10 about so at a high level what you can do is you can see we are creating a chain this chain has the usual prompt has a\n1:03:16 model we do some string outputting and then we start doing this funny thing called runnable parallel under the hood\n1:03:22 basically what this is doing is it's creating different branches to run you know if we're looking at Pros cuz this\n1:03:28 is going to be a pro con list and we're just going to run run our Pros in one chain and we're going to run our cons in\n1:03:34 another chain and eventually merge those Pro cons into a final list so that's why we have two branches one for pro a pro\n1:03:40 branch and then a con branch and that's where they're going to go off and generate pros and cons and we're going to come back and put into a final list\n1:03:46 so that was high level just talking through the code so let's hop back over to what this looks like under the hood\n1:03:52 when we're actually like you know visually what does this look like so that chain that we were just looking at\n1:03:57 what we're going to do is it's going to take in a prompt this prompt is going to take in a product name from there it's going to construct a prompt using in our\n1:04:04 case I think we're going to do the MacBook Pro and then from there it's up to the model to produce a list of features about that product from there\n1:04:11 we're going to grab the content like we normally do from the result from the model and just pull out the content string and what's cool is we're going to\n1:04:18 pass that string to our two separate chains so I don't know if you saw back over a second ago but we had a pro chain\n1:04:24 and a con chain that's exactly what we're doing here so the pros it's going to take in you know the list of features\n1:04:30 and in our case we're going to pull out all the pros from those features and then what we're going to do on the other side in parallel is find all the cons\n1:04:37 for all those features and then finally once we're done with both of those different chains that are running in\n1:04:42 parallel we're going to grab both of the outputs and then put them together in a final Lambda runnable so we're going to\n1:04:50 get both of those outputs and then put them in a basically a nice print statement for our users to read pretty easily so that's exactly what we're\n1:04:56 doing now let's hop back to the code and do a deeper dive now you have a good understanding of like what's happening\n1:05:01 at a super high level visually all right let's hop back okay let's dive into this part by part and I think the best way to\n1:05:07 go about it is just start you know at the first item in each chain and then walk our way through so when it comes to\n1:05:13 the prompt template this is where we're going to be doing our normal prompt template stuff meaning you know we're just going to say like hey you're an\n1:05:18 expert on doing product reviews and I need you to basically list the main features for this product we're going to\n1:05:24 create a prompt for it in our case like I said this is going to be a MacBook Pro and then we're going to pass that prompt over to our model who's going to go off\n1:05:30 and actually list those features out then we're going to do our string output parser none of this should be new so far\n1:05:36 but then we're going to hop into this runnable parallel basically you know runnables are how chains work under the\n1:05:42 hood so in this case we're doing parallel meaning we have a bunch of branches that we can spin off our\n1:05:47 different chains so in in this case we're going to have a pro branch and a cons branch and you can actually dive in\n1:05:54 and see what each one of these these are so this Pros Branch under the hood it's a runnable Lambda and what we're doing\n1:06:00 is we're actually just creating in our case we're creating a new chain right here that's exactly what it is I can\n1:06:06 actually rename it for you guys so you can actually see exactly what it is we'll call this one a chain that one a\n1:06:12 chain perfect just so it makes a lot more sense and it's even more clear what's actually happening so we're creating a chain for our prob branch in\n1:06:18 this case we're saying like hey take in the input which in this case is a feature list I want you to analyze those\n1:06:25 probes so analyze Pros all it does is it creates a new prompt template just like we normally do and once we have that\n1:06:31 prompt template what we do is we pass that prompt template over to our model for our model to go off and generate\n1:06:36 those pros and then finally per usual we use the string output parser to convert that output into a nice string and the\n1:06:43 con branch is the exact same thing we take in that list of features about our product we pass that list of features\n1:06:49 over to this prompt template so you know given these features list the cons of each one of those features from there\n1:06:56 pass it over to our model string output so all this is completely normal we've done this a ton of times now here's\n1:07:01 what's different after we run our parallel branches what we end up with is actually one big dictionary and that\n1:07:08 dictionary if we'll print it out I'll print it out for you guys so you can actually see what I'm talking about right here so we'll say final output\n1:07:14 what you can see is it'll actually include something called branches and this branches will under the hood\n1:07:21 actually list out our pros and cons cuz that's how we're going to save them and you'll see it in a second whenever I print it out but that's the final part\n1:07:27 what we're going to do is pass in our Pro output and our con output into this combined Pros con function and what this\n1:07:34 is going to do is just print out a nice list string final output for us so we can compare like yep should we get the\n1:07:40 Mac should we not get the Mac so let's run it and actually dive into the outputs so let's come down here let's\n1:07:45 clear all old outputs up and start running it now what's going to do under the hood like I said it's going to start running and the first print output we\n1:07:52 should see is this final output right here where we're going to have our Pros chain and our cons chain we're going to\n1:07:58 be able to view both of them that's what's going to happen in here and then finally afterwards we should expect another final output I just want to show\n1:08:04 you guys under the hood how the inputs are getting passed into this and how we can actually view like yeah it does make\n1:08:10 sense that we need to grab the Branch's object and then within the Branch's object we need to grab the pros and cons\n1:08:15 so let's scroll up so you guys can see exactly what I'm talking about takes a few seconds Okay cool so here's where\n1:08:21 everything's in a dictionary that's why it's all cluttered to see but you can see final output what it does you can\n1:08:27 see it's a dictionary inside the dictionary you can see the first object inside of it is called branches and then\n1:08:34 within branches we can grab pros and then if you scroll down a little bit later you can also see cons if I just\n1:08:40 search for it you'll be able to see it cons let's see yeah so you can see here's the cons object so how does this\n1:08:46 actually work under the hood well it's because everything in this runnable parallel is equal to we basically\n1:08:52 created a dictionary right here because you can see we have branches is equal to and then we're creating our object so\n1:08:58 Pros are going to be saved as the result of this Pro chain and cons are going to come from this Con chain so that's\n1:09:05 exactly how it works so I hope that makes sense once again this is getting a little bit deeper into the weeds when it\n1:09:10 comes to working with Lane chain and we're starting to do a little bit more like fanciness with python so if you have any questions on any of this please\n1:09:16 definitely go over to the school community that I've created for you guys drop a comment you know take a screenshot of what's con confusing you\n1:09:22 and I'd be happy to help or hop on our weekly coaching calls but I hope you guys see the value in this because in\n1:09:28 the past you would have had to run your Pros chain that would have taken 30 seconds then you had to run your cons\n1:09:33 chain that would take another 30 seconds but now when we can run stuff in parallel we can cut it down and make things a lot more efficient if you're\n1:09:39 going to be building applications for your users where they're expecting a speedy output this is exactly what you're going to want to do to help\n1:09:45 automate and speed up some of your task all right so enough of talking about running chains in parallel let's go over to learning how we can start branching\n1:09:52 our chains based on different conditions let's go ahead and dive into task numberb five next see\n1:09:58 y all right guys so welcome to the fifth example in the chain module and this\n1:10:03 final example what we're going to do is cover branching and I think the best way to cover this topic before diving into\n1:10:09 the code is to actually look at what this code looks like visually so let's hop back over here to excal draw and let\n1:10:16 me run you through what's actually happening under the hood so there are two parts to this chain the first part\n1:10:22 is what we're going to call the classification chain and what we're trying to do is we're trying to take in feedback from our customers so they're\n1:10:29 going to say like man that was awesome or like oh that was the worst product I ever bought and depending on what type of feedback they give us we're going to\n1:10:36 have different prompts to generate different types of messages so we're going to have a positive Branch we're\n1:10:41 going to have a negative Branch a neutral branch and then if something goes wrong we're going to have an escalation Branch so here's a quick run\n1:10:47 through of what's Happening visually and then we'll hop back over to the code so visually what we're going to do is do the normal chain for our classification\n1:10:54 where we're going to have have a feedback prompt what we're going to do is in this prompt we're obviously going\n1:11:00 to populate the prompt with the user's input in this case it's going to be their feedback about what we're doing\n1:11:06 then we're going to categorize that feedback into one of you know one of these four keywords positive neutral\n1:11:11 negative or escalate so that's how we're going to basically classify everything then we're going to pull out the\n1:11:17 category with a string parser like we've done a ton of times by now and here's where the New Concept comes in we're\n1:11:23 going to it's going to be called a runnable brand and basically the way this works is it's think of it as like an if statement so\n1:11:29 if the you know if a conditions met we're going to run a different chain so this is a really good way to like really\n1:11:36 you know make your chains more intelligent and in different scenarios run different chains so in this case\n1:11:41 just you know going deeper we're going to check if the category was positive if it was fantastic we're going to run the\n1:11:47 positive chain was it negative cool we'll run the negative chain so forth and so forth so I hope this all makes\n1:11:52 sense now let's hop back over to the code so you can see what this actually look looks like inside of Lang chain all\n1:11:57 right so welcome back to the code and I think what you'll notice is this is going to be super straightforward so\n1:12:03 what we have to start is we have a bunch of different prompt templates each one of these prompt templates for positive\n1:12:09 feedback negative feedback neutral feedback and escalate feedback all these different prop templates come from right\n1:12:15 here so as you can see across the board we have all these different prompts that's what we're doing we're building up each one of these chains for the\n1:12:21 different types of feedback all right let's keep going then eventually what we're going to do is we also need to create the prompts for our\n1:12:27 classification part so this is you know how you know your helpful assistant help us classify the sentiment of this\n1:12:34 feedback is it positive negative neutral or do we need to escalate it and this is how we're you know using our prompts to\n1:12:39 pass in our users feedback cool so here's where we're going to get into the new part which is the runnable branch so\n1:12:46 this is the part that is different than anything you've seen so far so what you're going to do is you're going to create a runnable branch and the\n1:12:52 synopsis of how this works is you're going to have a input so in this case we're just going to use a Lambda\n1:12:58 function and then you have a conditional right here so in this case we're going to say like hey is the word positive in\n1:13:05 the input that you just gave me if so fantastic I'm going to run and this is basically oops sorry this is basically\n1:13:11 the positive chain I'll just write right here back chain fantastic so this is\n1:13:17 actually what's happening under the hood we didn't full out you know call each one of these their own chains but each one of these is a neutral chain a\n1:13:24 positive chain and a negative chain so here's you know this because this is once again we're using Lane chain\n1:13:30 expression language to create a positive chain so as you can see we're just going to take in the promp template that we saw above we're going to pass that to a\n1:13:37 model and then we're going to get back the output so this is a message that we could respond to our users saying like oh you loved our product fantastic I\n1:13:44 really appreciate it and we hope you you know continue to buy further products and services from us okay so now you've\n1:13:50 kind of seen how we're going to use runnable branches to conditionally run different chain such as positive chains\n1:13:56 check to see if it's negative to run the negative chain see if it's neutral to run the negative chain the other part that I haven't mentioned yet is there's\n1:14:02 a default so when you're using runnable branches if any of these messages didn't\n1:14:08 trigger off like if we didn't run you know the positive negative or neutral you basically can create a default\n1:14:13 branch and in this case we're going to call it the escalate branch and what this one's going to do is it's actually\n1:14:19 just going to make a nice message for us that we can see where it's going to like hey generate a message to escalate this to a human agent so that's just kind of\n1:14:25 how it works and this could definitely be super helpful in a customer service type of situation so I just wanted to show you guys this cuz it's super\n1:14:31 helpful and helps you build more complex tools all right so now that we got the runnable branches out of the way let's go back to see how we can make this work\n1:14:37 so basically in the end we're going to have our classification chain and then you know which is just going to be this\n1:14:42 whole part right here this is the entire classification chain right here and then the entire bottom part all of this huge\n1:14:49 square right here is going to be this second chain so we're chaining chains together to make super long complicated\n1:14:55 ones I hope you guys like this all right so what we're going to do next is I'm just going to go ahead and run it so you guys can see the results and if you want\n1:15:01 to play around with this code for yourself what you'll notice is I went ahead and typed up a bunch of different types of reviews so you can experiment\n1:15:07 with each of the different branch pths so let's go ahead and run it for a positive review fast just so you can see you know how it works so we're just\n1:15:14 going to go open up our terminal and this time what we're going to do we're going to do Python and this is the third\n1:15:19 module so chains five to run this branching program so we're going to run it and what we would expect to happen is\n1:15:25 it to ex to go down the positive branching path and generate a nice positive message so you can see you know\n1:15:31 I hope this finds you well I just want to personally thank you for your positive feedback so you can see it's actually it's actually working and what\n1:15:37 we could do too if you want to try out something different let's try out a bad review just so you can kind of see how\n1:15:42 it would go down the in this case go down the negative branching path so I replaced it with the negative comment\n1:15:48 it's terrible I broke it after just one use and then you can run it again so let's see you know thank you for your\n1:15:54 feedback I'm sorry that your experience was not up to your expectations we take all this feedback so as you can see this\n1:15:59 is a way we can handle a bunch of different scenarios using GL chain expression so yeah that concludes the\n1:16:05 third module where we're covering chains and what we're going to do next is move over to rag which is going to be what\n1:16:11 we're learning about retrieval augmented generation this is going to be a huge module I hope you guys are pumped for it\n1:16:16 let's go ahead and dive into it now all right guys welcome to module number four where we're going to start\n1:16:22 learning about retrieval augmented Generation all also known as rag so in this quick overview section what I want\n1:16:28 to do is I want to talk about what is rag why do we need it and then I want to do a high Lev overview of like just\n1:16:34 running through a simple example just so everything makes sense whenever we start to dive into these code examples that\n1:16:40 I've set up over here for you guys okay so let's just go ahead and just talk about what is rag at a super high level\n1:16:45 and kind of why do we need it well these large language models that we have set up like chat gbt Gemini and llama 3 all\n1:16:52 of these models have a con straint on how much knowledge they already have and that can be a problem whenever we want\n1:16:59 to start answering questions or whenever we have questions about additional things such as like hey what's happening\n1:17:05 today in the news well these llms have a cut off date of you know a few months or even a year ago so it has no idea what's\n1:17:12 happening right now also when it comes to maybe just like inside of your company you have some specialized\n1:17:19 documents for y's processes or you have some other information about a product You're Building well the LMS obviously\n1:17:25 don't know anything about it so it's up to us to use rag to feed in additional\n1:17:31 new information to these lolms so under the hood all we're trying to do is just give these lolms additional information\n1:17:37 so they can answer our question that's all that's going on at a high level so as you can see we can feed in things\n1:17:42 like PDFs so whenever you hear people talking to PDFs or chatting with them that's exactly what's happening they're using rag so we can feed in websites we\n1:17:49 can feed in source code and video transcripts and the list goes on and on but okay that's enough at a high level\n1:17:55 so what I want to quickly do next is let's dive over to a big example that I set up for you guys where we're going to\n1:18:01 walk through exactly how rag works at a super high level so when we hop over to the code and we go through all these\n1:18:06 different examples you'll have a high level understanding of like oh that's why we're doing what we're doing so let's go ahead and dive into that high\n1:18:12 level overview right now all right guys so welcome to this quick overview of how rag works now I've\n1:18:19 put together this flowchart for you guys so that we can understand two critical components of working with Rag and I'm\n1:18:26 going to walk through both of them at a high level and kind of go super quick because we're going to go through this a bunch more times as we actually dive\n1:18:32 into the code but the main thing I want us to take away from this demo is for you understand the Core highle Concepts\n1:18:39 such as terms like vector store embeddings llms chunks tokens we're\n1:18:44 going to learn we're going to just quickly say all these words and it'll mean a lot more when we dive into the code and that's where we're going to be covering in part one and then part two\n1:18:50 of this quick demo I want to walk through like great well once we have actually saved all of the information\n1:18:56 from our PDF over to this special database called a vector store well how can we actually start asking questions\n1:19:03 and pulling out information so that we can get a AI generated response that's\n1:19:09 informed meaning if we're you know chatting with a document about how to like cook a specific food well we want\n1:19:16 to pull out the specific instructions for that food along with our question to generate an AI response so that's what\n1:19:22 we're going to be talking about at a high level and we're going to go pip part and this will hopefully all make sense and I do think one of the core\n1:19:29 parts to help this make sense of like why we're doing what we're doing is to actually start at the vector store and\n1:19:36 work our way back up so you understand why we're doing it because and this will make more sense so hang on for two\n1:19:42 minutes and it'll make all sense okay so at the end of the day everything that we're trying to do comes to this part\n1:19:47 right here it's the bridge and connection between our database which stores all the information about our\n1:19:53 Tech Source plus this Retriever and this retriever is where we're going to like\n1:19:59 hey man please give me all the information about you know let's just say we're working with Harry Potter and\n1:20:05 we ask like hey which Professor also is a werewolf well what we want to do in this example is we want to be able to\n1:20:12 look up things such as like professor and werewolf how well if we were manually doing this what we would do\n1:20:18 just like command F in a PDF and just search for those words a thousand times but there's going to be a ton of entries\n1:20:24 and what we're trying to do is really find a werewolf and a professor that's the information we're looking for okay\n1:20:29 so what we're trying to do and this is a keyword here we're trying to look for similarity inside of the original teex\n1:20:37 source that we're looking for that's all we're trying to do we want to look for similar pieces of information inside\n1:20:43 this original PDF that talk about professors and werewolves okay so that's all we're trying to do here now well how\n1:20:49 does this actually come into play when it comes to rag well what's going to happen under the hood is this PDF of\n1:20:55 Harry Potter is going to be broken up into a thousand different chunks because you can see this PDF is has about 10\n1:21:01 million tokens and that's way too big for us to answer any questions or like analyze what's going on because chat gbt\n1:21:09 has a window of about 8,000 tokens and 10 million is obviously way too big to pass into it so what we need to do\n1:21:15 because we're just trying to pull out the similar pieces of information and pass it over to jbt so it can generate\n1:21:20 an AI response about it so what we need to do is we need to split this up into a bunch of smaller chunks that are about a\n1:21:27 th000 each what's great about this is because like I said chbt can take in 8,000 prompts 8,000 tokens so if we pass\n1:21:35 in one chunk up to three or four chunks there's still plenty of room for us to also pass in a question about those\n1:21:42 chunks okay hang on we're getting there all right well each one of these chunks is just plain text so you can see like\n1:21:48 chunk one is just like you know Harry you're a wizard and you know and basically how this is working under the hood is we're just starting at the\n1:21:55 beginning of the book taking out a thousand words putting it a chunk putting it getting the next few thousand words putting in another chunk well the\n1:22:01 problem is we said our original goal is we went to look up similar phrases well how the heck does AI compare and search\n1:22:09 for similar phrases well this is where the term embedding comes into play an embedding is nothing more than the\n1:22:17 numerical representation of text so that was a mouthful what does that actually mean well if we were to like in this\n1:22:23 super simple example if we were to embed the word dog cat in house you can see\n1:22:28 that a dog has this embedding a cat has this embedding and a house has this\n1:22:34 embedding and you'll notice they're nothing more than just a list of numbers but they actually under the hood have\n1:22:39 some meaning and relationship to to one another so for example a dog and a cat\n1:22:44 are super similar the only thing that's different are these last two numbers but even then they're not that different cuz\n1:22:50 they got four legs they're furry they live in a house but a house on the other hand it's got a roof it's got walls\n1:22:56 brick they're completely different subjects so if we were to ever like search about dogs or animals with four\n1:23:03 legs well these two things would come together these are super similar so we would get responses about dogs and cats\n1:23:10 so with the point I'm just trying to get across is embeddings allow us to easily in search for similar items and the way\n1:23:18 we're able to do this cuz like machines love numbers and numbers are easy to compare and see how different and\n1:23:24 similar things are okay so what we're going to do is we're going to convert these chunks of text over to embeddings\n1:23:31 so that we can easily compare them and contrast them and once we have all these different chunks of text embedded what\n1:23:38 we're going to do is start saving the embedding plus the original text to a vector store and the reason why this is\n1:23:44 important is because going back to our original problem what we want to do is we want to be able to ask questions and\n1:23:50 retrieve related documents from this Vector store all right so now let's walk through this flow down here now that\n1:23:56 we're in part two and you'll see exactly why everything that we just did above is super important so going back to us with\n1:24:02 our Harry Potter fans if we ask a question which Professor is also a werewolf well that question will be\n1:24:09 embedded itself because what we're trying to do is see we're trying to look up similar documents to our question and\n1:24:17 the way we're able to do that is because our embedding we now have a numerical representation of Professor and werewolf\n1:24:24 and and then eventually cuz now it's embedded now whenever we go to our use our it's going to be called a retriever\n1:24:30 whenever we use our retriever to look up similar embeddings in the vector store it'll go like oh well chapter 4\n1:24:36 paragraph 1 2 and 3 all talk about this professor that's a werewolf let me give you back those chunks of data or those\n1:24:44 Blobs of text so that you can then ask questions about them so that's all that's happening under the hood we go\n1:24:50 step one here's my question here's my embedding that has my question great let's return this chunk and this chunk\n1:24:57 because those were the most similar and once we have those chunks returned what we can do is pass them over to chat gbt\n1:25:04 and what's going to happen under the hood that you'll see is all that's doing is is just adding those pieces of text\n1:25:09 at the very beginning of our prompt and then at the very bottom we'll have our question and then we can now ask\n1:25:15 questions about those pieces of information so what it would really be is like which Professor is a werewolf\n1:25:21 and then these three chapters or these three PA paragraphs will have all the information we need to generate an AI\n1:25:27 informed response now that was a ton of information don't worry we're going to go part by part when we look at the code\n1:25:33 it's just at a high level the main thing I want you to understand is we have huge Tech sources that have way too much\n1:25:38 information for any AI to consume we need to break it up into smaller chunks those chunks need to be converted over\n1:25:45 to eddings the reason we need to do that is because we need to see how similar each different chunk is to one another\n1:25:51 so that later whenever we go to ask questions about those different pieces\n1:25:56 of text and embeddings we can go oh yep these two are super similar here's the information you requested back so that\n1:26:02 you can ask a question so that was a mouthful let's actually go ahead and start diving over to the code now so we\n1:26:08 can see all of this in action and I promise you after you see this a few times it'll all make sense and you'll be\n1:26:13 able to start building your own rag use cases so let's go ahead and hop back over to the code all right guys so welcome to the\n1:26:20 first code example inside of the rag module and I'm super excited you guys to see how everything ties together that we\n1:26:27 just walked through in the highle drawing so this is exactly where you're going to connect the dots you're just to paint a road map of what we're about to\n1:26:33 do is I have broken up the first code example into two parts as you can see there's a 1 a and a 1B 1 a directly ties\n1:26:41 to the initial breakdown of what we were setting up top in our initial drawing\n1:26:46 drawing where we were first converting some Tech Source over to our Vector store so that's going to be in 1 a and\n1:26:52 then 1B is going to be all about actually asking questions and retrieving documents from that Vector store so\n1:26:59 everything we talked about when it come to like embedding chunking saving stuff through the vector store that's all\n1:27:04 going to happen in 1 a and then building our retrievers once again embedding you're going to see that all in 1 B okay\n1:27:10 so we're going to go ahead now and start walking through this code part by part and some of this code is a little bit\n1:27:15 more advanced just because there's a lot more moving Parts but don't worry I'm going to cover everything and once again\n1:27:21 if you have questions you can always head over to the fre School Community I created for you guys just take a screenshot or a Tim stamp of the video\n1:27:27 and say Here's Where I'm stuck I don't understand this and myself or someone else in the community will be sure to help get you unstuck so you can keep\n1:27:33 going and building out your awesome projects all right so let's go through this part by part and also walk through some of the folder structure just so it\n1:27:40 all makes sense okay so the first thing that we're going to be doing is inside of 1A is we need to set up some file\n1:27:47 paths so that we can connect to different parts of our project so the main things that we need to connect to\n1:27:53 is our initial text source so this is the document we're going to be asking questions about in this case we're going to be asking questions about the Odyssey\n1:27:59 and you can see how we access that book is by looking inside of our current directory go to the books folder and\n1:28:06 then look for the Odyssey text so if right now U my current directory is inside of the rag folder if I open up\n1:28:13 books you can see that I actually have the Odyssey text file right in here so that's how we're going to access that and then next thing is you can see I\n1:28:19 have this what I'm calling a persistent directory so that's actually just where I'm going to be storing our chroma\n1:28:25 database and same thing this one we just look at the current database open up the DB folder whenever you start to save\n1:28:32 files this will actually Auto get created so you might not see DB out the gate and then next we're going to save\n1:28:38 everything to the chroma DB folder so that's what you can see now this is since I've already ran this code before\n1:28:44 I already have this but I'm going to delete it so I'm on the same page as you guys Okay cool so let's keep chugg along\n1:28:50 in our code and so you can understand what's going on so if you remember the first part of what we're trying to do in\n1:28:55 this highle code is go from a Tech Source over to our Vector storm so if we've already went ahead and chunked our\n1:29:03 document converted everything to embeddings and saved it to our Vector store there's no reason to rerun 1A so\n1:29:10 this is why I have this code right here it's just going to say hey does a persistent directory already exist in\n1:29:16 this case the chroma DB already exists if it does just completely skip all of this because there's no reason for us to\n1:29:23 go off and you know there's no reason for us to go through and pay because embeddings do cost money to convert from\n1:29:28 text to embeddings so we're just going to completely skip it but if we do not have that persistent directory what we're going to do is go off and actually\n1:29:35 set everything up so the first thing that we're going to do is we're going to start at the top and actually load in\n1:29:42 the file path and what I'm going to do to help this make sense is we're just going to quickly hop back and forth from part to part so it all makes sense so\n1:29:48 first thing we are going to load that file path which is connected to our Odus text and we're going to use a text\n1:29:54 loader a text loader is just a great way to actually like Point like yep here's the file path to my document that I want\n1:30:01 to load later on we'll talk about things like web loaders to where you can actually pull information from the websites but just I'm just want to like\n1:30:07 expose you to the realm of the possible but right now we're just going to load a text document once we have that file loaded what we're going to do is we want\n1:30:14 to actually start splitting up this document into small chunks exactly like\n1:30:20 we talked about here so so far we've loaded it and now what we're going to do is start chunking it and there's a bunch\n1:30:25 of different chunking strategies that we will talk about later on in the text splitting Deep dive but for right now\n1:30:31 just know we're going to try and chunk our basically each section of the book\n1:30:37 every 1,000 characters and just to go like a little you know help explain a little bit more just so we're on the\n1:30:42 same page you can also set this thing called chunk overlap and all that does\n1:30:47 is let's just say this is a chunk of text and let's just say in the middle of\n1:30:52 a paragraph Was A characters what it'll do is it'll cut off so you kind of get cut off in the middle of a sentence and\n1:30:59 then when you load the next chunk it's kind of like well I have a query about the end of the first chunk but it's\n1:31:05 about the second chunk too so it just I don't know it's it's just difficult for us to understand semantically what's happening in a chunk so what you'll\n1:31:12 notice a lot of times people do is they'll set some overlap so what that's what that'll mean is just like you'll\n1:31:18 always grab a little bit of the next section too that way there's there's just always overlap and a lot of times\n1:31:23 you'll get better performance results so a lot of times just like give you numbers a lot of people will do things such as like 200 or 400 or common values\n1:31:31 so yeah but right now for example we're just going to keep everything lean and mean and set the chunk size to 1,000 tokens fantastic so once we have set up\n1:31:38 our text splitter what we're going to do is actually split up our original documents into a ton of those chunks\n1:31:43 that you see right here each one's going to be a th000 tokens now once we have all of our chunks also docs whatever you\n1:31:50 want to call them what we can do next is we need to start moving over to in embedding them now in this casee we're\n1:31:55 just going to use the open Ai embeddings and we'll actually go through later on in an embedding Deep dive to show you\n1:32:01 other options that you have but for right now just know that we're going to use open AI to convert our text chunks\n1:32:07 over to embeddings and we're going to be using the text embedding three small model and there's actually a few\n1:32:14 different models that you can use there's one that's three the three small which is the cheapest one and I that's\n1:32:20 one I primarily use and there's also a three large just just know as we talked about earlier there's this things called\n1:32:27 numerical representations where just which were just long arrays of like a dog is equal to like an array of like 1\n1:32:33 two three well the large model just has a lot more values so we can get a lot more precise with our embeddings so just\n1:32:39 just know three small is probably completely fine for the projects that you're working on okay let's keep\n1:32:44 chugging along so once we have set up our embeddings what we're going to do is we're going to use our chroma Vector\n1:32:50 store to actually save all of our documents with our beddings and we're going to say you are going to save the\n1:32:57 results over here into this persistent directory so what I'm going to do is I'm going to run this just so you guys can\n1:33:02 see what it looks like under the hood so we're going to run python this is the fourth module and this is 1A and we're\n1:33:08 going to run R rag Basics I'm going to zoom out so we can actually see it so you can see we are creating our embeddings we're finished creating our\n1:33:14 embeddings and then now we're going to create our Vector store if you scroll up actually a little bit you can see we\n1:33:20 saved a little bit of our first chunk cuz in our code we said you you know how many chunks do we create in this case\n1:33:26 826 where each chunk has a th000 tokens and you can see the first sample chunk and this first sample chunk is actually\n1:33:32 the very first part of the book so if you go open the book you can see at like line one this is the exact same so yeah\n1:33:39 we've actually just you know this is working hope you guys are hype to see it's all all coming together all right so let's go back down to the bottom so\n1:33:46 you can see our Vector stor is created and we can confirm that by going over to our database folder and checking for our\n1:33:53 chroma DB and you can see at the beginning DB chroma DB yep it made it\n1:33:59 and just to show you guys if I actually try running it again the way we've set up the code is it'll give us an alert saying Yep this Vector store is already\n1:34:05 created no need to redo it cool so now that we have that set up let's go ahead and move on to Part B where we can\n1:34:11 actually start asking questions and pulling information from the vector store so let's walk over here to 1B and\n1:34:17 start going through this once again part by part so per usual what we need to do is we need to have we need to point\n1:34:23 point to our Vector store so that we can actually start you know referencing it so in this case copying the same thing\n1:34:29 look inside our current directory then actually access the database folder and look for chrom ADB once we have that set\n1:34:36 up we need to specify which embedding we're going to be using and we're just it's important to use the same one just\n1:34:42 so that you know you wouldn't want to change languages when you're asking questions so like we just need to be very specific and use the same thing\n1:34:48 consistent or else you're just going to get some weird results fantastic next what we're going to do is we're going to use chroma DB once again we're just\n1:34:55 going to spin it up saying yep here is the persistent directory where you've already saved all of your edings in\n1:35:01 those text chunks and here's the embedding function so that whenever you get asked queries you know how to go off\n1:35:07 and do a similarity check and see which relevant documents are the most similar all right so in this case all we're\n1:35:12 going to do is for this question we're just going to ask the Odyssey so the main character is Odus we just want to know who is his wife so this is the\n1:35:19 query that we're going to be asking to our Vector store and I do think it is important for you guys if you haven't\n1:35:24 seen what Vector stores look like I actually think they did a great job this is the chroma DB website and I think\n1:35:32 they did a great job of helping you guys visualize what's going on so under the hood you can see we're going to ask a\n1:35:37 query so who's ODS is's wife we're going to convert that query over to an\n1:35:42 embedding once we have that embedding what we're going to do is access chroma DB and it will pull out the documents\n1:35:49 and they're connected embeddings cuz like that's how we can do the similarity score door and we can see like oh this\n1:35:55 one's the most similar and here's the related document let me pass that back and once you get passed back you know it\n1:36:00 eventually gets passed back to your whatever llm you're working with and generate that AI response so I just thought this was a neat little graphic I\n1:36:06 wanted to show you guys let's go back to ours though so we can keep cranking this out so what we need to do next is work\n1:36:12 on that Retriever and the retriever is what's going to take in our query and\n1:36:18 eventually return back those related documents but when we're setting up our retriever we have a few different\n1:36:23 different options for what we can do and per usual what we're going to do is I have a deep dive later on so you can see\n1:36:29 all the possibilities when it comes to working with Achievers but for right now just know we have a few different search\n1:36:35 types that give us different results and for this example we're going to be doing a similarity score threshold all that\n1:36:41 does is pretty much exactly what it says it's going to do a similarity check to find the most related documents and it's\n1:36:47 only going to return documents that you know up to us how similar so this number you can see we have something called a\n1:36:53 score threshold well the higher this number is the higher the text document has to be to our query and we'll play\n1:37:00 around with this just so you can see it in action and the other argument that you'll see is this random letter K with\n1:37:05 a number so what the heck is that well what it's going to do is it's going to return the K closest documents so if\n1:37:12 there was 100 documents that were you know that were Above This threshold what this will do is it will return the top\n1:37:18 three closest results so that's how you have to kind of set up your retriever if you want to uh to do this and feel free\n1:37:24 to bump this number up or down just remember depending on which llm you're working with you're kind of capped at you know right around usually 8,000\n1:37:31 tokens so in this example we're going to be pulling out 3,000 tokens whenever we're all said and done and passing it\n1:37:36 to our llm okay so now that we have set up our retriever set up the parameters what we can do is we're going to call\n1:37:43 that invoke function remember we talked about that a lot in the chat model section invoke is the magic word inside of link chain for actually taking action\n1:37:50 on the models that you set up so in this case on our we're going to pass in the query invoke it and it's going to give\n1:37:56 us back the relevant documents so I'm going to go ahead and run this and then we're going to play around with some of these parameters just so you can see\n1:38:02 what's happening under the hood and I do want to point out all we're doing in this example is just showing the\n1:38:08 relevant documents later on we'll actually pass these documents over to a chat model so we can start interacting\n1:38:14 with them but for right now I just want to show you guys that yes our Vector store is returning related documents and\n1:38:20 we'll just look through them okay so let's come down here clear this up we're going to run inside of our fourth module\n1:38:27 we're going to run 1B to actually start grabbing those documents and let's run it okay so what I would expect is to\n1:38:33 eventually get back three different documents that relate to OD deus's wife so you can see we have document one\n1:38:40 document two document three that's cool it also includes a source we'll talk more about that later but the main thing\n1:38:46 right now that you can see is his wife is Penelope so you can actually like kind of see like we're talking about in\n1:38:52 this document M we'll actually just do a quick search so you can look up like the term wife it gets mentioned in a few of\n1:38:58 these documents so yeah you are indeed blessed in the possession of a wife endowed with such rare excellent of\n1:39:04 understanding and so faithful as Penelope so that's like you can see this document perfectly describes you know\n1:39:10 like this is his wife so in the other documents too kind of talk about you know some of the other they they refer\n1:39:16 to wife so they're not as similar but you can see this was the closest one that's why it's document number one all\n1:39:22 right now that you've seen that work what I want to do is show you how we can play around with some of these parameters to get different results so I\n1:39:28 can actually bump this up to k equal 10 and I'll bump this down so we basically\n1:39:33 we're we're lowering the threshold and saying we want more results so what we're going to do is I would expect to get back a bunch more documents this\n1:39:39 time that are related to a wife so you can see yeah we got back 10 more documents this time let's see how many\n1:39:45 times his wife's mentioned penelopy yeah so these These are getting less relevant\n1:39:51 seven out of the 10 documents referred to her okay so what I want to do next is I want to show you guys what happens if\n1:39:57 you get a little too stringent so let's just say we only want to have documents that are like super super close to our\n1:40:04 initial question so if we were to run this updated code where we've updated the threshold to 0.09 meaning like very\n1:40:10 very similar what we'll do is we'll probably run into the problem of like hey you were too strict with your search\n1:40:16 results I couldn't find anything so we're going to give it a second and whenever it gets back to us it'll\n1:40:21 probably tell us hey man cannot find that oops it looks like there's a quick bug let me just close this out clear it\n1:40:27 out let me try this one more time and now it should hopefully give us back an\n1:40:32 answer yeah so you can see this time for whatever reason it just hit a bug yeah there are no relevant documents that retrieve using the relevant score 09 so\n1:40:39 we know we got too strict so that's uh just you know as you're building your own rag applications if you're not\n1:40:44 getting back results you might just be a little too strict with your similarity score okay cool well that's it for\n1:40:50 module number one now what we're going to do from here is just to paint a road map for you guys we're going to head into part two where we're going to start\n1:40:56 adding metadata to our document so we can actually kind of see like oh this is\n1:41:02 where this Source came from so we're going to go ahead and learn how to add metadata and view it in our request\n1:41:07 let's go ahead and start working on that now hey guys so welcome to the second example inside of the rag tutorial and\n1:41:14 what we're going to work on in this one is start setting up metadata now why is metadata important in context of rag\n1:41:21 well it all has to come back to the fact that lolms hallucinate and what I mean by that is Hallucination is a really\n1:41:27 nice way of saying they just get stuff wrong but what's cool is whenever we start adding in metadata to our rag\n1:41:32 queries what we're basically doing is allowing our llms to respond with like here is the source of information for\n1:41:39 where I'm generating this information I'm passing back to you so if we were asking a question about a meeting that\n1:41:46 happened in the past at our company it would tell us like oh John said this and Bob responded with this and it would say\n1:41:53 in here's the source where I got this information from so you could always just click the source go check it out on\n1:41:58 your own and be like oh yeah that is exactly what happened and dive into more of the details that went on in that meeting so that's metadata at a high\n1:42:04 level now let's actually dive into the code example so you can actually see how we set this up in practice and you're going to notice this is super similar to\n1:42:11 the rag Basics tutorial we're just adding in a little bit more complexity so you can understand all the capabilities of working with rag okay so\n1:42:19 per usual what we're going to do is set up our current directories but this time what you're going to notice is instead\n1:42:24 of loading a specific book we're loading a bunch of books and that's because what we're trying to do in the end of this\n1:42:29 tutorial is ask about a certain character and a certain book and we would expect the documents to we would\n1:42:35 expect our retriever eventually to only respond with documents related to that character so you'll see exactly what I'm\n1:42:42 talking about we're going to ask about and Romeo and Juliet so we would expect to only get back information about Romeo and Juliet and and the metadata related\n1:42:48 to it but you'll see that set up in just a second okay so what we're going to do this time is we're going to start using a different database so this time we're\n1:42:55 going to be using chroma DB with metadata I've already set mine up and you'll set Yours up in just a second but\n1:43:00 the main thing this code is identical to the other one we're just going to check to see hey does this persistent directory not exist if not let's just go\n1:43:08 through that main process we talked about last time which is going to be you know setting up our Vector store if we\n1:43:13 already have the database completely skip it so let's walk through what's happening under the hood when we are setting up this Vector store because\n1:43:18 there are a few differences in this one the main one is to start off is we are going going and searching through each\n1:43:24 one of the files in our books directory and we're going to grab all the files that end in a.txt so we're going to be\n1:43:30 grabbing all of these different books right here and what we're going to do with these book files is we're going to\n1:43:35 iterate through each of them and start copying the same process we did last time except for one tweak where we're\n1:43:41 going to add metadata here's what I mean first we're going to actually set up a new file path by joining in you know the\n1:43:46 name of the book file with our local books directory then we're going to load that book just like we did in the last\n1:43:53 example but what we're going to do now is once we have that document loaded we're going to add some metadata to it\n1:43:58 in this case we're going to set up this source for this document so whenever we're loading for example the\n1:44:04 Frankenstein book we're going to say the source of this book is Frankenstein or whenever we're looking at the ilad what\n1:44:10 we're going to do is we and you know retrieve a document it's going to say the source of the retrieved information\n1:44:15 comes from the ilad and you could definitely get a lot fancier with this you could break up your documents and set it up to be per chapter or per you\n1:44:23 know if you had a long meeting you could set it up to intro you know speaker one speaker two you could do it however you\n1:44:29 wanted to just so whenever you're setting up your metadata you could have like chapter and so forth and so forth\n1:44:35 but in this example we're just going to stick to a book Source cool once we have set up our metadata we're just going to add it to our documents array or list\n1:44:41 and then from there we're going to copy the same process first we're going to iterate through each one of those documents that we have and we're going\n1:44:47 to use our character splitter to chunk everything up into 1,000 tokens once we have our Docs we're going to do the\n1:44:53 exact same thing where we're going to pass in our docs plus our embeddings over to our Vector store down here so\n1:45:01 that we can go ahead and save everything okay so I hope this is making sense because now we're going to hop to 2B so we can actually start asking questions\n1:45:07 to our Vector store and pull out documents so what we're going to do in this example is once again set up our\n1:45:13 file directories and make sure we point to that new Vector store that we just set up the one that's going to be having\n1:45:19 metadata so this one right here and what we're going to do is we're going to use our our new embeddings plus our Vector\n1:45:26 store so that we can instantiate our new chroma instance so we can start passing questions to it now this Vector store\n1:45:33 retriever is the exact same as last time all we're going to do is just use similarity score I want the top three\n1:45:38 result and here's my threshold I set it super low for this one just cuz for whatever reason it was being a little\n1:45:43 weird however let's go ahead and run this code so you can see what it's doing under the hood so we will do python we\n1:45:50 are in the rag module this is the second exle to be so we're going to go ahead and run it and what I would expect to\n1:45:56 get back is information on how did Juliet die plus the metadata okay so you\n1:46:01 can actually kind of see it right here like in document number two Juliet's\n1:46:06 talking she obviously thinks Romeo killed himself and out of true love down\n1:46:12 here she stabs herself so like you can like it actually went ahead and just gave us the exact piece of information\n1:46:18 we were looking for which is crazy that it did that so efficiently and the part that's EXT cool is you can see this is\n1:46:24 the source where this information came from so you can go back and double check the AI to make sure it didn't just come up and hallucinate with some some weird\n1:46:31 facts yeah that's how adding metadata to your rag queries works at a basic level\n1:46:36 I hope you guys appreciated that and what we're going to do next is actually start doing some deep dives into each of\n1:46:41 the elements that we have been using to basically do our rag queries so we're going to head over next and do a deep\n1:46:48 dive into text splitting then we'll you know keep going down the chain from there so you can see all As specs and\n1:46:53 how we can add in some variety to our rag queries so let's go ahead and move over to part number\n1:46:59 three so welcome to example number three inside the rag module this example is\n1:47:04 all about going deep into different options when it comes to using the text splitter so what I what I mean by that\n1:47:11 is there are a bunch of different ways you can actually go about splitting up your large documents and you're going to\n1:47:16 in this example explore all of them so how to split it up based on characters sentences you know paragraphs and more\n1:47:22 we're going to be doing all that in this example and we're actually going to be spinning up a bunch of different Vector stores so that you can see how all of\n1:47:29 the different text splitting methods compare contrast when we actually make a query so you'll see exactly what I'm\n1:47:35 talking about here in just a second and per usual we're just going to start at the top of the code work our way down so that you understand everything that's\n1:47:41 going on all right so let's go ahead and dive into the code so the first thing that we're going to be doing is setting up our basically our normal file\n1:47:48 directories so we're going to just say like Yep this is the book we want to analyze and here is the path to our\n1:47:54 databases and you'll notice that we don't have a specific database called out yet and that's because we're going\n1:48:00 to be dynamically spinning up about four or five Vector stores down below okay so\n1:48:05 what we're going to do just make sure everything exists and we're going to do the normal you know kind of set up our\n1:48:10 normal loaders this time we're just going to load our book Romeo and Juliet grab the document and stand up and\n1:48:16 specify our Vector store or sorry our embeddings now here's where things get interesting what we're going to do as\n1:48:23 you can see I have I think five different examples to show you guys and each one of these examples kind of\n1:48:29 specifies which text splitter we're going to be doing and when it's useful to use this text splitter just so you\n1:48:35 guys always have a reference to come back to of like oh yeah I think it was text splitter number three that was good for this scenario and you can always\n1:48:41 just come back and actually explore but here's what's happening at a high level and then we'll dive into this Vector store function that's a little bit kind\n1:48:47 of it's it's different so what you'll notice is we're going to specify a specific type of text splitter along\n1:48:54 with the parameters we want to use and after that we're then going to actually go ahead and split up the document and\n1:49:00 once we have all those split up chunks what we're going to do is then call this create Vector store with all of our\n1:49:06 chunks and then we're going to give this database a name and once we do that and pass over that information we're going\n1:49:12 to call this create Vector store and what you can see in here is we're going to finally specify our persistent\n1:49:19 directory here and then actually go ahead and Save save everything to our new chroma database so that's exactly\n1:49:26 what's happening under the hood okay so let's walk through each one of these and I'll tell you when you should use each\n1:49:32 method and then at the very end we're actually going to go off and query the vector store okay so to start off the\n1:49:38 first type of splitting we're going to do is just the normal character-based splitting in the past what we've been\n1:49:44 doing I'm just going to show you what we've been doing for 2A in the past so you can see what we've nor been doing is\n1:49:50 character based splitting so this is nothing new this time we're just adding some overlap and you can see this is a\n1:49:56 great method to use when the type of your content doesn't really like there's no syntactical kind of meaning for\n1:50:02 example like if you're reading a book we talked about maybe keeping paragraphs together because we kind of want to like keep chunks of information together so\n1:50:09 this is just great when we just need to Chunk Up and split a bunch of random information the next example I want to show you guys is sentence-based text\n1:50:16 splitting so this one is when we actually want to split Things based on you know a sentence so this way you know\n1:50:21 maybe pair graphs are too long so we're just going to split everything based up on sentences so we're going to look for\n1:50:27 you know periods explanation marks question marks so and that's how we're going to split things up and once again\n1:50:32 we're going to be splitting things into chunk sizes of a thousand we're going to call this our chroma sentence then keep\n1:50:39 chugging along the next one is going to be our token text splitter and so this actually splits based on a token so in\n1:50:46 this one what's going to happen is we'll actually maybe split in the middle of a word so if let's say a word word was\n1:50:53 super long and you know maybe the first part of that sentence was right over the\n1:50:58 the chunk limit well half of the words going to get cut out so I would not recommend this for a lot of you know a\n1:51:05 lot of like documents based but I just wanted to show you guys that this does exist all right the next one that I just\n1:51:10 want to show you guys is going to be the recursive character splitter this is the one that I would actually most people use I've seen people use this the most\n1:51:17 and I would definitely recommend using number four the most cuz like it tries to naturally split around sentences and\n1:51:22 paragraphs within the character limit so it does a really good job of like making sure the information that gets pulled\n1:51:27 out just kind of it makes sense it would make sense for this cluster of information to be stored together okay\n1:51:33 cool and the final one I just want to show you guys is going to be custom teex splitting and all we're doing with just to show you guys we can make our own\n1:51:39 Splitters and these Splitters you can specify like oh I want to split chunks based on at the end of a paragraph so\n1:51:45 this is what this you know double backspace it's just a it's a double Escape or two new lines so in our case\n1:51:50 we're assuming that's a new paragraph okay cool and we're going to save that custom Tech splitter to our chroma DB\n1:51:56 custom fantastic so what we're going to do next I'm just going to show you is for each one of our databases that we just sped up Vector stores we're going\n1:52:03 to go off and query those Vector stores with this question how did Juliet die\n1:52:08 and the way we're going to do that is we're going to pass in the name of the vector store along with our query to this function right here and all it's\n1:52:15 going to do is look up our persistent directory that's going to be our Vector store and we're just going to do the\n1:52:20 normal thing that we've done so far to where we have have our Vector store we set up our retriever set up you know our\n1:52:26 search parameters for similarity score and then we're just going to grab the relevant documents and spit them out so\n1:52:31 that's exactly what we're going to do so let's go ahead and start running this query to embed everything first off using the different text spits and start\n1:52:37 asking questions so let's go ahead and run this and this is our third example so let's go ahead and do this is in the\n1:52:43 rag module and this is example number three so let's go ahead and run this super fast and this one will take longer\n1:52:49 because we're doing five six different types of settings but you can see that it's working we're creating everything\n1:52:55 so far we're based doing it for characters next we're going to do it based on sentences tokens and then from\n1:53:00 there our recursive and then finally we're going to do our custom text splitter and once this is all done we're going to then finally ask you know hey\n1:53:06 so how did how did she die and you can see let's just go result by result real fast just so you can kind of compare\n1:53:13 each one so the first one for character this is what we've been using so far and this does look super similar to what\n1:53:19 we've been doing it's just a lot of text from there what you can see when it comes over to the sentence one this one\n1:53:25 actually looks a lot more structured and then finally let's just keep going along when it comes to our token what I would\n1:53:31 expect to see at the very end of this one is like it get cut off at a random word so in this case it got cut off\n1:53:37 right at my but if this had been a longer word it would have got cut off and we wouldn't have been able to like you know we could have potentially lost\n1:53:42 some information when it comes to our recursive text butter you can see that this has done a really good job you know\n1:53:49 cutting at the very end of a of a sentence so that did a really good job or verse I can't remember what happens\n1:53:55 inside of a how Shakespeare wrote and then finally when it comes to doing our custom DB it didn't it you know it kind\n1:54:02 of just the way it chunked it it just didn't do a good job and that's because the way we sped up to split based on two\n1:54:08 new returns it just didn't do a good job of grabbing information but the main thing I just want you to learn is there's a ton of different ways that you\n1:54:14 can actually go about using different text Splitters to grab information my recommendation for you is to always use\n1:54:20 the recursive text splitter whenever for you're starting out and then you can get a little bit fancy using the token and\n1:54:26 text splitter and some of the sentence ones but just main thing always use the recursive but enough of that let's go\n1:54:32 ahead and dive into the next module where we're going to start talking about the different types of embeddings that you guys can use inside of your your rag\n1:54:38 setup so let's go ahead and dive into example number four right now so welcome to example number four\n1:54:46 inside the rag module and in this example we're going to do a deep dive into embedding just so you can see\n1:54:52 what's possible okay and just to give a little bit of background so far we have strictly used open AIS embeddings and in\n1:54:59 this example I'm going to show you how you can start to use custom embeddings why you'd want to do that and show you some other models inside of the open AI\n1:55:06 embeddings that you might want to use okay so let's quickly run through the code so you can see what's going on and\n1:55:11 then at the end we'll run it so you can actually compare and contrast two different results using two different embeddings so you can see which one\n1:55:18 works better and which one doesn't okay so what we're going to do is our per usual we're going to load in a book that\n1:55:25 we want to ask questions about and what we're going to do in this one uh per usual you'll see we have a database\n1:55:30 directory because we're going to end up creating two different Vector stores in this example and after that what we're\n1:55:35 going to do is per usual go ahead and load our document and we're going to split it up in this case we're just\n1:55:41 going to use the character splitter and once we have that set up what we're going to do is we're going to end up\n1:55:46 using this function called create Vector store more than that in a second the important part is I have two different\n1:55:53 embeddings down here so let's walk through each one super fast so the first one is once again like I said we're just\n1:55:59 going to be using the open AI embedding and so far we've been using the let me\n1:56:04 go ahead and just show you the exact one we've been using we've been using it's called text let's see text embedding\n1:56:10 three small this is the embedding that we've been using the entire tutorial and now we're branching out and we're going\n1:56:16 to be using the Ada embedding okay so let's actually go ahead and look how these compare price wise and actually\n1:56:23 look at what the different options you have so over here on the open AI website you can actually look up embedding and\n1:56:30 you can see there's a few different examples so this one is going to be the Ada example version two this one is one\n1:56:36 of the more pricey ones and you can actually go under the hood and and look at the different options they have so\n1:56:43 Ada so you can see if you scroll deeper down they have different qualities so the adaa example has it performs a\n1:56:51 little it it doesn't perform as well as some of the other options but you're able to add more data compared to the\n1:56:59 large model so it's all a procon but personally I just go with a small model for most of the stuff I work on just\n1:57:05 because it's the cheapest and you get really good results but the are other options and I def want you to be aware of that okay all right let's hop back to\n1:57:11 it so what we're going to do in this case is we're going to create a new open AI embedding using this new model and\n1:57:18 that's option one and like I said these open a Bings they're best just for general purpose with really good\n1:57:24 accuracy and then but remember you have to pay because this is all happening on\n1:57:29 open AI servers now option two what this embedding is going to be is we're actually going to download an embedding\n1:57:36 model from hugging face hugging face is just a like I'll just go ahead and show you guys what it is so you can see it in\n1:57:43 action so let me pull up these models for yall real fast so as you can see whenever we head over to hugging face\n1:57:48 which is just a great place to go ahead and download models that other people have created ated locally on your machine so you can run them completely\n1:57:54 for free so you can see there's a bunch of different models here and we can actually search for embeddings we\n1:58:00 actually already searching for beddings and you can see there's just a ton of them yeah dozens but in our example what\n1:58:05 we're going to do is use one of the more common ones which is just going to be a sentence Transformer and all it's going to do is its main thing is it's going to\n1:58:13 like I said it's basically just going to run locally and perform in beddings personally I don't really use these that much I just want to show you what's\n1:58:18 possible the main Pro and the reason why you would want to use one of these models from hugging face is because you\n1:58:24 can run it locally for free which is one of the biggest benefits it's usually not as performative so you're kind of\n1:58:30 trading performance for cost so it really just depends where you fall on that Spectrum okay so what we're going\n1:58:35 to do next is now that we have gone off and specify the two different embedding models that we want to use and the\n1:58:41 different names we want to call these different models for our Vector stores we're going to go ahead and create those databases now you've seen this before\n1:58:48 what we're going to do just pass in the the embedding and this is going to just passing right here so we're going to set up the vector store name and then we're\n1:58:54 going to say which embedding to use and that's how we're going to go off and create our two Vector stores one for open Ai and one for hugging face now it\n1:59:01 is important to mention whenever you do use this embedding because it is running locally on your computer you're going to download a pretty big file I think it's\n1:59:08 half a gigabyte but that's how big the file is to perform your embeddings locally so it just beware if you're\n1:59:14 going to download this code and run it it does uh take up a good bit of space and might be even more okay so what\n1:59:20 we're going to do next is we're going to do our normal comparison where we're just going to ask a question to our\n1:59:25 Vector stores and we're just going to compare the results of open AI to our hugging face example so let's go ahead\n1:59:31 and go ahead and open this up and we're going to run our fourth example so four\n1:59:37 because we're in the rag module fourth example going to go off and run it now what you'll notice is I've already gone\n1:59:43 ahead and actually created these models in the past so it's not going to generate a new hugging face Transformer\n1:59:50 cuz I've already done it in the past but you will see that we are going to be able to query these documents so in our\n1:59:57 open AI example output let's see what words we get back so we didn't actually\n2:00:03 get a good example in either one of these so what we can do is update our Vector store and we can actually update\n2:00:10 our K to get back more results clear it run it again and this time we can actually hopefully search to see if we\n2:00:17 got back any information about OD deus's wife who in this case is penelopy and it's so funny because sometimes you'll\n2:00:22 get back an example that does include the right information so you can see now it just changes per run so you can see\n2:00:28 now in our hugging face example we got some information about Penelope we got one of the documents contained\n2:00:34 information about her and then if we go back up you can see that our open AI\n2:00:40 example came back with a few more examples talking about Penelope so overall they both got towards the right\n2:00:45 answer it just open a returned back a lot more relevant information so kind of what we expected it's more performative\n2:00:51 at the cost of spinning a little bit more M on it okay cool enough with doing our embedding Deep dive what we're going\n2:00:57 to do next is we're going to move over to working with retrievers and learning about different search types and\n2:01:02 different arguments we can pass in to our retrievers and this is really going to help up our game when we're working with different rag applications all\n2:01:08 right let's go ahead and head over to example number five so welcome to this fifth example in\n2:01:15 the rag module in this example we're going to be doing a deep dive into retrievers and understanding all the\n2:01:22 different ways we can actually update how we search for different documents inside of our Vector stores so that's\n2:01:27 the main goal of this quick Deep dive okay so what we're going to do as a high level just to also understand we're going to by the end of this example\n2:01:34 showcase how by fine-tuning different search parameters and our retrievers how we're going to get different results so\n2:01:40 you're going to learn more about different types of search queries and different ways to basically you know pass in custom parameters for those\n2:01:46 different types of searches and we're going to be able to at the end of this compare and contrast the different results okay cool so let's quickly run\n2:01:52 through this one so the main thing we're going to do our normal part of setting up a accessing our Vector store in this\n2:01:59 case we're going to reuse our old Vector store which included all of our metadata so this is one that had all of our books\n2:02:05 with the different sources outside that we're going to use our usual embedding model we're going to spin up our chroma\n2:02:11 DB instance and then from here what we're going to do I'm going to close that out and we're going to come back to\n2:02:16 this query Vector store but what you can see is we're trying out three or four\n2:02:22 different examples of searching inside of our Vector store so what you'll notice is let me just show you like a a\n2:02:28 compare and contrast so the first thing is we pass in the name of the vector store that we want to search the query\n2:02:35 so this is like hey what question do you have what type of embeddings are we going to use and then here comes the\n2:02:41 important part we're going to pass in what type of search basically what type\n2:02:46 of search model we want to use and then any parameters so let's actually hop into this query Vector stor you can see\n2:02:52 all this in action under the hood so like I said you can see the search type and the search type parameters right\n2:02:57 here so we're dynamically spinning up and creating our retriever because that's the main area that we're focusing\n2:03:03 on in this tutorial we want to compare and contrast all the different options and not only do we want to like access\n2:03:09 these retrievers we want to go off eventually and fetch information with these different types of retrievers so\n2:03:15 that we can compare and contrast the relevant documents just so we can see like oh yeah this search type performs better than this search type okay so\n2:03:22 let's hop back down to our three examples below so that you can actually see how they work so so far what we've\n2:03:28 been using is and the rest of our code has been let's scroll down so I can show you we've been using similarity score\n2:03:34 with threshold basically so this is how we say yep I want to get the top three results and I only want to get back\n2:03:40 information that's over this score threshold that way we only get back relevant data however sometimes you\n2:03:46 don't want to actually do that threshold scoring if you know for a fact like yes every question I'm going to be asking is\n2:03:53 relevant to that database you really don't need the threshold so this is a very quick way to say yep here is my\n2:04:01 data store I want to just grab all the similar results don't care how relevant they are I but I do want to grab the top\n2:04:08 three results that's exactly what's going to happen with this similarity score and there's no way of really filtering out the like oh yeah this\n2:04:14 one's not that relevant okay the next one that we want to do is working with\n2:04:19 the max marginal relev an search query so what does this one do why is it\n2:04:25 important well what it does is it tries to not only get the most relevant information but it tries to kind of\n2:04:31 spread out meaning if for example how we're going to ask about questions about\n2:04:37 Juliet's death well what this type of search score would do instead of sorry I'm going compare and contrast with this\n2:04:43 similarity score let's just say all the top three documents about her death were all right next to each other so it's\n2:04:50 just going to grab all of that information and maybe leave out some additional context and what's cool about\n2:04:56 this Max margin result is not only is it looking for super relevant information but it's kind of also looking for\n2:05:02 adjacent information around her death so it's not just going to be like yeah she died because of blank it's going to\n2:05:08 maybe skip around to like a few paragraphs later that's also talking about her death but it might not be the\n2:05:14 most relevant information to help generate a more well-rounded response so very cool search method and you can\n2:05:20 actually see Let's uh scroll down here so you can kind of see so yeah so K is how many different responses we want to\n2:05:27 get fetch K specify the number of documents to initially fetch so this is you know how we can actually kind of\n2:05:33 specify like we're going to grab a ton of documents and then inside of that we're going to return the top three and\n2:05:38 we're going to like space out those results so you're going to get a well Diversified set of results so this is\n2:05:44 what you can see kind of right here so feel free to play with this Lambda multiplier like I said the main thing it\n2:05:50 does is control the diversity so if you want a ton of spread out information to get like as much wide range of context\n2:05:56 around the topic as possible you're going to want to bump that number to zero to get the maximum diversity and if\n2:06:01 you want super similar information keep it to one okay cool so enough of that I'm excited to show you guys this one\n2:06:07 actually running so you can see how it performs and the final one this is the one that we've been using the entire time which is just our similarity score\n2:06:15 with a threshold okay so let's go ahead and actually run this so you can see how it performs and we're going to actually\n2:06:20 this time because there's going to be so much information um over here in my terminal and we'll look at the results over here so what we're going to do is\n2:06:27 run python this is in the rag section and we are on the fifth example so we're\n2:06:33 going to go ahead and run this code and it's going to go off and actually oh my bad I need to actually load in my\n2:06:38 environment variables I didn't do that real fast we'll just quickly fix that on the fly so we'll do load oops\n2:06:46 load. EnV and then once we have that we'll come over here and import it fantastic that was my bad save it now\n2:06:53 when we run it again it'll work and it'll actually go ahead and show all the different results for the different Vector stores okay so let's run through\n2:07:00 this part by part just so you can kind of see how it works so the first example that we're going to look at we have to\n2:07:06 come all the way up just because there are a ton of different examples so the first one is just the regular similarity score so you can see it is going to\n2:07:13 return three different documents that are the most similar to our initial quest which was how did Juliet die so\n2:07:20 we're going to get back a ton of information and then hopefully one of these will actually include the way that she died\n2:07:25 yeah so she stabs her health so yeah you can see document two include the exact pieces of information that we needed so\n2:07:31 the next one that we were wanting to do was the max marginal relevant score and this is the one that allows us to get\n2:07:36 not only the piece of information we want but some of more of the contextual information around her death so let's\n2:07:42 actually see if this result includes anything around her dagger so this one does talk about you know laying her\n2:07:48 dagger down and it also yeah this one didn't perform as well as the other one\n2:07:54 so this would be a good way for us to go off and actually potentially like oh maybe let's just get some less diverse\n2:08:00 information so we really hone in so like I said everything's a game and you have to optimize and tune those parameters to\n2:08:06 get the results that you're looking for and the third one which is our usual one which is hey let's go off and actually\n2:08:12 get everything that's within a similarity score this one is going to return per usual the one where she she\n2:08:19 snatches a dagger and stabs herself so like I said there's a few different ways you can work with these different\n2:08:24 retrieval messages retrieval methods to experiment with grabbing information from your vector store so I just want\n2:08:30 you guys to be aware of a few of the most common approaches but enough of that what we're going to do next is Hop on to the next example where we're\n2:08:37 actually going to not only be retrieving information now but we're going to actually ask a one-off question of like\n2:08:43 hey what happened and we're going to get an AI generated response let's go ahead and dive into this example\n2:08:50 next so welcome to example number six inside of the rag module I'm super\n2:08:55 excited for this example because you're finally going to learn how to tie all the information that we've been storing\n2:09:00 in our Vector store we're going to be grabbing that information and passing it over to an llm so we can actually\n2:09:06 finally generate an AI generated response so I'm pumped for you guys to see it so now that you know what we're\n2:09:11 going to be doing let's actually dive into the code so you can see how you can start chatting with your data okay so\n2:09:17 per usual what we're going to do is we are going to go off and grab our Vector store location per usual we're going to\n2:09:24 be chatting with our Vector store that has all the different options from all the different books what we're going to\n2:09:29 be doing is use our usual text embedding from there we're going to spin up an instant of our Vector store from there\n2:09:35 we're going to be passing in a new question and this one is just going to be hey how can I learn more about Lang\n2:09:40 chain and now that we have our query we're going to set up a new retriever in this one we're just going to use the\n2:09:47 similarity search method which is going to like I said just how we just learned about it's just going to grab the most\n2:09:53 relevant document and it's going to return only one result and from there what we're going to do is use the\n2:09:58 Retriever and go ahead and search our Vector store for that query and return relevant documents so it's important to\n2:10:05 realize that this is a list of documents now in our example we're actually going to print it so you can see the\n2:10:10 underlying document but here's where the magic happens what we're doing under the\n2:10:15 hood is we are generating a r query and we're not only going to just use our\n2:10:21 query we're going to pass in the content from our Vector store that we just pulled out combine them and we're going\n2:10:27 to use our chat models to generate a response so this is going to be awesome and let me just walk you through the\n2:10:33 prompt so you can understand what's happening at a high level so the first thing is we are creating our prompt and we're going to say you know here's some\n2:10:39 documents that might help you answer this question here's the question now here's the relevant documents we could\n2:10:44 have probably used a prompt template to do this and you'll see we're going to do some of that later on but we could have even done it up here and then what\n2:10:51 you're going to see is we're going to join basically we're just going to do some string manipulation here to where we're going to grab all the content from\n2:10:57 our relevant document and really just spit it out in a nice text format and if you remember each one of these documents\n2:11:04 because we set this up earlier when we were doing our embeddings each one of these is going to be a th tokens long so\n2:11:09 we're going to have plenty of space to pass in this information into our query because hey we're only getting one result so 1K tokens and we have up to\n2:11:17 eight so 8,000 tokens so what we're going to do is once we have this combined input we are going to spin up\n2:11:23 our new chat model and we're going to use the latest version of open AI so ct40 and we are going to go ahead and\n2:11:30 generate our messages and what what we're going to do is pass in our messages with our combined input\n2:11:36 containing our query and all the information from our Vector store into our model and we're going to invoke it\n2:11:42 and what this is going to do is generate a result for us which is going to contain the AI response and for this\n2:11:48 example let's just only generate the content only so you can just see the actual like response that it's going to\n2:11:53 say like well if you want to learn more about langing chain here is my recommendation so what we're going to do\n2:11:59 is go ahead and run this is module number four for rag this is example number six for a one-off question so\n2:12:06 let's go ahead and run it so what you can see is it'll hopefully spit back some relevant documents and then at the end it'll do the AI generated response\n2:12:13 with information from our documents so this is so cool so let me just show you the AI generated response first and\n2:12:20 we'll hop back up to the relevant doc so I was a little bit uh a little cheeky and what I did is I put in a document\n2:12:26 about myself inside of the book section and you can see I have something oh\n2:12:31 where did it go for you guys yeah Lang chain demo and what you can see in here is you know hey if you\n2:12:37 want to learn more about Lang chain you can go over here to the official documentation or if you want more\n2:12:42 in-depth tutorials and insights on Lang chain check out my YouTube channel and uh here's a link to it don't forget to\n2:12:49 like And subscribe especially if youve made it this far this video don't forget to uh to like And subscribe if you want to learn more about AI yeah as you can\n2:12:55 see it's generating an AI response that officially responds to our query in a conversational way and you can see this\n2:13:02 is the exact document text that we were able to like manipulate and turn and use for our AI response and if we actually\n2:13:10 look inside this text demo you can see I put a bunch of information and we got back the part in our response for\n2:13:17 further exploration yeah so in our the way just the way the chunking worked is we got this piece of text and yeah so I\n2:13:23 hope you guys think that is super cool for our oneoff example what we're going to do next is I'm going to show you how\n2:13:29 you can have a full-on conversation with your rag data so let's go off and start working on example number\n2:13:36 seven so welcome to example number seven inside the rag module this is definitely\n2:13:42 the most complicated example in this section however it is the most useful\n2:13:47 one and probably the example you'll use the most often when ever actually building out a rag solution inside of\n2:13:53 your applications for your users okay so what we're going to do in this one is we're going to quickly speed through all\n2:13:59 the parts that are similar and then we're going to focus on the parts of this code example that are different that allow us to actually have a\n2:14:05 conversation with our rag application so that we can you know ask a question follow up with it get additional\n2:14:11 information from our Vector store and just keep going so that's what we're striving to do here in this example so\n2:14:16 let's go ahead and dive in so in part one what we're doing is per usual we're just spinning up a Vector store that\n2:14:21 points to all the books we've defined what we're going to do is we're just going to use our usual retriever which\n2:14:27 is just going to be a similarity one to get the top results for this example we're just going to use CH gbt 40 as our\n2:14:32 llm okay so now let's dive into this part of the code that's all new so the main thing that we're trying to do here\n2:14:38 I think it's best if we actually start at the bottom to understand what's happening so what we're trying to do is\n2:14:43 we are trying to set up this retrieval chain all this is going to do is we want to be able to retrieve information from\n2:14:50 our Vector store we want to have awareness of all the conversations we've had up to this point and based on the\n2:14:56 information from our Vector store and our conversation we want to generate a result that is all we're trying to do\n2:15:02 and that's what's happening under the hood okay so how is this happening well there's a few things that need to happen\n2:15:09 part one is we need to be able to grab information from our Vector store well\n2:15:14 how are we going to do that well we're going to be using this library and function called create stuff documents\n2:15:21 chain what does that mean I know it's a weird term but basically what it's doing under the hood is it makes a chain for\n2:15:28 us that will take in a list of documents and pass it to a model so that's what it's doing under the hood okay so we\n2:15:35 have documents and we need to feed those over to an llm that's exactly what this Chain's going to do for us so cuz we\n2:15:42 have open Ai and now you'll be like okay well where do these documents you know where does this conversation and\n2:15:47 documents come from well let's keep working back up so next is to create this document chain we also need to\n2:15:54 provide some information about like what the heck's going on so that our llm is aware of what it needs to do well this\n2:15:59 is where our we are going to make a prompt for us we're just going to call this the QA prompt so you know question\n2:16:06 answering prompt and we're just going to say hey you're an assistant who does question answering use the following pieces of retrieve context to answer the\n2:16:12 question if you don't know the answer just say you don't know use three sentences maximum and keep the answer concise so we're really just going to\n2:16:18 say hey here's the Act exact piece of information you need to know from this rag query Okay cool so this prompt is\n2:16:25 then you know we're going to use our prompt templates back from module number two to pass in our basically just pass\n2:16:32 in our QA system prompt here to say this is your system context this is what you should be doing and then we're going to\n2:16:38 pass in our chat history our chat history will happen later on and that's as we chat with the our llm we're just\n2:16:44 going to slowly build out a list of messages and then finally we're just going to continually add in the human inputs every time the human has a\n2:16:50 question it's just going to get passed in as an input right here okay cool so now you understand how we have this\n2:16:56 question answering chain so this is going to kind of set up the whole part where we're like responding to questions\n2:17:02 now we need to have what we're going to do and call the history aware retriever so what is this well the history aware\n2:17:10 retriever this is where we're actually going to start passing in and working with our Vector store retriever remember\n2:17:17 retriever is how we interface and pull information from our Vector store and we're also you'll see here in just a\n2:17:22 second but we're going to do something very similar to where we're going to have our our AI our llm in this case who's going to be doing the thinking and\n2:17:28 generating some responses we're going to have a retriever which is pulling down the information from our Vector store and then we're going to have a prompt\n2:17:34 which is kind of sets the scene for what's going to happen next so let's go part by part so this all makes sense\n2:17:40 okay so for this history aware retriever we'll go start with a prompt so this prompt is saying like hey you're going\n2:17:45 to have some chat history and it's up to you to basically interpret what's being\n2:17:51 said and not answer the question but just kind of provide context for what's going on so this is all you're doing\n2:17:58 it's just you it's up to you to reformulate the questions so that we can properly search for information inside\n2:18:05 of the vector store so that's all we're doing you're going to get a question just rephrase it for the vector store so\n2:18:10 that we can retrieve the proper information so I hope that makes sense and then let's just go back and look at the other parts uh the retriever we've\n2:18:17 already set up we're just going to get the similarity score and we're just going to grab the top three result results and then when it comes to our llm we're just using chat GPT 40 okay so\n2:18:25 now that we have all of these different things set up we can actually go ahead now that we have our full retrieval\n2:18:31 chain that has two parts our history aware retriever which is pulling information from the vector store and we\n2:18:37 have our qu question answer chain which is actually like taking in user input and actually responding and generating\n2:18:44 answers to our users now that we have this entire retrieval chain set up we're going to call it our rag chain what we\n2:18:50 can do is start having a conversation with our llm and our Vector store so\n2:18:55 this is the exciting part so I hope you guys like it so what we're going to do is start asking uh we're going to go\n2:19:01 ahead and run the program so you can see it in action but I just want to show you at a quick level what's happening first\n2:19:06 we have a chat history and that chat history was important because we were using it up here inside of our\n2:19:12 contextualized prompt just so we can have a historical reference to what's being said okay and we're also using it\n2:19:18 in our question a prompt Okay so so that we're constantly adding messages to this\n2:19:23 so you can see here whenever we invoke our our rag chain we're passing in the users query and we're also passing in\n2:19:30 that chat history very similar to what we did when we were setting up our first conversations with our chat models in\n2:19:35 section one second what we're doing next is once we get the result what we're going to do is just print it out for the user to see and then we are going to\n2:19:42 append and update our chat history so that we have our initial query and we get back the result from the AI and we\n2:19:49 continually just add it in a pin it to chat history so that it's aware and we can have a full-on conversation all right enough talking let's go ahead and\n2:19:56 dive into the example so you can see that it's working so what I'm going to do is just go ahead and open up the terminal clear everything out and start\n2:20:01 our conversation so this is in module 4 and this is example number seven so what\n2:20:07 we can do is go ahead and run it and then you know it's going to start our\n2:20:13 chat conversation for us so we'll ask the same question we did last time how can I learn more about Lang\n2:20:21 chain what this is going to do is go go off and search through our Vector store and actually spit out a you know a\n2:20:27 response to us so yeah hey to learn more you're responding to us concisely like we asked using the information from the\n2:20:32 vector store it's basically consolidating to 3 CES as maximum answering the exact question that's\n2:20:38 awesome I also just finished Romeo and\n2:20:45 Juliet how did uh she die so let's just this is kind of going out out on a limb\n2:20:51 not sure if this one will work I just want to show you guys that we can actually have a conversation with it and we'll come back to my initial question\n2:20:56 just a second so that's awesome hope fingers crossed it'll come back and actually respond like yeah she kills\n2:21:02 herself with Romeo's dagger so the cool part is we have specified I just want to show you something we specified do\n2:21:09 not basically yeah if you don't know the answer just say you don't know and what's cool about this is the fact that\n2:21:15 it's not making up information it's pulling information strictly from our rag documents and going from there so we\n2:21:21 can try and ask something else so who is Brandon again and this is where we're\n2:21:26 actually going to start using some more of the conversational awareness because it's going to have to go Brandon oh yeah\n2:21:32 this is his YouTube channel so we would expect uh we would expect it to refer up to two messages ago yeah Brandon is the\n2:21:38 creator you can find his videos here so yeah so it's kind of doing the the full-on conversation part as well as the\n2:21:44 additional retrieval part where it's actually accessing our Vector store so I hope this makes sense to you guys this\n2:21:49 is like I said by far one of the most complicated examples in this whole module and really this whole course so\n2:21:55 if you have any questions please head over to school it's a preschool community and pop a question take a\n2:22:01 screenshot and say I'm stuck here don't understand it or I've been building this on my own and I keep getting stuck here\n2:22:06 please help and myself or one of the other developers in the community will try and help you get you unstuck but\n2:22:12 enough of this one let's go ahead and hop over to the last module we have which is all about web scraping and\n2:22:18 actually using information we get from the the web in our Vector stores so let's go ahead and dive over to example\n2:22:24 number eight right now hey guys so welcome to the final\n2:22:29 example in the rag module what we're going to be doing in this video is going over two methods for you to go off and\n2:22:36 scrape information from the web and add it to your vector store so that you can start asking questions and interacting\n2:22:41 with the data and what we're going to do is I'm going to show you a basic example first so you can see it's working and show you some of the cons and then we're\n2:22:47 going to go over to using a tool called fire crawl which is is awesome and want to show you how it's going to perform a\n2:22:53 lot better and provide much more better results really okay let's go ahead and dive in and go part byart so the main\n2:22:59 thing that we're going to see for the beginning is same thing per usual we're just going to set up our directories and\n2:23:05 set up our file paths and this one what we're going to do next is we need to start specifying a place where we want\n2:23:12 to start scraping the web in this case we're just going to go on apple.com and what we're going to do is instead of using the text loader like we've been\n2:23:18 using for everything else we're just going to use the web based loader which is going to go off and scrape the\n2:23:25 basically scrape a website for us so it's you know instead of looking through a book and loading it it's going to just\n2:23:30 go over to a website and load the information so that's what's happening under the hood and then everything else\n2:23:35 is going to be identical we're just going to split the text once we have split up the text into all the documents\n2:23:41 we're going to start passing over basically those split up chunks plus our embeddings over to a vector store that\n2:23:47 you can see right here so yeah we're just going to create a new Vector store from scratch and that way we can start accessing it once we have set up our\n2:23:53 Vector store what we can do is start asking questions to it and we're just going to use a similarity score and this\n2:23:58 one we're just going to grab the top three results and you know we're not going to chat with this data at all we're just in this example we're just\n2:24:04 going to get back results of like oh yeah these these documents have the relevant information okay so I'm going\n2:24:09 to run this one we're going to look through the results together and then we're going to hop over to fir crawl after we kind of go over the pros and cons so let's go ahead and clear this up\n2:24:17 and then we're running the you know we're in the module this is the eth example and this is the basic one so\n2:24:24 what we're going to do I'm going to zoom out so we can actually go through the results together so it's going to spit\n2:24:29 out a ton of information so this is just like the first one so you can see that it's going like okay well U we got five\n2:24:37 chunks from the Apple website here's the first one where you can kind of see like oh yeah we're talking about the Apple\n2:24:42 iPhone 15 Pro we're looking at you know just some of the main key phrases and\n2:24:48 we're adding all this information to our database so that's just an example one and then when it comes to actually\n2:24:54 asking questions to our Vector store we said what products are announced it's just spitting back this information so\n2:24:59 it's just like it's saying Apple intelligence it's saying iPhone 15 iPad Pro so it's it's really not like saying\n2:25:05 what's new just kind of spitting out what's on the website and uh yeah just going to keep doing that and just return\n2:25:11 a bunch of a bunch of information not super helpful so but it did work we can actually interact and it did a good job\n2:25:17 of web scraping so plus plus on both of those but the important part is I want to show you example number two which is\n2:25:23 using fir crawl just so you guys can see and compare and contrast the different results now fir crawl it is a free tool\n2:25:30 that you can use however they do have like you know they have some free usage for you however you do have like a paid\n2:25:37 version so let me just show you yeah so they do have like a free plan and a hobby plan but once you see how cool\n2:25:42 they are like like some of the results I think you like dang if you're doing a lot of web based scraping this is the tool for you make sure like super easy\n2:25:49 their key you know key slogan is like hey we turn websites into llm ready data so like you know here's a website that's\n2:25:55 gross oh we'll actually pull like a markdown version of this data for you so that it's easy to use and interact with\n2:26:02 inside of your llms so enough talking about let's actually go ahead and and use it and the one thing I do want to\n2:26:07 point out is once you've signed up a major free account you will have to copy and paste this API key over to your\n2:26:13 environment variables folder that we have or file that we've set up and you'll just go ahead and paste it in there so you'll just update the fir\n2:26:19 crawl and Pi just put your key in your you'll just put yours right here okay cool enough of that let's go ahead and\n2:26:25 actually start running this so you can see how this fire crawl example compares to the other one and the only thing that\n2:26:31 I'm going to just say different uh cuz some of this code is like well this is weird what's what's happening I will run\n2:26:36 through it real fast you know set up your create your new persistent Vector database for your vector store we're\n2:26:42 going to call this one fir crawl and this time because we can't just do web loader which goes off and does everything for us we actually have to uh\n2:26:48 be a little bit more specific using the fir craw loader tool so this is a package that you can install and if you\n2:26:55 actually go back to our py project. tommo you will see that we have where is it down here yeah you will see that we\n2:27:01 have a fir crawl package so this is how we're accessing this tool but hey we're going to use the fir craw loader here's\n2:27:06 our API key so you can validate that we are allowed to do this here's the website I want you to go scrape and they\n2:27:11 have a few different modes ones I think called crawl and one's called scrape crawl just go to a whole website and\n2:27:17 mode goes through like a single page so I would defin I'd use scrape to start off with CU you can easily blow all your\n2:27:23 tokens if you crawl so once you have scraped that website we're going to load all that information like we normally do\n2:27:30 and what we're going to do in this one is we're actually going to chunk through that document that we have pulled out\n2:27:36 and we're going to add a lot of metadata that you'll see later on so it's actually pretty cool this the setup they\n2:27:41 have and then what you're going to do next split everything up into different chunks pass over those chunks plus the\n2:27:48 embeddings over to to a new Vector store and once we have that Vector store set up we can start asking and making\n2:27:55 queries to it okay so enough of that let's go ahead and start you know let's go ahead and start testing it out so\n2:28:01 what we're going to do in this one we're going to do Python and the rag module this is the eighth example and this is\n2:28:06 fir crawl so we're going to go ahead and run it and it'll take a few seconds to go off and scrape the website beginning\n2:28:13 to crawl the website and I might need to update my question just because uh wwc 24 was a little bit ago and uh I'm sure\n2:28:21 a lot has happened since okay so the main thing is we grabbed 14 chunks of information compared to you know\n2:28:28 whenever we were running the other example we only grabbed you know three or five so we're grabbing a lot more information because the fir crawl does a\n2:28:35 much better job of actually you know pulling out information and it actually I don't know how it works under the hood\n2:28:41 but they do a much better job of like getting around like a lot of websites only load HTML whenever you try to use a\n2:28:47 tool like web loader they only give you h HML so you're missing out a lot of the JavaScript code that gets rendered so\n2:28:53 that's how we're grabbing so much more information when it comes to fir craw so under the hood you can see now whenever\n2:28:58 we ask you so you can see in a sample chunk that like hey we grabbed well for whatever reason it chunked it too much\n2:29:04 but you can see whenever we look at like relevant documents we can start to see some of the updated questions that are\n2:29:11 related now it didn't do a fantastic job but let's update it to talk about Apple\n2:29:16 intelligence tell yeah whenever I wrote this first query I made it for uh basically I did it back\n2:29:23 in the day whenever wwc 24 was about to happen so now we're going to do a second question and this time what it's going\n2:29:29 to do is provide information strictly about Apple intelligence so yeah document two all about Apple\n2:29:35 intelligence so like I said much cooler you can actually see a lot more of like\n2:29:41 the links now if you want to go off and actually like interact and actually pull this information and using a sales page\n2:29:47 isn't the best example using things like like Reddit or places like that where you want to actually like pull actually\n2:29:53 a lot of content from like a lot of users conversation that's a great place to try out fir crawl so but enough of\n2:29:58 that I hope you guys enjoyed seeing how you can actually start pulling information from the web you can go\n2:30:04 really deep into a rabbit hole when it comes to this but just for now when it comes to like scraping Basics this is\n2:30:09 plenty to get you guys started but yeah enough for module number four where we learned about Rag and the next step\n2:30:16 we're going to head over to start working with agents and tools and this is going to blow your mind so let's go ahead and\n2:30:22 head over to module number five hey guys so welcome to this fifth\n2:30:27 module in this Lang chain master class in this module you're going to learn everything you need to know about using\n2:30:33 agents and tools and before we dive into actually going through all the awesome code examples that I've set up for you\n2:30:38 guys what we're going to do at a super high level is go through what the heck are agents what are tools how do they\n2:30:44 relate and I will say before I like actually dove in to work with agents for the first time I thought they were going\n2:30:49 to be this super magical thing that I would never understand it was going to be super overwhelming but it really\n2:30:54 wasn't once you U now that you've gotten this far inside the master class and you have a really good understanding of prompts and you have a good\n2:31:01 understanding of chat models and so forth this isn't going to be too complex so stick around I think you guys are\n2:31:06 going to be like oh that's actually pretty easy to understand so let's dive into it what is an agent well under the\n2:31:11 hood an agent is nothing more than an llm that has been provided a specific prompt to guide Its Behavior so the best\n2:31:19 way I like to think of it is a state machine and if you've never heard of a state machine it's basically just a machine that has certain States and it\n2:31:25 can perform different actions at each state so let's just say it's at State one well at State one it can do\n2:31:31 something else and once it's done with that it goes to the next state and basically it will just perform a certain Loop of task and that's basically what a\n2:31:39 state machine is in the same way that's exactly how agents work we are creating a prompt that defines certain States and\n2:31:45 behaviors for our agent to do and what it's going to do is just flow through all of those different states and at\n2:31:51 each state perform a different action so let's walk through it in this actual diagram and we're going through it part by part so the first thing that most\n2:31:57 prompts have inside of an when they Define our agents is they usually have an action State and this action state\n2:32:02 says well this is where you can actually perform an action so think of it if we're creating a writer agent well the\n2:32:09 whole point of that agent is to write so usually we'll say our end goal is to write a bullet list about what's going\n2:32:15 on in the news today well this agent will know it needs to write a bullet list so let's just say it starts off\n2:32:21 with our input and just talks about the recent Apple event and it writes a bullet list well then what it'll go is\n2:32:26 go to the next state where it can make observations about its actions and from here what it'll do is go that was a\n2:32:33 little weird you only produced four bullet points then we'll come back down to the thought and when we are in the\n2:32:39 thought stage this is usually we'll plan out plan is the key word here we usually plan out what upcoming actions or\n2:32:46 behaviors we need to do next so in this case we'll go oh for whatever reason we only had four bullet points I need to\n2:32:52 rewrite my article or my bullet list and make sure I have five bullet points this time so let's add one more and then\n2:32:59 we'll just go back up action we'll update our bullet list and have one more so we'll hit a total of five observation\n2:33:04 phase yep we have all five bullets things look good and then now that we get back to our thought stage you know\n2:33:10 we're going to say everything looks great we look like we're done and our thought phase will go okay cool I no longer need to work as an agent here's\n2:33:16 my final answer and that's a super simplified version but really under the hood that's exactly what's happening an\n2:33:22 agent gets a specific prompt with an llm that tells it how to behave we pass in a\n2:33:27 goal and it'll just work towards that goal and just cycle through these stages and there's a bunch of different types\n2:33:32 of Agents but this is the core Loop that you'll hear about the most right here okay now what the heck is a tool why do\n2:33:38 we care about uh care about it when it comes to agents and I when doing this course at first I thought about breaking\n2:33:43 them up but I didn't just because they're pretty much useless without each other and you'll understand why in just\n2:33:48 a second so a tool is nothing more than a basically some usually some code and\n2:33:54 that provides our agents with additional functionality so basically these tools provide our agents with superpowers is\n2:34:01 the best way I like to think about them we give them upgrades is the best way I like to like kind of think of it my in my head so a few common tools that\n2:34:08 you'll hear of are tools that will allow you to search the internet so think tavali serper duck ducko there's a ton\n2:34:15 of these tools and they allow our agent to interact with the outside world cuz if you remember these llms they have\n2:34:21 constraints they're just you know U they have cut off dates and they basically you know can only think thoughts and\n2:34:28 write text and this is how we allow them to interact with the outside world so that's what the search internet tool can\n2:34:33 do from there we could also set up our agents to actually execute code so we\n2:34:38 can hook them up to an interpreter to like oh here's the python code let's go over here oh yeah you can actually now\n2:34:45 write and draw some you can you know run some code you can generate some graphs with plot Le like you can do a lot of\n2:34:51 stuff when it comes to having your agents interact with code the final one you could also have tools that allow\n2:34:56 your agents to go interact with databases so obviously R so far we've kind of like hardcoded and wired up R in\n2:35:03 the past with in our rag section to work with a vector store but you can actually set up tools to allow your agent to\n2:35:09 interact with a vector store a SQL database like there's a ton that you can do so it's just up to you to create\n2:35:15 those tools and then give those uh tools to your agents and so just give you an example if we were trying to you know\n2:35:22 find today's news find today's like top five most talked about articles and then\n2:35:28 plot them inside of a a python plotly chart for like how correlated they were\n2:35:33 something silly but basically you know we could do that with tools because going through the same Loop we would\n2:35:38 understand first I need to go off and find all the news articles and find the most talked about ones well whenever I\n2:35:45 take that action I'll search the internet come back I now have information about the outside world\n2:35:50 thought hm okay well now that I have information about the outside world I need to start mapping the information I\n2:35:56 found to a to some sort of chart where we can visualize how correlated everything is just you know Silly\n2:36:01 example but when now we come back to the action step and goes okay well I now know the five most talked about articles\n2:36:08 now I can start actually generating code to make a visualization out of this\n2:36:13 information so then I'll start executing code oh NOP it didn't work it didn't properly run go through it a few times\n2:36:20 okay great now everything's working here's all the python code so you can generate some graphics so that's exactly how it works under the hood and you're\n2:36:27 just going to keep going through this core Loop and we've now supercharged our agent with tools to go off and interact\n2:36:32 with the outside world and actually start taking some action so that's enough of everything at a high level what we're going to do in the rest of\n2:36:38 this example is I've provided about five to six different code examples that you'll see where we're actually like\n2:36:43 code everything up that we just talked about so enough talking let's go ahead and dive into our first example so you can see all this in\n2:36:51 action all right guys so welcome to the first example inside of the agent and Tool module now what we're going to do\n2:36:58 inside of this example is just walk through everything at a super high level because this is going to be the first\n2:37:04 time we're going to be introducing agents and tools inside of actual code so we're going to go through it super slowly and then after that we go through\n2:37:11 this example we're going to later do a deep dive into agents so you can understand all the different ways we can\n2:37:16 use them and then we're going to do a deeper dive into tools so you can learn learn how to use existing tools and learn how to create your own so that's\n2:37:22 what we're about to do let's go ahead and dive into this part by part so you guys can Master agents and tools all\n2:37:28 right so some of this is going to be super similar to start off we're going to be loading our environment variables because we need to use open AI now the\n2:37:36 first thing that we're going to do in here is go off and create a tool and for this one we're just going to create a\n2:37:41 super simple tool that allows us to access the current date and time for our local computer CU if you think about it\n2:37:48 our llms you they were generated in the past they have constraints of and cut off dates and they actually have no way\n2:37:55 of knowing what time it is currently so whenever we're actually using these llms in Agent form we can add a tool time so\n2:38:03 that our agents can access our current time this is going to be super helpful as you go off to build larger and larger\n2:38:09 agents where they need to interact so this is just a super basic one so let's walk through what's happening so the\n2:38:14 first thing is we're going to create just a regular function that all it does is it reports date time because date\n2:38:20 time is just a standard library and we're just going to grab this the current time and then we're going to\n2:38:26 return the current time in this format to where it's hour minute minute so that's all we're trying to do and that's\n2:38:31 the whole purpose on this function we'll dive later into why had to set up our parameters like this for the function\n2:38:37 but we'll come back to that later all right cool well now once we have defined our specific function that we want to do\n2:38:43 we have to wrap it inside of a tool and later we'll get into actually like using existing tools but for right now this is\n2:38:49 just like how you can make your own simple tool with your own code but basically all you do is you call the tool class which comes from up here\n2:38:56 blank chain core tools that's how we can create a tool and it's just up to you to give your tool a name a descriptive name\n2:39:02 about like you know what does this tool do that way whenever the agent's executing and it's like oh I need to\n2:39:08 solve a Time problem oh it makes sense for me to use the tool that talks about time so that's why it's super important\n2:39:15 for us to give the name and description to be very representative of what what going to happen under the hood from\n2:39:21 there we pass in the function that we want the tool to perform whenever it gets called on so in this case whenever\n2:39:27 we say like hey give me the time under the hood it's going to trigger this function which is going to return our\n2:39:33 string representation of the current time okay so that's how we make a tool that's how we add it to our tools list\n2:39:39 and once we eventually have our tools list we can actually pass this over to our agents and you'll see that here in just a little bit okay so now we've got\n2:39:46 tools out of the way at a super basic level now we're going to work on creating our agents so if you look at\n2:39:52 this you might be like what the heck are we doing what are we what are we pulling what is this word react well uh react\n2:39:59 stands for reason and action and all we're doing here is we are pulling out a\n2:40:04 prompt because you remember from earlier earlier from prompt templates all we're doing is pulling out a prompt template\n2:40:10 that tells our llm how to act so if you actually go and read what this uh what's\n2:40:17 going to happen at this prompt you'll head over to to a website like this I'll actually copy the link for you guys so\n2:40:22 you can read it yourselves but you can see kind of how we talked about earlier an agent's nothing more than an lolm\n2:40:29 with a super specific prompt telling it how to behave so this is exactly what's happening under the hood this prompt\n2:40:35 template that we're going to create an agent around you know it's just like hey answer the following the best you can\n2:40:40 here's the tools you have access to and then that Loop that we kind of talked about earlier where you're taking action\n2:40:46 you're making observations planning out thoughts and providing a fin answer this is exactly what you know what this\n2:40:52 prompt template is telling you to do so you know hey use the following format to basically perform actions so you know\n2:40:59 taking the input think about it take action here's the you know like as we're\n2:41:04 working with tools sometimes when whenever we're taking an action like get time well sometimes we might have to\n2:41:11 pass it in parameters to our functions so like I want to get the time in Tokyo\n2:41:16 I want to get the weather in San Francisco so that's what actions like you know use the tool and then action\n2:41:21 input is like whatever parameters we want to pass over to it so just hope hopefully that makes sense observation\n2:41:27 hey look at the results of the action thought you know go go off and here's what I need to do next basically plan\n2:41:33 yeah so and you can kind of see we're telling our llm to repeat this over and over and over until we get a final\n2:41:38 answer okay so that's what's happening under the hood and we're actually able to like the part that's nice is it's\n2:41:45 kind of like GitHub where we're able to just pull down and reuse other people's code that's working but in this case\n2:41:51 we're just pulling down other people's prompts which is pretty cool if you think about it okay so now that we have our prompt which is going to tell our\n2:41:57 llm how to act we need to keep chugging along and we actually need to specify which llm we want to create our agent\n2:42:04 around and this time we're just going to use cat gbt 40 that's the latest open AI\n2:42:09 model that's out at the time of this recording and then from there what we're going to do is go off and actually use\n2:42:15 the agent we've defined we're going to pass in the prompt that we fine and we're also going to pass in all the\n2:42:21 tools we've set up to actually go off and create our agent so if you look under the hood what's actually happening\n2:42:28 is this create react agent it's coming from the the Lang chain agents repo so\n2:42:33 I'll just scroll up to the top so you guys can see it yeah this is coming all from Lang chain agents and we're really\n2:42:39 just importing these two classes to create our agents and run them but the main thing for our agents is we're\n2:42:45 sticking to that react part to where we are going to you know reason about what we want to do take action and just\n2:42:51 continue that cycle over and over in order to actually achieve our goals so this is kind of what what it looks like\n2:42:57 under the hood if you want to actually dive in here and read more about what's going on but really we're just combining\n2:43:02 all the ingredients of everything that's needed to make an agent and just putting it into one one class so that we can\n2:43:09 actually start passing information to it and running our agent and that's exactly what we're going to do next once we have\n2:43:14 our agent it's up to us to then pass in everything over to an agent executor so\n2:43:21 an agent executor is just basically going to help manage the Run of an agent as it goes off to solve problems so if\n2:43:28 you come over here you can see an agent executor we're just going to say like hey this is allowing us to use tools and\n2:43:34 and if you actually dive into the code here too you can see this is where we're going to be like actually performing The\n2:43:40 Run and accessing the tools putting information back and forth and and continuing to go from there but really\n2:43:45 don't need to like dive into it the main thing is just understand that like yes whenever I want to run my agent I need\n2:43:51 to use an agent executor Okay cool so once we have our agent executor set up what we can do is once again use our\n2:43:57 magical word when it comes to Lang chain and we need to invoke our agent executor to start actually spinning everything up\n2:44:04 and what you'll notice is instead of just passing in a string we're passing in a dictionary and the keyword in our\n2:44:11 dictionary is input this is the main thing that our agents expect to read is\n2:44:16 an input so in our case what time is it is the question that our agents are going to be answering so enough of that\n2:44:22 what we're going to do is let's go ahead and run it so you can see all of this happen in real time so let's open up our terminal and we're going to run python\n2:44:30 this is the fifth module because we're learning about agents and tools and this is the first example so let's go ahead and run it and see what happens so what\n2:44:36 it's going to do as you go off and actually use agents you'll see a lot of colorcoded text usually coming out of\n2:44:42 the output as it's going through that reasoning taking action and making observations so you'll usually see things like purple green and white and\n2:44:48 this is is uh anytime the agents's performing an action you can see like all right what's the action I want to\n2:44:54 perform Let Me zoom in for you guys so what's the action I want to perform I went to get the time so that's pretty\n2:44:59 cool that it was able to look through all the available tools that it had and pick the correct one and then what it\n2:45:04 did next is it passed in an action input and because we didn't specify any\n2:45:10 parameters for AR tools it just gave in none but you'll notice sometimes when you're creating tools and we'll dive\n2:45:15 into this here in a little bit if you do not have any parameters for your function sometimes they'll mess up so we just\n2:45:21 accept all parameters for arguments and keyword arguments but we just don't do anything with them so that's just a quick work around but what's cool is\n2:45:28 once we you know perform the action pass in the action input you can see here in blue we get the local time back for when\n2:45:35 I'm recording right now and then now that we have that answer from our tool the agent goes I now know the final\n2:45:42 answer and once it knows it's the final answer because if you actually come back over here the thought is I know I now know the final answer it then gives us\n2:45:49 back the final answer so then that's super cool cuz you can see right here it said the current time is 8:31 p.m. so\n2:45:55 that's the agent thinking and then finally under the hood I've said that 100 times right now but what's happening\n2:46:01 is it finally now that it has the answer it generates a object that it returns\n2:46:06 back to us and you can see this object contains nothing more than the original input plus the output of the final\n2:46:12 answer from this agent so that's what's happening and what we're going to do next is um and I hope you guys first off\n2:46:17 I hope you think that is aome awesome because it's very cool that we can have agents reason and take action in the\n2:46:24 world and uh you know everything we're going to do from here we're only going to add in more complexity and show off\n2:46:29 cooler features just so that you guys can match uh Master agents and tools and just to dive in what we're going to do\n2:46:34 next like I said we have two different folders for you guys to go over all the different ways that we can use agents\n2:46:40 and tools and the first one we're going to do is we're going to do a deeper dive into agents so that you can understand\n2:46:47 different ways that we can like figure these agents to work and eventually we're going to hop over to showing you\n2:46:52 how to use more custom tools so let's go ahead and head over to our first agent Deep dive example and start looking at\n2:46:58 react chat hey guys so welcome to the second example in this agent and tools module\n2:47:06 and in this example we're going to go a little bit deeper into working with different types of agents and really\n2:47:12 expanding what they're capable of doing so in this example we're actually going to focus on swapping up the underlying\n2:47:19 prompt that we're using in the agent sprinkle a few more tools and then kind of add some chat capabilities to it so\n2:47:26 that our agents can now go off and do some internet searching for us and give us answers so let's go ahead and dive\n2:47:31 into this example so you can U understand what's going on okay so to start we're going to do the exact same\n2:47:38 thing we did last time but we're going to add in some more tools so this time we're going to do another get current time tool exact same thing and then next\n2:47:45 we're going to add in the search Wikipedia tool and this this is really just a function to go off and use the\n2:47:50 Wikipedia library and what it's going to do is basically get a summary about whatever topic we're interested in so we\n2:47:57 get ask a questions about famous people events times and Wikipedia is going to give us back a quick summary you know\n2:48:03 two sentences of information it has on that topic okay so now that we've defined those functions we now need to\n2:48:09 package them into our tools list so and we need to format everything to be in the proper tool class so that's where\n2:48:16 we're going to set up the name a description so that our agent can go oh yeah that's the tool I want to use and\n2:48:22 then the actual underlying functionality we want to trigger cool so now that we have our tools defined for our agents we\n2:48:27 can now move over to actually creating our agent and what you'll notice in this example is we're using a different type\n2:48:35 of agent this time for our prompt excuse me so last time we were just using the react agent but this time we're using\n2:48:41 the structur chat agent so what this prompt is focused on it's actually having a chat so if we head over to uh\n2:48:49 over here where we actually can see the underly prompt template it looks a lot different than the last one so you can\n2:48:55 see like hey you're responding to a human with helpful information you have access to the tools in our tools list\n2:49:01 and I need you to basically create a a Json blob for these tools to go off and perform actions that's enough of that\n2:49:08 and then what's going to happen next is like all right cool here's you're going to search for a question go off and\n2:49:15 basically have thoughts about it take action so this is where we're going to use that Json blob and this is basically\n2:49:20 going to be like you know information from the past that we've taken like so whenever we get a response back we're\n2:49:26 going to save everything as a Jason blob and I'll keep it at that but the main thing is it's going to allow us to have\n2:49:32 a conversation with our agents and it's going to go off and you know use tools to perform actions on our behalf okay so\n2:49:38 that's what's happening under the hood now what are we going to do next per usual we're going to uh spin up a chat\n2:49:44 model I'll actually go ahead and make sure we're using the right model so we're going to GPT 40 make sure using\n2:49:51 the latest one and then from there what we're going to do is we're going to use this new we haven't used it before but\n2:49:57 it's called a conversation buffer memory all it does is it allows us to store our\n2:50:03 chat history in in memory that's all that's happening so in the past we've done things such as you know we'll just\n2:50:10 do like chat history and we'll save it to a list well this is just a more and\n2:50:15 you know we'll always add in our system messages and human messages well this conversation buffer memory it\n2:50:21 just it does a better job of doing it so uh we're just going to use it for this one it makes things simpler to set up\n2:50:27 for us okay well now that we have everything initialized let's start combining things to go off and create\n2:50:33 our agents so that we can start running them so per usual we're going to create our agent and we're going to pass in the\n2:50:39 main ingredients which are going to be our prompt our tools that we defined in the specific llm we want to use in the\n2:50:45 underlying agent and then from there what we can do is create our agent executor and this is going to be you can\n2:50:52 see we're adding in more tools to our agent executor this time these are the same as last time our agents and tools\n2:50:59 but now we're going to add in memory to our agent executor and this is how we're going to keep track of our previous\n2:51:05 responses and messages with our agent because we're going to be talking to this one and what you can see from here\n2:51:11 is we're now going to now that we have our agent executor been up and it's ready to run what we're going to do is\n2:51:17 start go ahead and start our chat conversation with the user so in our case we're going to start chatting with\n2:51:22 our agent so this is going to look exactly like our initial chat example we did except now we've supercharged it\n2:51:28 with an agent that's not just talking to a vector store it's going off and searching the internet for us to get\n2:51:33 responses so this is super cool and I hope you guys can see the value of it so let's go ahead and hop down here and\n2:51:39 start running our agent and actually seeing how it works so I'm going to make this a lot bigger because you'll see how\n2:51:47 cool it is cuz it's going to actually show us behind the scene what the agent is thinking and before I trigger it I do want to show you guys one thing let's go\n2:51:54 ahead and get the code ready so agent Deep dive and this is example number one the thing that per usual we're going to use the magic word when it comes to\n2:52:01 running Lang chain tools so the agent executor and per usual we do have to pass in the input and you'll notice as\n2:52:08 you use these agents tools a lot more everything's kind of structured as an input and an output so that's how we can\n2:52:13 show the response from the AI we're just going to say yep grab the output and per usual when the user submits a message\n2:52:20 we're going to add a human message and whenever the AI generates a message we're going to add an AI message okay let's go ahead and run it so you can see\n2:52:26 what's happening under the hood so we're going to start off asking a question we're just going to say who is George\n2:52:32 Washington and this will go off and use Wikipedia so that's cool it found the proper tool to use and as you can see it\n2:52:38 passed in the action input of like well who are we trying to query about and it's so cool that the AI went from a\n2:52:46 question like who is George Washington to picking out the core topic of that question and then passed it as the\n2:52:53 action input so it's amazing that this AI can figure out based on what parameters we need to pass in yeah just\n2:52:58 grab the core part of it so that's super cool now in the yellow we're actually getting back a response from our tool so\n2:53:05 our tool from Wikipedia says you know like hey here's everything you need to know about him and so forth and so forth\n2:53:12 and then once we have the final answer it generates a response to us so you can see the bot came back and said yep give\n2:53:19 us everything that Wikipedia said so uh let's see we can actually ask because this is a conversation we can add to our\n2:53:26 original question so how old was he when he died so because this is a\n2:53:32 conversation we can actually refer to our previous messages so in this case it didn't even have to go off to Wikipedia\n2:53:38 this time because it was like oh wait already know the information so I can just use use that to respond and we can\n2:53:44 ask other things such as who is Elon mus and how old is he right now so this\n2:53:51 question is a little bit more interesting because it's a two-part question first you have to go off and figure out who is the main person that\n2:53:58 we're talking about so in this case it's Elon Musk for whatever reason it's kind of struggling right now to to find out\n2:54:04 who he is but usually this is just the part of Agents where it's just going to go in a loop until it finally gets an answer for whatever reason it couldn't\n2:54:10 figure out that one so we'll just ask a different question because I do want to show you guys the key key underlying part who is Steve Jobs\n2:54:19 and how old was he when he died and the main reason I'm doing these two-part questions is because I want to show you\n2:54:25 guys how these agents can understand what we're trying to do yeah and for some reason I think Wikipedia is just\n2:54:31 crapping out on me but the main thing to know is what would happen under the hood normally is these agents would go off\n2:54:38 and find the result for the first part which is who is Steve Jobs and once it knows the answer it would then go off\n2:54:43 and actually find a you know using the response it would then trigger the next part and the question so you can see\n2:54:49 like yep he's an American entrepreneur obviously known for founding apple and he died at the age of 56 so that's\n2:54:56 that's very cool that I can do a two-part question and you can even go deeper to where it triggers off a second\n2:55:01 Wikipedia call but I'm not going to go into that for this example so I hope you guys are like wow these agents are\n2:55:07 powerful they can act on my behalf and go you know find information inform for me and I can just talk to them so I hope\n2:55:13 you find that super cool and what we're going to do next is dive into the next agent example that I've set up for you\n2:55:19 guys where you can actually talk to a document store so we're going to work on this one now hey guys so welcome to the third\n2:55:26 example in this agents and tools module in this example we're going to be diving into how we can connect our agents up to\n2:55:34 a vector store so that they can kind of work together to answer questions about our data so that's exactly what we're\n2:55:39 going to be setting up in this example so let's go ahead and dive in and what you'll notice is a lot of the information that I'm going to be showing\n2:55:45 in this module was built off of everything that we've kind of done in module 4 with Rag and all the previous\n2:55:51 modules before that so the log this is going to look super familiar so I'm not going to dive super deep into it but to\n2:55:56 start off we are going to be setting up all of our file path directories so we can point to our Vector store so this is\n2:56:03 the same Vector store that we did earlier that read all those different books for us so we're going to go off\n2:56:08 from there set up our embedding so that we can actually you know uh whenever we ask a question it we can embed it and\n2:56:16 compare it to all of our other documents to you know grab the most similar answers so we're going to then spin up our Vector store with our Vector\n2:56:22 database and the specific embedding function we want to use and then we're going to use the exact same retriever\n2:56:28 that we used the entire time in the past we're just going to use the similarity one this time which is just going to\n2:56:33 grab the most similar results not worry about a threshold and in this case we're going to grab the three top results and\n2:56:39 if you remember we used a 1,000 tokens per result uh per document so this is\n2:56:44 going to give us 3,000 tokens worth of information for our agent to use all right let's keep chugging along from\n2:56:50 here we're going to start working on creating our agents or sorry llm so in this case we're just going to use Chad\n2:56:56 gbt 4.0 and then now we're going to be kind of combining two different examples so in the past we had an example where\n2:57:04 you could ask specific questions about the doc store to the vector store that we've set up and but now we're going to\n2:57:09 be combining it with our agents so this is the exact same demo we did before where we kind of first set up like you\n2:57:16 know just like contextual what we're doing here in the first place so like hey you are working with chat history to\n2:57:23 solve an answer that's all you're doing and the second part was the history aware retriever so this is like hey go\n2:57:29 look at the previous questions that we've worked with plus you can look at the uh you can use the retriever to go\n2:57:34 off and answer or grab information from our Vector store so this is definitely going to go a little in the weeds but we\n2:57:39 already covered this in detail in our previous rag example right here where we did rag conversation so you've already\n2:57:46 done this in the past we're just now building on top of it okay cool per usual we we're just going off and\n2:57:52 creating stuff from our document chain this is how we can actually if you go under the hood this is how you can go\n2:57:58 about passing a list of documents over to a model so that they can be processed once we' set up all that we can set up\n2:58:04 our retrieval chain which will basically be able to interact with our Vector store okay enough of talking about\n2:58:10 setting up our Vector store now it's time for us to go off and set up our agent so we can have our agent\n2:58:16 communicate with our Vector store on on our behalf and perform actions and and lookups information so in this one what\n2:58:22 we're going to do is we are going to use the same react agent that we did in the first time which is just going to it's\n2:58:27 going to think about stuff take action make observations and just keep performing that in a loop and per usual when we're working with agents we have\n2:58:34 to set up a tool well this time we're going to set up a Custom Tool and what's interesting is under the hood this tool\n2:58:40 is going to do is it's actually going to invoke our rag chain so what's very cool is now anytime our agent has a problem\n2:58:48 where it needs to answer a question what it's going to do is it's going to go well I don't know the answer to that question but you know what I bet this\n2:58:55 tool does because it's useful for whenever you need to answer questions about the context of whatever the\n2:59:01 question is so what's cool is all we're going to do here is this we're going to invoke this rag chain and what we're\n2:59:07 going to do to it is we're going to pass in the input so it's going to be the person's question but we're also going\n2:59:12 to pass in the chat history so that we can have some uh contextual awareness of\n2:59:18 previous messages so that's super cool so now that we have made a new tool for\n2:59:23 our agent we're going to go through the normal process of setting up and creating our agents so that we can\n2:59:28 actually start running them so now that we have all of our agents set up with a proper tooling we can now go into another while loop and in this while\n2:59:35 loop we're going to once again start chatting to our agent so I'm going to come down here we're going to clear\n2:59:40 things up and now we're going to actually start running this example so you can see it in action so we're going to do python this is the agents and\n2:59:48 tools module we're working inside of the agent deep dive and we're going to start using the react doc store so let's go\n2:59:55 ahead and run this one and you can see so we can start asking questions and what's nice already at the gate we are\n3:00:01 you know accessing the vector store so because there was that one file I set up where I talked about Lang chain that's\n3:00:07 the first question I'm going to ask so how can I learn more about Lang\n3:00:14 chain so what it's going to do is it's going to well this is a question I need to get an answer about and then what\n3:00:20 it's going to do is actually trigger the AI to go off and get that information\n3:00:25 and what's cool is like it responded with the exact part of like yeah this is you know to learn more about Lang chain\n3:00:31 that what the document said was like yeah go watch Brandon's YouTube channel here's a link to the YouTube channel so\n3:00:37 as you can see that's pretty freaking cool and then you can follow up because it is a conversation so you can say who\n3:00:42 is Brandon and then it'll it'll go off and actually you know say oh he has a\n3:00:48 YouTube channel where he talks about like AI does not mention the name Brandon oh I guess I should have\n3:00:54 capitalized it so so yeah that's how it works at a super high level when it comes to working with the dock store\n3:01:01 this example was a little weird but I just wanted to show it to you guys just so you can understand like oh yeah you can use you can really start connect\n3:01:07 these agents with different basically different tools and use these agents in different ways and I would do want to\n3:01:12 show you guys real fast when it does come to the agent Deep dive I do want to show you whenever we set everything to\n3:01:18 verbose verbose normally yeah so here in the agents and tools I just want to show\n3:01:23 you guys when this is actually going off and grabbing the information so we'll do this here for both and I'm going to\n3:01:29 rerun this example just so you guys can see that it is it's actually grabbing from the doc store because I I like to\n3:01:34 see the agent think that's one of my favorite things so whenever we run this again we'll now do python fifth module\n3:01:40 fifth module agent Deep dive example number two we're going to run it I'm going to ask who is branded again so um\n3:01:46 or how do I learn more about L chain how do I learn more about Lang\n3:01:52 chain and then now you can see it's actually whenever it's running you can see that it's saying like Okay I need to\n3:01:58 answer a question about I need to to answer a question because I have no idea what is Lang chain in the context of\n3:02:04 what I'm learning about okay cool well I'm going to call the answer question tool the input I'm going to pass in is\n3:02:10 how do I learn more about Lang chain and then so you can see cuz we passed in an\n3:02:15 input coming back down here to our code yeah so here's our input the query is now the same as this query the chat\n3:02:21 history this was our first question so it's not updating it yet and then the context well this is all the information\n3:02:27 that we get back from our Vector store so this is some information from our Vector store some of it looks like it is\n3:02:35 yeah this is all documents from our Vector store and once it has those 3,000 tokens worth of information and then\n3:02:41 converts that into a final answer that it then spits back to us that we can talk to here so yeah I hope you guys I\n3:02:47 think that's super cool to see how the agent's thinking and operating you know whenever it's in verbos mode but enough\n3:02:53 of doing the agent Deep dive what we're going to do from here is we're going to head over and start going deeper into\n3:02:58 different ways that you can create your tools so that you guys can Master this and create your own tools and really\n3:03:04 supercharge your agents so let's go ahead and head into that next hey guys so welcome to the third\n3:03:11 example inside of the agents and tools module and in this example I'm going to show you guys the most basic way we can\n3:03:18 go about creating tools and that's going to be using the tool Constructor and you've already done this a few times but\n3:03:23 now we're just going to go into a deeper dive of understanding like oh that's actually what's going on and here's how\n3:03:29 I can start making tools so what I want to do is to start off I just want to show you the three functions that we're\n3:03:35 going to try and add to our tool set so the first one is just going to be greet User it's going to take in a name and\n3:03:41 then it's just going to come back and say like hello we're going to do reverse string all this is going to do is it's\n3:03:47 going to take in some text and reverse it and split it back and then finally we're going to set up a concatenate\n3:03:53 strings tool that just takes in two strings and concatenates them together and spits out a string so well obviously\n3:04:00 you know that's just regular python let's dive into the tool section where we're going to start using the tool\n3:04:05 Constructor so this is exactly what we've done so far whenever we've created tools in the past where we've kind of\n3:04:12 set up a name set up a description and then Define the specific function that we want to call but the main thing I\n3:04:17 want what to bring your focus to whenever we're using the tool Constructor is this is a great way to go\n3:04:23 about creating simple tools because all we're doing is just saying like here's the name here's the description and then\n3:04:28 we're just passing it a function we're not specifying anywhere the like oh yeah this takes in two parameters one's an\n3:04:35 one's a string one's one's a number like we're just relying 100% on the llm underneath to understand what the tool\n3:04:43 needs and provide it to it so and most of the time that actually works really well and that's whenever you just have a\n3:04:49 simple tool this is the best way to go about it it's super simple to do and the part that I did want to show you guys as well is if you're not using this\n3:04:57 complete basic way to create a tool you can go off and use something called the structure tool and the structure tool is\n3:05:04 great whenever you want to kind of set up more complex functions so like this one takes in two parameters so you know\n3:05:11 if you're doing anything that takes in more than one parameter I would recommend going with a structured tool and the part that makes this different\n3:05:17 different is it takes in all the same information as before except then now it takes in this schema and this schema\n3:05:24 just we're just going to use pedantic which is just a great way to like define basically like typed models um so we're\n3:05:30 just going to say like hey here are our arguments for concatenating string I expect to have two two properties A and\n3:05:38 B where a is the first string and B is the second string so we're clearly defining oh wow whenever I pass in I\n3:05:44 need to basically give two strings and that's how it's going to how it's going to work so now that you've kind of seen\n3:05:50 how this works I'm going to go ahead and actually trigger this to run and so you can actually see all these tools working\n3:05:56 in action and the only thing I do want to mention that we haven't shown so far is uh I am using a new prompt for this\n3:06:03 one it's called open aai tools agent and what this one is doing is it's going to\n3:06:09 it's basically just focused more on using tools so you can kind of see you're a helpful assistant here's your\n3:06:14 chat history here's your human inputs so this one all it does really is it's just\n3:06:19 yeah it kind of just it works really well whenever you're trying to use tools and so what we're going to do is go\n3:06:25 ahead and Trigger this and what we're expecting to see back is three different responses one we're going to tell the\n3:06:31 agent executor to greet Alice so greet is going to hopefully trigger our first\n3:06:36 tool then eventually we're going to say reverse the string hello and then we're going to say concatenate and we're\n3:06:42 expecting our agent to go up and use the appropriate tools to make this happen and we're going to log the entire process okay so what we're going to do\n3:06:48 come down here clear things up and we're going to run our example so we're going\n3:06:54 to do Python and this is the fifth module this is a tool Deep dive and we're going to look at our first example\n3:07:00 together so like I said under the hood this is going to go off and Trigger each one of these so let's look and see how\n3:07:06 it did so for the first one we said hey we're trying to greet Alice so you can\n3:07:11 see that it said I need to you know invoke greeting user with the keyword Alicia or Alice and what it's going to\n3:07:19 do is come back with an answer because if you come back to our original function all it was supposed to do is\n3:07:24 pass in hello and the person's name and that's exactly what it did so thumbs up for the first one the next one is\n3:07:30 reversing the string so you can see that like yep we're invoking reverse string with hello it did it it performed the\n3:07:36 code and now we print it back and then finally what we're going to do for the last one is concatenate strings so you\n3:07:43 can see I need to invoke concatenate strings and it did a really good job of creating a dictionary where it specified\n3:07:50 what is a and what is B and this all comes back to the fact that we set up our structur tool and we passed in an\n3:07:57 argument schema that was the magic that allowed this to happen and now you can see that it did a great job of splitting\n3:08:03 it uh concatenating it together okay cool so that was hopefully a quick Deep\n3:08:08 dive into creating tools and now we're going to go into the next module where\n3:08:13 we're going to start learning how you can use the tool decorator to kind of simplify some of this so let's go ahead and start working on example number two\n3:08:21 in the tool deep dive right now hey guys so welcome to the third and\n3:08:26 final example in the tool Deep dive and in this example you're going to learn how to create tools but you're going to learn how to do it where you have the\n3:08:33 maximum control over how these tools behave and what we're going to do in this module is we're going to set up two\n3:08:38 tools the first one is going to be a simple Search tool where we're going to go off and use Tavi to search the\n3:08:44 internet and the next one is just going to be a multiply numbers tool just a super super basic one so that's what we're going to be doing in this example\n3:08:50 and I just want to walk you through the major parts together real fast cuz some of this is pretty similar so the first thing that we're going to be doing is\n3:08:56 setting up our pedantic models and this is what we're going to be doing to define the specific inputs for both of\n3:09:02 our tools so this is nothing new you've seen this so far all right now let's actually dive into the part where we're\n3:09:08 going to create our tools and how we have maximum control over them so the main way we're going to be creating our\n3:09:13 tools is by using the base tool class now hop over to the L chain doc so you\n3:09:18 can kind of see it so the main way it works is we're going to be using the subass base tool to generate new tools\n3:09:25 so we're going to inherit from based tools to create new classes that are tools basically so what you can see as\n3:09:31 Lang chain says like hey this is the way you can have Maxum control over your tools but you know it takes a little bit\n3:09:37 more work and the part that's interesting is you can kind of see like we are basically using this example I\n3:09:43 just want to like walk you through the setup of it but the part that's nice is you can actually set up your tools to have different functionality so you can\n3:09:49 set up a tool to have a run so you can see this is a private run method and\n3:09:55 what you can see is we can actually say hey here's what you need to do here's your inputs don't worry about run\n3:10:01 manager I've never really found it helpful and then you can Define the output and what's super cool is you can actually if you wanted to go even harder\n3:10:08 you could actually Define the output as a pedantic model and if it fails during the run to generate the proper response\n3:10:14 it'll redo it so this is a really cool way if you plan on doing doing you know spitting out some Json or you know you\n3:10:20 want to make sure that you print out an object that for sure has a like a person's contact info so it for sure had\n3:10:26 to have the person's first name last name and email you could set up a pedantic model as the output and you can\n3:10:32 do that all here inside of your custom tools when you're inheriting from base tool and then finally you can set up\n3:10:37 things to run synchronously or you can set them up to run asynchronously we're just going to stick to synchronously for\n3:10:42 this example okay so let's hop back over to our example so the first thing that we're going to be doing is setting up a\n3:10:48 Search tool because we want our agent to go off and access the internet well what we're going to do is we are going to do\n3:10:54 exactly kind of what we did in the tool Constructor where we have to give our tool a name because we need to let our\n3:10:59 AI know like Yep this is the name of the tool and here's when I would need to use it whenever I need to answer questions\n3:11:05 about current events so you know anything that's not inside of the ai's knowledge base and then finally what is\n3:11:11 the argument schema well this is exactly what I want to be passed in the simple search input which just contains a query\n3:11:18 okay now whenever this tool gets triggered and calls underlying run method what's going to happen is we\n3:11:25 expect to get that query of a string of what we need to go search now here's where things get interesting this is\n3:11:30 where we've kind of defined what this function should do so in our case we're going to use Tav and if you head over to\n3:11:36 their documents this is how you can connect your llms to the web and it just makes it super simple to go off and\n3:11:42 search the internet uh and then just I'm sure some of you guys are curious how much it cost it's free every month for\n3:11:48 1,000 calls and then it goes from up from there but no I've I've used them and I've really liked them so far so definitely recommend checking them out\n3:11:54 and to get started it's super simple you're going to basically just make sure you have access to this class you're\n3:12:00 going to set up an account so you get API key and then once you have an API key you can just start using their client and making request and that's\n3:12:07 exactly what we're doing in rcode we're importing that client that we would have installed I've already added to the uh\n3:12:12 Tomo file so it's already going to work for you guys now we're going to and then once you have have your environment\n3:12:17 variable set up and grab it you're just going to make sure you paste it in here and once you've done that you can now actually start using the client to make\n3:12:24 requests to the Internet so let's go ahead and we're going to run through the second tool real fast and then we're going to run it so you can see that like\n3:12:30 wow we are actually communicating with the internet and it's super cool all right and the final one this is just a super simple example I just wanted to\n3:12:36 show you guys like yeah you can create a tool to multiply numbers this one's super simple I just wanted to to add it in here for you guys but once you\n3:12:42 created your two tools what you need to do is you need to go off and basically in your tools list we're going to create\n3:12:48 two new tools so as you can see each one of these is a class so to create a new instance of the class we're just kind of\n3:12:54 come down here and just you know create them down here so once we have two instances of these classes we're going to do the normal thing with our agents\n3:13:01 set up the llm we want to use give it the prompt to guide its instructions set up our tool calling agent um this one is\n3:13:09 a little bit different from what we've done in the past in the past we've been doing the the react agent so the tool\n3:13:14 calling agent it just specializes in using tools so that's what it's kind of doing and then finally what we're going\n3:13:20 to do is once we have our agent set up we're going to use the agent executor to actually start uh handling our runs all\n3:13:26 right so let's go ahead and run our agent with these two different examples the first example we're going to be\n3:13:32 using is search for Apple intelligence and I'm going to take off these quotes just so we have to make our uh AI learn\n3:13:40 a little bit more about Apple intelligence and then finally we're just going to pass in a sentence and we\n3:13:46 expect this sentence to get converted over to numbers that you can see specifically floats that we can then\n3:13:52 start to use to to to multiply so that's what we I expect to see happen whenever we run this so let's go ahead and run it\n3:13:58 real fast so this is in the agent module and we are in the tools section and this is the final one let's go ahead and run\n3:14:05 it so we can see the search results and look at just the math so you can see out the gate what it did is it said all\n3:14:11 right I need to do a simple surge because that was the name of our tool let's go back up here a simple surge and\n3:14:17 we just want to what is the query of what we want to look up oh we want to look up Apple intelligence so it pulled out the key topic and then Tav our\n3:14:24 Search tool came back to us and said well here's your query here's the what you want to look up and then it gave us\n3:14:29 back the results so it went off and searched the internet in a way to where we get quick titles URLs so if we wanted\n3:14:37 to we can go off and actually explore some of these URLs and dive deeper but you can see it came back to us with some\n3:14:43 more information about each each of these things and today is is the middle of June so you can see this is grabbing\n3:14:49 all recent information about what's happening with the new Apple intelligence release okay cool and then\n3:14:54 once it gave us back the actual like this is the raw score and actually you can see it here here's some of the\n3:15:00 relevant information so just putting it in a nice format for us um that's what's\n3:15:05 happening with the tool and once it has that information um it's just going to spit out the final response for us so\n3:15:12 you can kind of see it all right here this is the final response and the outputs right here it just going to look\n3:15:17 exactly this is basically the output if you printed it out that's what you would see uh um that you you were to pass it\n3:15:22 out to your user okay and enough of that the tool went super simple the reason why I kind of like this example is\n3:15:28 because usually tools struggle with like if you just to run this as a regular tool decorator for whatever reason it\n3:15:34 kind of struggles to recognize like oh yeah that's a float so I just thought this was a cool example of like oh yeah we have to convert a string to a\n3:15:40 numerical representation so yeah and we get back to the correct answer and it's a float which is what we want okay all\n3:15:46 right so that's it for the tool example and this is actually it for the entire uh agent and Tool Deep dive just to like\n3:15:53 plant some seeds in your brains of like what else is possible crew AI is by far one of my favorite pack uh products out\n3:15:59 there when it comes to working with agents because they make it super cool for us to actually set up agents to work\n3:16:05 with other agents so right now we have one agent you know performing a task but what happens if you had like a writer\n3:16:11 agent collaborating with a researcher agent who each specialized like the uh if the if the researcher agent\n3:16:17 specialized in exploring the web and the writer uh specialized in writing articles that were had a good Rhythm\n3:16:23 used proper like languaging to write interesting articles well these guys could work together to generate an\n3:16:29 awesome report so that's just scratching the service of what cre I can do but I just wanted to like plant some seeds for\n3:16:35 like what's next after you guys learn like how to make an individual agent well multi-agents tools are the next and\n3:16:42 I definitely recommend creai and I have a ton of videos on crei inside of my YouTube channel so I definitely recommend and checking that out next but\n3:16:48 yeah that's it for the fifth module so let's go ahead and wrap things up hey guys so I hope you guys have enjoyed\n3:16:54 this Lang chain master class we covered a ton of information in all five of the different modules hopefully you guys are\n3:17:00 now Pros when it comes to working with chat models prompt templates chains Rag\n3:17:06 and your agents and tools and just as a recap all the source code in this video is completely for free there's a link\n3:17:11 down the description below while you're down there it would mean a lot to me if y'all can hit that like And subscribe\n3:17:17 especially if you've made it this far in in this video also there's that free school Community where you can meet like-minded AI developers and we have\n3:17:24 those weekly free coaching calls so you'll definitely want to take advantage of that and then outside of that I have\n3:17:29 a ton of other AI related content on my YouTube channel everything from Full stack AI tutorials all the way to like\n3:17:36 crew AI Deep dive so you're definitely going want check out one of those after this video but enough of that I hope you\n3:17:42 guys have a great day and I can't wait to see youall around in the next one see you