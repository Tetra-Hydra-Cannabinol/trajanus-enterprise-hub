Title: LangChain v0.3 â€” Getting Started
Channel: James Briggs
Video ID: i2jGTcJsDPM
URL: https://www.youtube.com/watch?v=i2jGTcJsDPM
Duration: 33:47
Level: BEGINNER
Application: LangChain
Topics: Local Setup, Colab Setup, OpenAI LLMs, LLM Prompting, Temperature, Prompt Templates, LCEL Chains, Structured Outputs, Pydantic, Image Generation, DALL-E
Ingested: 2025-12-21
Source: Playwright Browser Extraction
==============================================================================

0:00 Okay, so moving on to our next chapter, getting started with a line chain. In this chapter, we're going to be
0:05 introducing a line chain by building a simple LM powered assistant that will do
0:11 various things for us. It will multimodal generating some text, generating images, generate some
0:18 structured outputs. It will do a few things. Now, to get started, we will go over to the course repo. All of the
0:25 code, all the chapters are in here. There are two ways of running this either locally or in Google Collab. We
0:32 would recommend running in Google Collab because it's just a lot simpler with environments, but you can also run it
0:38 locally. And actually for the capstone, we will be running it locally. There's
0:43 no way of us doing that in Collab. So if you would like to run everything locally, I'll show you how quickly. Now,
0:50 if you would like to run in collab, which I would recommend at least for the the first notebook chapters, just skip
0:58 ahead. There will be chapter points in the timeline of the video. So, for
1:04 running running it locally, we just come down to here. So, this actually tells you everything that you need. So, you
1:11 will need to install UV, right? So this is the package manager that we recommend
1:17 like the Python and package management library. You don't need to use UV, you
1:22 know, it's it's up to you. UV is is very simple. It works really well. So I would
1:28 recommend that. So you would install it with this command here. This is on Mac, so it will be different otherwise if you
1:36 are on Windows or otherwise you can uh look at the installation guide there and it'll tell you what to do. And so before
1:42 we actually do this, what I will do is go ahead and just clone this
1:48 repo. So we'll come into here. I'm going to create like a temp directory for me
1:53 cuz I already have the chain course in there. And what I'm going to do is just get clone line chain course. Okay. So
2:01 you will also need to install git if you don't have that. Okay. So we have that.
2:08 Then what we'll do is copy this. Okay. So this will install Python 3.12.7 for
2:13 us with this command. Then this will create a new VM within that or using
2:21 Python 3.12.7 that we've installed. And then UV sync will actually be looking at
2:28 the pi projectl file. That's like the uh the package installation for the repo
2:33 and using that to install everything that we need. Now we should actually make sure that we are within the line
2:40 chain course directory and then yes we can run those three and there we go. So everything
2:47 should install with that. Now if you are in cursor you can just do cursor dot or
2:56 we can run code. If VSS code I'll just be running this and then I've opened up
3:03 the course. Now within that course you'll have your notebooks and then you just run through these making sure you
3:09 select your kernel Python environment and making sure you're using the correct VN from here. So that should pop up
3:16 already as this VM bin Python and you'll click that and then you can run through.
3:22 When you are running locally don't run these you don't need to. You've already installed everything. So you don't this
3:28 specifically is for collab. So that is running things locally. Now let's have a
3:33 look at running things in collab. So for running everything in collab, we have
3:40 our notebooks in here. We click through and then we have each of the chapters through here. So starting with the first
3:47 chapter, the introduction, which is where we are now. So what you can do to open this in
3:53 collab is either just click this collab button here or if you really want to uh
4:00 for example maybe this it is not loading for you. What you can do is you can copy
4:05 the URL at the top here you can go over to collab you can go to
4:11 open GitHub and then just paste that in there and press
4:16 enter. And there we go. We have our notebook. Okay. So, we're in now. Uh
4:23 what we will do first is just install the prerequisites. So, we have line chain. I just load line chain packages
4:30 here. LChain core line chain openai because we're using open AI and line
4:36 chain community which is needed for running uh what we're running. Okay. So, that has installed everything for us. So
4:43 we can move on to our first step which is initializing our LM. So we're going
4:50 to be using GT40 mini which is like a small but fast but also cheaper model.
4:58 Uh that is also very good from OpenAI. So what we need to do here is get an API
5:04 key. Okay. So for getting that API key we're going to go to OpenAI's website.
5:10 And you can see here that we're opening platform.openai.com. openai.com and then we're going to go into settings,
5:15 organization, API keys. So, you can copy that or just click it from here. Okay. So, I'm going
5:22 to go ahead and create a new secret key. Actually, just in case you're kind of
5:28 looking for where this is, it's settings, organization, API keys again. Okay. Create a new API key. I'm going to
5:35 call it line train course. I'll just put it under semantic
5:41 router. That's just my organization. You you put it wherever you want it to be.
5:46 And then you would copy your API key. You can see mine here. I'm obviously
5:51 going to revert that before you see this, but you can try and use it if you really like. So, I'm going to copy that.
5:56 And I'm going to place it into this little box here. You could also just place it put your uh full API key in
6:04 here. It's up to you. But this little box just makes things easier. Now that
6:10 what we've basically done there is just passed in our API key. We're setting our open model GT40 mini. And what we're
6:17 going to be doing now is essentially just connecting and setting up our LLM parameters with line chain. So we run
6:26 that. We say okay we're using a GP4 mini and we're also setting ourselves up to
6:32 use two different LM here or two of the same LM with slightly different
6:37 settings. So the first of those is an LLM with a temperature setting of zero. The temperature setting basically
6:44 controls almost the randomness of the output of your LLM. And the way that it
6:51 works is when an LLM is predicting the sort of next token or next word in
6:59 sequence, it'll provide a probability actually for all of the tokens within the LM's knowledge base or what the LM
7:05 has been trained on. So what we do when we set a temperature of zero is we say you are going to give us the token with
7:14 highest probability according to you. Okay. Whereas when we set a temperature
7:20 of 0.9 what we're saying is okay there's actually an increased probability of you
7:26 giving us a token that according to your generated output is not the token with
7:32 the highest probability according to the LM. But what that tends to do is give us
7:37 more sort of creative outputs. So that's what the temperature does. So we are creating a normal LM and then a more
7:45 creative LM with this. So what are we going to be building? We're going to be
7:50 taking a article draft. So like a draft article uh from the Aurelio learning
7:58 page and we're going to be using line chain to generate various themes that we might um find helpful as we're you know
8:06 we have this article draft and we're editing it and just kind of like finalizing it. So what are those going
8:11 to be? You can see them here. We have the title for the article description and SEO friendly description
8:18 specifically. The third one, we're going to be getting the LM to provide us advice on an existing paragraph and
8:25 essentially writing a new paragraph for us from that existing paragraph. And what it's going to do, this is a
8:30 structured output part, is going to write a new version of that paragraph for us. And it's going to give us advice
8:36 on where we can improve our writing. Then we're going to generate a thumbnail or hero image for our article. So, you
8:44 know, nice image that you would put at the top. So here we're just going to input our article. You can you can put
8:51 something else in here if you like. Essentially this is just a big article
8:56 that's written a little while back on agents. And now we can go ahead and
9:02 start preparing our prompts which are essentially the instructions for our LLM.
9:07 So, line chain comes with a lot of different uh like utilities for prompts
9:13 and we're going to dive into them in a lot more detail, but I do want to just give you uh the essentials now just so
9:19 you can understand what we're looking at at least conceptually. So, prompts for chat agents are at a minimum broken up
9:26 into three components. Those are the system prompt. This provides instructions to our LM on how it should
9:32 behave, what its objective is, and how it should go about achieving that objective. Generally, system prompts are
9:39 going to be a bit longer than what we have here depending on the use case. Then we have our user prompts. So, these
9:45 are user written messages. Uh, usually sometimes we might want to pre-populate those if we want to encourage a
9:52 particular type of um conversational patterns from our agent. But for the
9:58 most part, yes, these are going to be user generated. Then we have our AI prompts. So these are of course AI
10:05 generated. And again, in some cases, we might want to generate those ourselves
10:10 beforehand or within a conversation if we have a particular reason for doing so. But for the most part, you can
10:17 assume that these are actually user and AI generated. Now, line chain provides
10:22 us with templates for each one of these prompt types. Let's go ahead and have a
10:29 look at what these look like within line chain. So, to begin, we are looking at
10:35 this one. So, we have our system message prompt template and human messages, the
10:41 the user that we saw before. So, we have these two system prompt, keeping it quite simple here. You are a system that
10:48 helps generate article titles, right? So our first component where we want to generate his article title. So we're
10:54 telling the AI, you know, that's what we want it to do. And then here, right? So
11:00 here we're actually providing kind of like a template for a
11:05 user input. So yes, as I mentioned, user input can
11:12 be um it can be fully generated by a user. It might be kind of not generated
11:18 by user. We might be setting up a conversation beforehand which then a user would later use or in this scenario
11:25 we're actually creating a template and the what the user will provide us will
11:31 actually just be inserted here inside article and that's why we have this import variables. So, what this is going
11:40 to do is okay, we have all of these instructions around here. They're all going to be provided to OpenAI as if it
11:46 is the user saying this. Um, but it will actually just be this here that a user
11:52 will be providing. Okay. And we might want to also format this a little nicer. It kind of depends. This will work as it
11:58 is, but we can also put, you know, something like this to make it a little bit clearer to the LM. Okay. what is the
12:06 article and where are the prompts? So, we have that and you can see in this
12:12 scenario there's not that much difference between what the system prompt and the user prompt is doing. And this is it's a particular scenario. It
12:19 varies when you get into the more conversational stuff as we will do later. Uh you'll see that the user
12:25 prompt is generally more fully userenerated or mostly user generated.
12:32 And much of these types of instructions we might actually be putting into the system prompt it varies and we'll see
12:39 throughout the course many different ways of using these different types of prompts in various different
12:45 places. Then you'll see here so I just want to show you how this is working. We
12:51 can use this format method on our user prompt here to actually insert something
12:57 within the uh article input here. So we're going to go use prompt format and
13:03 then we pass in something for article. Okay. And we can also maybe format this a little nicer but I'll just show you
13:09 this for now. So we have our human message and then inside content this is the the text that we had. You can see
13:15 that we have all this right and this is what we wrote before. We wrote all this except from this part. We didn't write
13:22 this. Instead of this we had article. All right. So let's format this a little
13:28 nicer so we can see. Okay. So this is exactly what we wrote up here. Exactly
13:33 the same except from now we have test string instead of article. So later when
13:38 we insert our article it's going to go inside there. That's all it's doing. It's like it's an it's an string in
13:45 Python. Okay. And this is again this is one of the things where people might complain about lang chain. you know,
13:50 this sort of thing can be, you know, it seems excessive because you could just do this with me stringing, but there
13:56 are, as we'll see later, particularly when you're streaming, just really helpful features that come with using
14:03 line chains kind of builtin uh prompt templates or at least uh message objects
14:09 that we'll see. So, you know, we we need to uh keep that in mind. Again, as
14:16 things get more complicated, line chain can be a bit more useful. So, chat prompt template. Uh, this is basically
14:23 just going to take what we have here, our system prompt, user prompts. You could also include some AI prompts in
14:29 there. And what it's going to do is merge both of those and then when we do
14:36 format, what it's going to do is put both of those together into a chat history. Okay. So, let's see what that
14:42 looks like first uh in a more messy way. Okay. So you can see we have just the
14:49 content right so it doesn't include the whole you know before we had human message we're not include we're not
14:55 seeing anything like that here instead we're just seeing the string so now let's switch back to
15:02 print and we can see that what we have is our system message here is just
15:07 prefixed with this system and then we have human and it's prefixed by human and then it continues right so that's
15:13 that's all it's doing it's just kind of merging those in some sort of chat log we could also put in like AI messages
15:18 and they would appear in there as well. Okay, so we have that. Now that is our
15:23 prompt template. Let's put that together with an LLM to create what would be in
15:29 the past line chain be called an LLM chain. Uh now we wouldn't necessarily call it an LLM chain because we're not
15:35 using the LLM chain abstraction. It's not super important. If that doesn't make sense, we will go into it in more
15:41 detail later particularly in the in the L cell chapter.
15:46 So what this chain will do you think line chain is just chains we're chaining
15:52 together these multiple components it will perform the steps prompt formatting so that's what I just showed you lm
16:01 generation so sending our prompt to open AAI getting a response and getting that
16:06 output so you can also add another step here if you want to format that in a particular way we're going to be
16:13 outputting that in a particular format so that we feed it into the next step more easily. But there are also things
16:18 called output passes which pass your output in a more dynamic or complicated
16:25 way depending on what you're doing. So this is our first look at Els. I don't
16:30 want us to focus too much on the syntax here because we will be doing that later. But I do want you to just
16:36 understand what is actually happening here and logically what are we writing.
16:43 So all we really need to know right now is we define our inputs with the first
16:49 dictionary segment here. Right? So this is a you know our inputs which we have
16:55 defined already. Okay. So if we come up to our
17:01 uh user prompt here we said the input variable is our article right and we might have also added input variables to
17:06 the system prompt here as well. In that case, you know, let's say we had you are an AI assistant
17:14 called name, right? That helps generate article
17:19 titles. In this scenario, we might have an input variables name here, right? And
17:27 then what we would have to do down here is we would also have to pass that
17:33 in, right? So it also we would have article but we would also have name. So
17:40 basically we just need to make sure that in here we're including the variables that we have defined as input variables
17:47 for our our first prompts. Okay. So we can actually go ahead and let's add that
17:52 uh so we can see it in action. So we'll run this again and just include that or
17:59 or reinitialize our first prompt. So we see that and if we just have a look at
18:05 what that means for this format function here it means we'll also need to pass in a name. Okay we call it Joe. Okay so Joe
18:13 the AI right so you are an AI system called Joe now. Okay so we have Joe our
18:19 AI that is going to be fed in through these input variables. Then we have this pipe operator. The pipe operator is
18:25 basically saying whatever is to the left of the pipe operator, which in this case would be this, is going to go into
18:33 whatever is on the right of the pipe operator. It's that simple. Uh again, we'll we'll dive into this and kind of
18:39 break it apart in the LSL chapter, but for now, that's all we need to know. So, this is going to go into our first
18:45 prompt that is going to format everything. It's going to add the name and the article that we provided into
18:51 our first prompt. And it's going to output that, right? Okay, it's going to output that. We have our pipe operate
18:56 here. So the output of this is going to go into the input of our next step. It's our creative LM. Then that is going to
19:05 generate some tokens. It's going to generate our output. That output is going to be an AI message. And as you
19:12 saw before, if I take this bit out, within those message objects, we
19:19 have this content field. Okay. So we are actually going to extract the content
19:24 field out from our AI message to just get the content and that is what we do
19:30 here. So we get the AI message out from ILM and then we're extracting the content from that AI message object and
19:36 we're going to passing it into a dictionary that just contains article title like so. Okay, we don't need to do
19:42 that. We can just get the AI message directly. I just want to show you how we are using this sort of chain in Els. So
19:52 once we have set up our chain, we then call it or execute it using the invoke
19:58 method. Into that we will need to pass in those variables. So we have our article already, but we also gave our AI
20:04 a name now. So let's add that and we'll run this. Okay, so Joe has generated us a
20:13 article title. Unlocking the future, the rise of neurosymbolic AI agents. Cool.
20:19 much better name than what I gave the article, which was AI agents are neurosymbolic systems. No, I don't think
20:26 I did too bad. Okay, so we have that. Now, let's continue. And what we're
20:33 going to be doing is building more of these types of LM chain pipelines where
20:39 we're feeding in some prompts. We're generating something, getting something, and and doing something with it.
20:45 So, as mentioned, we have the title. We're now moving on to the description. So, we're going to generate a
20:50 description. So, we have our human message prompt template. So, this is actually going to go into a similar format as before. We
20:59 probably also want to redefine this because I think I'm using the same system message there. So, let's uh let's
21:06 go ahead and do modify that. Or what we could also do is let's
21:12 just remove the name now because I've shown you that. So, what we could do is
21:18 you're an AI assistant that helps build good articles, right? Build good
21:25 articles. And we could just use this as our, you know, generic system prompt. Now, so let's say that's our new system
21:32 prompt. Now, we have our user prompt. You're tasked with creating a description for the article. The article is here for you to examine article. Here
21:39 is the article title. Okay. So, we need the article title now as well in our input variables. And then we're going to
21:45 output an SEOfriendly article description. And we're just saying, you know, just to be certain here, do not
21:50 output anything other than the description. So, you know, sometimes an LLM might say, hey, look, this is what
21:56 I've generated for you. The reason I think this is good is because so on and so on and so on, right? If you're
22:01 programmatically taking some output from an LLM, you don't want all of that fluff around what the LM has generated. You
22:08 just want exactly what you've asked it for, okay? because otherwise you need to pass out of code and it can get messy
22:14 and also just far less reliable. So we're just saying don't output anything
22:19 else. Then we're putting all of these together. So system prompt and a second user prompt. This one here putting those
22:26 together into a new chat prompt template. And then we're going to feed all that in to another cell chain as we
22:33 have here to well to generate our our description. So let's go ahead. We
22:38 invoke that as before. or we'll just make sure we add in the article title that we got from before and let's see
22:45 what we get. Okay, so we have this explore the transformative potential of neuros symbolic AI agents in a little
22:52 bit long to be honest, but yeah, you can see what it's doing here, right? And of course, we could then go in. We see this
22:58 is kind of too long like SEO friendly description. Not not really. So, we can
23:05 modify this. Output the SEO friendly description. Um, make sure we don't
23:12 exceed. Let me put that on a new line. Make sure we don't exceed say 200
23:18 characters or maybe it's even less for SEO. I don't I don't have a clue. I'll just say 120 characters. Do not output
23:24 anything other than the description. Right. So we could just you know go back modify our prompting see what that
23:30 generates again. Okay. So much shorter probably too short now but that's fine.
23:35 Cool. So we have that. We have a summary process that now you know in this dictionary formula that we have here.
23:42 Cool. Now the third step, we want to consume that first article variable with
23:49 our full article and we're going to generate a few different output fields.
23:55 So for this, we're going to be using the structured output feature. So let's
24:01 scroll down. We'll see what that is or what that looks like. So structured
24:06 output is essentially we're forcing the alignment like it has to output a dictionary with these you know
24:13 particular fields. Okay and we can modify this quite a bit but in this
24:19 scenario what I want to do is I want there to be an original paragraph right so I just want it to regenerate the
24:24 original paragraph cuz I'm lazy and I don't want to extract it out. Then I want to get the new edited paragraph.
24:31 This is the ln generated improved paragraph. And then we want to get some feedback because we we don't want to
24:37 just automate ourselves. We want to augment ourselves and get better with AI
24:43 rather than just being like ah you do you do this. So that's what we do here. And you can see that here we're using
24:50 this pyantic object. And what pyantic allows us to do is define these particular fields. And it also allows us
24:56 to assign these descriptions to a field. And line chain is actually going to go ahead read all of this, right? Even
25:03 reads. So, for example, we could put integer here and we could actually get a numeric score for our paragraph, right?
25:11 We can try that, right? So, let's uh let's let's just try that quickly. I'll show you. Um, so numeric numeric score.
25:18 In fact, let's even just ignore let's not put anything here. So, I'm going to put constructive feedback on the
25:25 original paragraph, but I just put in here. So, let's see what happens. Okay. So we have that and what I'm going to do
25:31 is I'm going to get our creative LM. I'm going to use this with structured output method and that's actually going to
25:37 modify that LM class. Create a new LM class that forces that LM to use this
25:42 structure for the output. Right? So passing in paragraph into here using this we're creating this new structured
25:49 LM. So let's run that and see what happens. Okay. So we're going to modify
25:55 our chain accordingly. Maybe what I can do is also just remove this bit for now.
26:02 So we can just see what the structured LLM outputs directly. And let's
26:08 see. Okay, so now you can see that we actually have that paragraph object,
26:14 right? The one we defined up here, which is kind of cool. And then in there we have the original paragraph, right? So
26:20 this is where this is coming from. I definitely remember writing something
26:25 that looks a lot like that. So I think that is uh correct. We have the edited paragraph. So this is okay what it
26:30 thinks is better. And then interestingly the feedback is three which is weird
26:37 right because uh here we said the constructive feedback on the original paragraph. But what we're doing when we
26:43 use this with structured output what lang chain is doing is essentially performing a tool call to open AAI and
26:50 what a tool call can do is force a particular structure in the output of an LM. So when we say feedback has to be an
26:58 integer, no matter what we put here, it's going to give us an integer because how do you provide constructive feedback
27:04 with an integer? It doesn't really make sense. But because we've set that limitation, that restriction here, that
27:12 is what it does. It just gives us the uh a numeric value. So I'm going to shift that to string and then let's rerun
27:19 this. See what we get. Okay, we should now see that we actually do get constructive feedback. All right. So
27:25 yeah, you can see it's quite quite long. So the original paragraph effectively communicates limitations with neuro AI
27:31 systems in performing certain tasks. However, it could benefit from slightly improved clarity and conciseness. For
27:38 example, the phrase was becoming clear can be made more direct by changing it to became evident. Well, yeah, true.
27:46 Thank you very much. So yeah, now we actually get that that feedback which is is pretty nice. Now, let's add in this
27:54 final step to our chain. Okay. And it's just going to pull out our paragraph object here and
28:01 extracting to a dictionary. We don't necessarily need to do this. Honestly, I actually kind of prefer it within this paragraph object, but just so we can see
28:10 how we would pass things on the other side of the chain. Okay. So, now we can
28:15 see we've extracted that out. Cool. So we have all of that interesting feedback
28:23 again. But let's leave it there for the text part of this. Now let's have a look
28:29 at the sort of multimodal features that we can work with. So this is, you know,
28:34 maybe one of those things that's kind of seems a bit more abstracted, a little bit complicated where it maybe could be
28:40 improved. But, you know, we're not going to really be focusing too much on the multi mode stuff. We'll sub be focusing
28:45 on language. But I did want to just show you very quickly. So we want this article to look better. Okay, we want to
28:54 generate a prompt based on the article itself that we can then pass to Darly
29:02 the the image generation model from OpenAI that will then generate an image like a like a thumbnail image for us.
29:09 Okay. So the first step of that is we're actually going to get an LM to generate
29:14 that. Right. So we have our prompt that we're going to use for that. So I'm going to say generate a prompt with less
29:19 than 500 characters to uh generate an image based on the following article.
29:25 Okay. So that's our prompt. You know, super simple. Uh we're using the generic prompt template here. You can use that.
29:32 You can use user uh prompt template. It's up to you. This is just like the generic prompt template. Then what we're
29:39 going to be doing is based on what this outputs, we're then going to feed that
29:44 in to this generate and display image function via the image prompt parameter
29:50 that is going to use the Darly API wrapper from Langchain. It's going to run that image prompt and we're going to
29:57 get a URL out from that essentially and then we're going to read that using SK
30:02 image here. Right? All right. So, it's going to read that image URL. Going to get the image data and then we're just going to display it. Okay. So, pretty
30:11 straightforward. Now, again, this is a cell thing here that we're doing. We
30:17 have this runnable lambda thing. When we're running functions within LEL, we
30:23 need to wrap them within this runnable lambda. I, you know, I don't want to go too much into what this is doing here
30:30 because we do cover in the LE cell chapter. But it's just you know all you really need to know is we have a custom
30:36 function wrap it in runnable lambda and then what we get from that we can use
30:41 within this here right the the lolt syntax. So what are we doing here? Let's
30:47 figure this out. We are taking our original that image prompt that we defined just up here right input
30:53 variable to that is article. Okay we have our article data being input here.
31:00 Feeding that into our prompt. From there, we get our message that we then feed into our LM. From the LM, it's
31:07 going to generate us a like an image prompt. Uh like a prompt for generating our image for this article. We can even
31:15 let's uh let's print that out so that we can see what it generates because I'm also kind of curious. Okay. So, we'll
31:22 just run that and then let's see. It will feed in that content into our
31:28 runnable, which is basically this function here. And we'll see what it generates. Okay. Don't expect anything
31:35 amazing from Darly. It's not it's not the best to be honest, but at least we
31:41 see how to use it. Okay. So, we can see the prompt that was used here. Create an
31:46 image that visually represents a concept of muros symbolic agents. It depicts a futuristic interface where large know
31:53 interacts with traditional code symbolizing integration of oh my gosh uh
31:58 something computation include elements like a brain to represent neuronet networks gears or circuits or symbolic
32:06 logic and a web of connections illustrating vast use cases of AI
32:11 agents. Oh my gosh look at all that big prompt. Then we get this. So,
32:17 you know, Darly is interesting. I would say we could even take this. Let's just see what that comes up with in something
32:24 like midjourney. You can see these way cooler images that we get from just
32:29 another image generation model. Far better, but pretty cool honestly. So, in terms of generation images, the phrasing
32:37 the the prompt itself is actually pretty good. The image, you know, could be better, but that's it. Right. So with
32:45 all of that, we've seen a little introduction to what we might building with ling chain. So that's it for our
32:51 introduction chapter. As I mentioned, we don't want to go too much into what each of these things is doing. I just really
32:59 want to focus on okay, this is kind of how we're building something with line
33:04 chain. This is the overall flow. uh but we don't really want to be focusing too
33:10 much on okay what exactly LSL is doing or what exactly uh you know this prompt
33:16 thing is that we're setting up. We're going to be focusing much more on all of those things and much more in the
33:23 upcoming chapters. So for now we've just seen a little bit of what we can build
33:28 before diving in in more detail.
