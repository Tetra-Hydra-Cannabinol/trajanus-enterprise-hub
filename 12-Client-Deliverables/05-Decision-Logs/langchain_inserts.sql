-- LangChain Transcript Inserts
-- Generated: 2025-12-21
-- Total chunks: 33

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Cyv-dgv80kE',
  1,
  'LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]',
  '### Result',
  '### Result',
  '{"channel": "James Briggs", "video_id": "Cyv-dgv80kE", "duration": "4:46:46", "level": "ADVANCED", "application": "LangChain", "topics": ["LangChain v0.3", "LLM Basics", "LCEL", "Prompts", "Chat Memory", "Agents", "Agent Executor", "Streaming", "Async", "LangSmith", "Capstone Project"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Cyv-dgv80kE',
  2,
  'LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]',
  '"0:00 welcome to the AI Engineers guide for the L chain this is a four course that\n0:05 will take you from the assumption that you know nothing about Lang chain to\n0:11 being able to proficiently us...',
  '"0:00 welcome to the AI Engineers guide for the L chain this is a four course that\n0:05 will take you from the assumption that you know nothing about Lang chain to\n0:11 being able to proficiently use the framework either you know within line\n0:17 chain within line graph or even elsewhere uh from the fundamentals that\n0:23 you will learn in this course now this course will be broken up into multiple\n0:28 chapters we''re going to start by talking a little bit about what line chain is and when we should really be using it\n0:35 and when maybe we don''t want to use it we''ll talk about the pros and cons and also about the the why the line chain\n0:41 ecosystem not just about the line chain framework itself from there we''ll\n0:46 introduce Lang chain we''ll just have a look at a few examples before diving into essentially the basics of the\n0:53 framework now I will just note that all this is for Lang chain 0.3 so that is latest current version\n1:01 although that being said we will cover a little bit of where line chain comes from as well so we''ll be looking at pre\n1:09 0.3 uh version methods for doing things so that we can understand okay that''s\n1:15 the old way doing things how do we do it now now that we''re in version 0.3 and also how do we dive a little deeper into\n1:22 those methods as well and kind of customize those from there we''ll be diving into what I believe is the S of\n1:30 future of AI I mean it''s it''s it''s a now and the short term potentially even\n1:36 further into the future and that is Agents we''ll be spending a lot of time on agents so we''ll be starting with a\n1:43 simple introduction to agents so that is how can we build an agent that''s simple\n1:51 what are the main components of Agents what do they look like and then we''ll be diving much deeper into them and we''ll\n1:57 be building out our own Agent X computer which kind of like the framework around\n2:03 the AI components of an agent we''re building our own and once we''ve done our\n2:08 Deep dive on agents we''ll be diving into Lang chain expression language which\n2:14 we''ll be using throughout this course so line chain expression language is the recommended way of using line chain and\n2:21 the expression language or L cell takes kind like a break from standard python\n2:27 syntax so there''s a bit of weirdness in there and yes we''ll be using it throughout the course but we''re leaving\n2:35 the ELO chapter until this you know kind of later on in the course because we really want to dive into the\n2:41 fundamentals of Elsa by that point but the idea is that by this point you already have a good grasp of at least\n2:47 how to use the basics of lell before we really dig in that point then we''ll be\n2:53 digging in streaming which is an essential ux feature of AI applications\n2:58 in general streaming it can just improve the user experience massively and it''s not just about\n3:05 streaming tokens you know that that interface where you have word by word the AI is generating text on the screen\n3:12 streaming is more than just that it is also the ability if you''ve seen the\n3:17 interface of perplexity where as the agent is thinking you''re getting an update of what the agent is thinking\n3:24 about what tools it is using and how it is using those tools that''s also another essential feature that we need to have a\n3:31 good understanding of streaming to build so we''ll also be taking a look at all of that then we''ll finally we''ll be topping\n3:38 it off with a Capstone project where we will be building our own AI agent\n3:45 application that is going to incorporate all of these features we''re going to have an agent that can use tools web\n3:51 search we''ll be using streaming and we''ll see all of this in you know a nice\n3:57 interface that we can that we can work with so that''s an overview the course of course it''s very high level what I''ve\n4:03 just gone through there''s a ton of stuff in here and truly this course can take you from you know wherever you are with\n4:09 L chain at the moment whether you''re a beginner or you''ve used it a bit or even inter mediate and you''re probably going\n4:16 to learn a fair bit from it so without any further Ado let''s dive into the\n4:22 first chapter okay so the first chapter of the course we''re going to focus on\n4:28 when should we actually use Lang chain and when should we use something else now through this chapter we''re not\n4:35 really going to focus too much on the code we you know every other chapter is very code focused but this one is a\n4:42 little more just theoretical what is line chain where''s fit in when should I use it when should I not so I want to\n4:48 just start by Framing this line chain is one of if not the most popular open\n4:56 source framework within the python ecosystem at least for AI it works\n5:01 pretty well for a lot of things and also works terribly for a lot of things as well to be completely honest there are\n5:07 massive Pros massive cons to using Lang chain here we''re just going to discuss a few of those and see how Lang chain\n5:14 maybe Compares a little bit against other Frameworks so the very first\n5:19 question we should be asking ourselves is do we even need a framework a is a\n5:24 framework actually needed when we can just hit an API you have the open AI API\n5:30 other apis mral so on and we can get a response from an llm in five lines of\n5:36 code on average for those is incredibly incredibly simple however that can\n5:42 change very quickly when we start talking about agents or retrieval augmented generation research assistance\n5:50 all this sort of stuff those use cases those methods can\n5:56 suddenly get quite complicated when we outside of Frameworks and that''s not necessarily a\n6:03 bad thing right it can be incredibly useful to be able to uh just understand\n6:10 everything that is going on and build it yourself but the problem is that to do\n6:16 that you need time like you need to learn all the intricacies of building these things the intricacies of these\n6:22 methods and themselves like what you know how do they even work and that kind of runs in the opposite direction of\n6:28 what we see with AI at the moment which is AI is being integrated into the world at an incredibly fast rate and because\n6:37 of this most Engineers coming into the space are not from a machine learning or\n6:43 AI background most people don''t necessarily have any experience with these systems a lot of Engineers coming\n6:50 in that could be devops Engineers generic backend python Engineers even you front end Engineers coming in and\n6:57 building all these things which is is great but they don''t necessarily have the experience and that you know that\n7:02 might be you as well and that''s not a bad thing because the idea is that obviously you''re going to learn and\n7:07 you''re going to pick up a lot of these things and in this scenario there''s quite a good argument for using the\n7:14 framework because a framework means that you can get started faster and a\n7:19 framework like Lang chain it abstracts away a lot of stuff and that''s a that''s\n7:24 a big complaint that a lot of people will have with L chain but that abstract\n7:29 in away of many things is also what made sing chain popular because it means that\n7:34 you can come in not really knowing okay what you know rag is for example and you can Implement a rag pipeline get the\n7:41 benefits of it without really needing to understand it and yes there''s an argument against that as well just\n7:47 implementing something without really understanding it but as we''ll see throughout the course it is possible to\n7:54 work with line chain in a way as we will in this course where you kind of\n8:00 implement these things in an abstract way and then break them apart and start understanding the intricacies at least a\n8:07 little bit so that can actually be pretty good\n8:12 however again circling back to what we said at the start if the idea or your\n8:18 application is just a very simple you know you need to generate some text based on some basic input maybe you\n8:24 should just use an API that''s completely valid as well now we just said okay okay\n8:30 a lot of people coming to L chain might not be from an AI background so another question for a lot\n8:36 of these Engineers might be okay if I want to learn about you know rag agents\n8:41 all these things should I skip line chain and just try and build it from scratch myself well line chain can help\n8:49 a lot with that Learning Journey so you can start very abstract and as you gradually begin to\n8:57 understand the framework better you can strip away more and more of those abstractions and get more into the\n9:03 details and in my opinion this gradual shift towards more explicit code with\n9:11 less abstraction is a really nice feature and it''s also what we focus on\n9:17 right throughout this course that''s what we''re going to be doing going sing abstract stripping away the abstractions\n9:22 and getting more explicit with what we''re building so for example building an agent in L chain there''s in very\n9:30 simple and Incredibly abstract crate tools agent method that we can use and\n9:37 like it creates a tool agent for you it''s it doesn''t tell you anything so you can you can use that right and we will\n9:46 use that initially in the course but then you can actually go from that to\n9:51 defining your full agent execution logic which is basically a tools call to open\n9:58 AI you going to be getting that tool information back but then You'' got to figure out okay how am I going to\n10:03 execute that how am I going to Sol this information and then how am I going to iterate through this so we''re going to\n10:08 be seeing that stripping way abstractions as we work through as we build agents as we do as we bu like our\n10:16 streaming use case among many other things even chat memory we''ll see there as well so line chain can act as the\n10:24 onramp to your AI learning experience then what you might find and I do think\n10:31 this is quite true for most people is that if you if you''re really serious\n10:36 about AI engineering and that''s what you want to do like that''s your focus right which isn''t for everyone for certain a\n10:43 lot of people just want to understand a bit of AI and they want to continue doing what they''re doing and just integrate AI here and there and maybe\n10:50 those you know if that''s your focus you might sick with ly chain you know there''s not necessarily a reason to move\n10:55 on but in the other scenario where you''re thinking okay I want to get\n11:00 really good at this I want to just learn as much as I can and I''m going to\n11:06 dedicate basically my you know my short-term future of my career on becoming AI\n11:13 engineer then line chain might be the on-ramp it might be your initial learning curve but then after you''ve\n11:20 become competent with line chain you might actually find that you want to move on to other Frameworks and that\n11:25 doesn''t necessarily mean that you''re going to have wasted your time with L chain because one L chain is a thing helping\n11:32 you learn and two one of the main Frameworks that I recommend a lot of people to move on to is actually line\n11:38 graph which is still within the L chain ecosystem and it still uses a lot of L chain objects and\n11:45 methods and of course Concepts as well so even if you do move on from line\n11:50 chain you may move on to something like L graph which you can no line chain for\n11:56 anyway and let''s say you do move on to another framework in set said in that scenario the concepts that you learn\n12:02 from Lang chain are still pretty important so to just finish up this chapter I just want to summarize on that\n12:09 question of should you be using Lang chain what''s important to remember is that Lang chain does abstract a lot now\n12:16 this abstraction of L chain is both a strength and a weakness with more\n12:23 experience those abstractions can feel like a limitation and\n12:29 that is why we sort of go with the idea that L chain is really good to get\n12:35 started with but as a project grows in complexity or the engineers get more experience they might move on something\n12:41 like Lang graph which in any case is going to be using Lang chain to some degree so in either one of those\n12:48 scenarios L chain is going to be a core tool in an AI engineered toolkit so\n12:56 it''s worth learning in our opinion but of course it comes with its you know it comes with its weaknesses and it''s just\n13:03 good to be aware of that it''s not a perfect framework but for the most part you will learn a lot from it and you\n13:09 will be able to build a lot with it so with all of that we''ll move on to our\n13:16 first of Hands-On chapter with Lang chain where we''ll just introduce Lang\n13:21 chain some of the essential Concepts I''m not going to Dag too much into the syntax but we''re just going to understand a little bit of what we can\n13:27 do with it okay so moving on to our next next chapter getting started with a line chain in this chapter we''re going to be\n13:33 introducing a line Chain by building a simple LM powered assistant that will do\n13:39 various things for us it will multimodal generating some text generating images\n13:45 generate some stret shed outputs it will do a few things now to get started we will go over to the course repo all of\n13:53 the code all the chapters are in here there are two ways of running this either locally or in Google collab we\n14:00 would recommend running in Google collab because it''s just a lot simpler with environments but you can also run it\n14:06 locally and actually for the cap Zone we will be running it locally there''s no\n14:12 way of us doing that in collab so if you would like to run everything locally\n14:17 I''ll show you how quickly now if you would like to run in collab which I would recommend at least for the the\n14:23 first notebook chapters just skip ahead there will be chapter points\n14:29 in the timeline of the video so for running running it locally we just come\n14:34 down to here so this actually tells you everything that you need so you will\n14:40 need to install uvie all right so this is the package manager that we recommended by the python and package\n14:47 management Library you don''t need to use uvie it''s it''s up to you uvie is is very\n14:53 simple it works really well so I would recommend that so you would install it with this command here this is on Mac so\n15:01 it will be different otherwise if you are on Windows or otherwise you can uh\n15:07 look at the installation guide there and it''ll tell you what to do and so before we actually do this what I will do is go\n15:13 ahead and just clone this REO so we''ll come into here I''m going to\n15:19 create like a temp directory for me because I already have the line chain course in there and what I''m going to do\n15:26 is just get loan line chain course okay so you will also need to install git if\n15:31 you don''t have that okay so we have that then what we''ll do is copy this okay so\n15:38 this will install python 3.2.7 for us with this command then this will create\n15:45 a new VM within that or using python 3.2.7 that we''ve installed and then UV\n15:53 sync we actually be looking at the Pi Project at TL file that''s like the uh\n15:59 the package installation for the repo and using that to install everything\n16:04 that we need now we should actually make sure that we are within the line chain course directory and then yes we can run\n16:11 those three and there we go so everything should install with that now if you are\n16:20 in cursor you can just do cursor dot or we can run code do if mvs code I''ll just\n16:28 be running this this and then I''ve opened up the course now within that course you have your notebooks and then\n16:35 you just run through these making sure you select your kernel pth environment and making sure you''re using the correct\n16:41 VN from here so that should pop up already as this VM bin Python and you''ll\n16:47 click that and then you can run through when you are running locally don''t run these you don''t need to you''ve already\n16:54 installed everything so you don''t this specifically is for collab so that is\n16:59 running things locally now let''s have a look at running things in collab so for\n17:06 running everything in collab we have our notebooks in here we click through and then we have each of the chapters\n17:12 through here so starting with the first chapter the introduction which is where we are\n17:18 now so what you can do to open this in collab is either just click this collab\n17:24 button here or if you really want to for example Maybe this it is not loading for\n17:31 you what you can do is you can copy the URL at the top here you can go over to\n17:36 collab you can go to open GitHub and then just paste that in\n17:43 there and press enter and there we go we have our\n17:48 notebook okay so we''re in now uh what we will do first is just install the\n17:53 prerequisites so we have line chain just a load of line chain packages here line\n17:59 chain core line chain open a because we''re using open Ai and line chain Community which is needed for running\n18:06 what we''re running okay so that has installed everything for us so we can\n18:12 move on to our first step which is initializing our LM so we''re going to be\n18:19 using GT40 mini which is side of small but fast but also cheaper model uh that\n18:26 is also very good for open AI so what we need to do here is get an API key okay\n18:33 so for getting that API key we''re going to go to open''s website and you can see\n18:39 here that we''re opening platform. open.com and then we''re going to go into settings organization API\n18:45 keys so you can copy that I''ll just click it from here okay so I''m going to\n18:50 go ahead and create a new secret key actually just in case you''re kind of\n18:56 looking for where this is It''s settings organization API Keys again okay create\n19:01 a new API key I''m going to call it Line train\n19:06 course I''ll just put it on the semantic router that''s just my organization you you put it wherever you want it to be\n19:14 and then you would copy your API key you can see mine here I''m obviously going to\n19:19 reval that before you see this but you can try and use it if you really like so I''m going to copy that and I''m going to\n19:25 place it into this little box here you could also just and place it put your uh\n19:31 full API key in here it''s up to you but this little box just makes things easier\n19:36 now that what we''ve basically done there is just passed in our API key we''re setting our opening model GT40 mini and\n19:45 what we''re going to be doing now is essentially just connecting and setting up our llm parameters with L chain so we\n19:53 run that we say okay we''re using a GT4 mini and we''re also setting ourselves up\n19:59 to use two different LMS here or two of the same LM with slightly different\n20:05 settings so the first of those is an LM with a temperature setting of zero the\n20:10 temperature setting basically controls almost the randomness of the output of\n20:17 your llm and the way that it works is when an LM is predicting the sort of\n20:24 next token or next word in sequence know provide a probability actually for all\n20:29 of the tokens within the lm''s knowledge base or what the LM has been trained on\n20:35 so what we do when we set temperature of zero is we say you are going to give us\n20:41 the token with highest probability according to you okay whereas when we\n20:47 set a temperature of 0.9 what we''re saying is okay there''s actually an increased probability of you giving us a\n20:55 token that according to your generated output is not the token with the highest\n21:01 probability according to the lmm but what that tends to do is give us more sort of creative outputs so that''s what\n21:08 the temperature does so we are creating a normal llm and then a more creative\n21:13 llm with this so what are we going to be building we''re going to be taking a\n21:19 article draft so like a draft article uh from the aelio learning page and we''re\n21:27 going to be using line chain to generate various sces that we might um find helpful as we''re you know we have this\n21:34 article draft and we''re editing it and just kind of like finalizing it so what are those going to be you can see them\n21:40 here we have the title for the article the description and SEO friendly\n21:45 description specifically third one we''re going to be getting the LM to Providers\n21:50 advice on existing paragraph and essentially writing a new paragraph for us from the existing paragraph and what\n21:57 it''s going to do this is the structured output part is going to write a new version of that paragraph for us and\n22:03 it''s going to give us advice on where we can improve our writing then we''re going to generate a thumbnail hero image for\n22:10 our article so nice image that you would put at the top so here we''re just going\n22:16 to input our article you can you can put something else in here if you like essentially this is just a big article\n22:24 that''s written a little while back on agents and and now we can go ahead and\n22:29 start preparing out our prompts which are essentially the instructions for our llm so line chain comes with a lot of\n22:38 different uh like utilities for prompts and we''re going to dive into them in a lot more detail but I do want to just\n22:44 give you uh the Essentials now just so you can understand what we''re looking at at least conceptually so prompts for\n22:51 chat agents are at a minimum broken up into three components those are the\n22:57 system pront this provides instructions to our LM on how it should behave what its objective is and how it should go\n23:03 about achieving that objective generally system prompts are going to be a bit\n23:08 longer than what we have here depending on the use case then we have our user prompts so these are user written\n23:15 messages usually sometimes we might want to pre-populate those if we want to encourage a particular type of um\n23:22 conversational patterns from our agent but for the most part yes these are going to be using generated then we have\n23:29 our AI promps so these are of course AI generated and again in some cases we\n23:36 might want to generate those ourselves beforehand or within a conversation if we have a particular reason for doing so\n23:43 but for the most part you can assume that these are actually user and AI generated now the line chain provides us\n23:51 with templates for each one of these prompt types let''s go ahead and have a\n23:57 look at what these look like within line chain so to begin we are looking at this\n24:03 one so we have our system message prom template and human message which the the\n24:09 user that we saw before so we have these two system prom keeping it quite simple here you are AI system that helps\n24:16 generate article titles right so so our first component where we want to generate is article title so we''re\n24:22 telling the AI that''s what we want it to do and then here right so here we''re\n24:29 actually providing kind of like a template for a user input\n24:36 so yes as I mentioned user input can be\n24:41 um it can be fully generated by user it might be kind of not generated by user\n24:47 it might be setting up a conversation beforehand which a user would later use or in this scenario we''re actually\n24:54 creating a template and the what the user will providers will actually just be inserted\n25:00 here inside article and that''s why we have this import variables so what this\n25:07 is going to do is okay we have all of these instructions around here they''re all going to be provided to openai as if\n25:14 it is the user saying this but it will actually just be this here that user\n25:20 will be providing okay and we might want to also format this a little nicer it kind of depends this will work as it is\n25:27 but we can also put you know something like this to make it a little bit clearer to the llm okay what is the\n25:34 article where the prompts so we have that and you can see in this scenario\n25:41 there''s not that much difference between what the system prompt and user prom is doing and this is It''s a particular scenario it varies when you get into the\n25:49 more conversational stuff as we will do later uh you''ll see that the user prompt\n25:54 is generally more fully user generated or mostly user generated and much of\n26:01 these types of instructions we might actually be putting into the system prompt it varies and we''ll see\n26:07 throughout of course many different ways of using these different types of PRS in various different\n26:13 places then you''ll see here so I just want to show you how this is working we\n26:19 can use this format method on our user prompt here to actually insert something\n26:25 within the uh article input here so we''re going to go us prompt format and\n26:31 then we pass in something for article okay and we can also maybe format this a little nicer but I''ll just show you this\n26:37 for now so we have our human message and then inside the content this is the the text that we had right you can see that\n26:43 we have all this right and this is what we wrote before we wrote all this except from this part we didn''t write this\n26:50 instead of this we had article right so let''s format this a little nicer so we\n26:57 can see okay so this is exactly what we wrote up here exactly the same except\n27:02 from now we have test string instead of article so later when we insert our\n27:07 article it''s going to go inside there allly doing it''s like it''s an it''s an F string in Python okay and this is again\n27:15 this is one of those things where people might complain about Lang chain you know this sort of thing can be you it seems\n27:21 excessive because you could just do this with an nring but there are as we''ll see later particularly when you''re streaming\n27:27 just really helpful features that come with using line chains kind of built-in\n27:34 uh prompt templates or at least uh message objects that we''ll see so\n27:41 you we need to uh keep that in mind again as soon as you get more complicated line chain can be a bit more\n27:47 useful so chat prom template uh this is basically just going to take what we\n27:53 have here our system promt user prompts you could also include some AI prompts in there and what it''s going to do is\n27:59 merge both of those and then when we do format what it''s going to do is put both\n28:06 of those together into a chat history okay so let''s see what that looks like first uh in a more messy way okay so you\n28:15 can see we have just the content right so it doesn''t include the whole you know\n28:21 before we had human message we''re not include we''re not seeing anything like that here instead we''re just seeing the\n28:26 string so now let''s switch back to print and we can see that what we have\n28:33 is our system message here it''s just prefixed with this system and then we have human and it''s prefixed by human\n28:39 and then it continues right so that''s that''s all it''s doing it''s just kind of merging those in some sort of chat lug we could also put in like AI messages\n28:46 and they would appear in there as well okay so we have that now that is our\n28:51 prompt template let''s put that together with an LM to create what would be in\n28:57 past line ch be called an llm chain uh now we wouldn''t necessarily call it an llm chain because we''re not using the\n29:03 llm chain abstraction it''s not super important if that doesn''t make sense we we''ll go into it in more detail later\n29:10 particularly in the in the ELO chapter so what this chain will do you\n29:17 think L chain is just chains where''re chaining together these multiple components it will perform the STS\n29:24 prompt formatting so that''s what I just showed you LM generation so sending our\n29:31 prom to open AI getting a response and getting that output so you can also add\n29:37 another set here if you want to format that in a particular way we''re going to be outputting that in a particular\n29:42 format so that we can feed it into the next set more easily but there are also things called output passes which pass\n29:49 your output in a more dynamic or complicated way depending on what you''re doing so this is our first look at Elsa\n29:58 don''t want us to focus too much on the syntax here because we will be doing that later but I do want you to just\n30:04 understand what is actually happening here and logically what are we writing\n30:11 so all we really need to know right now is we Define our inputs with the first\n30:17 dictionary segment here right so this is a you know our inputs which we have\n30:23 defined already okay so if we come up to our\n30:29 user prompt here we said the input variable is our article right and we might have also added input variables to\n30:34 the system prompt here as well in that case you know let''s say we had your AI assistant\n30:42 called name right that helps generate article\n30:47 titles in this scenario we might have an input variables name here right and then\n30:55 what we would have to do down here is we would also have to pass that\n31:01 in right so also we would have article but we would also have name so basically\n31:09 we just need to make sure that in here we''re including the variables that we have Define as input variables for our\n31:16 our first prompts okay so we can actually go ahead and let''s add that uh so we can see it''s in action so we''ll\n31:23 run this again and just include that or or reinitialize is our first prompt so\n31:29 we see that and if we just have a look at what that means for this format\n31:35 function here it means we''ll also need to pass in a name okay and call it Joe\n31:40 okay so Joe the AI right so you are an AI system called Joe now okay so we have\n31:46 Joe our AI that is going to be fed in through these input variables then we have this pipe operator the pipe\n31:53 operator is basically saying whatever is to the left of the pipe operator which\n31:58 in this case would be this is going to go into whatever is on the right of the pipe operator it''s that''s simple again\n32:05 we''ll we''ll dive into this and kind of break it apart in the Elso chapter but for now that''s all we need to know so\n32:11 this is going to go into our first prompt that is going to format everything it''s going to add the name\n32:17 and the article that we provided into our first prompt then it''s going to Output that right going to Output that\n32:23 we have our P operate here so the output of this is going to go into the input of our Next Step it''s our creative\n32:30 LM then that is going to generate some tokens it''s going to generate our output\n32:36 that output is going to be an AI message and as you saw before if I take this bit\n32:44 out within those message objects we have this content field okay so we are\n32:50 actually going to extract the content field out from our AI message to just\n32:56 get the content and that is what we do here so we get the AI message out from ilm and then we''re extracting the\n33:02 content from that AI message object and we''re going to passing it into a dictionary that just contains article\n33:07 title like so okay we don''t need to do that we can just get the AI message directly I just want to show you how we\n33:15 are using this sort of chain in Elso so once we have set up our chain we then\n33:23 call it or execute it using the invoke method into that we will need to pass in\n33:28 those variables so we have our article already but we also gave our AI a name now so let''s add that and we''ll ruin\n33:36 this okay so Joe has generated us a article title unlocking the future the\n33:43 rise of neuros symbolic AI agents cool much better name than what I gave the article which was AI agents are neuros\n33:52 symbolic systems no I don''t think I did too bad okay so we have that\n33:58 now let''s continue and what we''re going to be doing is building more of these\n34:03 types of LM chain pipelines where we''re feeding in some prompts we''re generating\n34:10 something getting something and and doing something with it so as mentioned we have the title we''re\n34:16 now moving on to the description so to generate description so we have our human message prompt template so this is\n34:22 actually going to go into a similar format as before we also\n34:28 want to redefine this because I think I''m using the same system message there\n34:33 so let''s let''s go ahead and do modify that or what we could also do is let''s\n34:40 just remove the name now because I''ve showing that so what we could do is\n34:46 you''re an AI system that helps build good articles right build good\n34:53 articles and we could just use this as our you know generic system prompt now\n34:58 so let''s say that''s our new system prompt now we have our user prompt your task creating description for the\n35:04 article the article is here fure examine article here is the article title okay so we need the article title now as well\n35:10 in our input variables and then we''re going to Output an AO friendly article description and we''re just saying you\n35:17 just to be certain here do not output anything other than the description so you know sometimes an LM might say hey\n35:23 look this is what I''ve generated for you the reason I think this is good is because so on and so on so on right if you''re programmatically taking some\n35:31 output from an LM you don''t want all of that fluff around what the LM is generated you just want exactly what\n35:37 you''ve asked it for okay because otherwise you need to pass out with code and it can get messy and also just far\n35:44 less reliable so we''re just saying do iput anything else then we''re putting all these together so system prompt and\n35:50 the second user prompt this one here putting those together into a new chat\n35:55 prompt template and then we''re going to to feed all that in to another LOL chain\n36:00 as we have here to well to generate our our description so let''s go ahead we\n36:06 invoke that as before we''re just make sure we add in the article title that we got from before and let''s see what we\n36:13 get okay so we have this explore the transformative potential of neuros symbolic Ai ageny and a little bit long\n36:21 to be honest but yeah you can see what it''s doing here right and of course we could then go in we see this is kind of\n36:26 too long right a yeah SEO friendly description not not really so we can\n36:33 modify this output the SEO friendly description um make sure we don''t exceed\n36:41 let me put that on a new line make sure we don''t exceed say 200 characters or\n36:46 maybe it''s even less to se I don''t I don''t have a clue I''ll just say 120 characters I do not outly anything other\n36:53 than the description right so we could just you know go back modify our prompting see what that generates again\n36:59 okay so much shorter probably too short now but that''s fine cool so we have that we have a summary process that and\n37:06 that''s now you know in this dictionary form that we have here cool now the\n37:12 third step we want to consume that first article variable with our full article\n37:18 and we''re going to generate a few different output Fields so for this\n37:24 we''re going to be using the structured output feature so let''s scroll down\n37:29 we''ll see what that is what that looks like so structured output is essentially\n37:36 we''re forcing their lmic like it has to Output a dictionary with these you know\n37:41 particular Fields okay and we can modify this quite a bit but in this scenario\n37:47 what I want to do is I want there to be an original paragraph right so I just want it to regenerate the original\n37:53 paragraph cuz I''m lazy and I don''t want to extract it out then I want to get the new edited paragraph This is the LM\n38:00 generated improved paragraph and then we want to get some feedback because we we don''t want to just automate ourselves we\n38:07 want to augment ourselves and get better with AI rather than just being like I\n38:13 you do you do this so that''s what we do here and you can see that here we''re using this pantic object and what pantic\n38:21 allows us to do is Define these particular fields and it also allows us to assign these descriptions to a field\n38:27 and and line chain is actually going to go ahead read all of this right even reads so for example we could put\n38:33 integer here and we could actually get a numeric score for our paragraph right we\n38:39 can try that right so let''s uh let''s let''s just try that quickly I''ll show you so numeric numeric score in fact\n38:48 let''s even just ignore let''s not put anything here so I''m going to put constructive feedback on the original\n38:53 paragraph but I just put into here so let''s see what happens okay so we have that and what I''m going to do is I''m\n38:59 going to get our creative llm I''m going to use this with structured output method and that''s actually going to\n39:05 modify that llm class create a new llm class that forces that llm to use this\n39:10 structure for the output right so passing in paragraph into here using this we''re creating this new structure\n39:17 LM so let''s run that and see what happens okay so we''re going to modify\n39:23 our chain accordingly maybe what I can do let''s also just remove this bit for now\n39:30 so we can just see what the strictured llm outputs directly and let''s\n39:36 see okay so now you can see that we actually have that paragraph object\n39:42 right the one we defined up here which is kind of cool and then in there we have the original paragraph right so\n39:48 this is where this is coming from I definitely remember writing something\n39:53 that looks a lot like that so I think that is correct we have the edited par so this is okay what thinks it''s better\n40:00 and then interestingly the feedback is three which is weird right because uh\n40:06 here we said the constructive feedback on the original paragraph but what we''re doing when we use this with structured\n40:12 output but what Lang chain is doing is is essentially performing a tool core to open Ai and what a tool core can do is\n40:20 force a particular structure in the output of an LM so when we say feedback\n40:26 has to be an integer no matter what we put here it''s going to give us an integer because how do you provide\n40:31 constructive feedback with an integer it doesn''t really make sense but because we''ve set that limitation that\n40:38 restriction here that is what it does it just gives us the uh a numeric value so\n40:44 I''m going to shift that to string and then let''s rerun this see what we get okay we should now see that we actually\n40:50 do get constructive feedback all right so yeah you can see it''s quite quite long so the original paragraph\n40:57 effectively communicates the limitations of neuro AI systems in performing certain tests however it could benefit\n41:03 from slightly improved Clarity and conciseness for example the phrase was becoming clear can be made more direct\n41:09 by changing it to became evident yeah true thank you very much so yeah now we\n41:17 actually get that that feedback which is pretty nice now let''s add in this final\n41:22 setep to our chain okay and it''s just going to pull out our paragraph object here and\n41:29 extracting into a dictionary we don''t necessarily need to do this honestly I actually kind of prefer it within this paragraph object but just so we can see\n41:38 how we would pass things on the other side of the chain okay so now we can see\n41:44 we''ve extracted that out cool so we have all of that interesting feedback again\n41:52 but let''s leave it there for the text part of this now let''s have a look at at\n41:57 the sort of multimodal features that we can work with so this is you know maybe one of those things that kind of seems a\n42:04 bit more abstracted a little bit complicated where it maybe could be improved but you know we''re not going to\n42:10 really be focusing too much on the M time modal stuff sub be focusing on language but I did want to just show you\n42:16 very quickly so we want this article to look better okay we want to\n42:22 generate a prompt based on the article it''s self that we can then pass to DAR\n42:30 the the image generation model from open AI that will then generate an image like like a thumbnail image for us okay so\n42:39 the first step of that is we''re actually going to get an LM to generate that right so we have our prompt that we''re\n42:44 going to use for that so I''m say generate a prompt with less than 500 characters to uh generate an image based\n42:52 on the following article okay so that''s our prompt yeah super simple uh using\n42:57 the generic prompt template here you can use that you can use user uh prompt template it''s up to you this is just\n43:03 like the generic prom template then what we''re going to be doing is based on what\n43:10 this outputs we''re then going to feed that in to this generate and display image function via the image prompt\n43:17 parameter that is going to use the darly API rapper from line chain it''s going to\n43:23 run that image prompt and we''re going to get a a eurl out from that essentially and then we''re going to read that using\n43:29 SK image here right so we''re just going to read that image URL going to get the image data and then we''re just going to\n43:35 display it okay so pretty straightforward now again this is a lell\n43:43 thing here that we''re doing we have this runable Lambda thing when we''re running\n43:49 functions within lell we need to wrap them within this runable Lambda I you\n43:54 know I don''t want to go too much into what this is doing here because we do cover in the L cell chapter but it''s\n44:01 just you know all you really need to know is we have a custom function wrap in runable Lambda and then what we get\n44:07 from that we can use within this here right the the L Sal syntax so what are\n44:14 we doing here let''s figure this out we are taking our original that image prom that we defined just up here right input\n44:21 variable to that is article okay we have our article d being input here feeding\n44:28 that into our prompt from there we get our message that we then feed into our\n44:33 llm from the LM it''s going to generate us a like an image prompt like a prompt\n44:39 for generating our image for this article we can even Let''s uh let''s print that out so that we can see what it\n44:46 generates because I''m also kind of curious okay so we''ll just run that and\n44:52 then let''s see it will feed in that content into our room reable which is\n44:58 basically this function here and we''ll see what it generates okay don''t expect\n45:03 anything amazing from darly it''s not it''s not the best to be honest but we at\n45:08 least we see how to use it okay so we can see the prom that was used here\n45:14 create an image that visually represents the concept of neuros symbolic agents depict a futuristic interface where\n45:20 large D interacts with traditional code symbolizing integration of oh my gosh uh\n45:27 something computation include elements like a brain to represent neur networks gears or circuits or symbolic logic and\n45:35 web of connections illustrating vast use cases of AI agents oh my gosh look at\n45:41 all that big prompt then we get this so you know dar''s interesting I would say we\n45:48 could even take this let''s just see what that comes up with in something like mid\n45:53 Journey you can see these way cooler images that we get from just another image generation model far better but\n46:00 pretty cool honestly so in terms of Generation image the phrasing the The\n46:05 Prompt itself is actually pretty good the image you know could be better but\n46:11 that''s it right so with all of that we''ve seen a little introduction to what\n46:16 we might build in with lighing chain so that''s it for our introduction chapter as I mentioned we don''t want to go too\n46:22 much into what each of these things is doing just really want to focus on okay\n46:29 this is kind of how we''re building something with line chain this is the overall flow uh but we don''t really want\n46:37 to be focusing too much on okay what exactly lell is doing or what exactly uh\n46:42 you know this prompt thing is that we''re setting up we''re going to be focusing\n46:48 much more on all of those things and much more in the upcoming chapters so\n46:53 for now we''ve just seen a little bit of what we can build before diving in in more detail okay so now we''re going to\n47:00 take a look at AI observability using lsmith now lsmith is another piece of\n47:08 the broader Lang chain ecosystem its focus is on allowing us to see what our\n47:15 llms agents Etc are actually doing and it''s something that we would definitely recommend using if you are going to be\n47:22 using line chain Lang graph now let''s take a look at how we would set L Smith up which is incredibly simple so I''m\n47:29 going to open this in collab and I''m just going to install the prerequisites here you''ll see these are all the same\n47:36 as before but we now have the Lin Smith Library here as well now we are going to be using Lin Smith throughout the course\n47:43 so in all the following chapters we''re going to be importing limith and that will be tracking everything we''re doing\n47:49 but you don''t need Lin Smith to go through the course it''s an an optional dependency but as mentioned I would\n47:55 recommend it so we''ll come down to here and first thing that we will need is the line chain API key now we do need an API\n48:03 key but that does come with a reasonable free tier so we can see here they have\n48:09 each of the plans and this is the one that we are by default on so it''s free\n48:16 for one user up to 5,000 traces per month if you''re building out an\n48:21 application I think it''s fairly easy to go beyond that but it really depends on what you''re building so it''s a good\n48:28 place to start with and then of course you can upgrade as required so we would go to smith. L\n48:36 chain.com and you can see here that this will log me in automatically I have all\n48:41 of these tracing projects these are all from me running the various chapters of the course yours if you do use l Smith\n48:48 throughout course your L Smith dashboard will end up looking something like this now what we need is an API key so we go\n48:56 over to settings we have API keys and we''re just going to create an API key because we''re\n49:03 just going through some personal learning right now I would go with personal access token we can give a name or description if you want okay and\n49:10 we''ll just copy that and then we come over to our notebook and we enter our API key there and that is all we\n49:16 actually need to do that''s absolutely everything supposed the one thing to be aware of is that you should set your L\n49:22 chain project to whatever project you''re working within so of course Within within the course we have individual\n49:28 project names for each chapter but for your own projects of course you should make sure this is something that you\n49:34 recognize and is useful to you so L Smith actually does a lot without needing to do anything so we can\n49:40 actually go through let''s just initialize our LM and start invoking it and seeing what L Smith returns to us so\n49:48 we''ll need our open API key enter it here and then let''s just invoke\n49:55 hello okay so nothing has changed on this end right so us running the code there''s nothing different here however\n50:02 now if we go to Lang Smith I''m going to go back to my dashboard okay and you can\n50:08 see that the the order of these projects just changed a little bit and that''s because the most recently used project I\n50:15 this one at the top Lang chain course Lang Smith openai which is the current chapter we''re in that was just triggered\n50:21 so I can go into here and I can see oh look at this so we actually have something in the Lang Smith UI and we\n50:28 didn''t all we did was enter our L train apid that''s all we did and we set some environment variables and that''s it so\n50:34 we can actually click through to this and it will give us more information so you can see what was the\n50:39 input what was the output and some other metadata here you see you know there''s not that much in\n50:46 here however when we do the same for agents we''ll get a lot more information\n50:52 so I can even show you a quick example from the future chapters if we come through to agents\n51:00 intro here for example and we just take a look at one of\n51:05 these okay so we have this input and output but then on the left here we get all this information and the reason we\n51:12 get all this information because agents are they''re performing multiple LM calls etc etc so there''s a lot more going on\n51:20 so we can see okay what was the first LM call and then we get these tool use traces we get another LM another rmm\n51:27 call another tool use and another LM call so you can see all this information which is incredibly useful and\n51:33 Incredibly easy to do because all I did when setting this up in that agent chapter was simply set the API key and\n51:40 the environment variables as we have done just now so you get a lot out of\n51:46 very little effort with Lang Smith which is great so let''s return to our Lang Smith project here and let''s invoke some\n51:53 more now I''ve already shown you you know we''re going to see a lot of things just by default but we can also add other\n52:00 things that Lang Smith wouldn''t typically Trace so to do that we will\n52:05 just import a traceable decorator from Lang Smith and then let''s make these\n52:12 just random functions traceable within limith okay so we''ll run those we have\n52:19 three here so we''re going to generate a random number we''re going to modify how\n52:24 long a function takes and also generate a random number and then in this one\n52:30 we''re going to either return this no error or we''re going to raise an error\n52:36 so we''re going to see how limith handles these different scenarios so let''s just\n52:41 iterate through and run those a few times so we''re just going to run each one of those 10\n52:47 times okay so let''s see what happens so they''re running let''s go over to our Lin\n52:54 sth UI and see what is happening over here so we can see that everything is updating we adding that information\n52:59 through and we can see if we go into a couple of these we can see a little more information so have the input and the\n53:05 output took three seconds see random error here in this\n53:12 scenario random error passed without any issues let me just refresh the page\n53:18 quickly okay so now we have the rest of that information and we can see that occasionally if there is an error from\n53:24 our random error function it is is signified with this and we can see the\n53:30 traceback as well that was returned there which is useful okay so we can see if an error has been raised we have to\n53:35 see what that error is we can see the various latencies of these functions so\n53:42 you can see that varying throughout here we see all the inputs to each one\n53:48 of our functions and then of course the outputs so we can see a lot in there\n53:53 which is pretty good now another thing that we can do do is we can actually filter so if we come to here we can add\n54:01 a filter let''s filter for errors that would be value error and\n54:06 then we just get all of the cases where one of our functions has returned or\n54:12 raise an error or value error specifically okay so that''s useful and then yeah there''s there''s various other\n54:19 filters that we can add there so we could add a name for example if we want\n54:24 to look for the generate string delay function only we could also do\n54:30 that okay and then we can see the varying latencies of that function as well cool so we have\n54:37 that now one final thing that we might want to do is maybe we want to make\n54:43 those function names a bit more descriptive or easy to search for for example and we can do that by saying the\n54:50 name of the traceable decorator like so so let''s run that we''ll run this a few\n54:56 times and then let''s jump over to limith again going to limith project okay and you can\n55:02 see those coming through as well so then we could also search for those based on that new name so what was it chitchat\n55:09 maker like so and then we can see all that information being streamed through\n55:16 to limith so that is our introduction to limith there is really not all that much\n55:23 to go through here it''s very easy to sell up and as we scen it gives us a lot of observability into what we are building\n55:31 and we will be using this throughout the course we don''t rely on it too much it''s a completely optional dependency so you\n55:37 don''t want to use l space you don''t need to but it''s there and I would recommend doing so so that''s it for this chapter\n55:43 we''ll move on to the next one now we''re going to move on to the chapter on\n55:48 prompts in Lang chain now prompts they seem like a simple concept and they are\n55:53 a simple concept but there''s actually quite a lot to them when you start diving into them and they truly have\n56:00 been a very fundamental part of what has propelled us forwards from pre llm times\n56:07 to the current llm times you have to think until llms became widespread the\n56:14 way to fine-tune a AI model or ml model\n56:19 back then was to get loads of data for your particular use case spend a load of\n56:26 training your specific Transformer or part of the Transformer to essentially\n56:31 adapt it for that particular task that could take a long time depending on the\n56:38 the task it could take you you know months or in some times if it was a\n56:43 simpler task it might take probably days potentially weeks now the interesting\n56:48 thing with L LMS is that rather than needing to go through this whole\n56:54 fine-tuning process to to modify a model for one task over\n57:00 another task rather than doing that we just prompt it differently we literally tell the model hey I want you to do this\n57:07 in this particular way and that is a you know that''s a paradigm shift in what you''re doing it''s so much faster it''s\n57:14 going to take you you know a couple of minutes rather than days weeks or months and LMS are incredibly powerful when it\n57:22 comes to just generalizing to you know across these many different tasks so prompts which control those instructions\n57:31 are a fundamental part of that now line chain naturally has many functionalities\n57:37 around prompts and we can build very Dynamic prompting pipelines that modify\n57:43 the structure and content of what we''re actually feeding into our llm depending on different variables different inputs\n57:49 and we''ll see that in this chapter so we''re going to work through prompting\n57:54 within the scope of of a rag example so let''s start by just dissecting the\n58:01 various parts of a prompt that we might expect to see for a use case like rag so\n58:07 our typical prompt for rag or retrieval augmented generation will include rules\n58:14 for the LM and this is this you will see in most prompts if not all this part of\n58:22 the promt sets up the behavior of the llm that is how it should be responding\n58:28 to user queries what sort of Personality it should be taking on what it should be\n58:33 focusing on when it is responding any particular rules or boundaries that we want to set and really what we''re trying\n58:41 to do here is just to Simply provide as much information as possible to the llm\n58:48 about well what we''re doing we just want to give the llm context as to the the\n58:56 place that it finds itself in because an LM has no idea where it is it''s just it''s a it takes in some information and\n59:04 spits out information if the only information it receives is from the user you know user query it has you know\n59:10 doesn''t know the context what is the application that it is within what is its objective what is its aim what are\n59:17 the boundaries all of this we need to just assume the llm has absolutely no\n59:24 idea about because it it truly does not so as much context as we can provide but\n59:32 it''s important that we don''t overdo it it''s uh we see this all the time people will over prompt an llm you want to be\n59:39 concise you don''t want fluff and in general every single part of your prompt\n59:45 the more concise and less fluffy you can make it the better now those rules or\n59:50 instructions are typically in the system prompt of your llm now the second one is\n59:55 context which is rag specific the context refers to some sort of external\n1:00:00 information that you are feeding into your llm we may have received this information from like web search\n1:00:08 database query or quite often in this case of rag it''s a vector database this\n1:00:15 external information that we provide is essentially the r retrieval augmentation\n1:00:22 of rag we are augmenting the knowledge of our llm which the the knowledge of\n1:00:29 our LM is contained within the llm model weights we''re augmenting that knowledge with some external knowledge that''s what\n1:00:36 we''re doing here now for chat LMS this context is typically placed within a\n1:00:44 conversational context within the user or assistant messages uh and with\n1:00:52 more recent models it can also be placed within uh tool messages as well then we\n1:00:58 have the question this pretty straightforward this is the query from the user this is or is this usually a\n1:01:06 user message of course there might be some additional formatting around this you might add a\n1:01:12 little bit of extra context or you might add some additional instructions if you\n1:01:17 find that you L them sometimes VAR off the rules that you''ve set within the system prompt you might you know append\n1:01:24 or prefix something something here but for the most part it''s probably just going to be the user''s input and finally\n1:01:30 uh so these are all the inputs for our prompt here is going to be the output\n1:01:36 that we get so the answer from the assistant again I mean that''s not even specific to rag it''s just what you would\n1:01:43 expect in a in a chat llm or any LM and of course that would be an assistant\n1:01:49 message so putting all of that together in an actual prompt you can see everything we have here so we have the\n1:01:55 uh rules for our prompt here the instructions we''re just saying okay answer the question based on the context\n1:02:01 below if you cannot answer the question using the information answer with I don''t know then we have some context\n1:02:08 here okay in this scenario that context that we''re feeding in here because it''s\n1:02:14 the first message we might putting that into the system prompt but that may also be turned around okay if you if you for\n1:02:21 example have an agent you might have your question up here before the context\n1:02:27 and then that would be coming from a user message and then this context would follow the question and be recognized as\n1:02:34 a tool message it would be fed in that way as well kind of depends on on what\n1:02:40 sort of structure you''re going for there but you can do either you can feed it into the system message if it''s less\n1:02:45 conversational whereas if it''s more conversational you might feed it in as a tool message okay and then we have a\n1:02:51 user query which is here and then we''d have the AI answer okay and obviously\n1:02:56 that would be generated here okay so let''s switch across to the code we''re in\n1:03:01 the L chain course repo notebooks 03 prompts and I''m just going to open this in collab okay let scroll down and we''ll\n1:03:09 start just by installing the prerequisites okay so we just have the various libraries again as I mentioned\n1:03:16 before Lang Smith is optional you don''t need to install it but if you would like to see your tracers and everything in\n1:03:22 Lang Smith then I would recommend doing that and if you are using L Smith you will need to enter your API key here\n1:03:28 again if you''re not using Lang Smith you don''t need to enter anything here you just skip that cell okay cool and let''s\n1:03:36 jump into the basic prompting then so we''re going to start with this prompt\n1:03:42 answer used query based on the question below so we''re just structuring what we just saw uh in\n1:03:47 code and we''re going to be using the chat problem template because generally\n1:03:53 speaking we''re using chat llms in most most cases nowadays so we have our chat\n1:04:00 promp template and that is going to contain a list of messages system message to begin with which is just\n1:04:06 going to contain this and we''re feeding in the the context within that there and\n1:04:12 we have our user query here okay so we''ll run this and if we take a look\n1:04:21 uh here we haven''t specified what our input variables are okay but we can see\n1:04:28 that we have query and we have context up here right so we can see that okay\n1:04:33 these are the input variables we just haven''t explicitly defined them here so\n1:04:39 let''s just confirm with this that line chain did pick those up and we can see that it did\n1:04:45 so it has context and query as our input variables for the prompt template that\n1:04:50 we just defined okay so we can also see the structure of our temp plates let''s\n1:04:57 have a look okay so we can see that within messages here we have a system message\n1:05:03 prompt template the way that we Define this you can see here that we have from messages and this will consume various\n1:05:10 uh different structures so you can see here that it has a from messages it is a\n1:05:18 sequence of message like representation so we could pass in a system prompt\n1:05:24 template object and then a user prompt template object or we can just use a\n1:05:31 tupol like this and this actually defines okay this system this is a user and you could also do assistant or tool\n1:05:39 messages and stuff here as well using the same structure and then we can look in here\n1:05:44 and of course that is being translated into the system message prom template and human message prompt template okay\n1:05:53 we have our input variables in there and there and we have the template too okay\n1:05:59 now let''s uh continue we''ll see here what I what I just said so we''re\n1:06:05 importing our system message prompt template and human message prompt template and you can see we''re using the\n1:06:10 same from messages method here right and you can see it''s so sequence of message\n1:06:16 like representation it''s just you know what that actually means it can vary right so\n1:06:22 here we have system message prom template from template here from template query you know there''s various\n1:06:28 ways that you might want to do this it just depends on how explicit you want to\n1:06:33 be generally speaking I think for myself I would prefer that we\n1:06:40 stick with the objects themselves and be explicit but it is definitely a little\n1:06:45 harder to pass when you''re when you''re reading this so I understand why you might also prefer this it''s definitely\n1:06:52 cleaner and it is it does look simpler so it just depends I suppose on\n1:07:00 preference okay so we can see again that this is exactly the same okay with chat\n1:07:06 prom template and it contains this and this okay you probably want to see the\n1:07:12 exact output so it was messages okay exactly the same as what I\n1:07:19 output before cool so we have all that let''s see how we would invoke our l L with\n1:07:26 these we''re going to be using 40 mini again we do need our open API key so\n1:07:32 enter that and we''ll just initialize our LM we\n1:07:38 are going with a low temperature here so less Randomness or less\n1:07:43 creativity and you in in many cases this is actually what I would be doing the\n1:07:48 reason in this scenario that we''re going with a low temperature is we''re doing\n1:07:54 Rag and if you you remember before if we scroll up a little bit here our template says answer the user''s query based on\n1:08:00 the context below if you cannot answer the question using the provided answer information answer with I don''t know\n1:08:08 right so just from reading that we know that we want our llm to be as truthful\n1:08:15 and accurate as possible so a more creative llm is going to struggle with\n1:08:20 that and is more likely to hallucinate whereas a low creativity or\n1:08:26 low temperature llm will probably stick with the rules a little better so again it depends on your use case you know if\n1:08:33 you''re creative writing you might want to go with a higher temperature there but for things like rag where the\n1:08:40 information being output should be accurate and truthful it''s important I\n1:08:46 think that we keep temperature low okay I talk about that a little bit here so\n1:08:52 um of course lower temperature of zero makes the LM output more deterministic which in theory should lead to less\n1:08:59 hallucination okay so we''re going to go with L cell again here this is for those\n1:09:04 of you that use LINE chain pass this is equivalent to an llm chain object so our\n1:09:09 prompt template is being fed into our LM okay and from now we have this pipeline\n1:09:18 now let''s see how we would use that pipeline so going to get some uh create\n1:09:23 some context here so so this just some Contex around orelio\n1:09:30 AI mention that we built sematic routers SM junkers there AI\n1:09:37 platform and development services we mentioned I think we\n1:09:42 specifically outline this later on in the example so the Align chain experts little piece of information now most LMS\n1:09:49 would have not been trained on the recent internet so the fact that this\n1:09:54 came in September M 2024 is relatively recent so a lot of LMS out of the box\n1:10:00 you wouldn''t expect them to know that so that is a good little bit of information\n1:10:05 to ask about so we invoke we have our query so what do we do and we have that\n1:10:11 context okay so we''re feeding that into that pipeline that we defined here all right so when we invoke that that is\n1:10:18 automatically going to take query and context and actually feed it into our prompt template okay\n1:10:26 if we want to we can also be a little more explicit so you you will probably\n1:10:32 see me doing this uh throughout the course because I do like to be explicit\n1:10:37 with everything to be honest and you''ll probably see me doing\n1:10:47 this okay and this is doing the same thing or you''ll see it will in a moment\n1:10:54 this is doing the exact same thing again this is just a outo thing\n1:10:59 so all I''m doing in this scenario is I''m saying okay take from the dictionary\n1:11:08 query and then also take from that input dictionary the context\n1:11:16 key okay so this is doing the exact same thing uh the reason that we might want\n1:11:22 to write this is mainly for clarity to be honest just to explicit say okay\n1:11:27 these are the inputs because otherwise we don''t really have them in the code other than within our original prompts\n1:11:34 up here which is not super clear so I think it''s usually a good\n1:11:40 idea to just be more expc with these things and of course if you decide you''re going to modify things a little\n1:11:45 bit let''s say you modify this to input down the line you can still feed in the same input here you''re just you know\n1:11:52 mapping it between different Keys essentially or if you would like to just modify that I don''t know you need to\n1:11:58 locase it on the way in or something you can do so you have that I''ll just redefine\n1:12:08 actually and we''ll invoke again okay we see that it does the exact\n1:12:14 same thing okay so R AI so this is the AI message just generated by the llm\n1:12:21 okay expertise in building AI agents several open source framework router AI\n1:12:29 platform okay right so they have everything there other than the line\n1:12:34 train experts thing it didn''t mention that but we will yeah we''ll test it later on that okay so on to Future\n1:12:41 prompting this is a specific prompting technique now many sort of State ofthe art or also to LMS are very good at\n1:12:49 instruction following so you''ll find that fuch shop prompting is less common\n1:12:54 now than it used to be at least for the sort of bigger more safy art models but\n1:13:01 when you start using smaller models not really what we can use here but let''s say you''re using a open source model\n1:13:08 like llama 3 or llama 2 which is much smaller you will probably need to\n1:13:15 consider things like f shot prompting although that being said with the open AI models you''re at least the current\n1:13:23 open AI models this is not so important nonetheless it can be useful so the idea\n1:13:29 behind fuchia prompting is that you are providing a few examples to your llm of\n1:13:35 how it should behave before you are actually going\n1:13:41 into the main part of the conversation so let''s see how that would look so we\n1:13:46 create an example prom so we have our human in AI so human input AI response\n1:13:52 so we''re basically saying up okay this with this type of input you should provide this type of output that''s what\n1:13:58 we''re doing here and we''re just going to provide some examples okay so we have our input here''s query one here is the\n1:14:06 answer one right this is just I just want to show you how it works this is not what we''d actually feed into our LM\n1:14:13 then with both these examples and our example prompt we''d feed both of these\n1:14:18 into uh line chains few shot chat message prompt template okay and\n1:14:25 well you''ll see what we get out of it okay so we basically get it formats everything and structures everything for\n1:14:31 us okay and using this of course it depends\n1:14:38 on let''s say you see that your user is talking about a particular topic and you\n1:14:44 would like to guide your llm to talk about that particular topic and a particular way right so you could\n1:14:51 identify that the user is talking about that topic either like a keyword match or a semantic similarity match and based\n1:14:58 on that you might want to modify these examples that you feed into your few sh\n1:15:03 chat message promp template and then obviously for that could be what you do for topic a for topic B you might have\n1:15:10 another set of examples that you feed into this all all this time your example prompt is remaining the same but you''re\n1:15:16 you''re just modifying the examples that are going in so that they''re more relevant to whatever it is your user is\n1:15:21 actually talking about so that can be useful now let''s see an example of that so when we are using a tiny LM It''s\n1:15:29 ability would be limited although I think we are we''re probably fine here we''re going to say answer the US query\n1:15:36 based on the context below always answering mark down format you know being very specific the self system\n1:15:42 prompt okay that''s nice but what we''ve kind of said here is okay always\n1:15:48 answering mod down for I did do that but when doing so please provide headers\n1:15:54 short summary and follow bullet points then conclude okay so you see this here\n1:16:01 okay so we get this overview of already you have this and this it''s actually quite good but if we come down here what\n1:16:08 I specifically want is to always follow this structure right so we have the\n1:16:14 double header for the topic summary header a couple of bullet points and\n1:16:21 then I always want to follow this pattern where it''s like to conclude always it''s always bold you know I want to be very specific on\n1:16:27 what I want and to be you know fully honest with GT40 mini you can actually\n1:16:34 just prompt most of this in but for the sake of the example we''re going to provide a few shot um examples in our\n1:16:42 few shot prompt examples instead to get this so we''re going to provide one example here second example here and\n1:16:50 you''ll see we''re just following that same pattern we''re just setting up the pattern that the llm should use so we''re\n1:16:57 going to set that up here we have our main header a little summary some\n1:17:03 subheaders bullet points subheader bullet points subheader bullet points to conclude so on and so on same with this\n1:17:09 one here okay and let''s see what we\n1:17:15 got okay so this is the structure of our new F shop prompt template you can see\n1:17:24 what all this looks like let''s come down and we''re going to do we''re basically going to insert that directly into our\n1:17:31 chat prompt template so we have for messages system\n1:17:36 prompt user prompt and then we have in there these so let me actually show you\n1:17:44 very quickly right so we just have um this few shot chat to message prompt template\n1:17:50 which will be fed into the middle here run that and then feed all this back into our pipeline okay and this will you\n1:17:57 know modify the structure so that we have that bold to conclude at the end here okay we can see nicely here so we\n1:18:04 get a bit more of that exact structure that we were getting again with GT40\n1:18:10 models and many other opening air models you don''t really need to do this but you will see it in other examples we do have\n1:18:17 an example of this where we''re using a llama and we''re using I think llama 2 if\n1:18:23 I''m not wrong and you can see that adding this fuse shot promp template is\n1:18:29 actually a very good way of getting those smaller less capable models to follow your instructions so this is RAR\n1:18:37 when you''re working those smaller lenss this can be super useful but even for so models like\n1:18:42 gp40 if you do find that you''re struggling with the prompting it''s just not quite following exactly what you\n1:18:48 want it to do this is a very good technique for actually getting it to\n1:18:53 follow a very straight structure or behavior okay so moving on we have Chain of Thought prompting so this is a more\n1:19:02 common prompting technique that encourages the LM to think through its\n1:19:08 reasoning or its thoughts step by step so it''s Chain of Thought the idea behind\n1:19:13 this is that okay in math class when you''re a kid the teachers would always\n1:19:19 push you to put down your your working out right and there was a more reasons\n1:19:25 for that one of them is to get you to think because they they know in a lot of cases actually you know you''re a kid and you''re in Aran you don''t really care\n1:19:31 about this test and the you know they''re just trying to get you to slow down a\n1:19:37 little bit and actually put down your reasoning and that kind of forc you to think oh actually I''m skipping a little\n1:19:43 bit in my head because I''m trying to just do everything up here if I write it down all of a sudden it''s like oh\n1:19:48 actually I yeah I need to actually do that slightly differently you you realize okay you''re probably rushing now\n1:19:55 I''m not saying an LM is rushing but it''s a similar effect by an LM writing everything down they tend to actually\n1:20:01 get things right more frequently and at the same time also similar to when\n1:20:07 you''re a child and a teacher is reviewing your exam work by having the LM write down its reasoning you as a as\n1:20:15 a human or engineer you can see where the llm went wrong if it did go wrong\n1:20:21 which can be very useful when you''re trying to diagnose problems so with train of thought we should see uh less\n1:20:27 hallucinations and generally better performance now to implement train of thought in line chain there''s no\n1:20:32 specific like line chain objects that do that instead it''s it''s just prompting okay so let''s go down and just see how\n1:20:39 we might do that okay so be helpful assistant and answer users question you\n1:20:44 must answer the question directly without any other text or explanation okay so that''s our no Chain of Thought\n1:20:51 system problems I will just note here especially with open AI again this is one of those things where you''ll see it\n1:20:57 more with the smaller models most LMS are actually trained to use train thought prompting by default so we''re\n1:21:03 actually specifically telling it here you must answer the question directly without any other text or explanation\n1:21:09 okay so we''re actually kind of reverse prompting it to not use train of thought otherwise by default it actually will\n1:21:16 try and do that because it''s been trained to that''s how that''s how relevant Chain of Thought is okay so I''m\n1:21:22 going to say how many key strokes I need to type in type the numbers from 1 to 500 okay we set up our like llm chain\n1:21:31 Pipeline and we''re going to just invoke our query and we''ll see what we get\n1:21:36 total number of key strokes needed to type the numbers from one to 500 is\n1:21:43 1,511 uh the actual an as I''ve written here is 1,392 without chain thought is\n1:21:50 hallucinating okay now let''s go ahead and see okay with Chain of Thought apprting what does it do so be helpful\n1:21:57 assistant and answer user question to answer the question you must list systematically and in precise detail all\n1:22:05 sub problems that are needed to be solved to answer the question solve each sub problem individually you have to\n1:22:11 shout at the LM sometimes to get them to listen and in sequence finally use\n1:22:17 everything you''ve worked through to provide the final answer okay so we''re getting it we''re forcing it to kind of\n1:22:22 go through the full problem there can remove that not sure why that''s there so\n1:22:27 run that again I don''t know why we have context there I remove that and let''s\n1:22:35 see you can see straight away that''s taking a lot longer to generate output\n1:22:40 that''s because it''s generating so many more tokens so that''s just one one drawback of this but let''s see what we\n1:22:46 have so to determine how many keystrokes to tie those numbers we is breaking down\n1:22:52 several sub problems so count number of digits from 1 to 9 10 to 99 so so on and\n1:22:58 count the digits in number 500 okay interesting so that''s how it''s breaking it up some more digits count in the\n1:23:05 previous steps so we go through total digits and we see that''s okay nine\n1:23:12 digits for those for here 180 for here\n1:23:17 1,200 and then of course three here so it gets all those sums those digits and\n1:23:25 actually comes to the right answer okay so that that is you that''s the difference with with Chain of Thought\n1:23:30 versus without so without it we just get the wrong answer basically guessing with\n1:23:37 chain of thought we get the right answer just by the llm writing down its reasoning and breaking the problem down\n1:23:44 into multiple Parts which is I found that super interesting that it it does that so that''s pretty cool now I will\n1:23:53 just see so as I as we mentioned before most llms nowadays are actually training\n1:23:58 to use train of thought prompting by default so let''s just see if we don''t mention anything right be a helpful\n1:24:03 assistant and answer the users question so we''re not telling it not to think through it''s reasoning and we''re not\n1:24:09 telling it to think through its reasoning let''s just see what it does okay so you can see again it''s\n1:24:16 actually doing the exact same reasoning\n1:24:21 okay it doesn''t it doesn''t give us like the sub problems that the start but it is going through and it''s breaking\n1:24:27 everything apart okay which is quite interesting and we get the same correct answer so the formatting here is\n1:24:33 slightly different it''s probably a little cleaner actually although I think\n1:24:38 uh I don''t know I here we get a lot more information so both are fine and in this\n1:24:46 scenario we actually do get the the right answer as well so you can see that that Chain of Thought prompting has\n1:24:52 actually been quite literally trained into the model and you''ll see that with\n1:24:58 most well I think all save the-art lenss Okay cool so that is our our chapter on\n1:25:07 prompting again we''re focusing very much on a lot of the fundamentals of\n1:25:13 prompting there and of course tying that back to the actual objects and methods\n1:25:19 within langing but for now that''s it for prompting and we''ll move on to the next ch chapter in this chapter we''re going\n1:25:26 to be taking a look at conversational memory in line chain we''re going to be\n1:25:31 taking a look at the core like chat memory components that have really been\n1:25:37 in line chain since the start but are essentially no longer in the library and\n1:25:43 we''ll be seeing how we actually Implement those historic conversational\n1:25:49 memory Utilities in the new versions of Lang chain so 0.3 now as a pre-warning\n1:25:57 this chapter is fairly long but that is because conversational memory is just\n1:26:02 such a critical part of chatbots and agents conversational memory is what\n1:26:08 allows them to remember previous interactions and without it our chat\n1:26:13 boox and agents would just be responding to the most recent message without any\n1:26:18 understanding of previous interactions within a conversations so they would just not be coners ational and depending\n1:26:26 on the type of conversation we might want to go with various approaches to\n1:26:32 how we remember those interactions within a conversation now throughout\n1:26:38 this chapter we''re going to be focusing on these for memory types we''ll be\n1:26:43 referring to these and I''ll be showing you actually how each one of these works but what we''re really focusing on is\n1:26:49 rewriting these for the latest version of Lang chain using the what it''s called\n1:26:54 the runable with\n1:27:00 message history so we''re going to be essentially taking a look at the original implementations for each of\n1:27:07 these four original memory types and then we''ll be rewriting them with the the runnable memory history class so\n1:27:14 just taking a look at each of these four very quickly conversational buffer\n1:27:19 memory is I think the simplest and most intuitive of these meor types it is\n1:27:26 literally just you have your messages they come into this object they are sold\n1:27:33 in this object as essentially a list and when you need them again it will return\n1:27:38 them to you there''s nothing and nothing else to it''s super simple the conversation Buffet window memory okay\n1:27:44 so new word in the middle of the window this works in pretty much the same way\n1:27:51 but those messages that it has stored is not going to return all of them for you instead it''s just going to return the\n1:27:57 most recent let''s say the most recent three for example okay and that is defined by a parameter K conation of\n1:28:05 summary memory rather than keeping track of the entire uh interaction memory\n1:28:11 directly what it''s doing is as those interactions come in it''s actually going to take them and it''s going to compress\n1:28:18 them into a smaller little summary of what has been within that conversation\n1:28:24 and as every new interaction is coming in it''s going to do that going to keep iterating on that summary and then that\n1:28:31 is going to be return to us when we need it and finally we have the conversational summary buffer memory so\n1:28:37 this is it''s taking so the buffer part of this is actually referring to very\n1:28:43 similar thing to the buffer window memory but rather than it being a you know most K messages it''s looking at the\n1:28:50 number of tokens within your memory and it''s returning the most recent K\n1:28:57 tokens that''s what the buffer part is there and then it''s also merging that\n1:29:02 with the summary memory here so essentially what you''re getting is almost like a list of the most recent\n1:29:09 messages based on the token length rather than the number of interactions plus a summary which would you know come\n1:29:16 at the the top here so you get kind of both the idea is that obviously this\n1:29:21 summary here would maintain all of your interactions in a very\n1:29:27 compressed form so you''re you''re losing less information and you''re still maintaining you know maybe the very\n1:29:33 first interaction the user might have introduced themselves giving you their name hopefully that would be maintained\n1:29:40 within the summary and it would not be lost and then you have almost like higher resolution on the most recent um\n1:29:47 K or k tokens from your memory okay so let''s jump over to the code we''re going\n1:29:53 into the 04 chat memory notebook open that in collab okay now here we are let''s go ahead and install the\n1:30:01 prerequisites run all we again can or cannot use align\n1:30:06 Smith it is up to you enter that and let''s come down and start so first just\n1:30:13 initialize our LM using 40 mini in this example again low\n1:30:20 temperature and we''re going to start with conversation buffer memory right so this is the original version of this uh\n1:30:29 memory type so let me where are we we''re here so memory conversation both of\n1:30:36 memory and we''re returning messages that needs to be set to true so the reason that we set return messages true it it\n1:30:44 mentions up here is if you do not do this it''s going to returning your chat\n1:30:50 history as a string to an llm whereas well chat lm''s nowadays would expect\n1:30:58 message objects so yeah you just want to be returning these as messages rather\n1:31:03 than as strings okay otherwise yeah you''re going to get some kind of strange Behavior out from your llms if you\n1:31:10 return them strings so you do want to make sure that it''s true I think by default it might not be true but this is\n1:31:16 coming this is deprecated right it does tell you here as de creation warning\n1:31:21 this is coming from older BL chain but it''s a good place to start just to understand this and then we''re going to rewrite this with the runnables which is\n1:31:28 the recommended way of doing so nowadays okay so adding messages to our memory\n1:31:34 we''re going to write this okay so it''s just a just a conversation user AI user\n1:31:39 AI so on and so on random chat main things to not here is I do provide my\n1:31:45 name we have the the model''s name right towards the start of those interactions okay so I''m just going to add all of\n1:31:52 those with do it like this okay then we can just see we can load our history\n1:32:01 like so so let''s just see what we have there okay so we have human message AI message human message right this is\n1:32:08 exactly what we I showed you just here it''s just in that message format from line chain okay so we can do that\n1:32:16 alternatively we can actually do this so we can get our memory we initialize the conation buffer memory as we did before\n1:32:24 and we can actually add it directly the message into our memory like that so we can use this add us message add AI\n1:32:30 message so on and so on load again and it''s going to give us the exact same thing again there''s multiple ways to do\n1:32:37 the same thing cool so we have that to pass all of this into our LM again this\n1:32:42 is all deprecated so we''re going to learn how to properly in a moment but this is how L chain was doing in the\n1:32:49 past so to pass all of this into our LM we'' be using this conversation chain\n1:32:55 right again this is deprecated nowadays we would be using lell for this so I I\n1:33:02 just want to show you okay how this would all go together and then we would invoke okay what is my name again let''s\n1:33:08 run that and we''ll see what we get it''s remembering everything remember so this\n1:33:13 conversation buffer memory it doesn''t drop messages it just remembers everything right and honestly with the\n1:33:20 sort of high context Windows of many LMS might be what you do it depends on how\n1:33:25 long you expect the conversation to go on for but you could you probably in most cases would get away with this okay\n1:33:32 so what let''s see what we get um I say what is my name again okay let''s see\n1:33:38 what it gives me says your name is chains great thank you that works now as\n1:33:44 I mentioned all of this that I just showed you is actually deprecated that''s the old way doing things let''s see how\n1:33:49 we actually do this in modern or up toate L chain so we''re going to be using this runable with message history to\n1:33:57 implement that we will need to use LL and for that we will need to just Define\n1:34:02 prompt templates our LM as we usually would okay so we''re going to set up our system prompt which is just a helpful\n1:34:09 assist called Zeta okay we''re going to put in this messages\n1:34:15 placeholder okay so that''s important essentially that is where our messages\n1:34:20 are coming from our conversation Buffet for memory is going to be inserted right\n1:34:26 so it''s going to be that chat history is going to be inserted after our system prompt but before our most recent query\n1:34:33 which is going to be inserted last here okay so messages placeholder item that''s\n1:34:39 important and we use that throughout the course as well so we use it both for chat history and we''ll see later on we\n1:34:45 also use it for the intermediate thoughts that a agent would go through as well so important to remember that\n1:34:52 little thing well link our prompt template to our LM again if we would\n1:34:58 like we could also add in the I think we only have the query here oh we would\n1:35:04 probably also want our history as well but I''m not going to do that right now\n1:35:10 okay so we have our Pipeline and we can go ahead and actually Define our runnable with message history now this\n1:35:17 class or object when we are initializing it does require a few items we can see them here okay so we'' see that we have\n1:35:24 our Pipeline with history so it''s basically going to be uh you can you can see here right we have that history\n1:35:30 messages key right this here has to align with what we provided as a meses\n1:35:36 placeholder in our pipeline right so we have our pipeline prompt template here\n1:35:44 and here right so that''s where it''s coming from it''s coming from messes placeholder variable name is history\n1:35:49 right that''s important that links to this then for the input messages key\n1:35:55 here we have query that again links to this okay so both important to have\n1:36:03 that the other thing that is important is obviously we''re passing in that pipeline from before but then we also\n1:36:09 have this get session history basically what this is doing is it''s saying okay I need to get uh the list of messages that\n1:36:16 make up my chat history that are going to be inserted into this variable so that is a function that we Define okay\n1:36:22 and with within this function what we''re trying to do here is actually replicate\n1:36:28 what we have with the previous conversation buffer memory okay\n1:36:34 so that''s what we''re doing here so it''s very simple right so we have this in\n1:36:40 memory chat message history okay so that''s just the object that we''re going to be returning what this will do is it\n1:36:47 will set up a session ID the session ID is essentially like a unique identifier so that eachers ation or interaction\n1:36:55 within a single conversation is being mapped to a specific conversation so you don''t have overlapping let say have\n1:37:00 multiple users using the same system you want to have a unique session ID for each one of those okay and what it''s\n1:37:06 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here we are going\n1:37:13 to initialize that session with an inmemory chat message history okay\n1:37:21 that''s it and we return okay and all that''s going to do is it''s going to basically append our messages they will\n1:37:28 be appended within this chat map session ID and they''re going to get returned\n1:37:33 there''s nothing R there''s nothing else to it to be honest so we invoke our\n1:37:39 rable let''s see what we get I need to ruin\n1:37:45 this okay note that we do have this config so we have a session ID that''s to\n1:37:50 again as I mentioned keep different conversations separate Okay so we''ve run that now let''s run a few more so what is\n1:37:57 my name again let''s see if it remembers your name is James how can I help you today James okay so it''s what we''ve just\n1:38:06 done there is literally conversation buter memory but for upto-date L chain\n1:38:14 with L cell with Runner BS so you the recommended way of doing it nowadays so\n1:38:20 that''s a very simple example okay really and not that much to it it gets a little\n1:38:27 more complicated as we start thinking about the different types of memory although that being said it''s not\n1:38:33 massively complicated we''re only rarely going to be changing the way that we''re getting our interactions so let''s uh\n1:38:41 let''s dive into that and see how we will do something similar with the conation buff for window memory but first let''s\n1:38:47 actually just understand okay what is Con station buffer window memory so as I mentioned near the start it''s going to\n1:38:53 keep track of the last K messages so there''s a few things to keep in mind\n1:38:58 here more messages does mean more tokens send with each request and if we have\n1:39:03 more tokens in each request it means that we''re increasing the latency of our responses and also the cost so with the\n1:39:10 previous memory type we''re just sending everything and because we''re sending everything that is going to be\n1:39:15 increasing our cost it''s going to be increasing our latency for every message especially as a conversation gets longer\n1:39:21 and longer and we don''t we might not necessarily want to do that so with this conversation buffer window memory we''re\n1:39:28 going to just say okay just return me the most recent messages okay so let''s\n1:39:35 well let''s see how that would work here we''re going to return the most recent four messages okay we are again make\n1:39:41 sure we''ve turned messages is set to True again this is deprecated this is just the old way of doing it in a moment\n1:39:48 we''ll see the updated way of doing this we''ll add all of our\n1:39:54 messages okay so we have this and just see here right so we''ve added in all\n1:40:01 these messages there''s more than four messages here and we can actually see that here so we have human message AI\n1:40:07 human AI human AI human AI right so we''ve got four pairs of human AI\n1:40:14 interactions there but here we don''t have there''s more than four pairs so four pairs will take us back all the way\n1:40:21 to here I''m researching different types of conversational uh memory okay and if\n1:40:28 we take a look here the most the first message we have is I''m researching different types of conversational memory\n1:40:33 so it''s cut off these two here which will be a bit problematic when we ask you what our name is okay so let''s just\n1:40:40 see going to be using conversation chain object again again just remember that is\n1:40:45 deprecated and I want to say what is my name again let''s see let''s see what it\n1:40:51 says uh I''m sorry I but I don''t have access to your name or any personal information if you like you can tell me\n1:40:56 your name right so it doesn''t actually remember uh so that''s kind of like a negative of the conversation Buffet\n1:41:04 window memory of course the uh to fix that in this scenario we might just want to increase K maybe we say remember the\n1:41:12 previous eight interaction Pairs and it will actually remember so what is my\n1:41:17 name again your name is James so now it remembers we''ve just modified how much it is remembering but of course you know\n1:41:24 pros and cons to this it really depends on what you''re trying to build so let''s take a look at how we would actually\n1:41:30 implement this with the runable with message history okay so you getting a little\n1:41:38 more complicated here although it it''s it''s not it''s not complicated but well\n1:41:44 we''ll see okay so we have buffer window message history we''re creating a class here this class is going to inherit from\n1:41:51 the base chat message history object from line chain okay and in all of our\n1:41:57 other message history objects can do the same thing before with the inmemory\n1:42:02 message object that was basically replicating the buffer memory so we\n1:42:08 didn''t actually need to do anything we didn''t need to Define our own class here\n1:42:13 so in this case we do so we follow the same pattern that line chain follows\n1:42:19 with this base chat message history and you can see a few of the functions here that are important so add messages and\n1:42:26 clear are the ones that we''re going to be focusing on we also need to have messages which this object attribute\n1:42:31 here okay so we''re just implementing the synchronous methods here if we want this\n1:42:38 to be async if we want to support async we would have to add a add messages um a\n1:42:44 get messages and a clay as well so let''s go ahead and do that we have messages we\n1:42:49 have K again we''re looking at remembering the top K messages or most recent K messages only so it''s important\n1:42:56 that we have that variable we are adding messages through this class this is\n1:43:01 going to be used by line chain within our runnable so we need to make sure that we do have this method and all\n1:43:06 we''re going to be doing is extending the self messages uh list here and then we''re actually just going to be trimming\n1:43:13 that down so that we''re not remembering anything beyond those you know most recent K\n1:43:19 messages that we have set from here and then we also have the clear method\n1:43:25 as well so we need to include that that''s just going to clear the history okay so it''s not this isn''t complicated\n1:43:31 right it just gives us this nice default standard interface for message history\n1:43:37 and we just need to make sure we''re following that pattern okay I''ve included the uh this print here just so we can see what''s happening okay so we\n1:43:45 have that and now for that get chat history function that we defined earlier\n1:43:51 rather than using the buin method we''re going to be using our own object which is a buffer window message history which\n1:43:58 will be defined just here okay so if session ID is not in the chat map as we\n1:44:05 did before we''re going to be initializing our buffer window message history we''re setting K up here with a\n1:44:10 default value of four and then we just return it okay and and that is it so let''s run this we have our runable with\n1:44:18 message history we have all of these variables which are exactly the same as before four but then we also have these\n1:44:25 variables here with it''s history Factory config and this is where if we have um\n1:44:33 new variables that we''ve added to our message history in this case k that we\n1:44:38 have down here we need to provide that to line train and sell it this is a new configurable field okay and we''ve also\n1:44:46 added it for the session ID here as well so we''re just being explicit and have everything in that so we have that\n1:44:53 and we run okay now let''s go ahead and invoke and see what we get okay so\n1:45:00 important here this history Factory config that is kind of being fed through\n1:45:06 into our invoke so that we can actually modify those variables from here okay so we have config configurable session ID\n1:45:14 okay we just put whatever we want in here and then we also have the number K okay so remember the previous four\n1:45:22 interaction I think in this one we''re doing something slightly different I think we''re remembering the four\n1:45:27 interactions rather than the previous four interaction pairs okay so my name is James uh we''re going to go through\n1:45:34 I''m just going to actually clear this and now I''m going to start again and we''re going to use the exact same ad\n1:45:40 user message ad AI message that we used before we''re just manually inserting all that into our history so that we can\n1:45:46 then just see okay what is the result and you can see that k equal 4 is actually unlike before where we were\n1:45:53 having the uh saving the top four interaction pairs we now saving the most\n1:46:01 recent four interactions not pairs just interactions and honestly I just think\n1:46:06 that''s clearer I think it''s weird that the number four for K would actually save the most recent eight messages\n1:46:14 right I I think that''s odd so I''m just not replicating that weirdness we could\n1:46:19 if we wanted to I just don''t like it so I''m not doing that and anyway we can see\n1:46:26 from messages that we''re returning just the most four recent messages okay which\n1:46:31 should be these four Okay cool so we''ve just using the runable we''ve replicated\n1:46:38 the old way of having a window memory and okay I''m going to say what is my\n1:46:43 name again as before it''s not going to remember so we can come to here I''m sorry about I don''t have access to\n1:46:48 personal information so on and so on if you like to tell me your name doesn''t know now let''s try a new one where we\n1:46:56 initialize a new session okay so we''re going with ID K4 so that''s going to\n1:47:01 create a new conversation there and we''re going to say we''re going to set K to\n1:47:07 14 okay great I''m going to manually insert the other uh messages as we did\n1:47:13 before okay and we can see all of those and see at the top here we are still maintaining that hi my name is James\n1:47:19 message now let''s see if it remembers my name your name is James okay there we go\n1:47:27 cool so that is working we can also see so we just added this what is my name again let''s just see if did that get\n1:47:34 added to our list of messages right what is my name again nice and then we also\n1:47:39 have the response your name is James so just by invoking this because we''re using the the runable with message\n1:47:46 history it''s just automatically adding all of that into our message history\n1:47:52 which is nice cool all right so that is the buffer window memory now we are going to take a\n1:47:59 look at how we might do something a little more complicated which is the the summaries okay so when you think about\n1:48:06 the summary you know what are we doing we''re actually taking the messages we''re using that LM call to summarize them to\n1:48:14 compress them and then we''re storing them within messages so let''s see how we would actually uh do that so to start\n1:48:22 with let''s just see how it was done in Old Line chain so we have conversation\n1:48:28 summary memory go through that and let''s just see what we get so\n1:48:35 again same interactions right I''m just invoking invoking invoking I''m not adding these\n1:48:41 directly to the messages because it actually needs to go through a um like that summarization process and if we\n1:48:49 have a look we can see it happening okay current conversation so sorry current\n1:48:55 conversation hello there my name is James AI is generating current conversation the human introduces\n1:49:01 himself as James AI greets James warmly and expresses its Readiness to chat and assist inquiring about how his day is\n1:49:08 going right so it''s summarizing the the previous interactions and then we have\n1:49:14 you know after that summary we have the most recent human message and then the AI is going to generate its response\n1:49:21 okay and that continues your own Contin is going and you see that the the final summary here is going to be a lot longer\n1:49:27 okay it''s different that first summary of course asking about his Day Men researching different types of\n1:49:32 conversational memory the AI responds enthusiastically explaining that conversational memory includes\n1:49:37 short-term memory longterm memory contextual memory personalized memory and then inquires if James is focused on a specific type of memory Okay cool so\n1:49:46 we get essentially the summary is just getting uh longer and longer as we go but at some point the idea is that it''s\n1:49:53 not going to keep growing and it should actually be shorter than if you were saving every single interaction whilst\n1:49:59 maintaining as much all the information as possible but of course you''re not\n1:50:04 going to maintain all of the information that you would with for example the the\n1:50:09 buffer memory right with the summary you are going to lose information but\n1:50:15 hopefully less information than if you''re just cutting interactions so\n1:50:20 you''re trying to reduce your token count whilst maintaining as much information as\n1:50:25 possible now let''s go and ask what is my name again it should be able to answer\n1:50:30 because we can see in the summary here that I introduced myself as\n1:50:36 James okay respondents your name is James how is your research going okay so\n1:50:41 has that cool let''s see how we''d Implement that so again as before we''re\n1:50:46 going to go with that conversation summary message history we''re going to\n1:50:52 be importing this system message uh we''re going to be using that not for the LM that we''re chatting with but for the LM that will be generating our summary\n1:51:01 so actually that is not quite correct there is create a summary not that it\n1:51:06 matters it''s just the doct string so we have our messages and we also have the LM so different different attribute here\n1:51:12 to what we had before when we initialize a conversation summary message history we need to passing in our LM we have the\n1:51:19 same methods as before we have ADD messages and clear and what we''re doing is as messages coming we extend with our\n1:51:27 current messages but then we''re modifying those okay so we construct our like\n1:51:34 instructions to make a summary okay so that is here we have the system front uh\n1:51:40 giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring to\n1:51:45 maintain as much relevant information as possible okay then we have a human message here through that we''re passing\n1:51:52 the existing summary okay and then we''re passing in the new\n1:51:57 messages Okay cool so we format those invoke the\n1:52:05 llm here and then what we''re doing is in the messages we''re actually replacing\n1:52:11 the existing history that we had before with a new history which is just a single system summary message okay let''s\n1:52:20 see what we get as before we have that get chat history exactly the same as before the only real difference is that\n1:52:26 we''re passing in the llm parameter here and of course as we''re passing in the LM parameter in here it does also mean that\n1:52:33 we''re going to have to include that in the configurable field spec and that we''re going to need to include that when\n1:52:40 we''re invoking our pipeline okay so we run that pass in the\n1:52:48 LM now of course one side effect of generating summaries for everything is that way actually you know we''re\n1:52:54 generating more so you are actually using quite a lot of tokens whether or\n1:52:59 not you are saving tokens or not actually depends on the length of a conversation as the conversation gets\n1:53:05 longer if you''re storing everything after a little while that the token usage is actually going to increase so\n1:53:12 if in your use case you expect to have shorter conversations you would be\n1:53:17 saving money and tokens by just using this standard buffer memory\n1:53:23 whereas if you''re expecting very long conversations you would be saving tokens and money by using the summary history\n1:53:31 okay so let''s see what we got from there we have a summary of the conversation James introduced himself by saying hi\n1:53:37 name James a I responded War asking hi James Interac include details about token\n1:53:42 usage okay so we actually included everything here which we probably should\n1:53:48 not have done why did we do that as so in here we''re including all\n1:53:56 of the out in here so we using or including\n1:54:02 all of the content from the messages so I think maybe we just do X content for X in messages that\n1:54:12 should resolve that okay there we go so we quickly fli\n1:54:20 that so yeah before we pass them in the entire mage object which obviously includes all of this information whereas\n1:54:26 actually we just want to be passing into the content so we modified that and now\n1:54:31 we''re getting what we would expect okay cool and then we can keep going right so as we as we keep going\n1:54:38 the summary should get more like abstract like as we just saw here is\n1:54:43 literally just giving us the messages directly almost okay so we''re getting a bit of summary there and we can keep\n1:54:50 going we''re going to add just more messages to that we''ll see the you as we''ll get send those we''ll get a\n1:54:57 response send it again get a response and we just adding all of that invoking all of that and that will be of course\n1:55:03 adding everything into our message history Okay cool so we''ve run that\n1:55:09 let''s see what the latest summary is okay and then we have this so this is\n1:55:15 a summary that we have inside of our our chat history okay cool now finally let''s see\n1:55:24 what is my name again we can just double check you know it has my name in there so it should be able to tell\n1:55:33 us okay cool so your name is James pretty interesting so let''s have a quick\n1:55:39 look over at limith so the reason I want to do this is just to point out okay the\n1:55:45 different essentially token usage that we''re getting with each one of these okay so we can see that we have these Runner mess history which probably uh\n1:55:53 improved in naming there but we can see okay how long is each one of these taken\n1:55:59 how many tokens are they also using come back to here we have this runable\n1:56:04 message history this is we''ll go through a few of these maybe to here I think we\n1:56:11 can see here this is that first interaction where we''re using the buffer memory and we can see how many tokens we\n1:56:17 used here so 112 tokens when we''re asking what is my name again okay then\n1:56:23 we modified this to include I think it was like 14 interactions or something on\n1:56:29 those lines obviously increases the number of tokens that we''re using right so we can could see that actually happening all in Lang which is quite\n1:56:36 nice and we can compare okay how many tokens is each one of these using now this is looking at the buffer window and\n1:56:44 then if we come down to here and look at this one so this is using our summary\n1:56:49 okay so our summary with what is my name again actually use more tokens in this scenario right which is interesting\n1:56:55 because we''re trying to compress information the reason there more is because there''s not there hasn''t been that many interactions as the\n1:57:02 conversation length increases with the summary this total\n1:57:08 number of tokens especially if we prompt it correctly to keep that low that should remain relatively small\n1:57:15 whereas with the buffer memory that will just keep increasing and increasing as the as the conversation gets longer\n1:57:23 so useful little way of using Lang Smith there to just kind of figure out okay in\n1:57:29 terms of tokens and costs of what we''re looking at for each of these memory types okay so our final memory type acts\n1:57:37 as a mix of the summary memory and the buffer memory so what it''s going to do\n1:57:43 is keep the buffer up until an N number of tokens and then once a message\n1:57:50 exceeds the N number of tokens limit for the buffer it is actually going to be\n1:57:55 added into our summary so this memory has the benefit of remembering in detail\n1:58:03 the most recent interactions whilst also not having the limitation of using too\n1:58:10 many tokens as a conversation gets longer and even potentially exceeding context Windows if you try super hard so\n1:58:18 this is a very interesting approach now as before let''s try the original way of\n1:58:24 implementing this then we will go ahead and use our update method for\n1:58:30 implementing this so we come down to here and we''re going to do L chain memory import conversation summary\n1:58:37 buffer memory okay a few things here LM for summary we have the N number of\n1:58:44 tokens that we can keep before they get added to the summary and then return messages of course okay you can see\n1:58:51 again this is dicated we use the conversation chain and then we just passing our memory there and\n1:58:57 then we can chat okay so super straightforward first message we''ll add\n1:59:03 a few more here and we have to invoke because how\n1:59:08 memory type here is using NM to create those summaries as it goes and let''s see\n1:59:14 what they look like okay so we can see for the first message here we have human message and then an AI message\n1:59:22 then we come a little bit lower down again it''s same thing human message is the first thing in our history here then\n1:59:29 it''s a system message so this is at the point where we''ve exceeded that 300 token limit and the memory type here is\n1:59:36 generating those summaries so that summary comes in as a system message and we can see okay the human named James\n1:59:43 introduces himself and mentions he''s researching different types of conversational memory and so on and so on right okay cool so we have that then\n1:59:52 let''s come down a little bit further we can see okay so the summary there okay\n1:59:59 so that''s what we that''s what we have that is the implementation for the old\n2:00:05 version of this memory again we can see it''s deprecated so how do we implement this for our more recent versions of\n2:00:14 Lang chain and specifically 0.3 well again we''re using that runable message\n2:00:20 history and it looks a little more complicated than we were getting before but it''s actually just you know it''s\n2:00:27 nothing too complex we''re just creating a summary as we did with the previous\n2:00:33 memory type but the decision for adding to that summary is based on in this case\n2:00:39 actually the number of messages so I didn''t go with the the Lang chain version where it''s a number of tokens I\n2:00:46 don''t like that I prefer to go with messages so what I''m doing is saying okay let K messages\n2:00:52 okay once we exceed K messages the messages beyond that are going to be\n2:00:58 added to the memory Okay cool so let''s see we first initialize our conversation\n2:01:06 summary buffer message history class with llm and K okay so these two here so\n2:01:14 LM of course to create summaries and K is just the the limit of the number of messages that we want to keep before\n2:01:19 adding them to the summary or dropping them from now messages and adding them to the summary okay so we will begin with okay\n2:01:29 do we have an existing summary so the reason we set this in none is we can''t\n2:01:36 extract the summary the existing summary unless it already exists and the only\n2:01:41 way we can do that is by checking okay do we have any messages if yes we want to check if within those messages we\n2:01:48 have a system message because we''re we''re doing the same structure is what we have up here where the system message\n2:01:54 that first system message is actually our summary so that''s what we''re doing here we''re checking if there is a\n2:02:00 summary message already stored within our messages okay so we''re checking for that if we\n2:02:08 find it we''ll just do we have this little print statement so we can see that we found something and then we just\n2:02:14 make our existing summary I should actually move this to the first instance\n2:02:21 here yeah okay so that existing summary will be set to the first\n2:02:30 message okay and this would be a system message rather than a\n2:02:35 string cool so we have that then we want to add any new messages to our history\n2:02:43 okay so we''re extending the history there and then we''re saying okay if the length of our history is exceeds the K\n2:02:50 value that we set we''re going say okay we found that many messages we''re going to be dropping the latest it''s going to be the latest two\n2:02:57 messages this I will say here one thing or one problem with this is that we''re\n2:03:04 not going to be saving that many tokens if we''re summarizing every two messages so what I would probably do is in in an\n2:03:12 actual like production setting I would probably say let''s go up to 20 messages\n2:03:19 and once we hit 20 messages let''s take the previous 10 we''re going to summarize them and put them into our summary\n2:03:26 alongside any you know previous summary that already existed but in in you know this is also fine as well okay so we say\n2:03:36 we found those mes we''re going to drop the latest two messages okay so we pull\n2:03:41 the the oldest messages out I should say not the latest it''s the\n2:03:48 oldest not the latest I want to keep the latest drop the oldest so we pull out\n2:03:54 the oldest messages and keep only the most recent messages okay then I''m\n2:04:01 saying okay if we if we don''t have any old messages to summarize we don''t do\n2:04:06 anything we just return okay so this in the case that this has not been triggered we would hit this but in the\n2:04:14 case this has been triggered and we do have old messages we''re going to come to\n2:04:20 here okay okay so this is we can see have a system message prompt template\n2:04:26 saying giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring\n2:04:32 to maintain as much relevant information as possible so if you want to be more conservative with tokens we could modify\n2:04:39 this prompt here to say keep the summary to within the length of a single\n2:04:44 paragraph for example and then we have our human M prom template which is going to say okay here''s the existing\n2:04:50 conversation summary in here on new messages now new messages here is actually the old messages but the way\n2:04:57 that we''re framing it to the llm here is that we want to summarize the whole conversation right it doesn''t need to\n2:05:03 have the most recent messages that we''re storing within our buffer it doesn''t need to know about those that''s\n2:05:09 irrelevant to the summary so we just tell it that we have these Zoom mes and as far as this LM is concerned this is\n2:05:15 like the full set of interactions okay so then we would format those and invoke\n2:05:21 our LM and then we''ll print out our new summary so we can see what''s going on there and we would prend that new\n2:05:30 summary to our conversation history okay and and this will work so we can just\n2:05:37 prend it like this because we''ve already\n2:05:42 popped where was it up here if we have an existing\n2:05:47 summary we already pop that from the list it''s already been pulled out of that list so it''s okay for us to just we\n2:05:53 don''t need to say like we don''t need to do this because we''ve already dropped that initial system message if it\n2:06:00 existed okay and then we have the clear method as before so that''s all of the\n2:06:05 logic for our conversational summary buffer memory we redefine our get chat\n2:06:14 history function with the LM and K parameters there and then we''ll also\n2:06:20 want to set the configurable Fields again so that is just going to be of course session ID LM and\n2:06:28 K okay so now we can invoke the K value to begin with is going to be\n2:06:35 four okay so we can see no old messages to update summary with it''s good let''s\n2:06:42 invoke this a few times and let''s see what we get okay so now M to summary with\n2:06:51 found six messages dropping the aest 2 and then we have new summary in the conversation James Inu himself and first\n2:06:58 is interestes in researching different types of conversational memory right so you can see there''s quite a lot in here\n2:07:03 at the moment so we would definitely want to prompt the LM the summary LM to keep\n2:07:10 that short otherwise we''re just getting a ton of stuff right but we can see that that is\n2:07:17 you know it''s it''s working it''s functional so let''s go back and see if we can prompt it to be a little more\n2:07:23 concise so we come to here ensuring to maintain as much relevant information as\n2:07:28 possible however we need to\n2:07:33 keep our summary concise the\n2:07:39 limit is a single short paragraph okay\n2:07:45 something like this let''s try and let''s see what we get with\n2:07:50 that okay so message one again nothing to update see this so new summary you can\n2:07:56 see it''s a bit shorter it doesn''t have all those bullet\n2:08:01 points okay so that seems better let''s see so you can see the first summary is\n2:08:09 a bit shorter but then as soon as we get to the second and third summaries the\n2:08:14 second summary is actually slightly longer than the third one okay so we''re going to be we''re going to be losing a\n2:08:20 bit of information in this case more than we were before but we''re saving a ton of tokens so that''s of course a good\n2:08:28 thing and of course we could keep going and adding many interactions here and we should see that this conversation\n2:08:34 summary will be it should maintain that sort of length of around one short\n2:08:40 paragraph So that is it for this chapter on conation memory we''ve seen a few\n2:08:47 different memory types we''ve implemented their old deprecated version so we can see what they were like and then we''ve\n2:08:55 reimplemented them for the latest versions of Lang chain and to be honest using logic where we are getting much\n2:09:02 more into the weees and that is in some ways okay it complicates things that is\n2:09:07 true but in other ways it gives us a ton of control so we can modify those memory\n2:09:13 types as we did with that final summary buffer memory type we can modify those\n2:09:18 to our liking which is incredibly useful when you''re actually building\n2:09:23 applications for the real world so that is it for this chapter we''ll move on to the next one in this chapter we are\n2:09:30 going to introduce agents now agents I think are one of the most important\n2:09:37 components in the world of AI and I don''t see that going away anytime soon I\n2:09:43 think the majority of AI applications the intelligent part of\n2:09:49 those will be was always an implementation of an AI agent or mle AI\n2:09:55 agents so in this chapter we are just going to introduce agents within the\n2:10:00 context of line chain we''re going to keep it relatively simple we''re going to go into much more depth in agents in the\n2:10:09 next chapter where we''ll do a bit of a deep dive but we''ll focus on just introducing the Core Concepts and of\n2:10:16 course agents within line chain here so jumping thing straight into our notebook\n2:10:24 let''s run our prerequisites you''ll see that we do have an additional prerequisite here which is\n2:10:30 Google search results that''s because we''re going to be using the sub API to allow our llm as an agent to search a\n2:10:38 web which is one of the great things about agents is that they can do all of\n2:10:44 these additional things and LM by itself obviously cannot so we come down to here\n2:10:49 we have our lsmith parameters again of course so you enter your Lang chain API\n2:10:54 if you have one and now we''re going to take a look at tools which is a very\n2:11:00 essential part of Agents so tools are a way for us to augment our llms with\n2:11:07 essentially anything that we can write in code so we mentioned that that we''re going to have a Google Search tool that\n2:11:14 Google Search tool it''s some code that gets executed by our llm in order to\n2:11:19 search Google and get some results so a tool can be thought of as any code logic\n2:11:26 or any function in the C in the case of python any function that has been\n2:11:32 formatted in a way so that our LM can understand how to use it and then\n2:11:38 actually use it although the the LM itself is not using the tool it''s more\n2:11:43 our agent execution logic which uses the tool for the llm so we''re going to go\n2:11:49 ahead and actually create a few simple tools we''re going to be using what is called the tool decorator from Lang\n2:11:55 chain and there are a few things to keep in mind when we''re building tools so for\n2:12:02 Optimal Performance our tool needs to be just very readable and what I mean by readable is we need three main things\n2:12:10 one is a DOT string that is written in natural language and it is going to be used to explain to the Alm when and why\n2:12:18 and how it should use this tool we should also have clear parameter names\n2:12:23 those parameter names should tell the llm okay what each one of these\n2:12:28 parameters are they should be self-explanatory if they are not self-explanatory we should be including\n2:12:36 an explanation for those parameters within the doc string then finally we should have type annotations for both\n2:12:43 our parameters and also what we''re returning from the tool so let''s jump in\n2:12:48 and see how we would Implement all of that so we come down to here and we have line chain core tools import tool okay so\n2:12:56 these are just four incredibly simple tools we have the addition or add tool\n2:13:03 multiply the exponentiate and the subtract tools okay so a few calculator\n2:13:09 S tools now when we add this tool decorator it is turning each of these\n2:13:16 tools into what we call a structured tool object so we can see that\n2:13:21 here we can see we have this structured tool we have a name description okay and\n2:13:28 then we have this Al schema we''ll see this in a moment and a function right so this function is literally just the\n2:13:35 original function it''s it''s a mapping to the original function so in this case it it''s the add function now the\n2:13:41 description we can see is coming from our doc string and of course the name as well is just coming from the function\n2:13:47 name okay and then we can also see let''s just print the name and\n2:13:53 description but then we can also see the ARs schema right we can so this thing\n2:13:58 here that we can''t read at the moment to read it we''re just going to look at the\n2:14:03 model Json schema method and then we can see what that contains which is all of this information so this actually\n2:14:10 contains everything includes properties so we have the X it C or title for that\n2:14:16 and it also specifies the type okay so the type that we Define is float float\n2:14:23 for open AI gets mapped to number rather than just being float and then we also\n2:14:28 see that we have this required field so this is telling how LM which parameters\n2:14:33 are required which ones are optional so we yeah in some cases you would we can\n2:14:39 even do that here let''s do Z that is going to be float or none okay and we\n2:14:48 just going to say it is 0.3 all right well I''m going to remove this\n2:14:53 in a minute because it''s kind of weird but let''s just see what that looks like so you see that we now have X Y and\n2:15:02 Z but then in Z we have some additional information okay so it can be any of it\n2:15:08 can be a number or it can just be nothing the default value for that is 0.3 okay and then if we look here we can\n2:15:16 see that the required field does not include Z so it''s just X and Y so it''s\n2:15:21 describing the full function schema for us but let''s remove\n2:15:27 that okay and we can see that again with our exponentiate tool similar thing okay\n2:15:33 so how how are we going to invoke our tool so the llm the underlying LM is\n2:15:41 actually going to generate a string okay so we''ll look something like this this is going to be our llm output so it is\n2:15:50 it''s a string that is some Json and of course to load a string into a\n2:15:56 dictionary format we just use Json loes okay so let''s see that so this could be\n2:16:03 the output Fromm we load it into a dictionary and then we get an actual dictionary and then what we would do is\n2:16:11 we can take our exponentiate uh tool we access the underlying function and then\n2:16:17 we pass it the keyword arguments from our diction here\n2:16:24 okay and that will execute our tool that is the tool execution log you that line chain implements and then later on in\n2:16:31 the next chapter we''ll be implementing ourselves cool so let''s move on to creating an agent now we''re going to be\n2:16:38 constructing a simple tool calling agent we''re going to be using Lang chain expression language to do this now we\n2:16:45 will be covering Lang chain expression language or also more in a upcoming\n2:16:50 chapter but for now all we need to know is that our agent will be constructed\n2:16:57 using syntax and components that like this so we would start with our input\n2:17:03 parameters that is going to include our user query and of course the chat history because we need our agent to be\n2:17:09 conversational and remember previous interactions within the conversation these input parameters will also include\n2:17:15 a placeholder for what we call the agent scratch Pad now the agent scratch Pad is essentially where we are storing the\n2:17:22 internal thoughts or the internal dialogue of the agent as it is using tools getting observations from those\n2:17:28 tools and working through those multiple internal steps so in the case that we\n2:17:34 will see it will be using for example the addition tool getting the result using the multiply tool getting the\n2:17:40 result and then providing a final answer to us as a user so let''s jump in and see\n2:17:46 what it looks like okay so we''ll just start with defining our prompt so our prompt is going to include the system\n2:17:52 message there nothing we''re not putting anything special in there we''re going to\n2:17:57 include the chat history which is a messages placeholder then we include our\n2:18:03 human message and then we include a placeholder for the agent scratch Pad\n2:18:08 now the way that we implement this later is going to be slightly different for the scratch Pad we actually use this\n2:18:13 message''s placeholder but this is how we use it with the built-in create tool agent from BL chain next we sign our LM\n2:18:21 we do need our Opening Our API key for that so we''ll enter that here like so\n2:18:28 okay so come down Okay so we''re going to be creating this agent we need conversation memory and we are going to\n2:18:34 use the older conversation buffer memory class rather than the newer renable with message history class that''s just\n2:18:40 because we''re also using this older create tool calling agent and this is\n2:18:46 this is the older way of doing things in the next chapter we are going to be using the more recent basically what we\n2:18:54 already learned on chat history we''re going to be using all of that to implement our chat history but for now\n2:19:00 we''re going to be using the older method uh which is deprecated just as a pre-warning but again as I mentioned at\n2:19:07 the very solid of course we''re starting abstract and then we''re getting into the details so we''re going to initialize our\n2:19:15 agent for that we need these four things LM as we defined tools as we have\n2:19:20 defined prompt as we have defined and then the memory which is our old conversation\n2:19:26 buffer memory so with all of that we are going to go ahead and we create a tool\n2:19:32 calling agent and then we just provide it with everything okay there we go now H you''ll see here I didn''t pass\n2:19:40 in the the memory I''m passing it in down here instead so we''re going to start with this question which is what is 10.7\n2:19:47 MTI 7.68 eight okay so given the Precision of\n2:19:55 these numbers our l a normal LM would not be able to answer that or almost\n2:20:01 definitely will not be able to answer that correctly we need a external tool to answer that accurately and we''ll see\n2:20:08 that that is exactly what it''s going to do so we can see that the tool agent\n2:20:15 action message here we see that it decided okay I''m going to use the multiply tool and here at parameters\n2:20:21 that I want to use for that tool okay we can see X is 10.7 and Y is\n2:20:26 7.68 you can see here that this is already a dictionary and that is because\n2:20:32 Lang chain has taken the string from our llm C and already converted it into a\n2:20:38 dictionary for us okay so that''s just it''s happening behind the scenes there and you can actually see if we go into\n2:20:44 the details a little bit we can see that we have these arguments and this is the original string that was coming fromn\n2:20:50 okay which has already been of of course processed by line chain so we have that\n2:20:56 now the one thing missing here is that okay we''ve got that the LM\n2:21:03 wants us to use multiply and we''ve got what the LM wants us to put into modly but where''s the answer right there is no\n2:21:11 answer because the tool itself has not been executed because it can''t be executed by the llm but then okay didn''t\n2:21:19 we already Define our agent here yes redefined the part of our agent that is\n2:21:26 how llm has our tools and it is going to generate which tool to use but it actually doesn''t include the\n2:21:33 agent execution part which is okay the agent executor is a broader thing it''s\n2:21:41 it''s broader logic like just code logic which acts as a scaffolding within which\n2:21:47 we have the iteration through multiple steps of our llm calls followed by the\n2:21:54 llm outputting what tools use followed by us actually executing that for the llm and then providing the output back\n2:22:02 into the llm for another decision or another step so the agent itself here is\n2:22:08 not the full agentic flow that we might expect instead for that we need to\n2:22:15 implement this agent executor class this agent executor includes our agent from\n2:22:21 before then it also includes the tools and one thing here is okay we we already\n2:22:26 passed the tools to our agent why do we need to pass them again well the tools being passed to our agent up\n2:22:32 here that is being used so that is essentially extracting out those\n2:22:38 function schemers and passing it to our LM so that our LM knows how to use the tools then we''re down here we''re passing\n2:22:44 the tools again to our agent executor and this is rather than looking at how to use those tools this is just\n2:22:50 looking at okay I want the functions for those tools so that I can actually execute them for the llm or for the\n2:22:57 agent okay so that''s why it''s happening there now we can also pass in our memory\n2:23:02 directly so you see if we scroll up a little bit here I actually had to pass\n2:23:08 in the memory like this with our agent that''s just because we weren''t using the agent executor now we have the agent\n2:23:14 executor it''s going to handle that for us and another thing that''s going to handle for us is it intermediate steps\n2:23:21 so you''ll see in a moment that when we invoke the agent executor we don''t include the intermediate steps and\n2:23:27 that''s because it that is already handled by the agent executor now so we''ll come down we''ll set the both equal\n2:23:34 to true so we can see what is happening and then we can see here there''s no\n2:23:39 intermediate steps anymore and we we do still pass in the chat history like this\n2:23:46 but then the addition of those new interactions to our memory is going to be handled by the\n2:23:52 executor so let me actually show that very quickly before we jump in okay so\n2:23:59 that''s cently empty we''re going to execute this okay we entered that new Asian\n2:24:05 execute chain let''s just have a quick look at our messages again and now you can see that the agent\n2:24:11 executor automatically handled the addition of our human message and then the responding AI message for us okay\n2:24:19 which is useful now what happened so we can see that the multiply tool was\n2:24:25 invoked with these parameters and then this pink text here that we got that is\n2:24:30 the observation from the tool assist what the tool output back to us okay then this final message here it''s not\n2:24:37 formatted very nicely well this final message here is coming from our llm so the green is our llm output the pink is\n2:24:45 our tool output okay so the LM after seeing this output says 10.7 MTI by 7.68\n2:24:56 is approximately 82.8 okay cool use and then we can also\n2:25:03 see the the chat history which we we already just saw great so that has been\n2:25:08 used correctly we can just also confirm that that is correct okay 82\n2:25:15 1759 recurring which is exactly what we get here okay and we the reason for all\n2:25:20 that is obviously how multiply tool is just doing this exact operation cool so let''s try this with a\n2:25:29 bit of memory so I''m going to ask or I''m going to sayate to the agent hello my\n2:25:34 name is James we''ll leave that as the it''s not actually the first interaction because\n2:25:40 we already have these but it''s an early interaction with my name in there then\n2:25:48 we''re going to try and perform more tool calls within a single execution Loop and what you''ll see with when it is calling\n2:25:54 these tools is that it can actually use multiple Tools in parallel so for sure I think two or three of these were used in\n2:26:00 parallel and then the final subtract had to wait for those previous results so it would have been executed afterwards and\n2:26:08 we should actually be able to see this in Langs Smith so if we go here yeah we\n2:26:13 can see that we have this initial cord and then we have add and multiply and exponentially we all use in parallel\n2:26:20 then we have another call which use subtract and then we get the response okay which is pretty cool and\n2:26:27 then the final result there is11 now when you look at whether the\n2:26:33 answer is accurate I think the order here of calculations is not quite\n2:26:39 correct so if we put the actual computation here it gets it right but\n2:26:45 otherwise if I use natural language it''s like I''m doing maybe I''m phrasing it in a in a poor way\n2:26:52 okay so I suppose that is pretty important so okay if we put the computation in here we get\n2:26:59 the13 so it''s something to be careful with and probably requires a little bit of prompting to promting and maybe\n2:27:07 examples in order to get that smooth so that it does do things in the way that\n2:27:12 we might expect or maybe we as humans are just bad and misus the systems one\n2:27:18 or the other okay so now we''ve gone through that a few times let''s go and see if our agent can still recall our\n2:27:25 name okay and it remembers my name is James good so it still has that memory in there as well that''s good let''s move\n2:27:32 on to another quick example where we''re just going to use Google search so we''re going to be using the Ser\n2:27:39 API you can okay you can get the API key that you need from here so Ser ai.com\n2:27:46 usersign in and just enter that in here so you will get it up to 100 stes per\n2:27:54 month for free so just be aware of that if you overuse it I don''t think they\n2:28:00 charge you cuz I don''t think you enter your card details straight away but yeah just be aware of that\n2:28:07 limit now there are certain tools that line chain have already built for us so\n2:28:12 they''re pre-built tools and we can just load them using the load tools function so we do that like so we have our load\n2:28:19 tools and we just pass in the Ser API tool only we could pass in more there if we want to and then we also pass in our\n2:28:26 LM now I''m going to one use that tool but I''m also going to Define my own tool\n2:28:32 which is to get the current location based on the IP address now this is we''re in collab at the moment so it''s\n2:28:38 actually going to get the IP address for the collab instance that I''m currently on and we''ll find out where that is so\n2:28:45 that is going to get the IP address and then it''s going to provide the data back to our LM this format here so we''re\n2:28:51 going to latitude longitude City and Country okay we''re also going to get the current day and time so now we''re going\n2:28:59 to redefine our prompt I''m not going to include chat history here I just want this to be like a one shot\n2:29:06 thing I''m going to redefine our agent and agent executor using our new tools which is our set API plus to get current\n2:29:14 date time and get location from IP then I''m going to invoke our agent executor\n2:29:20 with I have a few questions what is the date and time right now how is the weather where I am and please give me\n2:29:27 degrees in celce so when it gives me that weather okay and let''s see what we\n2:29:33 get okay so apparently we''re in Council Bluffs in the\n2:29:39 US it is 13 fah which I think is absolutely freezing oh my gosh it is yes\n2:29:46 minus 10 so it''s super cold over there and you can see that okay it did give us\n2:29:53 Fahrenheits that is because the tool that we were using provided us with Fahrenheit which is fine but it did\n2:30:00 translate that over into a estimate of Celsius fours which is pretty cool so let''s actually output that so we get\n2:30:08 this which I is correct we do us approximately this and\n2:30:14 we also get an description of the conditions as well as partly cloudy with z % precipitation lucky for\n2:30:21 them and humidity of 66% okay well pretty cool so that is it\n2:30:27 for this introduction to Lang chain agents as I mentioned next chapter we''re going to dive much deeper into agents\n2:30:34 and also Implement that for Lang chain version 0.3 so we''ll leave this chapter here and jump into the next one in this\n2:30:41 chapter we''re going to be taking a deep dive into agents with the Lang chain and\n2:30:48 we''re going to be covering what an agent is we''re going to talk a\n2:30:53 little bit conceptually about agents the react agent and the type of agent that\n2:30:59 we''re going to be building and based on that knowledge we are actually going to build out our own agent execution logic\n2:31:07 which we refer to as the agent executor so in comparison to the previous video\n2:31:14 on agents in line chain which is more of an introduction this is far far more\n2:31:20 detailed we''ll be getting into the weeds a lot more with both what agents are and\n2:31:25 also agents within Lang chain now when we talk about agents a significant part\n2:31:31 of the agent is actually relatively simple code\n2:31:37 logic that iteratively runs llm calls\n2:31:42 and processes their outputs potentially running or executing tools the exact\n2:31:49 logic for each approach to building an agent will actually vary pretty\n2:31:55 significantly but we''ll focus on one of those which is the react agent now react\n2:32:02 is it''s a very common pattern and although being relatively old now most\n2:32:08 of the tool agents that we see used by openai and essentially every LM company\n2:32:15 they all use a very similar pattern now the reactor agent follows a patter and like this okay so we would have our user\n2:32:23 input up here okay so our input here is a question right aside from the Apple\n2:32:29 remote what other device you can control the program Apple remote was originally designed to interact with now probably\n2:32:35 most LMS would actually be able to answer this directly now this is from the paper which was a few years back now\n2:32:42 in this scenario assuming our LM didn''t already know the answer there are most steps\n2:32:48 that an llm or an agent might take in order to find out the answer okay so the\n2:32:54 first of those is we say our question here is what other device can control the program Apple remote was originally\n2:33:01 designed to interact with so the first thing is okay what was the program that the Apple remote was originally designed\n2:33:07 to interact with that''s the first question we have here so what we do is I\n2:33:13 need to search Apple remote and find a program it was use for this is a reasoning step so the llm is reasoning\n2:33:19 about what it needs to do I need to search for that and find a program useful so we are taking an action this\n2:33:26 is a tool call here okay so we''re going to use the search tool and our query will be apple remote and the observation\n2:33:33 is the response we get from executing that tool okay so the response here would be the Apple remote it''s designed\n2:33:39 to control the front row mediate Center so now we know the programmer for was\n2:33:45 originally designed to interact with now we''re going to go through another it\n2:33:50 okay so this is one iteration of our reasoning action and\n2:33:56 observation so when we''re talking about react here although again this sort of\n2:34:02 pattern is very common across many agents when we''re talking about react\n2:34:07 the name actually is reasoning or the first two characters of re reasoning\n2:34:13 followed by action okay so that''s where the react comes from so this is one of\n2:34:19 our react agent Loops or iterations we''re going to go and do another one so\n2:34:24 next step we have this information the LM is now provided with this information now we want to do a search for front row\n2:34:32 okay so we do that this is the reasoning step we per the action search front row\n2:34:38 okay tool search query front row observation this is the response front\n2:34:43 row is controlled by an apple remote or keyboard function keys all right cool so\n2:34:50 we know keyboard function keys are the other device that we were asking about up here so now we have all the\n2:34:58 information we need we can provide an answer to our user so we go through\n2:35:04 another iteration here reasoning and action our reasoning is I can now\n2:35:09 provide the answer of keyboard function keys to the user okay great so then we\n2:35:16 use the answer tool like Final Answer In more common tool agent use and the\n2:35:25 answer would be keyboard function keys which we then output to our user okay so\n2:35:32 that is the react Loop okay so looking at this how where are we actually calling\n2:35:40 an llm and what and in what way are we actually calling llm\n2:35:46 so we have our reasoning step our LM is generating the text here right so LM is\n2:35:52 generating okay what should I do then our LM is going to generate input\n2:35:59 parameters to our action step here that will th those input parameters and and\n2:36:05 the tool being used will be taken by our code logic our agent executor logic and\n2:36:10 they''ll be used to execute some code in which we will get an output that output\n2:36:15 might be taken directly to our observation or our llm might take that output and then generate an observation\n2:36:22 based on that it depends on how you''ve implemented everything so our LM could\n2:36:29 potentially being be being used at every single step there and of course that\n2:36:35 will repeat through every iteration so we have further iterations down here so\n2:36:41 you''re potentially using LM multiple times throughout this whole process which of course in terms of latency and\n2:36:47 token cost it does mean that you''re going to be paying more for an agent\n2:36:53 than you are with just a sun LM but that that is of course expected because you have all of these different things going\n2:36:59 on but the idea is that what you can get out of an agent is of course much better\n2:37:05 than what you can get out of an LM alone so when we''re looking at all of this all\n2:37:11 of this iterative Chain of Thought and Tool use all this needs to be controlled\n2:37:17 by what we call the agent executor okay which is our code logic which is hitting our llm processing its outputs and\n2:37:25 repeating that process until we get to our answer so breaking that part down what does it actually look like it looks\n2:37:32 kind of like this so we have our user input goes into our llm okay and then we move on to the\n2:37:39 reasoning and action steps is the action the answer if it is the answer so as we\n2:37:47 saw here where is the answer if the action is the answer so true we\n2:37:53 would just go straight to our outputs otherwise we''re going to use our select tool agent executor is going to handle\n2:37:59 all this it''s going to execute our tool and then from that we get our you know\n2:38:04 three reasoning action observation inputs and outputs and then we''re feeding all that information back into\n2:38:11 our llm okay in which case we go back through that Loop so we could be looping\n2:38:16 for a little while until we get to that final but okay so let''s go across to the\n2:38:22 code when be going into the agent executor notebook we''ll open that up in\n2:38:27 coab and we''ll go ahead and just install our prerequisites nothing different here is\n2:38:34 just L chain L Smith optionally as before again optionally line chain API\n2:38:40 key if you do want to use l Smith okay and then we''ll come down to our first\n2:38:47 section where it''s going to define a few quick tools I''m not necessarily going to\n2:38:52 go through these because we''ve already covered them in the agent introduction\n2:38:58 but very quickly Lang chain core tool is we''re just importing this tool decorator which transforms each of our functions\n2:39:05 here into what we would call a structured tool object this thing here\n2:39:12 okay which we can see just having a quick look here and then if we want to we can extract all of the sort of key\n2:39:19 information from that structure tool using these parameters here or attributes so name description AR\n2:39:25 schemer model Json streer which give us essentially how the llm should use our\n2:39:32 function okay so I''m going to keep pushing through that now very quickly\n2:39:40 again we did cover this in the intro video so I don''t want to necessar go over again into much detail but our\n2:39:48 agent EX future logic is going to need this part so we''re going to be getting a\n2:39:53 string from our llm we''re going to be loading that into to a dictionary object and we''re going to be using that to\n2:40:00 actually execute our tool as we do here using keyword\n2:40:05 arguments okay like that okay so with the tools out of the way let''s take a\n2:40:11 look at how we create our agent so when I say agent here I''m specifically\n2:40:16 talking about the part that is generating our reasoning St then generating which\n2:40:24 tool and what the input parameters to that tool will be then the rest of that\n2:40:29 is not actually covered by the agent okay the rest of that would be covered by the agent execution logic which would\n2:40:35 be taking the tool to be used the parameters executing the tool getting\n2:40:41 the response aka the observation and then iterating through that until the llm is satisfied and we have enough\n2:40:47 information to answer a question so looking at that our agent we look\n2:40:53 something like this it''s pretty simple so we have our input parameters including the chat history user query we\n2:41:00 have our input parameters including the chat history us query and actually would also have any intermediate STS that have\n2:41:07 happened in here as well we have our prompt template and then we have our llm binded with tools so let''s see how all\n2:41:15 this would look starting with we''ll Define our promp template searching look\n2:41:20 like this we have our system message your helpful assistant when answering these question you should use on to\n2:41:27 provide after using a tool tool outp will provide in the scratch Pad below okay which we naming here if you have an\n2:41:34 answer in scratch Pad you should not use any more tools and set answer directly to the user okay so we have that as our\n2:41:41 system message we could obviously modify that based on what we''re actually doing\n2:41:46 then following our system message we''re going to have our chat history so any previous interactions between the user\n2:41:52 and the AI then we have our current message from the user okay we should be\n2:41:57 fed into the input field there and then following this we have our agent stretch\n2:42:03 pad or the intermediate thoughts so this is where things like the llm deciding\n2:42:09 okay this is what I need to do this is how I''m going to do it AKA The Tool call and this is the observation that''s where\n2:42:15 all of that information will be going right so each of those to pass in as a\n2:42:21 message okay and the way that we look is that any tool call generation from the\n2:42:27 llm so when the llm is saying use this tool please that will be a assistant\n2:42:32 message and then the responses from our tool so the\n2:42:37 observations they will be returned as tool messages great so we''ll run that to\n2:42:44 Define our prompt template we''re going to Define our LM we''re going to be using\n2:42:49 J2 40 mini with a temperature of zero because we want less creativity here\n2:42:55 particularly when we''re doing tour calling there''s just no need for us to use a high temperature here so we need\n2:43:01 to enter our open ey API key which we would get from platform open ey.com we enter this then we''re going to continue\n2:43:09 and we''re just going to add tools to our LM here\n2:43:15 okay these and we''re going to bind them here then we have tool Choice any so\n2:43:22 tool Choice any we we''ll see in a moment I''ll go through this a little bit more in a second but that''s going to\n2:43:28 essentially force a tool call you can also put required which is actually a bit more uh it''s bit clearer but I''m\n2:43:35 using any here so I''ll stick with it so these are our tools we''re going through we have our inputs into the agent\n2:43:42 runable we have our prom template and then that will get fed into our llm so\n2:43:48 let''s run that now we would invoke the agent part of everything here with this okay so\n2:43:55 let''s see what it outputs this is important so I''m asking what is 10+ 10 obviously that should use the addition\n2:44:01 tool and we can actually see that happening so the agent message content is actually empty here this is where\n2:44:08 you''d usually get an answer but if we go and have a look we have additional keyword dos in there we have tool calls\n2:44:16 and then we have function arguments Okay so we''re calling a function Arguments for that function are this okay so we\n2:44:24 can see this is string again the way that we would pass that as we do Json loads and that becomes a dictionary and\n2:44:30 then we can see which function is being called and it is the add function and that is all we need in order to actually\n2:44:37 execute our function or our our tool okay we can see it''s a little more\n2:44:43 detail here now what do we do from here we''re going to map the to name to the\n2:44:49 tool function and then we''re just going to execute the tool function with the generated ARS I\n2:44:55 those I''ll also just point out quickly that here we are getting the dictionary directly which I think is coming from\n2:45:02 somewhere else in this which is prob which is here okay so even that step\n2:45:08 here where we''re passing this out we don''t necessarily need to do that because I think on the L chain side\n2:45:14 they''re doing it for us so we''re already getting that so Json loads we don''t\n2:45:20 necessarily need here okay so we''re just creating this tool name to function\n2:45:25 mapping dictionary here so we''re taking the well the tool names and we''re just mapping those back to our tool functions\n2:45:32 and this is coming from our tools list so that tools list that we defined here\n2:45:37 okay or can even just see quickly that that will include everything or each of\n2:45:43 the tools you define there okay that''s all it is now we''re going to execute\n2:45:49 using our name to Tool mapping okay so this here will get us the function so it will get us this\n2:45:56 function and then to that function we''re going to pass the arguments that we\n2:46:02 generated okay let''s see what it looks like all right so the response so the observation\n2:46:10 is 20 now we are going to feed that back\n2:46:15 into our llm using the tool message and we''re actually going to put a little bit of text around this to make it a little\n2:46:22 bit nice so we don''t necessarily need to do this to be completely honest we could\n2:46:27 just return the answer directly uh I don''t understand I don''t even think\n2:46:33 there would really be any difference so we we could do either in some cases that\n2:46:38 could be very useful in other cases like here it doesn''t really make too much difference particularly because we have\n2:46:44 this tool call ID and what this tool call ID is doing is it''s being used by AI is being read by the LM so that the\n2:46:52 LM knows that the response we got here is actually mapped back to the the tool\n2:47:01 execution that it''s identified here because you see that we have this ID right we have an ID here the LM is going\n2:47:08 to see the ID it''s going see the ID that we pass back in here and it''s going to\n2:47:13 see those two are connected so see okay this is the tool I called and this is the response I got from\n2:47:19 because of that you don''t necessarily need to say which tool you used here you can it it depends on what you''re\n2:47:27 doing okay so what do we get here we have okay just running everything again\n2:47:34 we''ve added our tool call so that''s the original AI message that includes okay user add tool and then we have the tool\n2:47:40 execution tool message which is the observation we map those to the agent\n2:47:46 scratch pad and then what do we get we have an AI message but the content is empty again which is interesting because\n2:47:53 we we said to our llm up here if you have an answer to the in the scratchpad\n2:47:59 you should not use any more tools and said answer directly to the user so why why is our\n2:48:06 llm not answering well the reason for that is down here we specify tool Choice\n2:48:15 equals any which again it''s the same tool Choice required which is telling\n2:48:22 the L land that it cannot actually answer directly it has to use a tool and\n2:48:28 I usually do this right I would usually put tool Choice equals any or required and for the LM to use a tool every\n2:48:36 single time so then the question is if it has to use a tool every time how does\n2:48:41 it answer our user well we''ll see in a moment first I just want to show you\n2:48:49 the two options essentially that we have the second is what I would usually use but let''s let''s start with the first so\n2:48:56 the first option is that we set tool Choice equal to Auto and this tells the Ln that it can either use a tool or it\n2:49:03 can answer the user directly using the the final answer or using that content\n2:49:09 field so if we run that like we''re specifying to choices Auto we run that\n2:49:15 let''s invoke okay initially you see ah wait there''s still no content that''s because\n2:49:21 we didn''t add anything into the agent scratch Pad here there''s no information right it''s all\n2:49:27 empty um actually it''s empty because sorry so here you have the chat history that''s empty we didn''t specify the agent\n2:49:36 scratch Cad and the reason that we can do that is because we''re using if you look here we''re using get so essentially\n2:49:42 it''s saying try and get agent scratch pad from this dictionary but if it hasn''t been provided we''re just going to\n2:49:48 give an empty list so that''s what that''s why we don''t need to specify it here but\n2:49:54 that means that oh okay the the agent doesn''t actually know anything here it hasn''t used a tool yet so we''re going to\n2:50:01 just go through our iteration again right so we''re going to get our tool output we''re going to use that to create\n2:50:07 the tool message and then we''re going to add our tool call from the AI and the\n2:50:14 observation we''re going to pass those to the agent scratch pad and this time we see we run that okay now we get the\n2:50:22 content okay so now it''s not calling you see here there''s no to call or anything going\n2:50:28 on we just get content so that is this is the standard\n2:50:34 way of doing or building a tool calling agent the other option which I mentioned\n2:50:40 this is what I would usually go with so number two here I would usually create a\n2:50:45 final answer tool so why would we even do that why would we\n2:50:53 create a final answer tool rather than just you know this method is actually perfectly you know it works so why would\n2:50:59 we not just use this there are a few reasons the main ones are that with\n2:51:05 option two where we''re forcing tool calling this removes possibility of an\n2:51:11 agent using that content field directly and the reason at least the reason I\n2:51:17 found this good when building agents in the past is that occasionally when you do want to use a tool it''s actually\n2:51:22 going to go with the content field and it can get quite annoying and and use the content field quite frequently when\n2:51:29 you actually do want it to be using one of the tools and this is particularly\n2:51:36 noticeable with smaller models with bigger models it''s not as common\n2:51:41 although does so happen now the second thing that I quite like about using a\n2:51:47 tool as your final answer is that you can enforce a\n2:51:52 structured output in your answer so this is something we''re stting I think the first yes the first line chain example\n2:52:01 where we were using the structured output tool of Lang chain and what that\n2:52:06 actually is the structured outputs feature of Lang chain it''s actually just a tool call right so it''s forcing a tool\n2:52:13 call from your LM it''s just abstracted away so you don''t realize that that''s what it''s doing but that is what it''s doing\n2:52:20 so I find that structured outputs are very useful particularly when you have a\n2:52:26 lot of code around your agent so when that output needs to go Downstream into\n2:52:32 some logic that can be very useful because you can you have a reliable\n2:52:39 output format that you know is going to be output and it''s also incredibly useful if you have multiple outputs or\n2:52:47 multiple fields that you need to generate for so those can be very useful\n2:52:53 now to implement this so to implement option two we need to create a final answer tool we as with our other tools\n2:53:02 we''re actually going to description and you can or you cannot do this so you can\n2:53:08 you can also just return non and actually just use the generated\n2:53:14 action as the essentially what you''re going to send out of your agent\n2:53:19 execution logic or you can actually just execute the tool and just pass that\n2:53:24 information directly through perhaps in some cases you might have some additional postprocessing for your final\n2:53:31 answer maybe you do some checks to make sure it hasn''t said anything weird you could add that in this tool\n2:53:37 here but yeah in in this case we''re just trying to pass those through directly\n2:53:43 so let''s run this we''ve added where are we\n2:53:48 Final Answer we''ve added the final answer tool to our named tool mapping so our agent can now use it we redefine our\n2:53:56 agent setting tool choice to any because we''re forcing the tool Choice here and let''s go with what is 10 + 10 see what\n2:54:04 happens okay we get this right we can also one thing nice thing here is that\n2:54:10 we don''t need to check is out up in the content field or is it in the tool course field we know it''s going to be in\n2:54:15 the tool course field because we''re forcing that tool use quite nice so okay we know we''re using the ad tool and\n2:54:22 these are the arguments great we go or go through our process again we''re going\n2:54:27 to create our tool message and then we''re going to add those messages into our scratch pad or intermediate sets and\n2:54:34 then we can see again ah okay content field is empty that is expected we we''re\n2:54:41 forcing tool users no way that this can be this can be or have anything inside\n2:54:46 it but then if we come down here to our to calls nice final answer arbs answer\n2:54:53 10 + 10 = 20 all right we also have this tools used where''s tools used coming\n2:55:00 from okay while I mentioned before that you can add additional things or or\n2:55:06 outputs when you''re using this tool use for your final answer so if you just\n2:55:11 come up here to here you can see that I asked the llm to use that Tool''s use\n2:55:18 field which I defined here it''s a list of strings use this to tell me what tools you used in your answer right so\n2:55:25 I''m getting the normal answer but I''m also getting this information as well which is kind of nice so that''s where\n2:55:30 that is coming from see that okay so we have our actual answer here and then we\n2:55:36 just have some additional information okay and we''ve also defined a type here it''s just a list of strings which is\n2:55:41 really nice it''s giving us a lot of control over what we''re outputting which is perfect that''s you know when you''re\n2:55:47 building with agents the biggest problem in most cases\n2:55:52 is control of your llm so here we''re getting a honestly pretty unbelievable\n2:56:01 amount of control over what our LM is going to be doing which is perfect for\n2:56:06 when you''re building in the real world so this is everything we need this\n2:56:13 is our answer and we would of course be passing that Downstream into whatever log\n2:56:19 our AI application would be using okay so maybe that goes directly to a front\n2:56:25 end and we''re displaying this as our answer and we''re maybe providing some information about okay where did this\n2:56:31 answer come from or maybe there''s some additional steps Downstream where we''re\n2:56:36 actually doing some more processing or Transformations but yeah we have that that''s great now everything we''ve just\n2:56:44 done here we''ve been executing everything one by one and that''s to help us understand what process we go through\n2:56:53 when we''re building an agent executor but we''re not going to want to\n2:56:59 do that all the time are we most of the time we probably want to abstract all this away and that''s what we''re going to\n2:57:05 do now so we''re going to build essentially everything we''ve just taken\n2:57:11 we''re going to abstra take that and Abstract it away into a custom agent\n2:57:16 executor class so let''s have a quick look at what we''re doing here although it''s it''s literally just what we we just\n2:57:23 did okay so custom maor executor we initialize it we set this m\n2:57:29 Max iterations I''ll talk about this in a moment we initialize it that is going to set out chat history to just being empty\n2:57:38 okay it''s a new agent there should be no chat history in this case then we actually Define our agent right so that\n2:57:44 poted logic that is going to be taking out inputs and generating what to do next AKA what tool call to do okay and\n2:57:52 we set everything as attributes of our class and then we''re going to Define an\n2:57:58 invoke method this invoke method is going to take an input which just a\n2:58:03 string so it''s going to be our message from the user and what it''s going to do is it''s\n2:58:09 going to iterate through essentially everything we just did okay until we hit\n2:58:15 the The Final Answer tool Okay so well what does that mean we have our\n2:58:21 tool call right which is we''re just invoking our agent right so it''s going to generate what tool to use and what\n2:58:28 parameters should go into that okay and that''s a that''s an AI message so we would append that to our\n2:58:36 agent stretch pad and then we''re going to use the information from our tool call so the name of the tool and the ARs\n2:58:42 and also the ID we''re going to use all of that information to execute our tool\n2:58:49 and then provide the observation back to our llm okay so we execute our tool here\n2:58:55 we then format the tool output into a tool message see here that I''m just\n2:59:01 using the the output directly I''m not adding that additional information there\n2:59:06 we need do need to always pass in the tool call ID so that our LM knows which\n2:59:12 output is mapped to which tool I didn''t mention this before in in this video at\n2:59:17 least but that is that''s important when we have multiple toour calls happening in parallel because that can happen when\n2:59:23 we have multiple toour calls happening in parallel let''s say we have 10 tool calls all those responses might come\n2:59:28 back at different times so then the order of those can get messed up so we\n2:59:34 wouldn''t necessarily always see that it''s a AI message beginning a tool call followed\n2:59:41 by the answer to that tool call instead it might be AI message followed by like 10 different tool call responses so you\n2:59:49 need to have those IDs in there okay so then we pass our tool output back to our\n2:59:57 agent scratch pad or intermediate steps I''m sing a print in here so that we can see what''s happening whilst everything\n3:00:03 is running then we increment this count number we''ll talk about that in a moment\n3:00:08 so com past that we say okay if the tool name here is final answer that means we\n3:00:15 should stop okay so so once we get the final answer that means we can actually\n3:00:21 extract our final answer from the the final tool call okay and in this case\n3:00:26 I''m going to say that we''re going to extract the answer from the tool call or\n3:00:33 the the observation we''re going to extract the answer that was generated we''re going to pass that into our chat\n3:00:40 history so we''re going to have our user message is the one the user came up with followed by our answer which is just the\n3:00:47 the natur answer field and that''s going to be an AI message but then we''re actually going to be including all of\n3:00:53 the information so this is the the answer natural language answer and also\n3:00:59 the tools used output we''re going to be feeding all of that out to some\n3:01:04 Downstream process as preferred so we have that now one thing that can happen if\n3:01:12 we''re not careful is that our agent executor might may run many many times\n3:01:19 and particularly if we''ve done something wrong in our logic as we''re building these things it can happen that maybe\n3:01:26 we''ve not connected the observation back up into our agent executor logic and in\n3:01:33 that case what we might see is our agent executor runs again and again and again and I mean that''s fine we''re going to\n3:01:38 stop it but if we don''t realize straight away and we''re doing a lot of llm cords\n3:01:44 that can get quite expensive quite quickly so what we can do is we can set a limit right so that''s\n3:01:50 what we''ve done up here with this Max iterations we said okay if we go past three max iterations by default I''m\n3:01:56 going to say stop all right so that''s that''s why we have the count here while\n3:02:01 count is less than the max iterations we''re going to keep going once we hit the number of Max iterations we stop\n3:02:08 okay so the while loop will will just stop looping okay so it just protects Us\n3:02:14 in case of that and it also potentially maybe it''s Point your agent might be\n3:02:19 doing too much to answer a question so this will force it to stop and just provide an answer although if that does\n3:02:26 happen I just realize there''s a bit of a fault in the logic here if that does happen we wouldn''t necessarily have the\n3:02:33 answer here right so we would probably want to handle that nicely but in this\n3:02:39 scenario a very simple use case we''re not going to see that happening so we\n3:02:44 initialize our custom agent executor and then we invoke\n3:02:50 it okay and let''s see what happens all right there we go so that just wrapped\n3:02:56 everything into a single single invoke so everything is handled for us uh we\n3:03:03 could say okay what is 10 you know we can modify that and say 7.4 for example\n3:03:12 and that we''ll go through we''ll use the multiply tool instead and then we''ll come back to the final answer again okay\n3:03:18 so we can see that with this custom agent executor we''ve built an agent and\n3:03:24 we have a lot more control over everything that is going on in here one thing that we would probably need to add\n3:03:34 in this scenario is right now I''m assuming that only one tool call will happen at once it''s also why I''m asking\n3:03:39 here I''m not asking a complicated question because I don''t want it to go and try and execute multiple tool Calles\n3:03:46 at once uh which which can happen so let''s just try\n3:03:51 this okay so this is actually completely fine so this did just execute it one after the other so you can see that when\n3:03:59 asking this more complicated question it first did the exponentiate tool followed\n3:04:05 by the ad tool and then they actually gave us our final answer which is cool also told us we use both of those tools\n3:04:12 which it did but one thing that we should just be aware of is that from\n3:04:18 open AI open AI can actually execute multiple tool calls in parallel so by\n3:04:24 specifying that we''re just using this zero here we''re actually assuming that we''re only ever going to be calling one\n3:04:31 tool at any one time which is not always going to be the case so you would probably need to add a little bit of exual logic there in case of scenarios\n3:04:38 if you''re building an an agent that is likely to be running parallel to calls\n3:04:44 but yeah you can see here actually it''s completely fine so it''s running one after the other okay so with that we\n3:04:50 built our agent executor I know there''s a lot to that and of course you can just\n3:04:56 use the very abstract agent executor in L chain but I think it''s very good to\n3:05:01 understand what is actually going on to build our own agent executor in this case and it sets you up nicely for\n3:05:08 building more complicated or use case specific agent logic as\n3:05:14 well so that is it for this chapter in this chapter we''re going to\n3:05:20 be taking a look at line chains expression language we''ll be looking at the runnables the serializable and\n3:05:27 parallel of those the runable pass through and essentially how we use l\n3:05:33 cell in its full capacity now to do that well what I want to do is actually start\n3:05:40 by looking at the traditional approach to building chains in L chain so to do\n3:05:47 do that we''re going to go over to the ELO chapter and open that Cur up okay so\n3:05:54 let''s come down we''ll do the prerequisites as before nothing measure in here the one thing that is new is Doc\n3:06:01 array because later on as you see we''re going to be using this as an example of\n3:06:08 the parallel capabilities in L cell if you want to use Lim Smith you just need\n3:06:14 to add in your lime train API key okay and then let''s okay so now let''s dive into the\n3:06:20 traditional approach to chains in line chain so the LM chain I think is\n3:06:28 probably one of the first things introduced in line chain if I''m not wrong this take it to prompt and feeds\n3:06:33 into an l and that that''s it it you can also you can add like output passing to\n3:06:40 that as well but that''s optional and I don''t think we''re going to cover here so\n3:06:47 what that might look like is we have for example this promp template here give me a small report on topic okay so that\n3:06:54 would be our prompt template we set up as we usually do with the prom templates\n3:07:01 as we''ve seen before we then Define our LM need our\n3:07:07 open a key for this which as usual we would get from platform.\n3:07:13 open.com then we go ahead I''m just just showing you that you can Ino the LM there then we go ahead\n3:07:20 actually Define a output POS so we do do this I wasn''t sure we did but we would\n3:07:26 then Define our LM chain like this okay so LM chain we adding our prompt adding\n3:07:32 our LM adding our alasa okay this is the traditional\n3:07:39 approach so I would then say Okay retrieve Org the generation and what''s going to do it''s going to give me a\n3:07:45 little report back on on rag okay t a moment but you can see that\n3:07:51 that''s what we get here we can format out nicely as we\n3:07:56 usually do and we get okay look we get a nice little report however the LM chain\n3:08:02 is one it''s quite restrictive right we have to have like particular parameters that have been predefined as being\n3:08:09 usable which is you know restrictive and it''s also been deprecated so you know\n3:08:16 this isn''t the standard way of doing this anymore but we can still use it however the preferred method to building\n3:08:23 this and building anything else really or chains in general in L chain is using El cell right and it''s super simple\n3:08:30 right so we just actually take the prompt lemon Apple P that we had before and then we just chain them together\n3:08:36 with these pipe operators so the pipe operator here is saying take what is output from here and input it into here\n3:08:43 take wi''s output from here and input it into here it''s all it does super simple\n3:08:48 so put those together and we invoke it in the same way and we''ll get the same\n3:08:54 output okay and that''s what we get there is actually a slight difference on what\n3:09:00 we''re getting out from there you can see here we got actually a dictionary but\n3:09:06 that is pretty much the same okay so we get that and as before we can display\n3:09:12 that in markdown with this okay so we saw just now that we have this pipe\n3:09:18 operator here it''s not really\n3:09:23 standard P python syntax to use this or at least it''s definitely not common it''s\n3:09:29 it''s it''s an aberration of the intended use of python I think but anyway it\n3:09:36 does it looks cool and when you understand it I kind of get why they do\n3:09:42 because it make it does make things quite simple in comparison to what it could be otherwise so I kind get it it''s\n3:09:48 a little bit weird but it''s what they''re doing and I''m teaching that so that''s\n3:09:53 what we''re going to learn so what is that pipe operator\n3:09:59 actually doing well it''s as I mentioned it''s\n3:10:04 taking the output from this putting it as input into into what is ever under right but how does that actually work\n3:10:12 well let''s actually implement it ourselves without line chain so we''re going to create this class called\n3:10:18 runnable this class when we initialize it it''s going to take a function okay so this is literally a python function it''s\n3:10:24 going to take that and it''s going to essentially turn it into what we would\n3:10:30 call a runnable in line chain and what does that actually mean well it doesn''t really mean anything it just means that\n3:10:38 when you use run the invoke method on it it''s going to call that function in the\n3:10:43 way that you would have done otherwise all right so using just function you know brackets open parameters brackets\n3:10:50 closed it''s going to do that but it''s also going to add this method this all method now this all method in typical\n3:10:59 python syntax now this all method is essentially going to take your runnable\n3:11:06 function the one that you initialize with and it''s also going to take an other function okay this other function\n3:11:13 is actually going to be a runnable I believe yes it''s going to be runnable just like this and what it''s going to do\n3:11:20 is it''s going to run this runnable based on the output of your current runable\n3:11:28 okay that''s what this or is going to do seems a bit weird maybe but I''ll explain\n3:11:34 in a moment we''ll see why that works so I''m going to chain a few functions\n3:11:39 together using this or method so first we''re just going to turn\n3:11:45 them all into runnables Okay so these are normal functions as you can see normal python functions we then turn\n3:11:51 them into this runnable using our runnable class then look what we can do right so\n3:11:58 we we''re going to create a chain that is going to be our runnable\n3:12:04 chained with another runnable chained with another runnable okay let''s see what happens so we''re going to invoke\n3:12:11 that chain of runnables with three so what is this going to do\n3:12:16 okay we start with five we''re going to add five to three so we''ll get eight\n3:12:22 then we''re going to subtract five from8 to give us three again and then we''re\n3:12:29 going to multiply three by five to give us 15 and we can inval that and we get\n3:12:38 15 okay pretty cool so that is interesting how does that relate to the\n3:12:44 pipe operator well that pipe operator in Python is actually a\n3:12:51 shortcut for the or method so what we just implemented is the pipe operator so\n3:12:56 we can actually run that now with the pipe operator here and we''ll get the same get 15 right so that''s that''s what\n3:13:03 line chain is doing like under the hood that is what that pipe operator is it''s\n3:13:08 just chaining together these multiple runnables as we''d call them using their own internal or operator okay which is\n3:13:17 cool I I I will give them that it''s kind of a cool way of doing this creative I wouldn''t have thought about it\n3:13:24 myself so yeah that is a pipe operator then we have these runnable things okay\n3:13:31 so this is a this is different to the runable I just defined here this is we Define this ourselves it''s not a lang\n3:13:37 chain thing we didn''t get this from Lang chain Instead This runnable Lambda\n3:13:44 object here that is actually exactly the same as what we just defined\n3:13:49 all right so what we did here this runnable this runnable Lambda is the same thing but in Lang\n3:13:58 chain okay so if we use that okay we use that to now Define three runnables from\n3:14:05 the functions that we defined earlier we can actually pair those together now using the the pipe operator you could\n3:14:12 also pair them together if you want with the or operator right\n3:14:18 so we could do what we did earlier we can invoke that okay or as we were doing\n3:14:25 originally we use pipe operator exactly the same so this runnable Lambda from line chain is just\n3:14:31 what we what we just built with the runable cool so we have that now let''s\n3:14:37 try and do something a little more interesting we''re going to generate a report and we''re going to try and edit that report using this this\n3:14:43 functionality okay so give me a small report about topic okay we''ll Z through\n3:14:48 here we''re going to get our report on\n3:14:54 AI okay so we have this you can see that AI is mentioned many times in\n3:15:00 here then we''re going to take a very simple function right so I''m extract\n3:15:07 fact this is basically going to take uh what is it see taking the\n3:15:14 first okay so we''re actually trying to remove the introduction here I''m not sure if this actually will work as\n3:15:20 expected but it''s it''s fine try it anyway but then more importantly we''re\n3:15:28 going to replace this word okay so we''re going to replace an old word with a new word our old word going to be Ai and the\n3:15:34 word is going to be Skynet okay so we can wrap both of these functions as\n3:15:40 runable lambas okay we can add those as additional steps inside our entire chain\n3:15:46 all right so we''re going to extract try and remove the introduction although I think it needs a bit more processing\n3:15:53 than just splitting here and then we''re going to replace the word we need that actually to be AI run that run\n3:16:01 this okay so now we get artificial intelligence Skynet refers to the\n3:16:07 simulation of human intelligent process by machines uh we have narrow Skynet weak\n3:16:12 Skynet and strong Skynet applications of Skynet Skynet Technologies is being applied in\n3:16:18 numerous Fields including all these things scary despite potential sky that poses\n3:16:24 several challenges systems can perpetrate exist and biases it ra significant privacy\n3:16:31 concerns it can be exploited for malicious purposes okay so we have all\n3:16:37 these you know it''s just a silly little example we can see also the introduction didn''t work here the reason for that is\n3:16:43 because our introduction includes multiple new lines here so I would actually if I want to remove the\n3:16:50 introduction we should remove it from here I think and this is a I I would\n3:16:56 never actually recommend you do that uh because it''s not it''s not very flexible\n3:17:01 it''s not very robust but just so I show you that that is actually working so\n3:17:07 this extract fact runnable right so now we''re essentially just removing the\n3:17:14 introduction right why what do we want to do that I don''t know but it''s there just so you can see that we can have\n3:17:20 multiple of these runnable operations running and they can be whatever you want them to be okay it is worth knowing\n3:17:28 that the inputs to our functions here were all single arguments okay if you\n3:17:35 have function that is accepting multiple arguments you can do that the way that I would probably do it or you can do it in\n3:17:42 multiple ways one of the ways that you can do that is actually write your function to except for arguments but\n3:17:49 actually do them through a single argument so just like a single like X which would be like a dictionary or something and then just unpack them\n3:17:56 within the function and and use them as needed that''s just yeah that''s one way you can do it now we also have these\n3:18:02 different uh runnable objects that we can use so here we have runnable parallel and runnable pass through kind\n3:18:10 of self-explanatory to some degree so let me let just go through those so runable parallel allow you to run\n3:18:17 multiple runnable instances in parallel runnable pass through May was less\n3:18:24 self-explanatory allows us to pass a variable through to the next runnable without modifying it okay so let''s see\n3:18:31 how they would work so we''re going to come down here and we''re going to set these two dock arrays obiously these two\n3:18:38 sources of information and we''re going to need our\n3:18:43 LM to pull information from both of these sources of information in parallel which is going to look like this so we\n3:18:49 have these two sources of information Vector store a vector store B this is\n3:18:55 our dock array a and dock array B these are both going to be fed in as\n3:19:00 context into our prompt then our LM is going to use all of that to answer the\n3:19:06 question okay so to actually Implement that we have our we need an embedding\n3:19:12 model so he open our embeddings we have our vetur a a vector B they''re not you\n3:19:18 know real vectors they''re not full-on vectors SS here we''re just passing in a very small amount of information to both\n3:19:26 so we''re saying okay we''re going to create an inmemory vect S using these\n3:19:31 two bits of information so when say half the information is here this would be an irrelevant piece of information then we\n3:19:37 had the relevant information which is deep seek re3 was released in December 2024 okay then we''re going to have some\n3:19:45 other information in our other Vector sore again irrelevant piece here and\n3:19:50 relevant piece here okay the Deep seek V3 LM is a mixure of experts model with\n3:19:56 671 billion parameters at its largest okay so based on that we''re also going\n3:20:03 to build this prompt string so we''re going to pass in both of those contexts into our prompt then I''m going to ask a\n3:20:10 question we don''t actually need we don''t need that bit and actually we don''t even\n3:20:15 need that bit what am I doing so we just need this so we have the both the contexts there and we would run them\n3:20:22 through our prompt template okay so we have our system promp template which is\n3:20:27 this and then we''re just going to have okay our question is going to go into here as a user message cool so we have that and then\n3:20:36 let me make this easier to read we''re going to convert both those\n3:20:41 STS to retrievers which just means we can retrieve stuff from them and we''re going to use this runnable parallel to\n3:20:49 run both of these in parallel right so these are being both being run in\n3:20:55 parallel but then we''re also running our question in parallel because this needs to be essentially passed through this\n3:21:02 component without us modifying anything so when we look at this here it''s almost\n3:21:07 like okay the this section here would be our runable parallel and these are being\n3:21:14 running parallel but also our query is being passed through so it''s almost like\n3:21:19 there''s another line there which is our runable pass through okay so that''s what we''re doing here these running in\n3:21:25 parallel one of them is a pass through I need\n3:21:30 to run here I just realized here we''re using\n3:21:35 the uh deprecated embeddings just switch it to this so L chain open\n3:21:42 AI we run that run this run that and now\n3:21:47 this is set up okay so we then put our initial so this\n3:21:56 using our runable parallel and runnable pass through that is our initial step we\n3:22:01 then have our prompt LM now pass which would being chained together with usual\n3:22:07 you know the usual type operator okay and now we''re going to invoke question what architecture does\n3:22:13 the mod deep seek release in December use okay okay so for the elm to answer\n3:22:19 this question it''s here to need to tell us what it needs the information about the Deep seek model that was released in\n3:22:24 December which we have specified in one half uh here and then it also needs to\n3:22:31 know what architecture that model uses which is defined in the other half over\n3:22:37 here okay so let''s run this okay there we go deep SE V3 model\n3:22:44 released in December 2024 is a mix experts model with 671 billion\n3:22:49 parameters okay so mixture of experts and this many parameters pretty cool so\n3:22:55 we''ve put together our pipeline using elol using the pipe operator the\n3:23:01 runnables specifically we''ve looked at the runable parallel runable pass through and also the runable lampas so\n3:23:08 that''s it for this chapter on lell and we''ll move on to the next one in this\n3:23:14 chapter we''re going to cover streaming and async in Lang chain now both using\n3:23:19 async code and using streaming are incredibly important components of I\n3:23:27 think almost any conversational chat interface or at least any good\n3:23:32 conversational chat interface for async if your application is not async and\n3:23:39 you''re spending a load of time in your API or whatever else waiting for llm\n3:23:45 calls because a lot of those are behind apis you are waiting and your\n3:23:50 application is doing nothing because you''ve written synchronous code and that\n3:23:55 well there are many problems with that mainly it doesn''t scale so asyn code\n3:24:00 generally performs much better and especially for AI where a lot of the\n3:24:05 time we''re kind of waiting for API calls so asyn is incredibly important for that\n3:24:10 for streaming now streaming is slightly different thing so let''s say I want to\n3:24:16 to tell me a story okay I''m using gbt 4 here it''s a\n3:24:22 bit slower so we can achieve string we can see that token by token this text is being produced and sent to us now this\n3:24:29 is not just a visual thing this is the LM when it is generating tokens or words\n3:24:38 it is generating them one by one and and that''s because these llms literally\n3:24:43 generate tokens one by one so they''re looking at all of the previous tokens in order to generate the next one and then\n3:24:48 generate next one generate next one that''s how they work so when we are\n3:24:54 implementing streaming we''re getting that feed of tokens directly from the LM\n3:24:59 through to our you know our back end or our front end that is what we see when when we see that token by token\n3:25:06 interface right so that''s one thing what one other thing that I can do that let\n3:25:12 me switch across to 40 is I can say okay we just got this story I''m going to\n3:25:18 ask are there any standard\n3:25:25 storytelling techniques to follow used above please\n3:25:32 use search okay so look we we get this very\n3:25:41 briefly there we saw that it was searching the web and the way it''s not because we told it okay we told the llm\n3:25:48 to use the search tool but then the llm output some tokens to say use the search\n3:25:55 tool that is going to use a Search tool and it also would have output the token saying what that search query would have\n3:26:02 been although we didn''t see it there but what the chat GPT interface is\n3:26:08 doing there so it received those tokens saying hey I''m going to use a Search tool it didn''t just send us those tokens\n3:26:15 like it does with the tokens here instead it used those tokens to show us\n3:26:21 that searching the web little text box so streaming is not just the streaming\n3:26:28 of these direct tokens it''s also the streaming of these intermediate steps\n3:26:34 that the llm may be thinking through which is particularly important when it\n3:26:40 comes to agents and agentic interfaces so it''s also a feature thing right\n3:26:45 streaming does doesn''t just look nice is also a feature then finally of course\n3:26:51 when we''re looking at this okay let''s say we go back to\n3:26:57 GT4 and I say okay use all of this\n3:27:04 information to generate a long story for\n3:27:10 me right and okay we are getting the first token now we know something is\n3:27:16 happening and we need start reading now imagine if we were not streaming anything here and we''re just waiting\n3:27:23 right we''re still waiting now we''re still waiting and we wouldn''t see anything we''re just like oh it''s just\n3:27:28 blank or maybe there''s a little loading spinner so we''d still be waiting and even now we''re still\n3:27:39 waiting right this is an extreme example but can you imagine just waiting\n3:27:45 for so long and not seeing anything as a user right now just now we would have\n3:27:51 got our answer if we were not streaming I mean that that would be painful as a\n3:27:56 user you you not want to wait especially in a chat interface you don''t want to wait that\n3:28:01 long it''s okay with okay for example deep research takes a long time to process but you know it''s going to take\n3:28:08 a long time to process and it''s a different user case right you''re getting a report this is a chat interface and\n3:28:15 yes most messages are not going to take that long to generate we''re also\n3:28:21 probably not going to be using GPT 4 depending on I don''t know maybe some people still do but in some scenarios\n3:28:29 it''s painful to need to wait that long okay and it''s also the same for agents it''s nice when you''re using agents again\n3:28:36 update on okay we''re using this tool it''s using this tool this is how it''s using them perplexity for example have a\n3:28:42 very nice example of this so okay what what''s this open I founder joins morati\n3:28:49 sub let''s see right so we see this is really nice it''s we''re using Pro search it''s searching for news sharing with the\n3:28:55 results like we''re getting all this information as we''re waiting which is really cool and it helps to understand\n3:29:02 what is actually happening right it''s not needed in all use cases but it''s super nice to have those intermediate\n3:29:09 steps right so then we''re not waiting and then I think this bit probably also streamed but it was just super fast so I\n3:29:15 I didn''t see it but that''s pretty cool so streaming is pretty important let''s\n3:29:22 dive into our example okay we''ll open that in cab enough we out so starting\n3:29:27 with the prerequisites same as always Lang chain optionally L Smith we''ll also\n3:29:34 enter our L chain API key if you''d like to use l Smith we''ll also enter our openi API key so that is platform.\n3:29:42 open.com and then as usual we can just invoke our l m right so we have that\n3:29:48 it''s working now let''s see how we would stream with a stream okay so whenever a\n3:29:56 method so stream is actually a method as well we could use that but it''s not acing right so whenever we see a method\n3:30:02 in line chain that has a prefixed onto what would be another method that''s like\n3:30:08 the async version of this so we can actually stream using async\n3:30:16 super easily using just LM a stream okay\n3:30:21 now this is just a an example in to be completely honest you probably will not\n3:30:27 be able to use this in an actual application but it''s just an example and we''re going to see how we would use this\n3:30:34 or how we would stream asynchronously in an application further down in this\n3:30:40 notebook so starting with this you can see here that we''re getting these tokens\n3:30:45 right we''re just appending it to token here we don''t actually need to do that I don''t think we''re using this but maybe\n3:30:51 we yeah we do it here it''s fine so we''re just pending the tokens as they come\n3:30:56 back from our LM pending it to this we''ll see what that is in a moment and\n3:31:02 then I''m just printing the token content right so the content of the\n3:31:07 token so in this case that would be l in this case it would be LP it would be Sans for so on and so on so you can see\n3:31:14 for the most part it''s it''s tends to be Word level but it can also be subword level as you see scent iment is one word\n3:31:23 of course so you know they get broken up in in various ways then adding this pipe character\n3:31:30 onto the end here so we can see okay where are our individual tokens then we\n3:31:36 also have flush so flush uh you can actually turn this off and it''s still going to stream you''re still going to\n3:31:41 see everything which going to be a bit more you can see it''s kind of a it''s like bit by bit when we use flush it\n3:31:49 forces the console to update what is being shown to us immediately all right\n3:31:54 so we get a much smoother um when we''re looking at this it''s much smoother versus when flush is\n3:32:02 not set true so yeah when you''re printing that is good to do just so you can see you don''t necessarily need to\n3:32:08 okay now we added all those tokens to the tokens list so we can have a look at each individual object that was returned\n3:32:15 turn to us right and this is interesting so we see that we have the AI message chunk right that''s an object and then\n3:32:22 you have the content the first one''s actually empty second one has that n for NLP and\n3:32:30 yeah I mean that''s all we rarely need to know they''re very simple objects but they''re actually quite\n3:32:35 useful because uh just look at this right so we can add each one of our AI\n3:32:40 message chunks right let''s see what that does it doesn''t create a list it creates this right so we still just have one AI\n3:32:48 message chunk uh but it''s combined the content within those AI message chunks\n3:32:55 which is kind of cool right so for example like we could remove\n3:33:01 these right and then we just see NLP so that''s kind of nice little feature there\n3:33:06 I do I actually quite like that but uh you do need to just be a little bit careful because obviously you can do\n3:33:13 that the wrong way and you''re going to get like a I don''t know all that is some weird token salad so yeah you need to\n3:33:21 just make sure you are going to be merging those into correct order unless you I don''t know unless you''re doing\n3:33:28 something weird Okay cool so streaming that that was streaming from a LM let''s\n3:33:34 have look at streaming with agents so we it gets a bit more complicated to be\n3:33:41 completely honest but we also need to things are going to get a bit more\n3:33:46 complicated so that we can implement this in for example an API right so is\n3:33:52 it''s kind of like a necessary thing in any case so to just very quickly we''re\n3:33:58 going to construct our agent executor like we did in the agent execution chapter and for that for the agent\n3:34:06 executor we''re going to need tools chat prompt template llm agent and the agent H itself okay very quickly I''m not going\n3:34:13 to go through these uh in detail we just def find our tools have ADD multiply exponentiate subtract and Final Answer\n3:34:20 tool merge those into a single list of tools then we have our prompt template\n3:34:26 again same as before we just have system message we have chat history we have you query and then we have the agent scratch\n3:34:34 pad for those intermediate sets then we Define our agent using L cell L cell\n3:34:40 works quite well with both streaming and async by the way it supports both out of\n3:34:46 the box which is nice so we Define our agent then coming down here we''re going\n3:34:54 to create the agan ice fter this is the same as before right so there''s nothing\n3:34:59 new in here I don''t think so just initialize our agent things there then\n3:35:05 it''s you know We''re looping through looping through yeah nothing\n3:35:11 nothing new there so we''re just executing invoking our agent seeing if\n3:35:17 there''s a tool call uh this is slightly we could shift this to before or after it doesn''t actually matter that\n3:35:23 much so we''re checking if it''s final answer if not we continue X to our tools\n3:35:29 and so on Okay cool so then we can invoke\n3:35:35 that okay we go what is 10 + 10 there we go right so we have our\n3:35:42 agent executor it is working now now when we are running our agent executor\n3:35:51 with every new query if we''re putting this into an API we''re probably going to need to\n3:35:57 provide it with a a fresh callback Handler okay so this is the corat Handler that''s going to handle taking\n3:36:04 the tokens that are being generated by a LMO agent and giving them to some other\n3:36:10 piece of code like for example the the streaming response for a API\n3:36:16 and our Corbat Handler is going to put those tokens in a queue in our case and\n3:36:22 then our for example the streaming object is going to pick them up from the queue and put them wherever they need to\n3:36:28 be so to allow us to do that with every\n3:36:33 new query or is needing to initialize everything when we actually initialize\n3:36:39 our agent we can add a configural field to our llm okay so we set the configural\n3:36:45 Fields here oh also one thing is that way we set streaming equal to true that''s very manting but just so you see\n3:36:52 that there we do do that so we add some configurable fields to our LM which means we can basically pass an object in\n3:37:00 for these on every new invocation so we set our configurable\n3:37:05 field it''s going to be called callbacks and we we just add a description right there''s nothing more to it so this will\n3:37:12 now allow us to provide that field when we''re invoking our agent okay now we\n3:37:21 need to Define our callback Handler and as I mentioned what is basically going\n3:37:26 to be happening is this callback Handler is going to be passing tokens into our a\n3:37:32 sync IO Q object and then we''re going to be picking them up from the que\n3:37:37 elsewhere okay so we can call it a q callback Handler okay and that is\n3:37:42 inhering from the async Callback Handler cuz we want all this to be done asynchronously because we''re we''re\n3:37:47 thinking here about okay how do we Implement all this stuff within apis and actual real world code and we we do want\n3:37:55 to be doing all this in aing so let me execute that and I''ll just explain a little bit of what we''re looking at so\n3:38:01 we have the initialization right it''s nothing nothing specific here we just\n3:38:07 what we really want to be doing is we want to be setting our Q object assigning that to the class attributes\n3:38:14 and then there''s also this Final Answer scene which we''re setting to fults so\n3:38:19 what we''re going to be using that for is we our llm will be streaming tokens\n3:38:26 towards whilst it''s using its tool calling and we might not want to display those immediately or we remember to\n3:38:32 display them in a different way so by setting this Final Answer scene to\n3:38:37 false whilst our LM is outputting those tool tokens we can handle them in a\n3:38:44 different way and then as soon as we see that it''s done with the tool calls and it''s on to the final answer which is\n3:38:49 actually another tool call but once we see that it''s on to the final answer tool call we can set this true and then\n3:38:56 we can start processing our tokens in a you know different way essentially okay so we have that then we have this AER\n3:39:04 method this is required for any async generator object\n3:39:11 so what that is going to be doing is going to iterating through right it''s a generator it''s going to be going\n3:39:16 iterating through and it''s going saying okay if our queue is empty right this is the que that we set up here if it''s\n3:39:22 empty wait a moment right we use the Sleep Method here and this is an async\n3:39:28 Sleep Method this is super important we''re using we are waiting for an asynchronous sleep all right so whilst\n3:39:35 we''re whilst we''re waiting for that 0.1 seconds our our code can be doing other things\n3:39:42 right that that is important if we if we use I think the standard is time dos sleep that is not asynchronous and so it\n3:39:50 will actually block the thread for that 0. one seconds so we don''t want that to happen generally our Q should probably\n3:39:57 not be empty that frequently given how quickly uh tokens are going to be added to the queue so the only way that this\n3:40:05 would potentially be empty is maybe our LM stops maybe there''s like a connection\n3:40:11 Interruption for it you know a brief second or something and no tokens are added so in that case we don''t actually\n3:40:17 do anything we don''t keep the checking the queue we just wait a moment okay and then we check again now if it was empty\n3:40:26 we wait and then we continue onto the next iteration otherwise it probably won''t be empty we get whatever is from\n3:40:33 our inside our queue we get that out pull it out then we say Okay if that\n3:40:40 token is a done token we''re going to return so we''re going to stop this\n3:40:45 generator right we''re finished otherwise if it''s something else we''re going to yield that token\n3:40:52 which means we''re we''re returning that token but then we''re continuing through that loop again\n3:40:57 right so that is our generator logic then we have some other methods here\n3:41:04 these are L these are line chain specific okay we have on LM new token\n3:41:10 and we have on LM end starting with on LM new token this is basically when an\n3:41:15 LM returns a token to us line chain is going to run or execute this method okay\n3:41:22 this is the method that will be called what this is going to do is it''s going to go into the keyword arguments and\n3:41:28 it''s going to get the chunk object so this is coming Fromm if there is something in that chunk it''s going to\n3:41:36 check for a final answer tool call First okay so we get our tool calls and we say\n3:41:43 if the name within our ch chunk right probably this will be emptying most of\n3:41:48 the tokens we return right so you remember before when we''re looking at the chunks here this is what we''re\n3:41:54 looking at right the content for us is actually always going to be empty and instead we''re actually going to get the\n3:41:59 additional keyword objects here and inside there we''re going to have our tool calling our tool calls as we s in\n3:42:06 the the previous videos right so that''s what we''re extracting we''re extracting that information that''s why we''re going\n3:42:12 additional keyword ARS right and get those tool the tool call information\n3:42:18 right or it will be nonone right so if if it is nonone I don''t think it ever\n3:42:24 would be none to be honest it would be strange if it''s none I think that means something would be wrong okay so here\n3:42:29 we''re using the wars operator so the wars operator what it''s doing here is whil we''re checking the if logic here\n3:42:38 whilst we do that it''s also assigning whatever is inside this it''s assigning\n3:42:43 over to Tool Calles and then with the if we''re checking whether tool cause is\n3:42:49 something or nonone right because we''re using get here so if if this get\n3:42:54 operation fails and there is no tool calls this object here will be equal to\n3:42:59 none which gets assigned to Tool calls here and then this this if none will\n3:43:05 return false and this logic will not run okay and it will just continue if this\n3:43:11 is true so if there is something returned here we''re going to check if that''s something returned is using the\n3:43:16 function name or tool name final answer if it is we''re going to set that final answer see equal to True otherwise we''re\n3:43:24 just going to add our chunk into Q okay we use put no weight here because we''re\n3:43:30 we''re using async otherwise if you were not using async I think you might just put weight or maybe even put put no okay\n3:43:39 you you use put if it''s a synchronous code but I I don''t think I''ve ever implemented a synchronous so it would\n3:43:46 actually just be put no weight for Asing okay and then return so we have\n3:43:52 that then we have on llm end okay so this is when line chain sees that the\n3:43:59 llm has returned or indicated that it is finished with the response line chain\n3:44:06 will call this so you you have to be aware that this\n3:44:12 will happen multiple times during an agent execution because if you think within our agent executor we''re hitting\n3:44:21 the LM multiple times we have that first step where it''s deciding oh I''m going to use the add tool or the multiply tool\n3:44:28 and then that response gets back towards we execute that tool and then we pass\n3:44:33 the output from that tool and all the original user query in the chat history pass that back to our LM again all right\n3:44:39 so that''s another call to our LM that''s going to come back it''s going to finish it''s going to give us something else\n3:44:45 right so there''s multiple llm cods happening throughout our agent execution\n3:44:50 logic so this on LM call will actually get called at the end of every single one of those llm calls now if we get to\n3:44:59 the end of our llm call and it was just a it was a tool invocation so we had the you know it\n3:45:05 called the ad tool we don''t want to put the done token into our Cube because\n3:45:13 when the done token is added to our Cube we''re going to stop iterating\n3:45:19 okay instead if it was just a tool call we''re going to say step end right and\n3:45:24 we''ll actually get this token back so this is useful on for example the front\n3:45:29 end you could have okay I''ve I''ve used the ad tool the these are the parameters\n3:45:35 and it''s the end of the step so you could have that your tool callers being used on some front end and then as soon\n3:45:42 as it sees step end it knows okay we''re done with here was a response right and and it can just show you that and we''re\n3:45:49 going to use that we''ll see that soon but let''s say we get to the final an tool we''re on the final answer tool and\n3:45:55 then we get this signal that the llm has finished then we need to stop iterating\n3:46:02 otherwise our our streaming generator is just going to keep going forever right nothing''s going to stop it or maybe it\n3:46:09 will time out I don''t think it will though so at that point we need to send okay\n3:46:15 stop right we need to say we''re done and then that will that will come back to here to our a iterator and to our asnc\n3:46:23 iterator and it will return and stop the generator okay so that''s the core logic\n3:46:31 that we have inside there I know there''s a lot going on there it''s but we need all of this so it''s important to be\n3:46:38 aware of it okay so now let''s see how we might actually call our agent with all\n3:46:45 of this streaming uh in this way so we''re going to initialize our queue\n3:46:52 we''re going to use that to initialize a streamer okay using the the custom streamer that we just sell custom\n3:46:57 callback Handler whatever you want to call it okay then I''m going to define a function so this is an asynchronous\n3:47:04 function it has to be if if we''re using async and what it''s going to do is it''s going to call our agent with a config\n3:47:12 here and we''re going to pass it that call the the Callback which is the streamer right note here I''m not calling\n3:47:19 the agent executor I''m just calling the agent right so the uh if we come back up\n3:47:24 here we''re calling this all right so that''s not going to include all tool execution logic and\n3:47:30 importantly we''re calling the agent with the config that uses callbacks right so\n3:47:37 this this configurable field here from our LM is actually being fed through it propagates through to our agent object\n3:47:43 as well to the runable so realizable all right so that''s what we''re executing here we see agent with\n3:47:49 config and we''re passing in those callbacks which is just one actually okay so that sets up our agent and then\n3:47:57 we invoke it with a stram okay like we did before and we''re just going to return everything so let''s uh run that\n3:48:05 okay and we see all the token or the chunk objects that are being returned and this is useful to understand what\n3:48:12 we''re actually doing up here right so when we''re doing this chunk\n3:48:17 message additional C keyword arguments right we can see that in here so this would be the chunk message object we get\n3:48:24 the additional keyword objects go into tool calls and we get the information here so we have the ID for that tool\n3:48:30 call as we saw in the previous chapters then we have our function right so the\n3:48:37 function includes the name right so we know what tool we''re calling from this first chunk but we don''t know the\n3:48:43 arguments right those arguments are going to be streamed to us so we can see them begin to come through in the next\n3:48:50 chunk so the next chunk is just it''s just a first token for for the ad\n3:48:55 function right and we can see these all come together over multiple steps and we\n3:49:01 actually get all of our arguments okay that''s pretty cool\n3:49:07 so actually one thing I would like to show you here as well so if we just do token\n3:49:13 equals token sorry and we\n3:49:20 do tokens. pen\n3:49:26 token okay we have all of our tokens in here now right you see that they''re all\n3:49:31 AI message chunks so we can actually add those together right so let''s we''ll go with\n3:49:37 these here and based on these we''re going to get all the arguments okay so this is kind of interesting so it''s one\n3:49:45 until I think like the second to last\n3:49:51 maybe right so we have these and actually we just want to add those\n3:49:56 together so I''m going to go with tokens one I''m just going to go\n3:50:04 four uh four token in we''re going to go from\n3:50:10 the second onwards I''m going to TK plus\n3:50:16 token right and let''s see what TK looks like at the end here\n3:50:23 TK okay so now you see I kind of merged with all those um arguments here sorry\n3:50:30 plus equal okay so run that and you can see here that it''s merged those arguments it\n3:50:36 didn''t get all of them so I kind of missed some at the end there but it''s merging them right so we can see that that logic where it''s you know before it\n3:50:43 was adding the content from various trunks it also does the same for the\n3:50:49 other parameters within your trunk object which is is I I think it''s pretty cool you can see here the name wasn''t\n3:50:55 included that''s because we started on token one or on token zero where the name was so if we actually started from\n3:51:02 token zero and let''s just let''s just pull them in there right so from one\n3:51:10 onwards we''re going to get a complete AI message chunk which includes the name\n3:51:16 here and all of those arguments and you you''ll see also here right populate everything which is pretty\n3:51:23 cool okay so we have that now based on this we''re going to want to modify our\n3:51:29 custom agent executor because we''re streaming everything right so we want to add\n3:51:35 streaming inside our agent executor which we''re doing here right so this is\n3:51:40 async death stream and we''re sharing async for token in the a string okay so\n3:51:47 this is like the very first instance if output is none we''re just going to be adding our token so the the chunk sorry\n3:51:56 to our output like the first token becomes our output otherwise we''re just\n3:52:02 appending our tokens to the output okay if the token content is empty which it\n3:52:08 should be right because we''re using tool cores all the time we''re just going to print content okay I just added these as\n3:52:15 so we see like print everything I just want to want to be able to see that I wouldn''t expect this to run because\n3:52:22 we''re saying it has to use tool calling okay so within our agent if we come up\n3:52:28 to here we said tool Choice any so it''s been forced to use tool calling so it should never really be returning\n3:52:34 anything inside the content field but just in case it''s there right so we''ll we''ll see if that is actually true then\n3:52:40 we''re just getting out our tool CES information okay from our trunk and we''re going to say okay if there''s\n3:52:46 something in there we''re going to print what is in there okay and then we''re going to extract our tool name if there\n3:52:51 is some if there is a tool name I''m going to show you the tool name then we''re going to go to ORS and\n3:52:57 if the ORS are not empty we''re going to see what we get in there okay and then\n3:53:03 from all of this we''re actually going to we merge all of it into our AI message right because we''re merging everything\n3:53:09 as we''re going through we''re merging everything into outputs as I showed you before okay cool and then we just\n3:53:14 awaiting our stream that will like kick it off okay and then we do the the standard agent execut stuff again here\n3:53:21 right so we''re just pulling out tool name Tool logs tool call ID and then we''re using all that to execute our tool\n3:53:27 here and then we''re creating a new tool message and passing that back in and then also here I move the break for The\n3:53:35 Final Answer into the final step so that is our custom Asian executor with streaming and let''s see what let''s see\n3:53:42 what it does okay St for b equal true so we see all those print\n3:53:49 statements okay so you can kind of see it''s a little bit messy but you can see\n3:53:55 we have tool calls that had some stuff inside it had add here and what we''re\n3:54:01 printing out here is we''re printing out the full AI message chunk with tool calls and then I''m just printing out\n3:54:06 okay what are we actually pulling out from from that so these are actually coming from the same thing okay and then\n3:54:12 the same here right so we''re looking at full message and then we''re looking okay we''re getting this argument out from it\n3:54:19 okay so we can see everything that is being pulled out you know chunk by chunk\n3:54:25 or token by to token and that''s it okay so we could just get everything like that however right so I''m I''m printing\n3:54:32 everything so we can see that it''s streaming what if I don''t print okay so we''re setting the bo or by default the\n3:54:39 both is equal to false here so what happens if we invoke now plus\n3:54:47 C okay cool we got\n3:54:54 nothing so the reason we got nothing is because we''re not\n3:54:59 printing but we don''t if you are if you''re building an an API for example\n3:55:06 you''re you''re pulling your tokens through you can''t print them to your\n3:55:14 like like a front end or or print them as to the output of your API printing\n3:55:20 goes to your terminal right your console window it doesn''t go anywhere else instead what we want to do is we\n3:55:28 actually want to get those tokens out right but if but how do we do that all\n3:55:33 right so we we printed them but another place that those tokens are is in our que all right because we set them up to\n3:55:41 go to the que so we can actually pull them out of\n3:55:47 our queue whilst our agent executor is running and then we can do whatever we\n3:55:52 want with them because our code is async so it can be doing multiple things at the same time so whilst our code is\n3:55:58 running the agent executor whilst that is happening our code can also be pulling out from our\n3:56:05 queue tokens that are in there and sending them to like an API for example\n3:56:12 right or whatever Downstream you you have so let''s see what that looks like\n3:56:17 we start by just initializing our que initializing our streamer with that que then we create a task so this is\n3:56:24 basically saying okay I I want to run this but don''t run it right now I''m not ready yet the reason that I say I''m not\n3:56:31 ready yet is because I also want to Define here my async Loop which is going\n3:56:37 to be printing those tokens right but this is async right so we we set this up\n3:56:43 this is like get ready to run this because it is async this is running right this is just running like it there\n3:56:50 it''s already running so we get this we continue we continue this none of this is actually executed\n3:56:57 yet right only here when we await the task that we set up here only then does\n3:57:04 our agent executor run and our async object here\n3:57:11 begin getting tokens right and here again printing but I don''t need to print\n3:57:16 I could I could have like a let''s say where this is within an API or\n3:57:22 something let''s say I''m I''m saying okay send token to\n3:57:29 XYZ token right that''s sending up token somewhere or if we''re maybe we''re yielding this to our some sort streamer\n3:57:37 object within our API right we can do whatever we want with those tokens okay I''m just printing them because I want to\n3:57:44 see them okay but just important here is that we''re not printing them within our\n3:57:50 agent executor we''re printing them outside the agent executor we''ve got them out and we can put them wherever we\n3:57:56 want which is perfect when you''re building an actual sort real world use KS we using an API or something else\n3:58:02 okay so let''s run that let''s see what we get look at that we get all of the\n3:58:07 information we could need and a little bit more right because now we''re using the agent executor and now we can also\n3:58:15 see oh we have this step end right so I know all I I know just from looking at this right this is my first tool use so\n3:58:24 what tool is it let''s have a look it''s the add tool and then we have these arguments I can then pass them right\n3:58:31 Downstream then we have the next tool use which is here down here so we can\n3:58:38 then pass them in the way that we like so that''s pretty cool\n3:58:45 let''s I mean let''s see right so we''re getting those fers out can we can we do\n3:58:50 something with them before I before I print them and show them yes let''s see\n3:58:55 okay so we''re now modifying our our Loop here same stuff right we''re still\n3:59:01 initializing our queue initializing our streamer initializing our task okay and we''re still doing this aing for token\n3:59:08 streamer okay but then we''re doing stuff with our tokens so I''m saying okay if if we''re on\n3:59:15 stream end I''m not actually going to print stream end I''m going to print new line okay otherwise if we''re getting a tool\n3:59:23 call here we''re going to say if that tool call is the tool name I am going to\n3:59:29 Sprint calling tool name okay if it''s the arguments I''m going to print the\n3:59:35 tool argument and I''m going to end up with nothing so that we don''t go to a new line so we''re actually going to be\n3:59:41 streaming everything okay so let''s just see what this looks\n3:59:47 like oh my bad I just added\n3:59:52 that okay you see that so it go it goes very\n3:59:58 fast so it''s kind of hard to see it I''m going to slow it down so you can see so you can see that we as soon as we get\n4:00:04 the toour name we stream that we''re calling the add tool then we stream token by token the actual Arguments for\n4:00:11 that tool then for the next one again we do the same we''re calling this tool name then we''re streaming token by token\n4:00:18 again we''re processing everything Downstream from outside of the agent\n4:00:24 executor and this is an essential thing to be able to do when we''re actually implementing streaming and acing and\n4:00:32 everything else in an actual application so I know that''s a lot but it''s\n4:00:39 important so that is it for our chapter on streaming and Asing\n4:00:44 I hope this all been useful thanks now we''re on to the final Capstone chapter\n4:00:50 we''re going to be taking everything that we''ve learned so far and using it to build a actual chat application now the\n4:00:59 chat application is what you can see right now and we can go into this and ask some pretty interesting questions\n4:01:05 and because it''s an agent because as iess is tools it will be able to answer them for us so we''ll see inside our\n4:01:12 application that we can ask questions s that require tool use such as this and\n4:01:17 because of the streaming that we''ve implemented we can see all this information real time so we can see that sub API tool is being used these are the\n4:01:24 queries we saw all that was in parallel as well so each one of those tools were\n4:01:29 being used in parallel we modified the code a little bit to enable that and we\n4:01:35 see that we have the answer we can also see the structured output being used here so we can see our answer followed\n4:01:41 by the tools used here and then we could ask followup questions as well because this is conversational so we say how is\n4:01:48 the weather in each of those\n4:02:00 cities okay that''s pretty cool so this is what we''re going to be building we\n4:02:05 are of course going to be focusing on the API the back end I''m not front end engineer so I can''t take you through\n4:02:11 that but the code is there so for those of you that do want to go through the front end code you can of course go and\n4:02:17 do that but we''ll be focusing on how we build the API that powers all of this\n4:02:23 using of course everything that we''ve learned so far so let''s jump into it the first thing we going to want to do is\n4:02:29 clone this repo so we''ll copy this URL this is repo orelio Labs line chain\n4:02:36 course and you''ll just clone your repo like so I''ve already done this so I''m\n4:02:42 not going to do it again instead I''ll just navigate to the line chain course repo now there''s a few\n4:02:50 setup things that you do need to do all of those can be found in the read me so\n4:02:56 we just open a new tab here and I''ll open the read me okay so\n4:03:02 this explains everything we need we have if you were running this locally already you will have seen this or you will have\n4:03:09 already done all of this but for those of you that haven''t we go through quickly now so you will need to install\n4:03:17 the UV Library so this is how we manage our pyth environment our packages we use\n4:03:24 UV on Mac you would install it like so if you''re on Windows or Linux just\n4:03:31 double check how you''d install over here once you have installed this you then go\n4:03:37 to install python so UV python install then we want to to create our VM our\n4:03:45 virtual environment using that version of python so the VM\n4:03:52 here then as you can see here we need to activate that virtual environment which\n4:03:57 I did miss from here so let me quickly add that so you just run that for me I''m\n4:04:03 using fish so I just add fish onto the end there but if you''re using bash or zsh I think you can you can just run\n4:04:09 that directly and then finally we need to sync I install all of our packages using UV\n4:04:17 sync and you see that will install everything for you great so we have that\n4:04:24 and we can go ahead and actually open cursor or vs code and then we should\n4:04:31 find ourselves within cursor or vs code so in here you''ll find a few things that\n4:04:39 we will need so first is environment variables so we can come over to here\n4:04:45 and we have open AI API key larning chain API key and ser API API key create\n4:04:50 a copy of this and you would make this your EMV file or if you want to run it\n4:04:57 with Source you can well I like to use mac. EnV when I''m on Mac and I just add\n4:05:03 export onto the start there and then enter my API Keys now I actually already\n4:05:09 have these in this local. EMV file which over in my terminal I''ll just activate with Source\n4:05:17 again like that now we''ll need that when we are running our API and application\n4:05:23 later but for now let''s just focus on understanding what the API actually\n4:05:29 looks like so navigating into the 09 Capstone chapter we''ll find a few things\n4:05:36 what we''re going to focus on is the API here and we have a couple of notebooks that help us just understand okay what\n4:05:44 are we actually doing here so let me give you a quick overview of the API\n4:05:49 first so the API we''re using fast API for this we have a few functions in here\n4:05:54 the one we''ll start with is this okay so this is our post Endo for invoke and\n4:06:01 this essentially sends something to our llm and begins a streaming response so\n4:06:07 we can go ahead and actually start the API and we can just see what this looks like so we''ll go into chapter 09 caps\n4:06:14 there and API after setting our environment variables here and we just want to do UV\n4:06:20 run uicorn main colon app reload we don''t need to reload but if we''re\n4:06:26 modifying the code that can be useful okay and we can see that our API is now running on Local Host Port\n4:06:34 8000 and if we go to our browser we can actually open the dots for our API so we\n4:06:42 go to 8,000 slash dos okay we just see that we have that single invoke method\n4:06:48 it stripes the content and it gives us a small amount of information there now we could try\n4:06:55 out here so if we say say hello we can\n4:07:00 run that and we''ll see that we get a response we get this okay now the thing\n4:07:09 that we''re missing here is that this is actually being streamed back to us okay so this is not a just a direct response\n4:07:16 this is a stream to see that we''re going to navigate over to here to this streaming test notebook and we''ll run\n4:07:25 this so we are using request here we are not just doing a you know the standard\n4:07:32 post request because we want to stream the output and then print the output as we are receiving them okay so that''s why\n4:07:40 this looks a little more complicated than just a typical request. post or request. getet so what we''re doing here\n4:07:48 is we''re starting our session which is our our post request and then we''re just\n4:07:53 iterating through the content as we receive it from that request when we\n4:07:59 receive a token because sometimes this might be non we print that okay and we\n4:08:04 have that flush equals TRS we have used in the past so let''s define that and\n4:08:10 then let''s just ask a simple question what is 5 + 5\n4:08:16 okay and we we saw that that was it was pretty quick so it generated this response first and then it went ahead\n4:08:23 and actually continued streaming with all of this okay and we can see that there\n4:08:29 these special tokens are being provided this is to help the front end basically\n4:08:35 decide okay what should go where so here where we''re showing these multiple steps\n4:08:43 of tool use and the parameters the way the front end is deciding how to display\n4:08:48 those is it''s just it''s being provided the single stream but it has the SE tokens has a SE has se name then it has\n4:08:57 the parameters followed by the sort of ending of the step token and it''s looking at each one of these and then\n4:09:03 the one step name that it treats differently is where it will see The Final Answer step name when it sees the\n4:09:10 final answer step name rather than displaying this tool interface it instead begins streaming the tokens\n4:09:16 directly at like typical chat interface and if we look at what we actually get\n4:09:22 in our final answer it''s not just the answer itself right so we have the answer here this is streamed into that\n4:09:31 typical chat output but then we also have tools used and then this is added\n4:09:36 into the little boxes that we have below the chat here so there''s quite a lot\n4:09:42 going on just within this little stream now we can try with some other questions\n4:09:47 here so we can say okay tell me about the latest news in the world you can see that there''s a little bit of a wait here\n4:09:53 whilst it''s waiting to get the response and then yeah it''s streaming a lot of stuff quite quickly okay so there''s a\n4:10:00 lot coming through here okay and then we can ask other questions like okay this\n4:10:05 one here how called is in Osa right now is five mtip by five right so these two\n4:10:10 are going to be executed in parallel and then it will after it has the answers for those the agent will use the another\n4:10:18 multiply tool to multiply those two values together and all of that will get streamed okay and then as we saw earlier\n4:10:26 we have the what is the current Daye and time in these places same thing so three questions three questions here what is\n4:10:33 the current date and time in Dubai what is the current date and time in Tokyo and what is the current date and time in Berlin those three questions get\n4:10:41 executed in parallel against St I search at all and then all answers get returned\n4:10:47 within that final answer okay so that is how our API is working now let''s dive a\n4:10:55 little bit into the code and understand how it is working so there are a lot of\n4:11:01 important things here there''s some complexity but at the same time we''ve tried to make this as simple as possible\n4:11:07 as well so let''s just fast API syntax here with the app post invoke so our\n4:11:13 invoke endpoint we consume some some content which just a string and then if you remember from the agent execut a\n4:11:21 deep dive which is what we''ve implemented here or a modified version of that we have to initialize our asyn q\n4:11:30 and our streamer which is the Q coreback Handler which I believe is exactly the same as what we defined in that earlier\n4:11:37 chapter there''s no differences there so we Define that and then we return and\n4:11:43 this streaming response object right again this is a fast API thing this is so that you are streaming a response\n4:11:50 that streaming response has a few attributes here which again are fast API\n4:11:55 things or just generic API things so some headers giving instructions to the\n4:12:01 API and then the media type here which is text event stream you can also use I\n4:12:06 think it''s text plane possibly as well but I believe this standard here would\n4:12:11 be to use event screen and then the more important part for us is this token\n4:12:17 generator okay so what is this token generator well it is this function that\n4:12:23 we defined up here now if you again if you remember that earlier chapter at the\n4:12:28 end of the chapter we set up a a for Loop where we were printing out\n4:12:34 different tokens in various formats so we kind of pro postprocessing them before deciding how to display them\n4:12:42 that''s exactly what doing here so in this block here We''re looping through\n4:12:50 every token that we''re receiving from our streamer We''re looping through and we''re just saying okay if this is the\n4:12:57 end of a step we''re going to yield this end of Step token which we we saw here\n4:13:03 okay so it''s this end of end of St token there otherwise if this is a tool call\n4:13:10 so again we''ve got that W operator here so what we''re doing is saying okay get the tool calls out from our current\n4:13:18 message if there is something there so if this is not nonone we''re going to execute what inside here and what is\n4:13:25 being executed inside here is we''re checking for the tool name if we have the tool name we return this okay so we\n4:13:32 have the start step token the start of Step name token the tool name or set\n4:13:39 name whichever those you want to call it and then the end of the set name token\n4:13:45 okay and then this of course comes through to the front end like that okay that''s what we have there\n4:13:52 otherwise we should only be seeing the tool name returned as part of first token for every step after that it\n4:13:59 should just be tool arguments so in this case we say okay if we have those tool\n4:14:05 or function arguments we''re going to just return them directly so then that is the part that would stream all of\n4:14:11 this here okay like these would be individual tokens right for example\n4:14:16 right so we might have the open curly brackets followed by query could be a token latest could be a token world\n4:14:24 could be a token news could be a token Etc okay so that is what is happening there this should not get executed but\n4:14:31 we have a we just handle that just in case so we have any issues with tokens\n4:14:37 being returned there we''re just going to print as error and we''re going to continue with the streaming but that\n4:14:43 should not really be happening cool so that is our like token streaming Loop\n4:14:51 now the way that we are picking up tokens from our stream object here is of\n4:14:56 course through our agent execution logic which is happening in parallel okay so\n4:15:02 all of this is asynchronous we have this async definition here so all of this is happening asynchronously so what has\n4:15:09 happened here is here we have created a task which is the agent ex you to invoke\n4:15:16 and we passing our content we passing that streamer which we''re going to be pulling tokens from and we also set\n4:15:21 Theos to true we can actually remove that but that would just allow us to see\n4:15:27 additional output in our terminal window if we want it I don''t think there''s\n4:15:33 anything particularly interesting to look at in there but particularly if you are debugging that can be useful so we\n4:15:41 create our task here but this does not begin the task right this is a asyn iio\n4:15:47 create task but this does not begin until we await it down here so what is\n4:15:53 happening here is essentially this code here is still being run and like a we''re\n4:16:00 in an asynchronous Loop here but then we await this task as soon as we await this task tokens will start being placed\n4:16:07 within our que which then get picked up by the streamer object here so then this\n4:16:13 begins receiving tokens I know asyn code is always a little bit more confusing\n4:16:21 given the strange order of things but that is essentially what is happening\n4:16:26 you can imagine all this is essentially being executed all at the same time so\n4:16:31 we have that is there anything else to go through here I don''t think so it''s all sort of boiler plates stuff for fast\n4:16:37 API rather than the actual AI code itself so we have that that''s our\n4:16:43 streaming function now let''s have a look at the agent code itself okay so agent code where would\n4:16:50 that be so we''re using this agent executor invoke and we''re importing this\n4:16:56 from the agent file so we can have a look in here for this now you can see\n4:17:02 straight away we''re pulling in our API Keys here just yeah make sure that you\n4:17:07 do have those now all of our C okay this is what we''ve seen before\n4:17:13 in that agent execut to Deep dive chapter this is all practically the same\n4:17:20 so we have our LM we''ve set those configurable fields as we did in the\n4:17:26 earlier chapters that configurable field is for our callbacks we have our prompt this has been modified a little bit so\n4:17:35 essentially just telling it okay make sure you use the tools provided we say You must use the final\n4:17:41 answer tool to provide a final answer to the user and one thing that I added that I notice every now and again so I have explicitly\n4:17:48 said Ed to answer to users current question not pre-used questions so I\n4:17:54 found with this setup it will occasionally if I just have a little bit of small talk with the agent and\n4:18:01 beforehand I was asking questions about okay like what was the weather in this place or that place the agent will kind\n4:18:07 of hang on to those previous questions and try and use a tool again to answer and that is just something that you can\n4:18:13 more or less prompt out of it okay so we have that this is all exactly the same\n4:18:19 as before okay so we have our chat history to make this conversational we have our human message and then our\n4:18:25 agent scratchpad so that agent can think through multiple tool use messages great\n4:18:31 so we also have the article class so this is to process results from Sur\n4:18:39 API we have our Ser API function here I will talk about that a little more in a\n4:18:44 moment because this is also a little bit different to what we covered before what we covered before with C API if you\n4:18:51 remember was synchronous because we''re using the Ser API client directly or the\n4:18:57 Ser API tool directly from line chain and because we want everything to be\n4:19:02 asynchronous we have had to recreate that tool in a asynchronous fashion\n4:19:09 which we''ll talk about a little bit later but for now let''s move on from that we see our final answer being used\n4:19:18 here so this is I think we defined the exact same thing before probably in that\n4:19:23 deep dive chapter again where we have just the answer and the tools that have been used great so we have that one thing\n4:19:31 that is a little different here is when we are defining our name to Tool\n4:19:38 function so this takes a tool name and it Maps it to a tool to function when we\n4:19:45 have synchronous tools we actually use tool Funk here okay so rather than tool\n4:19:52 cartin it would be tool Funk however we are using a synchronous\n4:19:58 tools and so this is actually tool co-routine and this is why this is why\n4:20:04 if you if you come up here I''ve made every single tool asynchronous now that is not really NE\n4:20:12 for a tool like final answer because there is no there''s no API calls happening an API call is a very typical\n4:20:19 scenario where you do want to use async because if you make an API call with a synchronous function your code is just\n4:20:26 going to be waiting for the response from the API while the API is processing\n4:20:32 and doing whatever it''s doing so that is an ideal scenario where you would want to use async because rather\n4:20:39 than your code just waiting for the response from the API it can instead go\n4:20:44 and do something else whilst it''s waiting right so that''s an ideal scenario where you''d use async which is\n4:20:50 why we would use it for example with a Ser API tool here but for final answer\n4:20:55 and for all of these calculator tools that we built there''s actually no need\n4:21:02 to have these as async because our code is just running through its executing\n4:21:07 this code there''s no waiting involved so it doesn''t necessarily make sense have these a synchronous however by making\n4:21:15 them asynchronous it means that I can do tool care routine for all of them rather than saying oh if this tool is\n4:21:22 synchronous use tool. Funk whereas if this one is async use tool. cartin so\n4:21:28 just simplifies the code for us a lot more but yeah not directly necessary but\n4:21:34 it does help us write cleaner code here this is also true later on because we actually have\n4:21:40 to await our tool code which we can see over here right so we\n4:21:47 have to await those tool calls that would get Messier if we were using the\n4:21:53 like some sync tools some async tools so we have that we have our Q callback\n4:21:59 Handler this is again that''s the same as before so I''m not going to go through\n4:22:04 I''m not going to go through that we covered that in the earlier Deep dive chapter we have our execute tool\n4:22:09 function here again that is a synchronous this just helps us you know clean up code a little bit this would I\n4:22:16 think in the Deep dive chapter we had this directly placed within our agent\n4:22:21 executor function and you can do that it''s fine it''s just a bit cleaner to kind of pull this out and we can also\n4:22:28 add more type annotations here which I like so execute tool expects us to\n4:22:33 provide an AI message which includes a tool call within it and it will return\n4:22:38 as a tool message okay agent exor this\n4:22:43 is all the same as before and we''re actually not even using verose here so we could fully remove it but I I will\n4:22:49 leave it of course if you would like to use that you can just add a ifos and then log or print some stuff where you\n4:22:57 need it okay so what do we have in here we have our streaming function so this\n4:23:03 is what actually calls our agent right so we have a query this will\n4:23:10 call our agent just here and we could even make this a little clearer so for example this could be\n4:23:17 configured agent because this is this is not the response this is a configured\n4:23:23 agent so I think this is may be a lot clearer so we are configuring our agent with our callbacks okay which is just\n4:23:30 our streamer then we''re iterating through the tokens are returned by our\n4:23:35 agent using a stream here okay and as we are iterating through this because we\n4:23:42 pass our streamer to the Callback here what that is going to do is every single\n4:23:48 token that our agent returns is going to get processed through our Q callback\n4:23:56 Handler here okay so this on LM new token on LMN these are going to get\n4:24:03 executed and then all of those tokens you can see here I''ll pass to our Q okay\n4:24:09 then we come up here and we have this a it so that this aor method here is used\n4:24:15 by our generator over in our API is used by this token\n4:24:21 generator to pick up from the queue the tokens that have been put in the queue\n4:24:28 by these other methods here okay so it''s putting tokens into the queue and\n4:24:34 pulling them out with this okay so that is just happening in\n4:24:39 parallel as well as this code is running here now the reason that we extract the\n4:24:45 tokens out here is that we want to pull out our tokens and we append them all to\n4:24:50 our outputs now those outputs that becomes a list of AI messages which are\n4:24:57 essentially the AI telling as what tool to use and what parameters to pass to each one of those tools this is very\n4:25:05 similar to what we covered in that deep dive chapter but the one thing that I have modified here is I''ve enabled us to\n4:25:12 use parallel tool calls so that is what we see here with this these four lines\n4:25:20 of code we''re saying okay if our tool call includes an ID that means we have a\n4:25:25 new tool call or a new AI message so what we do is we append that AI message\n4:25:32 which is the AI message chunk to our outputs and then following that if we\n4:25:37 don''t get an ID that means we''re getting the tool arguments so following that we''re just\n4:25:42 adding our AI message chunk to the most recent AI message Chunk from our outputs\n4:25:49 okay so what that will do is it it will create that list of AI messages would be\n4:25:56 like AI message one and then this will just append everything to that AI\n4:26:02 message one then we''ll get our next AI message chunk this will then just append\n4:26:08 everything to that until we get a complete AI message and so on and so on\n4:26:15 okay so what we do here is here we''ve collected all our AI message chunk\n4:26:21 objects then finally what we do is just transform all those AI message chunk objects into actual AI message objects\n4:26:28 and then return them from our function which we then receive over here so into the tool cuse variable okay now this is\n4:26:37 very similar to The Deep dive chapter again we''re going through that that count that Loop where we have a Max\n4:26:44 iterations at which point we will just stop but until then we continue\n4:26:49 iterating through and making more tool calls executing those tool calls and so on so what what is going on here let''s\n4:26:57 see so we got our tool calls there''s going to be a list of AI message objects then what we do with those AI\n4:27:04 message objects is we pass them to this ex cuute tool function if you remember what is that that is this function here\n4:27:13 so we pass each AI message individually to this function and that will execute\n4:27:19 the tool force and then return us that observation from the tool okay so that is what you see\n4:27:28 happening here but this is an async method so typically what you''d have to\n4:27:33 do is you''d have to do await X you tool and we could do that so we could do a\n4:27:39 okay let me let me make this a little bigger for us okay and so what we could do for example\n4:27:45 which might be a bit clearer is you could do tool OBS equals an empty list\n4:27:51 and what you can do is you can say for Tool call oops in tool calls the\n4:27:58 tool observation is we''re going to append execute tool call which would\n4:28:04 have to be in a weit so we'' actually put your weight in there and what this would do is actually the exact same thing as\n4:28:10 what we''re doing here the difference being that we''re doing this tool by Tool\n4:28:16 okay so we are we''re executing async here but we''re doing them sequentially\n4:28:23 whereas what we can do which is better is we can use asyn I gather so what this\n4:28:28 does is gathers all those Co routines and then we await them all at the same\n4:28:33 time to run them all asynchronously they all begin at the same time or almost\n4:28:38 exactly at the same time and we get those responses kind of in parallel but of course it''s\n4:28:44 saying so it''s not fully in parallel but practically in parallel cool so we have\n4:28:51 that and then that okay we get all of our tool observations from that so that''s all of our tool messages and then\n4:28:57 one interesting thing here is if we let''s say we have all of our AI messages\n4:29:04 of all of our tool cores and we just append all of those to our agent scratch Pad right so let''s say here we''re just\n4:29:11 like oh okay scratch Pad extend and then we would just have\n4:29:17 okay we'' have our tool calls and then we do agent stretch PCT\n4:29:23 send tool OBS all right so what what is happening here is this would essentially\n4:29:28 give us something that looks like this so we have our AI message say I''m just\n4:29:37 going to put okay we''ll just put tool call IDs in here to simplify a little bit\n4:29:42 this would be tool call ID a then we would have ai message tool call ID B\n4:29:50 then we''d have tool message let''s just remove this content\n4:29:55 field I don''t want that and Tool message tool call ID B right so it would look\n4:30:02 something like this so the the order is the tool message is not following the AI\n4:30:07 message which you would think okay we have this tool qual ID that''s probably fine actually when we''re running this if\n4:30:14 you add these to your agent scratch pad in this order what you''ll see is your response\n4:30:20 just hangs like nothing nothing happens when you come through to your second uh\n4:30:25 iteration of your agent call so actually what you need to do is these need to be\n4:30:30 sorted so that they are actually in order and it doesn''t actually doesn''t\n4:30:36 necessarily matter which order in terms of like a or b or c or whatever you use so you could have this order we have ai\n4:30:42 message tool message AI message tool message just as long as you have your tool call IDs are both together or you\n4:30:49 could know invert this for example right so you could have this right and that that will work\n4:30:55 as well it''s essentially just as long as you have your AI message followed by your tool message and both of those are\n4:31:02 sharing that tool call ID you need to make sure you have that order okay so\n4:31:08 that of course would not happen if we do this and instead what we need to do is\n4:31:14 something like this okay so I made this a lot easier to read okay so we''re\n4:31:20 taking the tool call ID we are pointing it to the tool observation and we''re\n4:31:26 doing that for every tool call and to Observation within like a zip of those\n4:31:31 okay then what we''re saying is for each tool call within our tool calls we are\n4:31:37 extending our agent scratch pad with that tool call followed by by the tool\n4:31:43 observation message which is the tool message so this would be our this is the AI message and that is\n4:31:50 the tool messages down there okay so that is what it''s happening and that is how we get this correct order which will\n4:31:58 run otherwise things will not run so that''s important to be aware of okay now\n4:32:05 we''re we''re almost done I know there''s we just been through quite a lot so we continue we incre increment our count as\n4:32:11 we were doing before then we need to check for the final answer tool okay and because we''re running these tools in parallel okay\n4:32:18 because we''re allowing multiple tool calls in one step we can''t just look at the most recent tool and look if it is\n4:32:25 it has the name Final Answer instead we need to iterate through all of our tool calls and check if any of them have the\n4:32:30 name final answer if they do we say okay we extract that final answer call we\n4:32:35 extract the final answer as well so this is the direct text content and we say\n4:32:40 okay we have found found the final answer so this will we set to True okay which should happen every time but let''s\n4:32:47 say if our agent gets stuck in a loop of calling multiple tools this might not\n4:32:53 happen before we break based on the max iterations here so we might end up\n4:32:59 breaking based on Max iterations rather than we found a final answer okay so\n4:33:04 that can happen so anyway if we find that final answer we break out of this\n4:33:09 for Loop here and then of course we do need to break out of our wow Loop which is here so we say if we found the final\n4:33:16 answer break okay cool so we have that\n4:33:22 finally after all of that so this is our you know we''ve executed our tool our\n4:33:27 agent steps and iterations has process we''ve been through those finally we come\n4:33:34 down to here where we say okay we''re going to add that final output to our chat history so this is just going to be\n4:33:41 the text content right so this here get direct answer but then what we do is we\n4:33:49 return the full final answer call the full final answer call is basically this here right so this answer and tools used\n4:33:57 but of course populated so we''re saying here that if we have a final answer okay\n4:34:03 if we have that we''re going to return the final answer call which was generated by our llm otherwise we''re\n4:34:09 going to return this one so this is in the scenario that maybe the agent got caught in a loop and just kept iterating\n4:34:16 if that happens we''ll say it will come back with okay no answer found and it will just return okay we didn''t use any\n4:34:23 tools which is not technically true but it''s this is like a exception handling\n4:34:29 event so it ideally it shouldn''t happen but it''s not really a big deal if we''re\n4:34:35 saying okay there were no tools use in my opinion anyway cool so we have all of\n4:34:41 that and yeah we just we initialize our agent executor and then I mean that that is\n4:34:48 our agent execution code the one last thing we want to go through is the Ser API tool which we will do in a moment\n4:34:56 okay so Ser API let''s see what let''s see\n4:35:01 how we build our Ser API tool okay so we''ll start with the synchronous Ser API\n4:35:10 now the reason we''re starting with this is that it''s actually it''s just a bit simpler so I''ll show you this quickly\n4:35:16 before we move on to the async implementation which is what we''re using within our app so we want to get our set\n4:35:23 API API key so I''ll run that and we just enter it at the top\n4:35:29 there and this will R so we''re going to use the sub API SDK first we''re\n4:35:36 importing Google search and these are the input prameters so we have our API key we''re using we say want use Google\n4:35:42 we our question is so query so Q for query we''re searching for the latest\n4:35:47 news in the world it will return quite a lot of stuff you can see there''s a ton of stuff in there right\n4:35:56 now what we want is contained within this organic results key so we can run\n4:36:02 that and we''ll see K is talking about you various things pretty recent stuff\n4:36:09 at the moment so we can tell okay that is that is in fact working now this is\n4:36:14 quite messy so what I would like to do first is just clean that up a little bit so we Define this article base model\n4:36:21 which is pantic and we''re saying okay from a set of results okay so we''re\n4:36:27 going to iterate through each of these we''re going to extract the title source\n4:36:32 link and the snippet so you can see title source link and snippet here\n4:36:42 okay so that''s all usedful we''ll run that and what we do is we go through\n4:36:48 each of the results in organic results and we just load them into our article using this class method here and then we\n4:36:55 can see okay let''s have a look at what those look like it''s much nicer okay we\n4:37:02 get this nicely formatted object here cool that''s great now all of this what\n4:37:10 we just did here so this is using sub apis SDK which is great super easy to\n4:37:15 use the problem is that they don''t offer a async SDK which is a shame but it''s\n4:37:22 not that hard for us to set up ourselves so typically with a synchronous requests\n4:37:29 what we can use is the aiio HTTP Library it''s well it''s you can see what we''re\n4:37:35 doing here so this is equivalent to requests Dot get okay that''s essentially\n4:37:43 what we''re doing here and the equivalent is literally this okay so this is the\n4:37:50 equivalent using requests that we are running here but we''re using asyn Code\n4:37:55 so we''re using AI Hep client session and then session. getet okay with this async\n4:38:03 width here and then we just await our response so this is all yeah this is what we do rather than this to make our\n4:38:11 code async so it''s really simple and then the output that we get is exactly the same\n4:38:16 right so we still get this exact same output so that means of course that we can use that articles method like this\n4:38:25 in the exact same way and we get we get the same result there''s no need to make this article from sub API result asnc\n4:38:34 because again like this this bit of code here is fully local it''s just our python\n4:38:39 running everything so this does not need to be async okay and we can see that we\n4:38:45 get literally the exact same result there so with that we have everything\n4:38:51 that we would need to build a fully asynchronous Sur API tool which is exactly what we do here for Lang chain\n4:38:58 so we import those tools and I mean there''s nothing is there anything different here no this is exactly what\n4:39:04 we we just said but I will run this because I would like to show you very quickly this okay so this is how we were\n4:39:12 initially calling our Tools in previous chapters because we we were okay mostly\n4:39:18 with using the the synchronous tools however you can see that the funk here\n4:39:26 is just empty right so if I do type just a non-type that is\n4:39:31 because well this is an async function okay it''s an async tool sorry so it was\n4:39:40 defined with async here what happens when you do that is you get\n4:39:45 this Co routine object so rather than Funk which is it isn''t here you get that\n4:39:52 cartine if we then modified this which would be kind of okay let''s just remove\n4:39:58 all the ayns here and the await if we modify that like so and then\n4:40:06 we look at the set API structure tool we go across we see that we now get that\n4:40:12 funk okay so that is that is just the difference between an async structured\n4:40:17 tool versus a sync structured tool we of course on\n4:40:22 async okay now we have K again so important to be aware of that and of\n4:40:29 course we we run using the sub API care\n4:40:34 routine so that is that''s how we build the sub API tool\n4:40:41 uh there''s nothing I mean that is exactly what we did here so I don''t need to I don''t think we need to go through that any further so yeah I think that is\n4:40:50 basically all of our code behind this API with all of that we can then go\n4:40:55 ahead so we have our API running already let''s go ahead and actually run also our\n4:41:02 front end so we''re going to go to documents orelo line chain course and\n4:41:07 then we want to go to Chapters 09 Capstone app and you will need to have npm\n4:41:14 installed so to do that what do we do we can take a look at this answer for example this is probably what I would\n4:41:21 recommend okay so I would run Brew install node followed by Brew install mpm if you''re on Mac of course it''s\n4:41:28 different if you''re on Linux or Windows once you have those you can do npm install and this will just install all\n4:41:34 of the oop sorry mpm install and this would just install all of the node\n4:41:40 packages that we need and then we can just run npm run Dev okay and now we\n4:41:48 have our app running on locost 3000 so we can come over to here open\n4:41:53 that up and we have our application can ignore this so in here we can begin just\n4:42:00 asking questions okay so we can start with quick question what is 5 +\n4:42:07 5 and you see so we have our streaming happening here it said the agent wants to use ad tool and these are the input\n4:42:14 parameters to the ad tool and then we get the streamed response so this is the\n4:42:20 final answer tool where we''re outputting that answer key and value and then here we''re outputting that tools used key and\n4:42:27 value which is just an array of the tools being used which just functions add so we have that then let''s ask\n4:42:35 another question this time we''ll trigger Ser API with tell me about the latest news in the world\n4:42:42 okay so we can see that''s using C API and a query is latest world\n4:42:48 news and then it comes down here and we actually get some citations here which is kind of cool so you can also come\n4:42:55 through to here okay and it teses through to here so that''s pretty cool unfortunately I\n4:43:01 just lost my chat so fine let me I can ask that\n4:43:07 question again\n4:43:16 okay we can see that to us set API there now let''s continue with the next question from our notebook which is how\n4:43:23 cold is in I like right now what is five M by five what do you get when multiplying those two numbers together\n4:43:30 I''m just going to modify that to say in Celsius so that I can understand thank\n4:43:36 you okay so for this one we can see what did we get so we got current temperature\n4:43:41 in ow we got multiply 5 by five which our second question and then we also got\n4:43:49 subtract interesting that I I don''t know why it did that it''s kind of weird so it\n4:43:54 it decided to use oh ah okay so this is\n4:44:00 okay so then here it was okay that kind of makes sense does\n4:44:05 that make sense roughly okay so I think the the conversion for Fahrenheits Celsius is\n4:44:11 say like subtract 32 okay yes so to go from Fahrenheit to\n4:44:18 Celsius you are doing basically Fahrenheit minus 32 and then you''re\n4:44:23 multiplying by this number here which the iume the AI did\n4:44:28 not oh it roughly did okay so subtracting 36 like 32 would have given us four and it gave us approximately two\n4:44:36 so if you think okay multiply by this it''s practically multiplying by 0.5 five\n4:44:41 so halfing the value and that would give us roughly 2 so that''s what this was\n4:44:47 doing here kind of interesting Okay cool so we''ve gone through we have seen how\n4:44:54 to build a fully fledged chat application using what\n4:44:59 we''ve learned throughout a course and we''ve built quite a lot if you think about this application you''re getting\n4:45:07 the real time updates on what tools are being used the parameters being input to those tools and then that is all being\n4:45:13 returned in a streamed output and even in a structured output for your final\n4:45:18 answer including the answer and the tools that we use so of course you know what we built here is fairly limited but\n4:45:26 it''s super easy to extend this like you could maybe something that you might want to go and do is take what we''ve\n4:45:33 built here like Fork this application and just go and add different tools to it and see what happens because this is\n4:45:40 very extensible you can do a lot with it but yeah that is the end of the course\n4:45:46 of course this is just the beginning of whatever it is you''re wanting to learn\n4:45:52 or build with AI treat this as the beginning and just go out and find all\n4:45:58 the other cool interesting stuff that you can go and build so I hope this course has been useful\n4:46:06 informative and gives you an advantage in whatever it is is you''re going out of this build so thank you very much for\n4:46:13 watching and taking the course and sticking through right to the end I know it''s pretty long so I appreciate it a\n4:46:21 lot and I hope you get a lot out of it thanks bye\n0:00 welcome to the AI Engineers guide for the L chain this is a four course that\n0:05 will take you from the assumption that you know nothing about Lang chain to\n0:11 being able to proficiently use the framework either you know within line\n0:17 chain within line graph or even elsewhere uh from the fundamentals that\n0:23 you will learn in this course now this course will be broken up into multiple\n0:28 chapters we''re going to start by talking a little bit about what line chain is and when we should really be using it\n0:35 and when maybe we don''t want to use it we''ll talk about the pros and cons and also about the the why the line chain\n0:41 ecosystem not just about the line chain framework itself from there we''ll\n0:46 introduce Lang chain we''ll just have a look at a few examples before diving into essentially the basics of the\n0:53 framework now I will just note that all this is for Lang chain 0.3 so that is latest current version\n1:01 although that being said we will cover a little bit of where line chain comes from as well so we''ll be looking at pre\n1:09 0.3 uh version methods for doing things so that we can understand okay that''s\n1:15 the old way doing things how do we do it now now that we''re in version 0.3 and also how do we dive a little deeper into\n1:22 those methods as well and kind of customize those from there we''ll be diving into what I believe is the S of\n1:30 future of AI I mean it''s it''s it''s a now and the short term potentially even\n1:36 further into the future and that is Agents we''ll be spending a lot of time on agents so we''ll be starting with a\n1:43 simple introduction to agents so that is how can we build an agent that''s simple\n1:51 what are the main components of Agents what do they look like and then we''ll be diving much deeper into them and we''ll\n1:57 be building out our own Agent X computer which kind of like the framework around\n2:03 the AI components of an agent we''re building our own and once we''ve done our\n2:08 Deep dive on agents we''ll be diving into Lang chain expression language which\n2:14 we''ll be using throughout this course so line chain expression language is the recommended way of using line chain and\n2:21 the expression language or L cell takes kind like a break from standard python\n2:27 syntax so there''s a bit of weirdness in there and yes we''ll be using it throughout the course but we''re leaving\n2:35 the ELO chapter until this you know kind of later on in the course because we really want to dive into the\n2:41 fundamentals of Elsa by that point but the idea is that by this point you already have a good grasp of at least\n2:47 how to use the basics of lell before we really dig in that point then we''ll be\n2:53 digging in streaming which is an essential ux feature of AI applications\n2:58 in general streaming it can just improve the user experience massively and it''s not just about\n3:05 streaming tokens you know that that interface where you have word by word the AI is generating text on the screen\n3:12 streaming is more than just that it is also the ability if you''ve seen the\n3:17 interface of perplexity where as the agent is thinking you''re getting an update of what the agent is thinking\n3:24 about what tools it is using and how it is using those tools that''s also another essential feature that we need to have a\n3:31 good understanding of streaming to build so we''ll also be taking a look at all of that then we''ll finally we''ll be topping\n3:38 it off with a Capstone project where we will be building our own AI agent\n3:45 application that is going to incorporate all of these features we''re going to have an agent that can use tools web\n3:51 search we''ll be using streaming and we''ll see all of this in you know a nice\n3:57 interface that we can that we can work with so that''s an overview the course of course it''s very high level what I''ve\n4:03 just gone through there''s a ton of stuff in here and truly this course can take you from you know wherever you are with\n4:09 L chain at the moment whether you''re a beginner or you''ve used it a bit or even inter mediate and you''re probably going\n4:16 to learn a fair bit from it so without any further Ado let''s dive into the\n4:22 first chapter okay so the first chapter of the course we''re going to focus on\n4:28 when should we actually use Lang chain and when should we use something else now through this chapter we''re not\n4:35 really going to focus too much on the code we you know every other chapter is very code focused but this one is a\n4:42 little more just theoretical what is line chain where''s fit in when should I use it when should I not so I want to\n4:48 just start by Framing this line chain is one of if not the most popular open\n4:56 source framework within the python ecosystem at least for AI it works\n5:01 pretty well for a lot of things and also works terribly for a lot of things as well to be completely honest there are\n5:07 massive Pros massive cons to using Lang chain here we''re just going to discuss a few of those and see how Lang chain\n5:14 maybe Compares a little bit against other Frameworks so the very first\n5:19 question we should be asking ourselves is do we even need a framework a is a\n5:24 framework actually needed when we can just hit an API you have the open AI API\n5:30 other apis mral so on and we can get a response from an llm in five lines of\n5:36 code on average for those is incredibly incredibly simple however that can\n5:42 change very quickly when we start talking about agents or retrieval augmented generation research assistance\n5:50 all this sort of stuff those use cases those methods can\n5:56 suddenly get quite complicated when we outside of Frameworks and that''s not necessarily a\n6:03 bad thing right it can be incredibly useful to be able to uh just understand\n6:10 everything that is going on and build it yourself but the problem is that to do\n6:16 that you need time like you need to learn all the intricacies of building these things the intricacies of these\n6:22 methods and themselves like what you know how do they even work and that kind of runs in the opposite direction of\n6:28 what we see with AI at the moment which is AI is being integrated into the world at an incredibly fast rate and because\n6:37 of this most Engineers coming into the space are not from a machine learning or\n6:43 AI background most people don''t necessarily have any experience with these systems a lot of Engineers coming\n6:50 in that could be devops Engineers generic backend python Engineers even you front end Engineers coming in and\n6:57 building all these things which is is great but they don''t necessarily have the experience and that you know that\n7:02 might be you as well and that''s not a bad thing because the idea is that obviously you''re going to learn and\n7:07 you''re going to pick up a lot of these things and in this scenario there''s quite a good argument for using the\n7:14 framework because a framework means that you can get started faster and a\n7:19 framework like Lang chain it abstracts away a lot of stuff and that''s a that''s\n7:24 a big complaint that a lot of people will have with L chain but that abstract\n7:29 in away of many things is also what made sing chain popular because it means that\n7:34 you can come in not really knowing okay what you know rag is for example and you can Implement a rag pipeline get the\n7:41 benefits of it without really needing to understand it and yes there''s an argument against that as well just\n7:47 implementing something without really understanding it but as we''ll see throughout the course it is possible to\n7:54 work with line chain in a way as we will in this course where you kind of\n8:00 implement these things in an abstract way and then break them apart and start understanding the intricacies at least a\n8:07 little bit so that can actually be pretty good\n8:12 however again circling back to what we said at the start if the idea or your\n8:18 application is just a very simple you know you need to generate some text based on some basic input maybe you\n8:24 should just use an API that''s completely valid as well now we just said okay okay\n8:30 a lot of people coming to L chain might not be from an AI background so another question for a lot\n8:36 of these Engineers might be okay if I want to learn about you know rag agents\n8:41 all these things should I skip line chain and just try and build it from scratch myself well line chain can help\n8:49 a lot with that Learning Journey so you can start very abstract and as you gradually begin to\n8:57 understand the framework better you can strip away more and more of those abstractions and get more into the\n9:03 details and in my opinion this gradual shift towards more explicit code with\n9:11 less abstraction is a really nice feature and it''s also what we focus on\n9:17 right throughout this course that''s what we''re going to be doing going sing abstract stripping away the abstractions\n9:22 and getting more explicit with what we''re building so for example building an agent in L chain there''s in very\n9:30 simple and Incredibly abstract crate tools agent method that we can use and\n9:37 like it creates a tool agent for you it''s it doesn''t tell you anything so you can you can use that right and we will\n9:46 use that initially in the course but then you can actually go from that to\n9:51 defining your full agent execution logic which is basically a tools call to open\n9:58 AI you going to be getting that tool information back but then You'' got to figure out okay how am I going to\n10:03 execute that how am I going to Sol this information and then how am I going to iterate through this so we''re going to\n10:08 be seeing that stripping way abstractions as we work through as we build agents as we do as we bu like our\n10:16 streaming use case among many other things even chat memory we''ll see there as well so line chain can act as the\n10:24 onramp to your AI learning experience then what you might find and I do think\n10:31 this is quite true for most people is that if you if you''re really serious\n10:36 about AI engineering and that''s what you want to do like that''s your focus right which isn''t for everyone for certain a\n10:43 lot of people just want to understand a bit of AI and they want to continue doing what they''re doing and just integrate AI here and there and maybe\n10:50 those you know if that''s your focus you might sick with ly chain you know there''s not necessarily a reason to move\n10:55 on but in the other scenario where you''re thinking okay I want to get\n11:00 really good at this I want to just learn as much as I can and I''m going to\n11:06 dedicate basically my you know my short-term future of my career on becoming AI\n11:13 engineer then line chain might be the on-ramp it might be your initial learning curve but then after you''ve\n11:20 become competent with line chain you might actually find that you want to move on to other Frameworks and that\n11:25 doesn''t necessarily mean that you''re going to have wasted your time with L chain because one L chain is a thing helping\n11:32 you learn and two one of the main Frameworks that I recommend a lot of people to move on to is actually line\n11:38 graph which is still within the L chain ecosystem and it still uses a lot of L chain objects and\n11:45 methods and of course Concepts as well so even if you do move on from line\n11:50 chain you may move on to something like L graph which you can no line chain for\n11:56 anyway and let''s say you do move on to another framework in set said in that scenario the concepts that you learn\n12:02 from Lang chain are still pretty important so to just finish up this chapter I just want to summarize on that\n12:09 question of should you be using Lang chain what''s important to remember is that Lang chain does abstract a lot now\n12:16 this abstraction of L chain is both a strength and a weakness with more\n12:23 experience those abstractions can feel like a limitation and\n12:29 that is why we sort of go with the idea that L chain is really good to get\n12:35 started with but as a project grows in complexity or the engineers get more experience they might move on something\n12:41 like Lang graph which in any case is going to be using Lang chain to some degree so in either one of those\n12:48 scenarios L chain is going to be a core tool in an AI engineered toolkit so\n12:56 it''s worth learning in our opinion but of course it comes with its you know it comes with its weaknesses and it''s just\n13:03 good to be aware of that it''s not a perfect framework but for the most part you will learn a lot from it and you\n13:09 will be able to build a lot with it so with all of that we''ll move on to our\n13:16 first of Hands-On chapter with Lang chain where we''ll just introduce Lang\n13:21 chain some of the essential Concepts I''m not going to Dag too much into the syntax but we''re just going to understand a little bit of what we can\n13:27 do with it okay so moving on to our next next chapter getting started with a line chain in this chapter we''re going to be\n13:33 introducing a line Chain by building a simple LM powered assistant that will do\n13:39 various things for us it will multimodal generating some text generating images\n13:45 generate some stret shed outputs it will do a few things now to get started we will go over to the course repo all of\n13:53 the code all the chapters are in here there are two ways of running this either locally or in Google collab we\n14:00 would recommend running in Google collab because it''s just a lot simpler with environments but you can also run it\n14:06 locally and actually for the cap Zone we will be running it locally there''s no\n14:12 way of us doing that in collab so if you would like to run everything locally\n14:17 I''ll show you how quickly now if you would like to run in collab which I would recommend at least for the the\n14:23 first notebook chapters just skip ahead there will be chapter points\n14:29 in the timeline of the video so for running running it locally we just come\n14:34 down to here so this actually tells you everything that you need so you will\n14:40 need to install uvie all right so this is the package manager that we recommended by the python and package\n14:47 management Library you don''t need to use uvie it''s it''s up to you uvie is is very\n14:53 simple it works really well so I would recommend that so you would install it with this command here this is on Mac so\n15:01 it will be different otherwise if you are on Windows or otherwise you can uh\n15:07 look at the installation guide there and it''ll tell you what to do and so before we actually do this what I will do is go\n15:13 ahead and just clone this REO so we''ll come into here I''m going to\n15:19 create like a temp directory for me because I already have the line chain course in there and what I''m going to do\n15:26 is just get loan line chain course okay so you will also need to install git if\n15:31 you don''t have that okay so we have that then what we''ll do is copy this okay so\n15:38 this will install python 3.2.7 for us with this command then this will create\n15:45 a new VM within that or using python 3.2.7 that we''ve installed and then UV\n15:53 sync we actually be looking at the Pi Project at TL file that''s like the uh\n15:59 the package installation for the repo and using that to install everything\n16:04 that we need now we should actually make sure that we are within the line chain course directory and then yes we can run\n16:11 those three and there we go so everything should install with that now if you are\n16:20 in cursor you can just do cursor dot or we can run code do if mvs code I''ll just\n16:28 be running this this and then I''ve opened up the course now within that course you have your notebooks and then\n16:35 you just run through these making sure you select your kernel pth environment and making sure you''re using the correct\n16:41 VN from here so that should pop up already as this VM bin Python and you''ll\n16:47 click that and then you can run through when you are running locally don''t run these you don''t need to you''ve already\n16:54 installed everything so you don''t this specifically is for collab so that is\n16:59 running things locally now let''s have a look at running things in collab so for\n17:06 running everything in collab we have our notebooks in here we click through and then we have each of the chapters\n17:12 through here so starting with the first chapter the introduction which is where we are\n17:18 now so what you can do to open this in collab is either just click this collab\n17:24 button here or if you really want to for example Maybe this it is not loading for\n17:31 you what you can do is you can copy the URL at the top here you can go over to\n17:36 collab you can go to open GitHub and then just paste that in\n17:43 there and press enter and there we go we have our\n17:48 notebook okay so we''re in now uh what we will do first is just install the\n17:53 prerequisites so we have line chain just a load of line chain packages here line\n17:59 chain core line chain open a because we''re using open Ai and line chain Community which is needed for running\n18:06 what we''re running okay so that has installed everything for us so we can\n18:12 move on to our first step which is initializing our LM so we''re going to be\n18:19 using GT40 mini which is side of small but fast but also cheaper model uh that\n18:26 is also very good for open AI so what we need to do here is get an API key okay\n18:33 so for getting that API key we''re going to go to open''s website and you can see\n18:39 here that we''re opening platform. open.com and then we''re going to go into settings organization API\n18:45 keys so you can copy that I''ll just click it from here okay so I''m going to\n18:50 go ahead and create a new secret key actually just in case you''re kind of\n18:56 looking for where this is It''s settings organization API Keys again okay create\n19:01 a new API key I''m going to call it Line train\n19:06 course I''ll just put it on the semantic router that''s just my organization you you put it wherever you want it to be\n19:14 and then you would copy your API key you can see mine here I''m obviously going to\n19:19 reval that before you see this but you can try and use it if you really like so I''m going to copy that and I''m going to\n19:25 place it into this little box here you could also just and place it put your uh\n19:31 full API key in here it''s up to you but this little box just makes things easier\n19:36 now that what we''ve basically done there is just passed in our API key we''re setting our opening model GT40 mini and\n19:45 what we''re going to be doing now is essentially just connecting and setting up our llm parameters with L chain so we\n19:53 run that we say okay we''re using a GT4 mini and we''re also setting ourselves up\n19:59 to use two different LMS here or two of the same LM with slightly different\n20:05 settings so the first of those is an LM with a temperature setting of zero the\n20:10 temperature setting basically controls almost the randomness of the output of\n20:17 your llm and the way that it works is when an LM is predicting the sort of\n20:24 next token or next word in sequence know provide a probability actually for all\n20:29 of the tokens within the lm''s knowledge base or what the LM has been trained on\n20:35 so what we do when we set temperature of zero is we say you are going to give us\n20:41 the token with highest probability according to you okay whereas when we\n20:47 set a temperature of 0.9 what we''re saying is okay there''s actually an increased probability of you giving us a\n20:55 token that according to your generated output is not the token with the highest\n21:01 probability according to the lmm but what that tends to do is give us more sort of creative outputs so that''s what\n21:08 the temperature does so we are creating a normal llm and then a more creative\n21:13 llm with this so what are we going to be building we''re going to be taking a\n21:19 article draft so like a draft article uh from the aelio learning page and we''re\n21:27 going to be using line chain to generate various sces that we might um find helpful as we''re you know we have this\n21:34 article draft and we''re editing it and just kind of like finalizing it so what are those going to be you can see them\n21:40 here we have the title for the article the description and SEO friendly\n21:45 description specifically third one we''re going to be getting the LM to Providers\n21:50 advice on existing paragraph and essentially writing a new paragraph for us from the existing paragraph and what\n21:57 it''s going to do this is the structured output part is going to write a new version of that paragraph for us and\n22:03 it''s going to give us advice on where we can improve our writing then we''re going to generate a thumbnail hero image for\n22:10 our article so nice image that you would put at the top so here we''re just going\n22:16 to input our article you can you can put something else in here if you like essentially this is just a big article\n22:24 that''s written a little while back on agents and and now we can go ahead and\n22:29 start preparing out our prompts which are essentially the instructions for our llm so line chain comes with a lot of\n22:38 different uh like utilities for prompts and we''re going to dive into them in a lot more detail but I do want to just\n22:44 give you uh the Essentials now just so you can understand what we''re looking at at least conceptually so prompts for\n22:51 chat agents are at a minimum broken up into three components those are the\n22:57 system pront this provides instructions to our LM on how it should behave what its objective is and how it should go\n23:03 about achieving that objective generally system prompts are going to be a bit\n23:08 longer than what we have here depending on the use case then we have our user prompts so these are user written\n23:15 messages usually sometimes we might want to pre-populate those if we want to encourage a particular type of um\n23:22 conversational patterns from our agent but for the most part yes these are going to be using generated then we have\n23:29 our AI promps so these are of course AI generated and again in some cases we\n23:36 might want to generate those ourselves beforehand or within a conversation if we have a particular reason for doing so\n23:43 but for the most part you can assume that these are actually user and AI generated now the line chain provides us\n23:51 with templates for each one of these prompt types let''s go ahead and have a\n23:57 look at what these look like within line chain so to begin we are looking at this\n24:03 one so we have our system message prom template and human message which the the\n24:09 user that we saw before so we have these two system prom keeping it quite simple here you are AI system that helps\n24:16 generate article titles right so so our first component where we want to generate is article title so we''re\n24:22 telling the AI that''s what we want it to do and then here right so here we''re\n24:29 actually providing kind of like a template for a user input\n24:36 so yes as I mentioned user input can be\n24:41 um it can be fully generated by user it might be kind of not generated by user\n24:47 it might be setting up a conversation beforehand which a user would later use or in this scenario we''re actually\n24:54 creating a template and the what the user will providers will actually just be inserted\n25:00 here inside article and that''s why we have this import variables so what this\n25:07 is going to do is okay we have all of these instructions around here they''re all going to be provided to openai as if\n25:14 it is the user saying this but it will actually just be this here that user\n25:20 will be providing okay and we might want to also format this a little nicer it kind of depends this will work as it is\n25:27 but we can also put you know something like this to make it a little bit clearer to the llm okay what is the\n25:34 article where the prompts so we have that and you can see in this scenario\n25:41 there''s not that much difference between what the system prompt and user prom is doing and this is It''s a particular scenario it varies when you get into the\n25:49 more conversational stuff as we will do later uh you''ll see that the user prompt\n25:54 is generally more fully user generated or mostly user generated and much of\n26:01 these types of instructions we might actually be putting into the system prompt it varies and we''ll see\n26:07 throughout of course many different ways of using these different types of PRS in various different\n26:13 places then you''ll see here so I just want to show you how this is working we\n26:19 can use this format method on our user prompt here to actually insert something\n26:25 within the uh article input here so we''re going to go us prompt format and\n26:31 then we pass in something for article okay and we can also maybe format this a little nicer but I''ll just show you this\n26:37 for now so we have our human message and then inside the content this is the the text that we had right you can see that\n26:43 we have all this right and this is what we wrote before we wrote all this except from this part we didn''t write this\n26:50 instead of this we had article right so let''s format this a little nicer so we\n26:57 can see okay so this is exactly what we wrote up here exactly the same except\n27:02 from now we have test string instead of article so later when we insert our\n27:07 article it''s going to go inside there allly doing it''s like it''s an it''s an F string in Python okay and this is again\n27:15 this is one of those things where people might complain about Lang chain you know this sort of thing can be you it seems\n27:21 excessive because you could just do this with an nring but there are as we''ll see later particularly when you''re streaming\n27:27 just really helpful features that come with using line chains kind of built-in\n27:34 uh prompt templates or at least uh message objects that we''ll see so\n27:41 you we need to uh keep that in mind again as soon as you get more complicated line chain can be a bit more\n27:47 useful so chat prom template uh this is basically just going to take what we\n27:53 have here our system promt user prompts you could also include some AI prompts in there and what it''s going to do is\n27:59 merge both of those and then when we do format what it''s going to do is put both\n28:06 of those together into a chat history okay so let''s see what that looks like first uh in a more messy way okay so you\n28:15 can see we have just the content right so it doesn''t include the whole you know\n28:21 before we had human message we''re not include we''re not seeing anything like that here instead we''re just seeing the\n28:26 string so now let''s switch back to print and we can see that what we have\n28:33 is our system message here it''s just prefixed with this system and then we have human and it''s prefixed by human\n28:39 and then it continues right so that''s that''s all it''s doing it''s just kind of merging those in some sort of chat lug we could also put in like AI messages\n28:46 and they would appear in there as well okay so we have that now that is our\n28:51 prompt template let''s put that together with an LM to create what would be in\n28:57 past line ch be called an llm chain uh now we wouldn''t necessarily call it an llm chain because we''re not using the\n29:03 llm chain abstraction it''s not super important if that doesn''t make sense we we''ll go into it in more detail later\n29:10 particularly in the in the ELO chapter so what this chain will do you\n29:17 think L chain is just chains where''re chaining together these multiple components it will perform the STS\n29:24 prompt formatting so that''s what I just showed you LM generation so sending our\n29:31 prom to open AI getting a response and getting that output so you can also add\n29:37 another set here if you want to format that in a particular way we''re going to be outputting that in a particular\n29:42 format so that we can feed it into the next set more easily but there are also things called output passes which pass\n29:49 your output in a more dynamic or complicated way depending on what you''re doing so this is our first look at Elsa\n29:58 don''t want us to focus too much on the syntax here because we will be doing that later but I do want you to just\n30:04 understand what is actually happening here and logically what are we writing\n30:11 so all we really need to know right now is we Define our inputs with the first\n30:17 dictionary segment here right so this is a you know our inputs which we have\n30:23 defined already okay so if we come up to our\n30:29 user prompt here we said the input variable is our article right and we might have also added input variables to\n30:34 the system prompt here as well in that case you know let''s say we had your AI assistant\n30:42 called name right that helps generate article\n30:47 titles in this scenario we might have an input variables name here right and then\n30:55 what we would have to do down here is we would also have to pass that\n31:01 in right so also we would have article but we would also have name so basically\n31:09 we just need to make sure that in here we''re including the variables that we have Define as input variables for our\n31:16 our first prompts okay so we can actually go ahead and let''s add that uh so we can see it''s in action so we''ll\n31:23 run this again and just include that or or reinitialize is our first prompt so\n31:29 we see that and if we just have a look at what that means for this format\n31:35 function here it means we''ll also need to pass in a name okay and call it Joe\n31:40 okay so Joe the AI right so you are an AI system called Joe now okay so we have\n31:46 Joe our AI that is going to be fed in through these input variables then we have this pipe operator the pipe\n31:53 operator is basically saying whatever is to the left of the pipe operator which\n31:58 in this case would be this is going to go into whatever is on the right of the pipe operator it''s that''s simple again\n32:05 we''ll we''ll dive into this and kind of break it apart in the Elso chapter but for now that''s all we need to know so\n32:11 this is going to go into our first prompt that is going to format everything it''s going to add the name\n32:17 and the article that we provided into our first prompt then it''s going to Output that right going to Output that\n32:23 we have our P operate here so the output of this is going to go into the input of our Next Step it''s our creative\n32:30 LM then that is going to generate some tokens it''s going to generate our output\n32:36 that output is going to be an AI message and as you saw before if I take this bit\n32:44 out within those message objects we have this content field okay so we are\n32:50 actually going to extract the content field out from our AI message to just\n32:56 get the content and that is what we do here so we get the AI message out from ilm and then we''re extracting the\n33:02 content from that AI message object and we''re going to passing it into a dictionary that just contains article\n33:07 title like so okay we don''t need to do that we can just get the AI message directly I just want to show you how we\n33:15 are using this sort of chain in Elso so once we have set up our chain we then\n33:23 call it or execute it using the invoke method into that we will need to pass in\n33:28 those variables so we have our article already but we also gave our AI a name now so let''s add that and we''ll ruin\n33:36 this okay so Joe has generated us a article title unlocking the future the\n33:43 rise of neuros symbolic AI agents cool much better name than what I gave the article which was AI agents are neuros\n33:52 symbolic systems no I don''t think I did too bad okay so we have that\n33:58 now let''s continue and what we''re going to be doing is building more of these\n34:03 types of LM chain pipelines where we''re feeding in some prompts we''re generating\n34:10 something getting something and and doing something with it so as mentioned we have the title we''re\n34:16 now moving on to the description so to generate description so we have our human message prompt template so this is\n34:22 actually going to go into a similar format as before we also\n34:28 want to redefine this because I think I''m using the same system message there\n34:33 so let''s let''s go ahead and do modify that or what we could also do is let''s\n34:40 just remove the name now because I''ve showing that so what we could do is\n34:46 you''re an AI system that helps build good articles right build good\n34:53 articles and we could just use this as our you know generic system prompt now\n34:58 so let''s say that''s our new system prompt now we have our user prompt your task creating description for the\n35:04 article the article is here fure examine article here is the article title okay so we need the article title now as well\n35:10 in our input variables and then we''re going to Output an AO friendly article description and we''re just saying you\n35:17 just to be certain here do not output anything other than the description so you know sometimes an LM might say hey\n35:23 look this is what I''ve generated for you the reason I think this is good is because so on and so on so on right if you''re programmatically taking some\n35:31 output from an LM you don''t want all of that fluff around what the LM is generated you just want exactly what\n35:37 you''ve asked it for okay because otherwise you need to pass out with code and it can get messy and also just far\n35:44 less reliable so we''re just saying do iput anything else then we''re putting all these together so system prompt and\n35:50 the second user prompt this one here putting those together into a new chat\n35:55 prompt template and then we''re going to to feed all that in to another LOL chain\n36:00 as we have here to well to generate our our description so let''s go ahead we\n36:06 invoke that as before we''re just make sure we add in the article title that we got from before and let''s see what we\n36:13 get okay so we have this explore the transformative potential of neuros symbolic Ai ageny and a little bit long\n36:21 to be honest but yeah you can see what it''s doing here right and of course we could then go in we see this is kind of\n36:26 too long right a yeah SEO friendly description not not really so we can\n36:33 modify this output the SEO friendly description um make sure we don''t exceed\n36:41 let me put that on a new line make sure we don''t exceed say 200 characters or\n36:46 maybe it''s even less to se I don''t I don''t have a clue I''ll just say 120 characters I do not outly anything other\n36:53 than the description right so we could just you know go back modify our prompting see what that generates again\n36:59 okay so much shorter probably too short now but that''s fine cool so we have that we have a summary process that and\n37:06 that''s now you know in this dictionary form that we have here cool now the\n37:12 third step we want to consume that first article variable with our full article\n37:18 and we''re going to generate a few different output Fields so for this\n37:24 we''re going to be using the structured output feature so let''s scroll down\n37:29 we''ll see what that is what that looks like so structured output is essentially\n37:36 we''re forcing their lmic like it has to Output a dictionary with these you know\n37:41 particular Fields okay and we can modify this quite a bit but in this scenario\n37:47 what I want to do is I want there to be an original paragraph right so I just want it to regenerate the original\n37:53 paragraph cuz I''m lazy and I don''t want to extract it out then I want to get the new edited paragraph This is the LM\n38:00 generated improved paragraph and then we want to get some feedback because we we don''t want to just automate ourselves we\n38:07 want to augment ourselves and get better with AI rather than just being like I\n38:13 you do you do this so that''s what we do here and you can see that here we''re using this pantic object and what pantic\n38:21 allows us to do is Define these particular fields and it also allows us to assign these descriptions to a field\n38:27 and and line chain is actually going to go ahead read all of this right even reads so for example we could put\n38:33 integer here and we could actually get a numeric score for our paragraph right we\n38:39 can try that right so let''s uh let''s let''s just try that quickly I''ll show you so numeric numeric score in fact\n38:48 let''s even just ignore let''s not put anything here so I''m going to put constructive feedback on the original\n38:53 paragraph but I just put into here so let''s see what happens okay so we have that and what I''m going to do is I''m\n38:59 going to get our creative llm I''m going to use this with structured output method and that''s actually going to\n39:05 modify that llm class create a new llm class that forces that llm to use this\n39:10 structure for the output right so passing in paragraph into here using this we''re creating this new structure\n39:17 LM so let''s run that and see what happens okay so we''re going to modify\n39:23 our chain accordingly maybe what I can do let''s also just remove this bit for now\n39:30 so we can just see what the strictured llm outputs directly and let''s\n39:36 see okay so now you can see that we actually have that paragraph object\n39:42 right the one we defined up here which is kind of cool and then in there we have the original paragraph right so\n39:48 this is where this is coming from I definitely remember writing something\n39:53 that looks a lot like that so I think that is correct we have the edited par so this is okay what thinks it''s better\n40:00 and then interestingly the feedback is three which is weird right because uh\n40:06 here we said the constructive feedback on the original paragraph but what we''re doing when we use this with structured\n40:12 output but what Lang chain is doing is is essentially performing a tool core to open Ai and what a tool core can do is\n40:20 force a particular structure in the output of an LM so when we say feedback\n40:26 has to be an integer no matter what we put here it''s going to give us an integer because how do you provide\n40:31 constructive feedback with an integer it doesn''t really make sense but because we''ve set that limitation that\n40:38 restriction here that is what it does it just gives us the uh a numeric value so\n40:44 I''m going to shift that to string and then let''s rerun this see what we get okay we should now see that we actually\n40:50 do get constructive feedback all right so yeah you can see it''s quite quite long so the original paragraph\n40:57 effectively communicates the limitations of neuro AI systems in performing certain tests however it could benefit\n41:03 from slightly improved Clarity and conciseness for example the phrase was becoming clear can be made more direct\n41:09 by changing it to became evident yeah true thank you very much so yeah now we\n41:17 actually get that that feedback which is pretty nice now let''s add in this final\n41:22 setep to our chain okay and it''s just going to pull out our paragraph object here and\n41:29 extracting into a dictionary we don''t necessarily need to do this honestly I actually kind of prefer it within this paragraph object but just so we can see\n41:38 how we would pass things on the other side of the chain okay so now we can see\n41:44 we''ve extracted that out cool so we have all of that interesting feedback again\n41:52 but let''s leave it there for the text part of this now let''s have a look at at\n41:57 the sort of multimodal features that we can work with so this is you know maybe one of those things that kind of seems a\n42:04 bit more abstracted a little bit complicated where it maybe could be improved but you know we''re not going to\n42:10 really be focusing too much on the M time modal stuff sub be focusing on language but I did want to just show you\n42:16 very quickly so we want this article to look better okay we want to\n42:22 generate a prompt based on the article it''s self that we can then pass to DAR\n42:30 the the image generation model from open AI that will then generate an image like like a thumbnail image for us okay so\n42:39 the first step of that is we''re actually going to get an LM to generate that right so we have our prompt that we''re\n42:44 going to use for that so I''m say generate a prompt with less than 500 characters to uh generate an image based\n42:52 on the following article okay so that''s our prompt yeah super simple uh using\n42:57 the generic prompt template here you can use that you can use user uh prompt template it''s up to you this is just\n43:03 like the generic prom template then what we''re going to be doing is based on what\n43:10 this outputs we''re then going to feed that in to this generate and display image function via the image prompt\n43:17 parameter that is going to use the darly API rapper from line chain it''s going to\n43:23 run that image prompt and we''re going to get a a eurl out from that essentially and then we''re going to read that using\n43:29 SK image here right so we''re just going to read that image URL going to get the image data and then we''re just going to\n43:35 display it okay so pretty straightforward now again this is a lell\n43:43 thing here that we''re doing we have this runable Lambda thing when we''re running\n43:49 functions within lell we need to wrap them within this runable Lambda I you\n43:54 know I don''t want to go too much into what this is doing here because we do cover in the L cell chapter but it''s\n44:01 just you know all you really need to know is we have a custom function wrap in runable Lambda and then what we get\n44:07 from that we can use within this here right the the L Sal syntax so what are\n44:14 we doing here let''s figure this out we are taking our original that image prom that we defined just up here right input\n44:21 variable to that is article okay we have our article d being input here feeding\n44:28 that into our prompt from there we get our message that we then feed into our\n44:33 llm from the LM it''s going to generate us a like an image prompt like a prompt\n44:39 for generating our image for this article we can even Let''s uh let''s print that out so that we can see what it\n44:46 generates because I''m also kind of curious okay so we''ll just run that and\n44:52 then let''s see it will feed in that content into our room reable which is\n44:58 basically this function here and we''ll see what it generates okay don''t expect\n45:03 anything amazing from darly it''s not it''s not the best to be honest but we at\n45:08 least we see how to use it okay so we can see the prom that was used here\n45:14 create an image that visually represents the concept of neuros symbolic agents depict a futuristic interface where\n45:20 large D interacts with traditional code symbolizing integration of oh my gosh uh\n45:27 something computation include elements like a brain to represent neur networks gears or circuits or symbolic logic and\n45:35 web of connections illustrating vast use cases of AI agents oh my gosh look at\n45:41 all that big prompt then we get this so you know dar''s interesting I would say we\n45:48 could even take this let''s just see what that comes up with in something like mid\n45:53 Journey you can see these way cooler images that we get from just another image generation model far better but\n46:00 pretty cool honestly so in terms of Generation image the phrasing the The\n46:05 Prompt itself is actually pretty good the image you know could be better but\n46:11 that''s it right so with all of that we''ve seen a little introduction to what\n46:16 we might build in with lighing chain so that''s it for our introduction chapter as I mentioned we don''t want to go too\n46:22 much into what each of these things is doing just really want to focus on okay\n46:29 this is kind of how we''re building something with line chain this is the overall flow uh but we don''t really want\n46:37 to be focusing too much on okay what exactly lell is doing or what exactly uh\n46:42 you know this prompt thing is that we''re setting up we''re going to be focusing\n46:48 much more on all of those things and much more in the upcoming chapters so\n46:53 for now we''ve just seen a little bit of what we can build before diving in in more detail okay so now we''re going to\n47:00 take a look at AI observability using lsmith now lsmith is another piece of\n47:08 the broader Lang chain ecosystem its focus is on allowing us to see what our\n47:15 llms agents Etc are actually doing and it''s something that we would definitely recommend using if you are going to be\n47:22 using line chain Lang graph now let''s take a look at how we would set L Smith up which is incredibly simple so I''m\n47:29 going to open this in collab and I''m just going to install the prerequisites here you''ll see these are all the same\n47:36 as before but we now have the Lin Smith Library here as well now we are going to be using Lin Smith throughout the course\n47:43 so in all the following chapters we''re going to be importing limith and that will be tracking everything we''re doing\n47:49 but you don''t need Lin Smith to go through the course it''s an an optional dependency but as mentioned I would\n47:55 recommend it so we''ll come down to here and first thing that we will need is the line chain API key now we do need an API\n48:03 key but that does come with a reasonable free tier so we can see here they have\n48:09 each of the plans and this is the one that we are by default on so it''s free\n48:16 for one user up to 5,000 traces per month if you''re building out an\n48:21 application I think it''s fairly easy to go beyond that but it really depends on what you''re building so it''s a good\n48:28 place to start with and then of course you can upgrade as required so we would go to smith. L\n48:36 chain.com and you can see here that this will log me in automatically I have all\n48:41 of these tracing projects these are all from me running the various chapters of the course yours if you do use l Smith\n48:48 throughout course your L Smith dashboard will end up looking something like this now what we need is an API key so we go\n48:56 over to settings we have API keys and we''re just going to create an API key because we''re\n49:03 just going through some personal learning right now I would go with personal access token we can give a name or description if you want okay and\n49:10 we''ll just copy that and then we come over to our notebook and we enter our API key there and that is all we\n49:16 actually need to do that''s absolutely everything supposed the one thing to be aware of is that you should set your L\n49:22 chain project to whatever project you''re working within so of course Within within the course we have individual\n49:28 project names for each chapter but for your own projects of course you should make sure this is something that you\n49:34 recognize and is useful to you so L Smith actually does a lot without needing to do anything so we can\n49:40 actually go through let''s just initialize our LM and start invoking it and seeing what L Smith returns to us so\n49:48 we''ll need our open API key enter it here and then let''s just invoke\n49:55 hello okay so nothing has changed on this end right so us running the code there''s nothing different here however\n50:02 now if we go to Lang Smith I''m going to go back to my dashboard okay and you can\n50:08 see that the the order of these projects just changed a little bit and that''s because the most recently used project I\n50:15 this one at the top Lang chain course Lang Smith openai which is the current chapter we''re in that was just triggered\n50:21 so I can go into here and I can see oh look at this so we actually have something in the Lang Smith UI and we\n50:28 didn''t all we did was enter our L train apid that''s all we did and we set some environment variables and that''s it so\n50:34 we can actually click through to this and it will give us more information so you can see what was the\n50:39 input what was the output and some other metadata here you see you know there''s not that much in\n50:46 here however when we do the same for agents we''ll get a lot more information\n50:52 so I can even show you a quick example from the future chapters if we come through to agents\n51:00 intro here for example and we just take a look at one of\n51:05 these okay so we have this input and output but then on the left here we get all this information and the reason we\n51:12 get all this information because agents are they''re performing multiple LM calls etc etc so there''s a lot more going on\n51:20 so we can see okay what was the first LM call and then we get these tool use traces we get another LM another rmm\n51:27 call another tool use and another LM call so you can see all this information which is incredibly useful and\n51:33 Incredibly easy to do because all I did when setting this up in that agent chapter was simply set the API key and\n51:40 the environment variables as we have done just now so you get a lot out of\n51:46 very little effort with Lang Smith which is great so let''s return to our Lang Smith project here and let''s invoke some\n51:53 more now I''ve already shown you you know we''re going to see a lot of things just by default but we can also add other\n52:00 things that Lang Smith wouldn''t typically Trace so to do that we will\n52:05 just import a traceable decorator from Lang Smith and then let''s make these\n52:12 just random functions traceable within limith okay so we''ll run those we have\n52:19 three here so we''re going to generate a random number we''re going to modify how\n52:24 long a function takes and also generate a random number and then in this one\n52:30 we''re going to either return this no error or we''re going to raise an error\n52:36 so we''re going to see how limith handles these different scenarios so let''s just\n52:41 iterate through and run those a few times so we''re just going to run each one of those 10\n52:47 times okay so let''s see what happens so they''re running let''s go over to our Lin\n52:54 sth UI and see what is happening over here so we can see that everything is updating we adding that information\n52:59 through and we can see if we go into a couple of these we can see a little more information so have the input and the\n53:05 output took three seconds see random error here in this\n53:12 scenario random error passed without any issues let me just refresh the page\n53:18 quickly okay so now we have the rest of that information and we can see that occasionally if there is an error from\n53:24 our random error function it is is signified with this and we can see the\n53:30 traceback as well that was returned there which is useful okay so we can see if an error has been raised we have to\n53:35 see what that error is we can see the various latencies of these functions so\n53:42 you can see that varying throughout here we see all the inputs to each one\n53:48 of our functions and then of course the outputs so we can see a lot in there\n53:53 which is pretty good now another thing that we can do do is we can actually filter so if we come to here we can add\n54:01 a filter let''s filter for errors that would be value error and\n54:06 then we just get all of the cases where one of our functions has returned or\n54:12 raise an error or value error specifically okay so that''s useful and then yeah there''s there''s various other\n54:19 filters that we can add there so we could add a name for example if we want\n54:24 to look for the generate string delay function only we could also do\n54:30 that okay and then we can see the varying latencies of that function as well cool so we have\n54:37 that now one final thing that we might want to do is maybe we want to make\n54:43 those function names a bit more descriptive or easy to search for for example and we can do that by saying the\n54:50 name of the traceable decorator like so so let''s run that we''ll run this a few\n54:56 times and then let''s jump over to limith again going to limith project okay and you can\n55:02 see those coming through as well so then we could also search for those based on that new name so what was it chitchat\n55:09 maker like so and then we can see all that information being streamed through\n55:16 to limith so that is our introduction to limith there is really not all that much\n55:23 to go through here it''s very easy to sell up and as we scen it gives us a lot of observability into what we are building\n55:31 and we will be using this throughout the course we don''t rely on it too much it''s a completely optional dependency so you\n55:37 don''t want to use l space you don''t need to but it''s there and I would recommend doing so so that''s it for this chapter\n55:43 we''ll move on to the next one now we''re going to move on to the chapter on\n55:48 prompts in Lang chain now prompts they seem like a simple concept and they are\n55:53 a simple concept but there''s actually quite a lot to them when you start diving into them and they truly have\n56:00 been a very fundamental part of what has propelled us forwards from pre llm times\n56:07 to the current llm times you have to think until llms became widespread the\n56:14 way to fine-tune a AI model or ml model\n56:19 back then was to get loads of data for your particular use case spend a load of\n56:26 training your specific Transformer or part of the Transformer to essentially\n56:31 adapt it for that particular task that could take a long time depending on the\n56:38 the task it could take you you know months or in some times if it was a\n56:43 simpler task it might take probably days potentially weeks now the interesting\n56:48 thing with L LMS is that rather than needing to go through this whole\n56:54 fine-tuning process to to modify a model for one task over\n57:00 another task rather than doing that we just prompt it differently we literally tell the model hey I want you to do this\n57:07 in this particular way and that is a you know that''s a paradigm shift in what you''re doing it''s so much faster it''s\n57:14 going to take you you know a couple of minutes rather than days weeks or months and LMS are incredibly powerful when it\n57:22 comes to just generalizing to you know across these many different tasks so prompts which control those instructions\n57:31 are a fundamental part of that now line chain naturally has many functionalities\n57:37 around prompts and we can build very Dynamic prompting pipelines that modify\n57:43 the structure and content of what we''re actually feeding into our llm depending on different variables different inputs\n57:49 and we''ll see that in this chapter so we''re going to work through prompting\n57:54 within the scope of of a rag example so let''s start by just dissecting the\n58:01 various parts of a prompt that we might expect to see for a use case like rag so\n58:07 our typical prompt for rag or retrieval augmented generation will include rules\n58:14 for the LM and this is this you will see in most prompts if not all this part of\n58:22 the promt sets up the behavior of the llm that is how it should be responding\n58:28 to user queries what sort of Personality it should be taking on what it should be\n58:33 focusing on when it is responding any particular rules or boundaries that we want to set and really what we''re trying\n58:41 to do here is just to Simply provide as much information as possible to the llm\n58:48 about well what we''re doing we just want to give the llm context as to the the\n58:56 place that it finds itself in because an LM has no idea where it is it''s just it''s a it takes in some information and\n59:04 spits out information if the only information it receives is from the user you know user query it has you know\n59:10 doesn''t know the context what is the application that it is within what is its objective what is its aim what are\n59:17 the boundaries all of this we need to just assume the llm has absolutely no\n59:24 idea about because it it truly does not so as much context as we can provide but\n59:32 it''s important that we don''t overdo it it''s uh we see this all the time people will over prompt an llm you want to be\n59:39 concise you don''t want fluff and in general every single part of your prompt\n59:45 the more concise and less fluffy you can make it the better now those rules or\n59:50 instructions are typically in the system prompt of your llm now the second one is\n59:55 context which is rag specific the context refers to some sort of external\n1:00:00 information that you are feeding into your llm we may have received this information from like web search\n1:00:08 database query or quite often in this case of rag it''s a vector database this\n1:00:15 external information that we provide is essentially the r retrieval augmentation\n1:00:22 of rag we are augmenting the knowledge of our llm which the the knowledge of\n1:00:29 our LM is contained within the llm model weights we''re augmenting that knowledge with some external knowledge that''s what\n1:00:36 we''re doing here now for chat LMS this context is typically placed within a\n1:00:44 conversational context within the user or assistant messages uh and with\n1:00:52 more recent models it can also be placed within uh tool messages as well then we\n1:00:58 have the question this pretty straightforward this is the query from the user this is or is this usually a\n1:01:06 user message of course there might be some additional formatting around this you might add a\n1:01:12 little bit of extra context or you might add some additional instructions if you\n1:01:17 find that you L them sometimes VAR off the rules that you''ve set within the system prompt you might you know append\n1:01:24 or prefix something something here but for the most part it''s probably just going to be the user''s input and finally\n1:01:30 uh so these are all the inputs for our prompt here is going to be the output\n1:01:36 that we get so the answer from the assistant again I mean that''s not even specific to rag it''s just what you would\n1:01:43 expect in a in a chat llm or any LM and of course that would be an assistant\n1:01:49 message so putting all of that together in an actual prompt you can see everything we have here so we have the\n1:01:55 uh rules for our prompt here the instructions we''re just saying okay answer the question based on the context\n1:02:01 below if you cannot answer the question using the information answer with I don''t know then we have some context\n1:02:08 here okay in this scenario that context that we''re feeding in here because it''s\n1:02:14 the first message we might putting that into the system prompt but that may also be turned around okay if you if you for\n1:02:21 example have an agent you might have your question up here before the context\n1:02:27 and then that would be coming from a user message and then this context would follow the question and be recognized as\n1:02:34 a tool message it would be fed in that way as well kind of depends on on what\n1:02:40 sort of structure you''re going for there but you can do either you can feed it into the system message if it''s less\n1:02:45 conversational whereas if it''s more conversational you might feed it in as a tool message okay and then we have a\n1:02:51 user query which is here and then we''d have the AI answer okay and obviously\n1:02:56 that would be generated here okay so let''s switch across to the code we''re in\n1:03:01 the L chain course repo notebooks 03 prompts and I''m just going to open this in collab okay let scroll down and we''ll\n1:03:09 start just by installing the prerequisites okay so we just have the various libraries again as I mentioned\n1:03:16 before Lang Smith is optional you don''t need to install it but if you would like to see your tracers and everything in\n1:03:22 Lang Smith then I would recommend doing that and if you are using L Smith you will need to enter your API key here\n1:03:28 again if you''re not using Lang Smith you don''t need to enter anything here you just skip that cell okay cool and let''s\n1:03:36 jump into the basic prompting then so we''re going to start with this prompt\n1:03:42 answer used query based on the question below so we''re just structuring what we just saw uh in\n1:03:47 code and we''re going to be using the chat problem template because generally\n1:03:53 speaking we''re using chat llms in most most cases nowadays so we have our chat\n1:04:00 promp template and that is going to contain a list of messages system message to begin with which is just\n1:04:06 going to contain this and we''re feeding in the the context within that there and\n1:04:12 we have our user query here okay so we''ll run this and if we take a look\n1:04:21 uh here we haven''t specified what our input variables are okay but we can see\n1:04:28 that we have query and we have context up here right so we can see that okay\n1:04:33 these are the input variables we just haven''t explicitly defined them here so\n1:04:39 let''s just confirm with this that line chain did pick those up and we can see that it did\n1:04:45 so it has context and query as our input variables for the prompt template that\n1:04:50 we just defined okay so we can also see the structure of our temp plates let''s\n1:04:57 have a look okay so we can see that within messages here we have a system message\n1:05:03 prompt template the way that we Define this you can see here that we have from messages and this will consume various\n1:05:10 uh different structures so you can see here that it has a from messages it is a\n1:05:18 sequence of message like representation so we could pass in a system prompt\n1:05:24 template object and then a user prompt template object or we can just use a\n1:05:31 tupol like this and this actually defines okay this system this is a user and you could also do assistant or tool\n1:05:39 messages and stuff here as well using the same structure and then we can look in here\n1:05:44 and of course that is being translated into the system message prom template and human message prompt template okay\n1:05:53 we have our input variables in there and there and we have the template too okay\n1:05:59 now let''s uh continue we''ll see here what I what I just said so we''re\n1:06:05 importing our system message prompt template and human message prompt template and you can see we''re using the\n1:06:10 same from messages method here right and you can see it''s so sequence of message\n1:06:16 like representation it''s just you know what that actually means it can vary right so\n1:06:22 here we have system message prom template from template here from template query you know there''s various\n1:06:28 ways that you might want to do this it just depends on how explicit you want to\n1:06:33 be generally speaking I think for myself I would prefer that we\n1:06:40 stick with the objects themselves and be explicit but it is definitely a little\n1:06:45 harder to pass when you''re when you''re reading this so I understand why you might also prefer this it''s definitely\n1:06:52 cleaner and it is it does look simpler so it just depends I suppose on\n1:07:00 preference okay so we can see again that this is exactly the same okay with chat\n1:07:06 prom template and it contains this and this okay you probably want to see the\n1:07:12 exact output so it was messages okay exactly the same as what I\n1:07:19 output before cool so we have all that let''s see how we would invoke our l L with\n1:07:26 these we''re going to be using 40 mini again we do need our open API key so\n1:07:32 enter that and we''ll just initialize our LM we\n1:07:38 are going with a low temperature here so less Randomness or less\n1:07:43 creativity and you in in many cases this is actually what I would be doing the\n1:07:48 reason in this scenario that we''re going with a low temperature is we''re doing\n1:07:54 Rag and if you you remember before if we scroll up a little bit here our template says answer the user''s query based on\n1:08:00 the context below if you cannot answer the question using the provided answer information answer with I don''t know\n1:08:08 right so just from reading that we know that we want our llm to be as truthful\n1:08:15 and accurate as possible so a more creative llm is going to struggle with\n1:08:20 that and is more likely to hallucinate whereas a low creativity or\n1:08:26 low temperature llm will probably stick with the rules a little better so again it depends on your use case you know if\n1:08:33 you''re creative writing you might want to go with a higher temperature there but for things like rag where the\n1:08:40 information being output should be accurate and truthful it''s important I\n1:08:46 think that we keep temperature low okay I talk about that a little bit here so\n1:08:52 um of course lower temperature of zero makes the LM output more deterministic which in theory should lead to less\n1:08:59 hallucination okay so we''re going to go with L cell again here this is for those\n1:09:04 of you that use LINE chain pass this is equivalent to an llm chain object so our\n1:09:09 prompt template is being fed into our LM okay and from now we have this pipeline\n1:09:18 now let''s see how we would use that pipeline so going to get some uh create\n1:09:23 some context here so so this just some Contex around orelio\n1:09:30 AI mention that we built sematic routers SM junkers there AI\n1:09:37 platform and development services we mentioned I think we\n1:09:42 specifically outline this later on in the example so the Align chain experts little piece of information now most LMS\n1:09:49 would have not been trained on the recent internet so the fact that this\n1:09:54 came in September M 2024 is relatively recent so a lot of LMS out of the box\n1:10:00 you wouldn''t expect them to know that so that is a good little bit of information\n1:10:05 to ask about so we invoke we have our query so what do we do and we have that\n1:10:11 context okay so we''re feeding that into that pipeline that we defined here all right so when we invoke that that is\n1:10:18 automatically going to take query and context and actually feed it into our prompt template okay\n1:10:26 if we want to we can also be a little more explicit so you you will probably\n1:10:32 see me doing this uh throughout the course because I do like to be explicit\n1:10:37 with everything to be honest and you''ll probably see me doing\n1:10:47 this okay and this is doing the same thing or you''ll see it will in a moment\n1:10:54 this is doing the exact same thing again this is just a outo thing\n1:10:59 so all I''m doing in this scenario is I''m saying okay take from the dictionary\n1:11:08 query and then also take from that input dictionary the context\n1:11:16 key okay so this is doing the exact same thing uh the reason that we might want\n1:11:22 to write this is mainly for clarity to be honest just to explicit say okay\n1:11:27 these are the inputs because otherwise we don''t really have them in the code other than within our original prompts\n1:11:34 up here which is not super clear so I think it''s usually a good\n1:11:40 idea to just be more expc with these things and of course if you decide you''re going to modify things a little\n1:11:45 bit let''s say you modify this to input down the line you can still feed in the same input here you''re just you know\n1:11:52 mapping it between different Keys essentially or if you would like to just modify that I don''t know you need to\n1:11:58 locase it on the way in or something you can do so you have that I''ll just redefine\n1:12:08 actually and we''ll invoke again okay we see that it does the exact\n1:12:14 same thing okay so R AI so this is the AI message just generated by the llm\n1:12:21 okay expertise in building AI agents several open source framework router AI\n1:12:29 platform okay right so they have everything there other than the line\n1:12:34 train experts thing it didn''t mention that but we will yeah we''ll test it later on that okay so on to Future\n1:12:41 prompting this is a specific prompting technique now many sort of State ofthe art or also to LMS are very good at\n1:12:49 instruction following so you''ll find that fuch shop prompting is less common\n1:12:54 now than it used to be at least for the sort of bigger more safy art models but\n1:13:01 when you start using smaller models not really what we can use here but let''s say you''re using a open source model\n1:13:08 like llama 3 or llama 2 which is much smaller you will probably need to\n1:13:15 consider things like f shot prompting although that being said with the open AI models you''re at least the current\n1:13:23 open AI models this is not so important nonetheless it can be useful so the idea\n1:13:29 behind fuchia prompting is that you are providing a few examples to your llm of\n1:13:35 how it should behave before you are actually going\n1:13:41 into the main part of the conversation so let''s see how that would look so we\n1:13:46 create an example prom so we have our human in AI so human input AI response\n1:13:52 so we''re basically saying up okay this with this type of input you should provide this type of output that''s what\n1:13:58 we''re doing here and we''re just going to provide some examples okay so we have our input here''s query one here is the\n1:14:06 answer one right this is just I just want to show you how it works this is not what we''d actually feed into our LM\n1:14:13 then with both these examples and our example prompt we''d feed both of these\n1:14:18 into uh line chains few shot chat message prompt template okay and\n1:14:25 well you''ll see what we get out of it okay so we basically get it formats everything and structures everything for\n1:14:31 us okay and using this of course it depends\n1:14:38 on let''s say you see that your user is talking about a particular topic and you\n1:14:44 would like to guide your llm to talk about that particular topic and a particular way right so you could\n1:14:51 identify that the user is talking about that topic either like a keyword match or a semantic similarity match and based\n1:14:58 on that you might want to modify these examples that you feed into your few sh\n1:15:03 chat message promp template and then obviously for that could be what you do for topic a for topic B you might have\n1:15:10 another set of examples that you feed into this all all this time your example prompt is remaining the same but you''re\n1:15:16 you''re just modifying the examples that are going in so that they''re more relevant to whatever it is your user is\n1:15:21 actually talking about so that can be useful now let''s see an example of that so when we are using a tiny LM It''s\n1:15:29 ability would be limited although I think we are we''re probably fine here we''re going to say answer the US query\n1:15:36 based on the context below always answering mark down format you know being very specific the self system\n1:15:42 prompt okay that''s nice but what we''ve kind of said here is okay always\n1:15:48 answering mod down for I did do that but when doing so please provide headers\n1:15:54 short summary and follow bullet points then conclude okay so you see this here\n1:16:01 okay so we get this overview of already you have this and this it''s actually quite good but if we come down here what\n1:16:08 I specifically want is to always follow this structure right so we have the\n1:16:14 double header for the topic summary header a couple of bullet points and\n1:16:21 then I always want to follow this pattern where it''s like to conclude always it''s always bold you know I want to be very specific on\n1:16:27 what I want and to be you know fully honest with GT40 mini you can actually\n1:16:34 just prompt most of this in but for the sake of the example we''re going to provide a few shot um examples in our\n1:16:42 few shot prompt examples instead to get this so we''re going to provide one example here second example here and\n1:16:50 you''ll see we''re just following that same pattern we''re just setting up the pattern that the llm should use so we''re\n1:16:57 going to set that up here we have our main header a little summary some\n1:17:03 subheaders bullet points subheader bullet points subheader bullet points to conclude so on and so on same with this\n1:17:09 one here okay and let''s see what we\n1:17:15 got okay so this is the structure of our new F shop prompt template you can see\n1:17:24 what all this looks like let''s come down and we''re going to do we''re basically going to insert that directly into our\n1:17:31 chat prompt template so we have for messages system\n1:17:36 prompt user prompt and then we have in there these so let me actually show you\n1:17:44 very quickly right so we just have um this few shot chat to message prompt template\n1:17:50 which will be fed into the middle here run that and then feed all this back into our pipeline okay and this will you\n1:17:57 know modify the structure so that we have that bold to conclude at the end here okay we can see nicely here so we\n1:18:04 get a bit more of that exact structure that we were getting again with GT40\n1:18:10 models and many other opening air models you don''t really need to do this but you will see it in other examples we do have\n1:18:17 an example of this where we''re using a llama and we''re using I think llama 2 if\n1:18:23 I''m not wrong and you can see that adding this fuse shot promp template is\n1:18:29 actually a very good way of getting those smaller less capable models to follow your instructions so this is RAR\n1:18:37 when you''re working those smaller lenss this can be super useful but even for so models like\n1:18:42 gp40 if you do find that you''re struggling with the prompting it''s just not quite following exactly what you\n1:18:48 want it to do this is a very good technique for actually getting it to\n1:18:53 follow a very straight structure or behavior okay so moving on we have Chain of Thought prompting so this is a more\n1:19:02 common prompting technique that encourages the LM to think through its\n1:19:08 reasoning or its thoughts step by step so it''s Chain of Thought the idea behind\n1:19:13 this is that okay in math class when you''re a kid the teachers would always\n1:19:19 push you to put down your your working out right and there was a more reasons\n1:19:25 for that one of them is to get you to think because they they know in a lot of cases actually you know you''re a kid and you''re in Aran you don''t really care\n1:19:31 about this test and the you know they''re just trying to get you to slow down a\n1:19:37 little bit and actually put down your reasoning and that kind of forc you to think oh actually I''m skipping a little\n1:19:43 bit in my head because I''m trying to just do everything up here if I write it down all of a sudden it''s like oh\n1:19:48 actually I yeah I need to actually do that slightly differently you you realize okay you''re probably rushing now\n1:19:55 I''m not saying an LM is rushing but it''s a similar effect by an LM writing everything down they tend to actually\n1:20:01 get things right more frequently and at the same time also similar to when\n1:20:07 you''re a child and a teacher is reviewing your exam work by having the LM write down its reasoning you as a as\n1:20:15 a human or engineer you can see where the llm went wrong if it did go wrong\n1:20:21 which can be very useful when you''re trying to diagnose problems so with train of thought we should see uh less\n1:20:27 hallucinations and generally better performance now to implement train of thought in line chain there''s no\n1:20:32 specific like line chain objects that do that instead it''s it''s just prompting okay so let''s go down and just see how\n1:20:39 we might do that okay so be helpful assistant and answer users question you\n1:20:44 must answer the question directly without any other text or explanation okay so that''s our no Chain of Thought\n1:20:51 system problems I will just note here especially with open AI again this is one of those things where you''ll see it\n1:20:57 more with the smaller models most LMS are actually trained to use train thought prompting by default so we''re\n1:21:03 actually specifically telling it here you must answer the question directly without any other text or explanation\n1:21:09 okay so we''re actually kind of reverse prompting it to not use train of thought otherwise by default it actually will\n1:21:16 try and do that because it''s been trained to that''s how that''s how relevant Chain of Thought is okay so I''m\n1:21:22 going to say how many key strokes I need to type in type the numbers from 1 to 500 okay we set up our like llm chain\n1:21:31 Pipeline and we''re going to just invoke our query and we''ll see what we get\n1:21:36 total number of key strokes needed to type the numbers from one to 500 is\n1:21:43 1,511 uh the actual an as I''ve written here is 1,392 without chain thought is\n1:21:50 hallucinating okay now let''s go ahead and see okay with Chain of Thought apprting what does it do so be helpful\n1:21:57 assistant and answer user question to answer the question you must list systematically and in precise detail all\n1:22:05 sub problems that are needed to be solved to answer the question solve each sub problem individually you have to\n1:22:11 shout at the LM sometimes to get them to listen and in sequence finally use\n1:22:17 everything you''ve worked through to provide the final answer okay so we''re getting it we''re forcing it to kind of\n1:22:22 go through the full problem there can remove that not sure why that''s there so\n1:22:27 run that again I don''t know why we have context there I remove that and let''s\n1:22:35 see you can see straight away that''s taking a lot longer to generate output\n1:22:40 that''s because it''s generating so many more tokens so that''s just one one drawback of this but let''s see what we\n1:22:46 have so to determine how many keystrokes to tie those numbers we is breaking down\n1:22:52 several sub problems so count number of digits from 1 to 9 10 to 99 so so on and\n1:22:58 count the digits in number 500 okay interesting so that''s how it''s breaking it up some more digits count in the\n1:23:05 previous steps so we go through total digits and we see that''s okay nine\n1:23:12 digits for those for here 180 for here\n1:23:17 1,200 and then of course three here so it gets all those sums those digits and\n1:23:25 actually comes to the right answer okay so that that is you that''s the difference with with Chain of Thought\n1:23:30 versus without so without it we just get the wrong answer basically guessing with\n1:23:37 chain of thought we get the right answer just by the llm writing down its reasoning and breaking the problem down\n1:23:44 into multiple Parts which is I found that super interesting that it it does that so that''s pretty cool now I will\n1:23:53 just see so as I as we mentioned before most llms nowadays are actually training\n1:23:58 to use train of thought prompting by default so let''s just see if we don''t mention anything right be a helpful\n1:24:03 assistant and answer the users question so we''re not telling it not to think through it''s reasoning and we''re not\n1:24:09 telling it to think through its reasoning let''s just see what it does okay so you can see again it''s\n1:24:16 actually doing the exact same reasoning\n1:24:21 okay it doesn''t it doesn''t give us like the sub problems that the start but it is going through and it''s breaking\n1:24:27 everything apart okay which is quite interesting and we get the same correct answer so the formatting here is\n1:24:33 slightly different it''s probably a little cleaner actually although I think\n1:24:38 uh I don''t know I here we get a lot more information so both are fine and in this\n1:24:46 scenario we actually do get the the right answer as well so you can see that that Chain of Thought prompting has\n1:24:52 actually been quite literally trained into the model and you''ll see that with\n1:24:58 most well I think all save the-art lenss Okay cool so that is our our chapter on\n1:25:07 prompting again we''re focusing very much on a lot of the fundamentals of\n1:25:13 prompting there and of course tying that back to the actual objects and methods\n1:25:19 within langing but for now that''s it for prompting and we''ll move on to the next ch chapter in this chapter we''re going\n1:25:26 to be taking a look at conversational memory in line chain we''re going to be\n1:25:31 taking a look at the core like chat memory components that have really been\n1:25:37 in line chain since the start but are essentially no longer in the library and\n1:25:43 we''ll be seeing how we actually Implement those historic conversational\n1:25:49 memory Utilities in the new versions of Lang chain so 0.3 now as a pre-warning\n1:25:57 this chapter is fairly long but that is because conversational memory is just\n1:26:02 such a critical part of chatbots and agents conversational memory is what\n1:26:08 allows them to remember previous interactions and without it our chat\n1:26:13 boox and agents would just be responding to the most recent message without any\n1:26:18 understanding of previous interactions within a conversations so they would just not be coners ational and depending\n1:26:26 on the type of conversation we might want to go with various approaches to\n1:26:32 how we remember those interactions within a conversation now throughout\n1:26:38 this chapter we''re going to be focusing on these for memory types we''ll be\n1:26:43 referring to these and I''ll be showing you actually how each one of these works but what we''re really focusing on is\n1:26:49 rewriting these for the latest version of Lang chain using the what it''s called\n1:26:54 the runable with\n1:27:00 message history so we''re going to be essentially taking a look at the original implementations for each of\n1:27:07 these four original memory types and then we''ll be rewriting them with the the runnable memory history class so\n1:27:14 just taking a look at each of these four very quickly conversational buffer\n1:27:19 memory is I think the simplest and most intuitive of these meor types it is\n1:27:26 literally just you have your messages they come into this object they are sold\n1:27:33 in this object as essentially a list and when you need them again it will return\n1:27:38 them to you there''s nothing and nothing else to it''s super simple the conversation Buffet window memory okay\n1:27:44 so new word in the middle of the window this works in pretty much the same way\n1:27:51 but those messages that it has stored is not going to return all of them for you instead it''s just going to return the\n1:27:57 most recent let''s say the most recent three for example okay and that is defined by a parameter K conation of\n1:28:05 summary memory rather than keeping track of the entire uh interaction memory\n1:28:11 directly what it''s doing is as those interactions come in it''s actually going to take them and it''s going to compress\n1:28:18 them into a smaller little summary of what has been within that conversation\n1:28:24 and as every new interaction is coming in it''s going to do that going to keep iterating on that summary and then that\n1:28:31 is going to be return to us when we need it and finally we have the conversational summary buffer memory so\n1:28:37 this is it''s taking so the buffer part of this is actually referring to very\n1:28:43 similar thing to the buffer window memory but rather than it being a you know most K messages it''s looking at the\n1:28:50 number of tokens within your memory and it''s returning the most recent K\n1:28:57 tokens that''s what the buffer part is there and then it''s also merging that\n1:29:02 with the summary memory here so essentially what you''re getting is almost like a list of the most recent\n1:29:09 messages based on the token length rather than the number of interactions plus a summary which would you know come\n1:29:16 at the the top here so you get kind of both the idea is that obviously this\n1:29:21 summary here would maintain all of your interactions in a very\n1:29:27 compressed form so you''re you''re losing less information and you''re still maintaining you know maybe the very\n1:29:33 first interaction the user might have introduced themselves giving you their name hopefully that would be maintained\n1:29:40 within the summary and it would not be lost and then you have almost like higher resolution on the most recent um\n1:29:47 K or k tokens from your memory okay so let''s jump over to the code we''re going\n1:29:53 into the 04 chat memory notebook open that in collab okay now here we are let''s go ahead and install the\n1:30:01 prerequisites run all we again can or cannot use align\n1:30:06 Smith it is up to you enter that and let''s come down and start so first just\n1:30:13 initialize our LM using 40 mini in this example again low\n1:30:20 temperature and we''re going to start with conversation buffer memory right so this is the original version of this uh\n1:30:29 memory type so let me where are we we''re here so memory conversation both of\n1:30:36 memory and we''re returning messages that needs to be set to true so the reason that we set return messages true it it\n1:30:44 mentions up here is if you do not do this it''s going to returning your chat\n1:30:50 history as a string to an llm whereas well chat lm''s nowadays would expect\n1:30:58 message objects so yeah you just want to be returning these as messages rather\n1:31:03 than as strings okay otherwise yeah you''re going to get some kind of strange Behavior out from your llms if you\n1:31:10 return them strings so you do want to make sure that it''s true I think by default it might not be true but this is\n1:31:16 coming this is deprecated right it does tell you here as de creation warning\n1:31:21 this is coming from older BL chain but it''s a good place to start just to understand this and then we''re going to rewrite this with the runnables which is\n1:31:28 the recommended way of doing so nowadays okay so adding messages to our memory\n1:31:34 we''re going to write this okay so it''s just a just a conversation user AI user\n1:31:39 AI so on and so on random chat main things to not here is I do provide my\n1:31:45 name we have the the model''s name right towards the start of those interactions okay so I''m just going to add all of\n1:31:52 those with do it like this okay then we can just see we can load our history\n1:32:01 like so so let''s just see what we have there okay so we have human message AI message human message right this is\n1:32:08 exactly what we I showed you just here it''s just in that message format from line chain okay so we can do that\n1:32:16 alternatively we can actually do this so we can get our memory we initialize the conation buffer memory as we did before\n1:32:24 and we can actually add it directly the message into our memory like that so we can use this add us message add AI\n1:32:30 message so on and so on load again and it''s going to give us the exact same thing again there''s multiple ways to do\n1:32:37 the same thing cool so we have that to pass all of this into our LM again this\n1:32:42 is all deprecated so we''re going to learn how to properly in a moment but this is how L chain was doing in the\n1:32:49 past so to pass all of this into our LM we'' be using this conversation chain\n1:32:55 right again this is deprecated nowadays we would be using lell for this so I I\n1:33:02 just want to show you okay how this would all go together and then we would invoke okay what is my name again let''s\n1:33:08 run that and we''ll see what we get it''s remembering everything remember so this\n1:33:13 conversation buffer memory it doesn''t drop messages it just remembers everything right and honestly with the\n1:33:20 sort of high context Windows of many LMS might be what you do it depends on how\n1:33:25 long you expect the conversation to go on for but you could you probably in most cases would get away with this okay\n1:33:32 so what let''s see what we get um I say what is my name again okay let''s see\n1:33:38 what it gives me says your name is chains great thank you that works now as\n1:33:44 I mentioned all of this that I just showed you is actually deprecated that''s the old way doing things let''s see how\n1:33:49 we actually do this in modern or up toate L chain so we''re going to be using this runable with message history to\n1:33:57 implement that we will need to use LL and for that we will need to just Define\n1:34:02 prompt templates our LM as we usually would okay so we''re going to set up our system prompt which is just a helpful\n1:34:09 assist called Zeta okay we''re going to put in this messages\n1:34:15 placeholder okay so that''s important essentially that is where our messages\n1:34:20 are coming from our conversation Buffet for memory is going to be inserted right\n1:34:26 so it''s going to be that chat history is going to be inserted after our system prompt but before our most recent query\n1:34:33 which is going to be inserted last here okay so messages placeholder item that''s\n1:34:39 important and we use that throughout the course as well so we use it both for chat history and we''ll see later on we\n1:34:45 also use it for the intermediate thoughts that a agent would go through as well so important to remember that\n1:34:52 little thing well link our prompt template to our LM again if we would\n1:34:58 like we could also add in the I think we only have the query here oh we would\n1:35:04 probably also want our history as well but I''m not going to do that right now\n1:35:10 okay so we have our Pipeline and we can go ahead and actually Define our runnable with message history now this\n1:35:17 class or object when we are initializing it does require a few items we can see them here okay so we'' see that we have\n1:35:24 our Pipeline with history so it''s basically going to be uh you can you can see here right we have that history\n1:35:30 messages key right this here has to align with what we provided as a meses\n1:35:36 placeholder in our pipeline right so we have our pipeline prompt template here\n1:35:44 and here right so that''s where it''s coming from it''s coming from messes placeholder variable name is history\n1:35:49 right that''s important that links to this then for the input messages key\n1:35:55 here we have query that again links to this okay so both important to have\n1:36:03 that the other thing that is important is obviously we''re passing in that pipeline from before but then we also\n1:36:09 have this get session history basically what this is doing is it''s saying okay I need to get uh the list of messages that\n1:36:16 make up my chat history that are going to be inserted into this variable so that is a function that we Define okay\n1:36:22 and with within this function what we''re trying to do here is actually replicate\n1:36:28 what we have with the previous conversation buffer memory okay\n1:36:34 so that''s what we''re doing here so it''s very simple right so we have this in\n1:36:40 memory chat message history okay so that''s just the object that we''re going to be returning what this will do is it\n1:36:47 will set up a session ID the session ID is essentially like a unique identifier so that eachers ation or interaction\n1:36:55 within a single conversation is being mapped to a specific conversation so you don''t have overlapping let say have\n1:37:00 multiple users using the same system you want to have a unique session ID for each one of those okay and what it''s\n1:37:06 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here we are going\n1:37:13 to initialize that session with an inmemory chat message history okay\n1:37:21 that''s it and we return okay and all that''s going to do is it''s going to basically append our messages they will\n1:37:28 be appended within this chat map session ID and they''re going to get returned\n1:37:33 there''s nothing R there''s nothing else to it to be honest so we invoke our\n1:37:39 rable let''s see what we get I need to ruin\n1:37:45 this okay note that we do have this config so we have a session ID that''s to\n1:37:50 again as I mentioned keep different conversations separate Okay so we''ve run that now let''s run a few more so what is\n1:37:57 my name again let''s see if it remembers your name is James how can I help you today James okay so it''s what we''ve just\n1:38:06 done there is literally conversation buter memory but for upto-date L chain\n1:38:14 with L cell with Runner BS so you the recommended way of doing it nowadays so\n1:38:20 that''s a very simple example okay really and not that much to it it gets a little\n1:38:27 more complicated as we start thinking about the different types of memory although that being said it''s not\n1:38:33 massively complicated we''re only rarely going to be changing the way that we''re getting our interactions so let''s uh\n1:38:41 let''s dive into that and see how we will do something similar with the conation buff for window memory but first let''s\n1:38:47 actually just understand okay what is Con station buffer window memory so as I mentioned near the start it''s going to\n1:38:53 keep track of the last K messages so there''s a few things to keep in mind\n1:38:58 here more messages does mean more tokens send with each request and if we have\n1:39:03 more tokens in each request it means that we''re increasing the latency of our responses and also the cost so with the\n1:39:10 previous memory type we''re just sending everything and because we''re sending everything that is going to be\n1:39:15 increasing our cost it''s going to be increasing our latency for every message especially as a conversation gets longer\n1:39:21 and longer and we don''t we might not necessarily want to do that so with this conversation buffer window memory we''re\n1:39:28 going to just say okay just return me the most recent messages okay so let''s\n1:39:35 well let''s see how that would work here we''re going to return the most recent four messages okay we are again make\n1:39:41 sure we''ve turned messages is set to True again this is deprecated this is just the old way of doing it in a moment\n1:39:48 we''ll see the updated way of doing this we''ll add all of our\n1:39:54 messages okay so we have this and just see here right so we''ve added in all\n1:40:01 these messages there''s more than four messages here and we can actually see that here so we have human message AI\n1:40:07 human AI human AI human AI right so we''ve got four pairs of human AI\n1:40:14 interactions there but here we don''t have there''s more than four pairs so four pairs will take us back all the way\n1:40:21 to here I''m researching different types of conversational uh memory okay and if\n1:40:28 we take a look here the most the first message we have is I''m researching different types of conversational memory\n1:40:33 so it''s cut off these two here which will be a bit problematic when we ask you what our name is okay so let''s just\n1:40:40 see going to be using conversation chain object again again just remember that is\n1:40:45 deprecated and I want to say what is my name again let''s see let''s see what it\n1:40:51 says uh I''m sorry I but I don''t have access to your name or any personal information if you like you can tell me\n1:40:56 your name right so it doesn''t actually remember uh so that''s kind of like a negative of the conversation Buffet\n1:41:04 window memory of course the uh to fix that in this scenario we might just want to increase K maybe we say remember the\n1:41:12 previous eight interaction Pairs and it will actually remember so what is my\n1:41:17 name again your name is James so now it remembers we''ve just modified how much it is remembering but of course you know\n1:41:24 pros and cons to this it really depends on what you''re trying to build so let''s take a look at how we would actually\n1:41:30 implement this with the runable with message history okay so you getting a little\n1:41:38 more complicated here although it it''s it''s not it''s not complicated but well\n1:41:44 we''ll see okay so we have buffer window message history we''re creating a class here this class is going to inherit from\n1:41:51 the base chat message history object from line chain okay and in all of our\n1:41:57 other message history objects can do the same thing before with the inmemory\n1:42:02 message object that was basically replicating the buffer memory so we\n1:42:08 didn''t actually need to do anything we didn''t need to Define our own class here\n1:42:13 so in this case we do so we follow the same pattern that line chain follows\n1:42:19 with this base chat message history and you can see a few of the functions here that are important so add messages and\n1:42:26 clear are the ones that we''re going to be focusing on we also need to have messages which this object attribute\n1:42:31 here okay so we''re just implementing the synchronous methods here if we want this\n1:42:38 to be async if we want to support async we would have to add a add messages um a\n1:42:44 get messages and a clay as well so let''s go ahead and do that we have messages we\n1:42:49 have K again we''re looking at remembering the top K messages or most recent K messages only so it''s important\n1:42:56 that we have that variable we are adding messages through this class this is\n1:43:01 going to be used by line chain within our runnable so we need to make sure that we do have this method and all\n1:43:06 we''re going to be doing is extending the self messages uh list here and then we''re actually just going to be trimming\n1:43:13 that down so that we''re not remembering anything beyond those you know most recent K\n1:43:19 messages that we have set from here and then we also have the clear method\n1:43:25 as well so we need to include that that''s just going to clear the history okay so it''s not this isn''t complicated\n1:43:31 right it just gives us this nice default standard interface for message history\n1:43:37 and we just need to make sure we''re following that pattern okay I''ve included the uh this print here just so we can see what''s happening okay so we\n1:43:45 have that and now for that get chat history function that we defined earlier\n1:43:51 rather than using the buin method we''re going to be using our own object which is a buffer window message history which\n1:43:58 will be defined just here okay so if session ID is not in the chat map as we\n1:44:05 did before we''re going to be initializing our buffer window message history we''re setting K up here with a\n1:44:10 default value of four and then we just return it okay and and that is it so let''s run this we have our runable with\n1:44:18 message history we have all of these variables which are exactly the same as before four but then we also have these\n1:44:25 variables here with it''s history Factory config and this is where if we have um\n1:44:33 new variables that we''ve added to our message history in this case k that we\n1:44:38 have down here we need to provide that to line train and sell it this is a new configurable field okay and we''ve also\n1:44:46 added it for the session ID here as well so we''re just being explicit and have everything in that so we have that\n1:44:53 and we run okay now let''s go ahead and invoke and see what we get okay so\n1:45:00 important here this history Factory config that is kind of being fed through\n1:45:06 into our invoke so that we can actually modify those variables from here okay so we have config configurable session ID\n1:45:14 okay we just put whatever we want in here and then we also have the number K okay so remember the previous four\n1:45:22 interaction I think in this one we''re doing something slightly different I think we''re remembering the four\n1:45:27 interactions rather than the previous four interaction pairs okay so my name is James uh we''re going to go through\n1:45:34 I''m just going to actually clear this and now I''m going to start again and we''re going to use the exact same ad\n1:45:40 user message ad AI message that we used before we''re just manually inserting all that into our history so that we can\n1:45:46 then just see okay what is the result and you can see that k equal 4 is actually unlike before where we were\n1:45:53 having the uh saving the top four interaction pairs we now saving the most\n1:46:01 recent four interactions not pairs just interactions and honestly I just think\n1:46:06 that''s clearer I think it''s weird that the number four for K would actually save the most recent eight messages\n1:46:14 right I I think that''s odd so I''m just not replicating that weirdness we could\n1:46:19 if we wanted to I just don''t like it so I''m not doing that and anyway we can see\n1:46:26 from messages that we''re returning just the most four recent messages okay which\n1:46:31 should be these four Okay cool so we''ve just using the runable we''ve replicated\n1:46:38 the old way of having a window memory and okay I''m going to say what is my\n1:46:43 name again as before it''s not going to remember so we can come to here I''m sorry about I don''t have access to\n1:46:48 personal information so on and so on if you like to tell me your name doesn''t know now let''s try a new one where we\n1:46:56 initialize a new session okay so we''re going with ID K4 so that''s going to\n1:47:01 create a new conversation there and we''re going to say we''re going to set K to\n1:47:07 14 okay great I''m going to manually insert the other uh messages as we did\n1:47:13 before okay and we can see all of those and see at the top here we are still maintaining that hi my name is James\n1:47:19 message now let''s see if it remembers my name your name is James okay there we go\n1:47:27 cool so that is working we can also see so we just added this what is my name again let''s just see if did that get\n1:47:34 added to our list of messages right what is my name again nice and then we also\n1:47:39 have the response your name is James so just by invoking this because we''re using the the runable with message\n1:47:46 history it''s just automatically adding all of that into our message history\n1:47:52 which is nice cool all right so that is the buffer window memory now we are going to take a\n1:47:59 look at how we might do something a little more complicated which is the the summaries okay so when you think about\n1:48:06 the summary you know what are we doing we''re actually taking the messages we''re using that LM call to summarize them to\n1:48:14 compress them and then we''re storing them within messages so let''s see how we would actually uh do that so to start\n1:48:22 with let''s just see how it was done in Old Line chain so we have conversation\n1:48:28 summary memory go through that and let''s just see what we get so\n1:48:35 again same interactions right I''m just invoking invoking invoking I''m not adding these\n1:48:41 directly to the messages because it actually needs to go through a um like that summarization process and if we\n1:48:49 have a look we can see it happening okay current conversation so sorry current\n1:48:55 conversation hello there my name is James AI is generating current conversation the human introduces\n1:49:01 himself as James AI greets James warmly and expresses its Readiness to chat and assist inquiring about how his day is\n1:49:08 going right so it''s summarizing the the previous interactions and then we have\n1:49:14 you know after that summary we have the most recent human message and then the AI is going to generate its response\n1:49:21 okay and that continues your own Contin is going and you see that the the final summary here is going to be a lot longer\n1:49:27 okay it''s different that first summary of course asking about his Day Men researching different types of\n1:49:32 conversational memory the AI responds enthusiastically explaining that conversational memory includes\n1:49:37 short-term memory longterm memory contextual memory personalized memory and then inquires if James is focused on a specific type of memory Okay cool so\n1:49:46 we get essentially the summary is just getting uh longer and longer as we go but at some point the idea is that it''s\n1:49:53 not going to keep growing and it should actually be shorter than if you were saving every single interaction whilst\n1:49:59 maintaining as much all the information as possible but of course you''re not\n1:50:04 going to maintain all of the information that you would with for example the the\n1:50:09 buffer memory right with the summary you are going to lose information but\n1:50:15 hopefully less information than if you''re just cutting interactions so\n1:50:20 you''re trying to reduce your token count whilst maintaining as much information as\n1:50:25 possible now let''s go and ask what is my name again it should be able to answer\n1:50:30 because we can see in the summary here that I introduced myself as\n1:50:36 James okay respondents your name is James how is your research going okay so\n1:50:41 has that cool let''s see how we''d Implement that so again as before we''re\n1:50:46 going to go with that conversation summary message history we''re going to\n1:50:52 be importing this system message uh we''re going to be using that not for the LM that we''re chatting with but for the LM that will be generating our summary\n1:51:01 so actually that is not quite correct there is create a summary not that it\n1:51:06 matters it''s just the doct string so we have our messages and we also have the LM so different different attribute here\n1:51:12 to what we had before when we initialize a conversation summary message history we need to passing in our LM we have the\n1:51:19 same methods as before we have ADD messages and clear and what we''re doing is as messages coming we extend with our\n1:51:27 current messages but then we''re modifying those okay so we construct our like\n1:51:34 instructions to make a summary okay so that is here we have the system front uh\n1:51:40 giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring to\n1:51:45 maintain as much relevant information as possible okay then we have a human message here through that we''re passing\n1:51:52 the existing summary okay and then we''re passing in the new\n1:51:57 messages Okay cool so we format those invoke the\n1:52:05 llm here and then what we''re doing is in the messages we''re actually replacing\n1:52:11 the existing history that we had before with a new history which is just a single system summary message okay let''s\n1:52:20 see what we get as before we have that get chat history exactly the same as before the only real difference is that\n1:52:26 we''re passing in the llm parameter here and of course as we''re passing in the LM parameter in here it does also mean that\n1:52:33 we''re going to have to include that in the configurable field spec and that we''re going to need to include that when\n1:52:40 we''re invoking our pipeline okay so we run that pass in the\n1:52:48 LM now of course one side effect of generating summaries for everything is that way actually you know we''re\n1:52:54 generating more so you are actually using quite a lot of tokens whether or\n1:52:59 not you are saving tokens or not actually depends on the length of a conversation as the conversation gets\n1:53:05 longer if you''re storing everything after a little while that the token usage is actually going to increase so\n1:53:12 if in your use case you expect to have shorter conversations you would be\n1:53:17 saving money and tokens by just using this standard buffer memory\n1:53:23 whereas if you''re expecting very long conversations you would be saving tokens and money by using the summary history\n1:53:31 okay so let''s see what we got from there we have a summary of the conversation James introduced himself by saying hi\n1:53:37 name James a I responded War asking hi James Interac include details about token\n1:53:42 usage okay so we actually included everything here which we probably should\n1:53:48 not have done why did we do that as so in here we''re including all\n1:53:56 of the out in here so we using or including\n1:54:02 all of the content from the messages so I think maybe we just do X content for X in messages that\n1:54:12 should resolve that okay there we go so we quickly fli\n1:54:20 that so yeah before we pass them in the entire mage object which obviously includes all of this information whereas\n1:54:26 actually we just want to be passing into the content so we modified that and now\n1:54:31 we''re getting what we would expect okay cool and then we can keep going right so as we as we keep going\n1:54:38 the summary should get more like abstract like as we just saw here is\n1:54:43 literally just giving us the messages directly almost okay so we''re getting a bit of summary there and we can keep\n1:54:50 going we''re going to add just more messages to that we''ll see the you as we''ll get send those we''ll get a\n1:54:57 response send it again get a response and we just adding all of that invoking all of that and that will be of course\n1:55:03 adding everything into our message history Okay cool so we''ve run that\n1:55:09 let''s see what the latest summary is okay and then we have this so this is\n1:55:15 a summary that we have inside of our our chat history okay cool now finally let''s see\n1:55:24 what is my name again we can just double check you know it has my name in there so it should be able to tell\n1:55:33 us okay cool so your name is James pretty interesting so let''s have a quick\n1:55:39 look over at limith so the reason I want to do this is just to point out okay the\n1:55:45 different essentially token usage that we''re getting with each one of these okay so we can see that we have these Runner mess history which probably uh\n1:55:53 improved in naming there but we can see okay how long is each one of these taken\n1:55:59 how many tokens are they also using come back to here we have this runable\n1:56:04 message history this is we''ll go through a few of these maybe to here I think we\n1:56:11 can see here this is that first interaction where we''re using the buffer memory and we can see how many tokens we\n1:56:17 used here so 112 tokens when we''re asking what is my name again okay then\n1:56:23 we modified this to include I think it was like 14 interactions or something on\n1:56:29 those lines obviously increases the number of tokens that we''re using right so we can could see that actually happening all in Lang which is quite\n1:56:36 nice and we can compare okay how many tokens is each one of these using now this is looking at the buffer window and\n1:56:44 then if we come down to here and look at this one so this is using our summary\n1:56:49 okay so our summary with what is my name again actually use more tokens in this scenario right which is interesting\n1:56:55 because we''re trying to compress information the reason there more is because there''s not there hasn''t been that many interactions as the\n1:57:02 conversation length increases with the summary this total\n1:57:08 number of tokens especially if we prompt it correctly to keep that low that should remain relatively small\n1:57:15 whereas with the buffer memory that will just keep increasing and increasing as the as the conversation gets longer\n1:57:23 so useful little way of using Lang Smith there to just kind of figure out okay in\n1:57:29 terms of tokens and costs of what we''re looking at for each of these memory types okay so our final memory type acts\n1:57:37 as a mix of the summary memory and the buffer memory so what it''s going to do\n1:57:43 is keep the buffer up until an N number of tokens and then once a message\n1:57:50 exceeds the N number of tokens limit for the buffer it is actually going to be\n1:57:55 added into our summary so this memory has the benefit of remembering in detail\n1:58:03 the most recent interactions whilst also not having the limitation of using too\n1:58:10 many tokens as a conversation gets longer and even potentially exceeding context Windows if you try super hard so\n1:58:18 this is a very interesting approach now as before let''s try the original way of\n1:58:24 implementing this then we will go ahead and use our update method for\n1:58:30 implementing this so we come down to here and we''re going to do L chain memory import conversation summary\n1:58:37 buffer memory okay a few things here LM for summary we have the N number of\n1:58:44 tokens that we can keep before they get added to the summary and then return messages of course okay you can see\n1:58:51 again this is dicated we use the conversation chain and then we just passing our memory there and\n1:58:57 then we can chat okay so super straightforward first message we''ll add\n1:59:03 a few more here and we have to invoke because how\n1:59:08 memory type here is using NM to create those summaries as it goes and let''s see\n1:59:14 what they look like okay so we can see for the first message here we have human message and then an AI message\n1:59:22 then we come a little bit lower down again it''s same thing human message is the first thing in our history here then\n1:59:29 it''s a system message so this is at the point where we''ve exceeded that 300 token limit and the memory type here is\n1:59:36 generating those summaries so that summary comes in as a system message and we can see okay the human named James\n1:59:43 introduces himself and mentions he''s researching different types of conversational memory and so on and so on right okay cool so we have that then\n1:59:52 let''s come down a little bit further we can see okay so the summary there okay\n1:59:59 so that''s what we that''s what we have that is the implementation for the old\n2:00:05 version of this memory again we can see it''s deprecated so how do we implement this for our more recent versions of\n2:00:14 Lang chain and specifically 0.3 well again we''re using that runable message\n2:00:20 history and it looks a little more complicated than we were getting before but it''s actually just you know it''s\n2:00:27 nothing too complex we''re just creating a summary as we did with the previous\n2:00:33 memory type but the decision for adding to that summary is based on in this case\n2:00:39 actually the number of messages so I didn''t go with the the Lang chain version where it''s a number of tokens I\n2:00:46 don''t like that I prefer to go with messages so what I''m doing is saying okay let K messages\n2:00:52 okay once we exceed K messages the messages beyond that are going to be\n2:00:58 added to the memory Okay cool so let''s see we first initialize our conversation\n2:01:06 summary buffer message history class with llm and K okay so these two here so\n2:01:14 LM of course to create summaries and K is just the the limit of the number of messages that we want to keep before\n2:01:19 adding them to the summary or dropping them from now messages and adding them to the summary okay so we will begin with okay\n2:01:29 do we have an existing summary so the reason we set this in none is we can''t\n2:01:36 extract the summary the existing summary unless it already exists and the only\n2:01:41 way we can do that is by checking okay do we have any messages if yes we want to check if within those messages we\n2:01:48 have a system message because we''re we''re doing the same structure is what we have up here where the system message\n2:01:54 that first system message is actually our summary so that''s what we''re doing here we''re checking if there is a\n2:02:00 summary message already stored within our messages okay so we''re checking for that if we\n2:02:08 find it we''ll just do we have this little print statement so we can see that we found something and then we just\n2:02:14 make our existing summary I should actually move this to the first instance\n2:02:21 here yeah okay so that existing summary will be set to the first\n2:02:30 message okay and this would be a system message rather than a\n2:02:35 string cool so we have that then we want to add any new messages to our history\n2:02:43 okay so we''re extending the history there and then we''re saying okay if the length of our history is exceeds the K\n2:02:50 value that we set we''re going say okay we found that many messages we''re going to be dropping the latest it''s going to be the latest two\n2:02:57 messages this I will say here one thing or one problem with this is that we''re\n2:03:04 not going to be saving that many tokens if we''re summarizing every two messages so what I would probably do is in in an\n2:03:12 actual like production setting I would probably say let''s go up to 20 messages\n2:03:19 and once we hit 20 messages let''s take the previous 10 we''re going to summarize them and put them into our summary\n2:03:26 alongside any you know previous summary that already existed but in in you know this is also fine as well okay so we say\n2:03:36 we found those mes we''re going to drop the latest two messages okay so we pull\n2:03:41 the the oldest messages out I should say not the latest it''s the\n2:03:48 oldest not the latest I want to keep the latest drop the oldest so we pull out\n2:03:54 the oldest messages and keep only the most recent messages okay then I''m\n2:04:01 saying okay if we if we don''t have any old messages to summarize we don''t do\n2:04:06 anything we just return okay so this in the case that this has not been triggered we would hit this but in the\n2:04:14 case this has been triggered and we do have old messages we''re going to come to\n2:04:20 here okay okay so this is we can see have a system message prompt template\n2:04:26 saying giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring\n2:04:32 to maintain as much relevant information as possible so if you want to be more conservative with tokens we could modify\n2:04:39 this prompt here to say keep the summary to within the length of a single\n2:04:44 paragraph for example and then we have our human M prom template which is going to say okay here''s the existing\n2:04:50 conversation summary in here on new messages now new messages here is actually the old messages but the way\n2:04:57 that we''re framing it to the llm here is that we want to summarize the whole conversation right it doesn''t need to\n2:05:03 have the most recent messages that we''re storing within our buffer it doesn''t need to know about those that''s\n2:05:09 irrelevant to the summary so we just tell it that we have these Zoom mes and as far as this LM is concerned this is\n2:05:15 like the full set of interactions okay so then we would format those and invoke\n2:05:21 our LM and then we''ll print out our new summary so we can see what''s going on there and we would prend that new\n2:05:30 summary to our conversation history okay and and this will work so we can just\n2:05:37 prend it like this because we''ve already\n2:05:42 popped where was it up here if we have an existing\n2:05:47 summary we already pop that from the list it''s already been pulled out of that list so it''s okay for us to just we\n2:05:53 don''t need to say like we don''t need to do this because we''ve already dropped that initial system message if it\n2:06:00 existed okay and then we have the clear method as before so that''s all of the\n2:06:05 logic for our conversational summary buffer memory we redefine our get chat\n2:06:14 history function with the LM and K parameters there and then we''ll also\n2:06:20 want to set the configurable Fields again so that is just going to be of course session ID LM and\n2:06:28 K okay so now we can invoke the K value to begin with is going to be\n2:06:35 four okay so we can see no old messages to update summary with it''s good let''s\n2:06:42 invoke this a few times and let''s see what we get okay so now M to summary with\n2:06:51 found six messages dropping the aest 2 and then we have new summary in the conversation James Inu himself and first\n2:06:58 is interestes in researching different types of conversational memory right so you can see there''s quite a lot in here\n2:07:03 at the moment so we would definitely want to prompt the LM the summary LM to keep\n2:07:10 that short otherwise we''re just getting a ton of stuff right but we can see that that is\n2:07:17 you know it''s it''s working it''s functional so let''s go back and see if we can prompt it to be a little more\n2:07:23 concise so we come to here ensuring to maintain as much relevant information as\n2:07:28 possible however we need to\n2:07:33 keep our summary concise the\n2:07:39 limit is a single short paragraph okay\n2:07:45 something like this let''s try and let''s see what we get with\n2:07:50 that okay so message one again nothing to update see this so new summary you can\n2:07:56 see it''s a bit shorter it doesn''t have all those bullet\n2:08:01 points okay so that seems better let''s see so you can see the first summary is\n2:08:09 a bit shorter but then as soon as we get to the second and third summaries the\n2:08:14 second summary is actually slightly longer than the third one okay so we''re going to be we''re going to be losing a\n2:08:20 bit of information in this case more than we were before but we''re saving a ton of tokens so that''s of course a good\n2:08:28 thing and of course we could keep going and adding many interactions here and we should see that this conversation\n2:08:34 summary will be it should maintain that sort of length of around one short\n2:08:40 paragraph So that is it for this chapter on conation memory we''ve seen a few\n2:08:47 different memory types we''ve implemented their old deprecated version so we can see what they were like and then we''ve\n2:08:55 reimplemented them for the latest versions of Lang chain and to be honest using logic where we are getting much\n2:09:02 more into the weees and that is in some ways okay it complicates things that is\n2:09:07 true but in other ways it gives us a ton of control so we can modify those memory\n2:09:13 types as we did with that final summary buffer memory type we can modify those\n2:09:18 to our liking which is incredibly useful when you''re actually building\n2:09:23 applications for the real world so that is it for this chapter we''ll move on to the next one in this chapter we are\n2:09:30 going to introduce agents now agents I think are one of the most important\n2:09:37 components in the world of AI and I don''t see that going away anytime soon I\n2:09:43 think the majority of AI applications the intelligent part of\n2:09:49 those will be was always an implementation of an AI agent or mle AI\n2:09:55 agents so in this chapter we are just going to introduce agents within the\n2:10:00 context of line chain we''re going to keep it relatively simple we''re going to go into much more depth in agents in the\n2:10:09 next chapter where we''ll do a bit of a deep dive but we''ll focus on just introducing the Core Concepts and of\n2:10:16 course agents within line chain here so jumping thing straight into our notebook\n2:10:24 let''s run our prerequisites you''ll see that we do have an additional prerequisite here which is\n2:10:30 Google search results that''s because we''re going to be using the sub API to allow our llm as an agent to search a\n2:10:38 web which is one of the great things about agents is that they can do all of\n2:10:44 these additional things and LM by itself obviously cannot so we come down to here\n2:10:49 we have our lsmith parameters again of course so you enter your Lang chain API\n2:10:54 if you have one and now we''re going to take a look at tools which is a very\n2:11:00 essential part of Agents so tools are a way for us to augment our llms with\n2:11:07 essentially anything that we can write in code so we mentioned that that we''re going to have a Google Search tool that\n2:11:14 Google Search tool it''s some code that gets executed by our llm in order to\n2:11:19 search Google and get some results so a tool can be thought of as any code logic\n2:11:26 or any function in the C in the case of python any function that has been\n2:11:32 formatted in a way so that our LM can understand how to use it and then\n2:11:38 actually use it although the the LM itself is not using the tool it''s more\n2:11:43 our agent execution logic which uses the tool for the llm so we''re going to go\n2:11:49 ahead and actually create a few simple tools we''re going to be using what is called the tool decorator from Lang\n2:11:55 chain and there are a few things to keep in mind when we''re building tools so for\n2:12:02 Optimal Performance our tool needs to be just very readable and what I mean by readable is we need three main things\n2:12:10 one is a DOT string that is written in natural language and it is going to be used to explain to the Alm when and why\n2:12:18 and how it should use this tool we should also have clear parameter names\n2:12:23 those parameter names should tell the llm okay what each one of these\n2:12:28 parameters are they should be self-explanatory if they are not self-explanatory we should be including\n2:12:36 an explanation for those parameters within the doc string then finally we should have type annotations for both\n2:12:43 our parameters and also what we''re returning from the tool so let''s jump in\n2:12:48 and see how we would Implement all of that so we come down to here and we have line chain core tools import tool okay so\n2:12:56 these are just four incredibly simple tools we have the addition or add tool\n2:13:03 multiply the exponentiate and the subtract tools okay so a few calculator\n2:13:09 S tools now when we add this tool decorator it is turning each of these\n2:13:16 tools into what we call a structured tool object so we can see that\n2:13:21 here we can see we have this structured tool we have a name description okay and\n2:13:28 then we have this Al schema we''ll see this in a moment and a function right so this function is literally just the\n2:13:35 original function it''s it''s a mapping to the original function so in this case it it''s the add function now the\n2:13:41 description we can see is coming from our doc string and of course the name as well is just coming from the function\n2:13:47 name okay and then we can also see let''s just print the name and\n2:13:53 description but then we can also see the ARs schema right we can so this thing\n2:13:58 here that we can''t read at the moment to read it we''re just going to look at the\n2:14:03 model Json schema method and then we can see what that contains which is all of this information so this actually\n2:14:10 contains everything includes properties so we have the X it C or title for that\n2:14:16 and it also specifies the type okay so the type that we Define is float float\n2:14:23 for open AI gets mapped to number rather than just being float and then we also\n2:14:28 see that we have this required field so this is telling how LM which parameters\n2:14:33 are required which ones are optional so we yeah in some cases you would we can\n2:14:39 even do that here let''s do Z that is going to be float or none okay and we\n2:14:48 just going to say it is 0.3 all right well I''m going to remove this\n2:14:53 in a minute because it''s kind of weird but let''s just see what that looks like so you see that we now have X Y and\n2:15:02 Z but then in Z we have some additional information okay so it can be any of it\n2:15:08 can be a number or it can just be nothing the default value for that is 0.3 okay and then if we look here we can\n2:15:16 see that the required field does not include Z so it''s just X and Y so it''s\n2:15:21 describing the full function schema for us but let''s remove\n2:15:27 that okay and we can see that again with our exponentiate tool similar thing okay\n2:15:33 so how how are we going to invoke our tool so the llm the underlying LM is\n2:15:41 actually going to generate a string okay so we''ll look something like this this is going to be our llm output so it is\n2:15:50 it''s a string that is some Json and of course to load a string into a\n2:15:56 dictionary format we just use Json loes okay so let''s see that so this could be\n2:16:03 the output Fromm we load it into a dictionary and then we get an actual dictionary and then what we would do is\n2:16:11 we can take our exponentiate uh tool we access the underlying function and then\n2:16:17 we pass it the keyword arguments from our diction here\n2:16:24 okay and that will execute our tool that is the tool execution log you that line chain implements and then later on in\n2:16:31 the next chapter we''ll be implementing ourselves cool so let''s move on to creating an agent now we''re going to be\n2:16:38 constructing a simple tool calling agent we''re going to be using Lang chain expression language to do this now we\n2:16:45 will be covering Lang chain expression language or also more in a upcoming\n2:16:50 chapter but for now all we need to know is that our agent will be constructed\n2:16:57 using syntax and components that like this so we would start with our input\n2:17:03 parameters that is going to include our user query and of course the chat history because we need our agent to be\n2:17:09 conversational and remember previous interactions within the conversation these input parameters will also include\n2:17:15 a placeholder for what we call the agent scratch Pad now the agent scratch Pad is essentially where we are storing the\n2:17:22 internal thoughts or the internal dialogue of the agent as it is using tools getting observations from those\n2:17:28 tools and working through those multiple internal steps so in the case that we\n2:17:34 will see it will be using for example the addition tool getting the result using the multiply tool getting the\n2:17:40 result and then providing a final answer to us as a user so let''s jump in and see\n2:17:46 what it looks like okay so we''ll just start with defining our prompt so our prompt is going to include the system\n2:17:52 message there nothing we''re not putting anything special in there we''re going to\n2:17:57 include the chat history which is a messages placeholder then we include our\n2:18:03 human message and then we include a placeholder for the agent scratch Pad\n2:18:08 now the way that we implement this later is going to be slightly different for the scratch Pad we actually use this\n2:18:13 message''s placeholder but this is how we use it with the built-in create tool agent from BL chain next we sign our LM\n2:18:21 we do need our Opening Our API key for that so we''ll enter that here like so\n2:18:28 okay so come down Okay so we''re going to be creating this agent we need conversation memory and we are going to\n2:18:34 use the older conversation buffer memory class rather than the newer renable with message history class that''s just\n2:18:40 because we''re also using this older create tool calling agent and this is\n2:18:46 this is the older way of doing things in the next chapter we are going to be using the more recent basically what we\n2:18:54 already learned on chat history we''re going to be using all of that to implement our chat history but for now\n2:19:00 we''re going to be using the older method uh which is deprecated just as a pre-warning but again as I mentioned at\n2:19:07 the very solid of course we''re starting abstract and then we''re getting into the details so we''re going to initialize our\n2:19:15 agent for that we need these four things LM as we defined tools as we have\n2:19:20 defined prompt as we have defined and then the memory which is our old conversation\n2:19:26 buffer memory so with all of that we are going to go ahead and we create a tool\n2:19:32 calling agent and then we just provide it with everything okay there we go now H you''ll see here I didn''t pass\n2:19:40 in the the memory I''m passing it in down here instead so we''re going to start with this question which is what is 10.7\n2:19:47 MTI 7.68 eight okay so given the Precision of\n2:19:55 these numbers our l a normal LM would not be able to answer that or almost\n2:20:01 definitely will not be able to answer that correctly we need a external tool to answer that accurately and we''ll see\n2:20:08 that that is exactly what it''s going to do so we can see that the tool agent\n2:20:15 action message here we see that it decided okay I''m going to use the multiply tool and here at parameters\n2:20:21 that I want to use for that tool okay we can see X is 10.7 and Y is\n2:20:26 7.68 you can see here that this is already a dictionary and that is because\n2:20:32 Lang chain has taken the string from our llm C and already converted it into a\n2:20:38 dictionary for us okay so that''s just it''s happening behind the scenes there and you can actually see if we go into\n2:20:44 the details a little bit we can see that we have these arguments and this is the original string that was coming fromn\n2:20:50 okay which has already been of of course processed by line chain so we have that\n2:20:56 now the one thing missing here is that okay we''ve got that the LM\n2:21:03 wants us to use multiply and we''ve got what the LM wants us to put into modly but where''s the answer right there is no\n2:21:11 answer because the tool itself has not been executed because it can''t be executed by the llm but then okay didn''t\n2:21:19 we already Define our agent here yes redefined the part of our agent that is\n2:21:26 how llm has our tools and it is going to generate which tool to use but it actually doesn''t include the\n2:21:33 agent execution part which is okay the agent executor is a broader thing it''s\n2:21:41 it''s broader logic like just code logic which acts as a scaffolding within which\n2:21:47 we have the iteration through multiple steps of our llm calls followed by the\n2:21:54 llm outputting what tools use followed by us actually executing that for the llm and then providing the output back\n2:22:02 into the llm for another decision or another step so the agent itself here is\n2:22:08 not the full agentic flow that we might expect instead for that we need to\n2:22:15 implement this agent executor class this agent executor includes our agent from\n2:22:21 before then it also includes the tools and one thing here is okay we we already\n2:22:26 passed the tools to our agent why do we need to pass them again well the tools being passed to our agent up\n2:22:32 here that is being used so that is essentially extracting out those\n2:22:38 function schemers and passing it to our LM so that our LM knows how to use the tools then we''re down here we''re passing\n2:22:44 the tools again to our agent executor and this is rather than looking at how to use those tools this is just\n2:22:50 looking at okay I want the functions for those tools so that I can actually execute them for the llm or for the\n2:22:57 agent okay so that''s why it''s happening there now we can also pass in our memory\n2:23:02 directly so you see if we scroll up a little bit here I actually had to pass\n2:23:08 in the memory like this with our agent that''s just because we weren''t using the agent executor now we have the agent\n2:23:14 executor it''s going to handle that for us and another thing that''s going to handle for us is it intermediate steps\n2:23:21 so you''ll see in a moment that when we invoke the agent executor we don''t include the intermediate steps and\n2:23:27 that''s because it that is already handled by the agent executor now so we''ll come down we''ll set the both equal\n2:23:34 to true so we can see what is happening and then we can see here there''s no\n2:23:39 intermediate steps anymore and we we do still pass in the chat history like this\n2:23:46 but then the addition of those new interactions to our memory is going to be handled by the\n2:23:52 executor so let me actually show that very quickly before we jump in okay so\n2:23:59 that''s cently empty we''re going to execute this okay we entered that new Asian\n2:24:05 execute chain let''s just have a quick look at our messages again and now you can see that the agent\n2:24:11 executor automatically handled the addition of our human message and then the responding AI message for us okay\n2:24:19 which is useful now what happened so we can see that the multiply tool was\n2:24:25 invoked with these parameters and then this pink text here that we got that is\n2:24:30 the observation from the tool assist what the tool output back to us okay then this final message here it''s not\n2:24:37 formatted very nicely well this final message here is coming from our llm so the green is our llm output the pink is\n2:24:45 our tool output okay so the LM after seeing this output says 10.7 MTI by 7.68\n2:24:56 is approximately 82.8 okay cool use and then we can also\n2:25:03 see the the chat history which we we already just saw great so that has been\n2:25:08 used correctly we can just also confirm that that is correct okay 82\n2:25:15 1759 recurring which is exactly what we get here okay and we the reason for all\n2:25:20 that is obviously how multiply tool is just doing this exact operation cool so let''s try this with a\n2:25:29 bit of memory so I''m going to ask or I''m going to sayate to the agent hello my\n2:25:34 name is James we''ll leave that as the it''s not actually the first interaction because\n2:25:40 we already have these but it''s an early interaction with my name in there then\n2:25:48 we''re going to try and perform more tool calls within a single execution Loop and what you''ll see with when it is calling\n2:25:54 these tools is that it can actually use multiple Tools in parallel so for sure I think two or three of these were used in\n2:26:00 parallel and then the final subtract had to wait for those previous results so it would have been executed afterwards and\n2:26:08 we should actually be able to see this in Langs Smith so if we go here yeah we\n2:26:13 can see that we have this initial cord and then we have add and multiply and exponentially we all use in parallel\n2:26:20 then we have another call which use subtract and then we get the response okay which is pretty cool and\n2:26:27 then the final result there is11 now when you look at whether the\n2:26:33 answer is accurate I think the order here of calculations is not quite\n2:26:39 correct so if we put the actual computation here it gets it right but\n2:26:45 otherwise if I use natural language it''s like I''m doing maybe I''m phrasing it in a in a poor way\n2:26:52 okay so I suppose that is pretty important so okay if we put the computation in here we get\n2:26:59 the13 so it''s something to be careful with and probably requires a little bit of prompting to promting and maybe\n2:27:07 examples in order to get that smooth so that it does do things in the way that\n2:27:12 we might expect or maybe we as humans are just bad and misus the systems one\n2:27:18 or the other okay so now we''ve gone through that a few times let''s go and see if our agent can still recall our\n2:27:25 name okay and it remembers my name is James good so it still has that memory in there as well that''s good let''s move\n2:27:32 on to another quick example where we''re just going to use Google search so we''re going to be using the Ser\n2:27:39 API you can okay you can get the API key that you need from here so Ser ai.com\n2:27:46 usersign in and just enter that in here so you will get it up to 100 stes per\n2:27:54 month for free so just be aware of that if you overuse it I don''t think they\n2:28:00 charge you cuz I don''t think you enter your card details straight away but yeah just be aware of that\n2:28:07 limit now there are certain tools that line chain have already built for us so\n2:28:12 they''re pre-built tools and we can just load them using the load tools function so we do that like so we have our load\n2:28:19 tools and we just pass in the Ser API tool only we could pass in more there if we want to and then we also pass in our\n2:28:26 LM now I''m going to one use that tool but I''m also going to Define my own tool\n2:28:32 which is to get the current location based on the IP address now this is we''re in collab at the moment so it''s\n2:28:38 actually going to get the IP address for the collab instance that I''m currently on and we''ll find out where that is so\n2:28:45 that is going to get the IP address and then it''s going to provide the data back to our LM this format here so we''re\n2:28:51 going to latitude longitude City and Country okay we''re also going to get the current day and time so now we''re going\n2:28:59 to redefine our prompt I''m not going to include chat history here I just want this to be like a one shot\n2:29:06 thing I''m going to redefine our agent and agent executor using our new tools which is our set API plus to get current\n2:29:14 date time and get location from IP then I''m going to invoke our agent executor\n2:29:20 with I have a few questions what is the date and time right now how is the weather where I am and please give me\n2:29:27 degrees in celce so when it gives me that weather okay and let''s see what we\n2:29:33 get okay so apparently we''re in Council Bluffs in the\n2:29:39 US it is 13 fah which I think is absolutely freezing oh my gosh it is yes\n2:29:46 minus 10 so it''s super cold over there and you can see that okay it did give us\n2:29:53 Fahrenheits that is because the tool that we were using provided us with Fahrenheit which is fine but it did\n2:30:00 translate that over into a estimate of Celsius fours which is pretty cool so let''s actually output that so we get\n2:30:08 this which I is correct we do us approximately this and\n2:30:14 we also get an description of the conditions as well as partly cloudy with z % precipitation lucky for\n2:30:21 them and humidity of 66% okay well pretty cool so that is it\n2:30:27 for this introduction to Lang chain agents as I mentioned next chapter we''re going to dive much deeper into agents\n2:30:34 and also Implement that for Lang chain version 0.3 so we''ll leave this chapter here and jump into the next one in this\n2:30:41 chapter we''re going to be taking a deep dive into agents with the Lang chain and\n2:30:48 we''re going to be covering what an agent is we''re going to talk a\n2:30:53 little bit conceptually about agents the react agent and the type of agent that\n2:30:59 we''re going to be building and based on that knowledge we are actually going to build out our own agent execution logic\n2:31:07 which we refer to as the agent executor so in comparison to the previous video\n2:31:14 on agents in line chain which is more of an introduction this is far far more\n2:31:20 detailed we''ll be getting into the weeds a lot more with both what agents are and\n2:31:25 also agents within Lang chain now when we talk about agents a significant part\n2:31:31 of the agent is actually relatively simple code\n2:31:37 logic that iteratively runs llm calls\n2:31:42 and processes their outputs potentially running or executing tools the exact\n2:31:49 logic for each approach to building an agent will actually vary pretty\n2:31:55 significantly but we''ll focus on one of those which is the react agent now react\n2:32:02 is it''s a very common pattern and although being relatively old now most\n2:32:08 of the tool agents that we see used by openai and essentially every LM company\n2:32:15 they all use a very similar pattern now the reactor agent follows a patter and like this okay so we would have our user\n2:32:23 input up here okay so our input here is a question right aside from the Apple\n2:32:29 remote what other device you can control the program Apple remote was originally designed to interact with now probably\n2:32:35 most LMS would actually be able to answer this directly now this is from the paper which was a few years back now\n2:32:42 in this scenario assuming our LM didn''t already know the answer there are most steps\n2:32:48 that an llm or an agent might take in order to find out the answer okay so the\n2:32:54 first of those is we say our question here is what other device can control the program Apple remote was originally\n2:33:01 designed to interact with so the first thing is okay what was the program that the Apple remote was originally designed\n2:33:07 to interact with that''s the first question we have here so what we do is I\n2:33:13 need to search Apple remote and find a program it was use for this is a reasoning step so the llm is reasoning\n2:33:19 about what it needs to do I need to search for that and find a program useful so we are taking an action this\n2:33:26 is a tool call here okay so we''re going to use the search tool and our query will be apple remote and the observation\n2:33:33 is the response we get from executing that tool okay so the response here would be the Apple remote it''s designed\n2:33:39 to control the front row mediate Center so now we know the programmer for was\n2:33:45 originally designed to interact with now we''re going to go through another it\n2:33:50 okay so this is one iteration of our reasoning action and\n2:33:56 observation so when we''re talking about react here although again this sort of\n2:34:02 pattern is very common across many agents when we''re talking about react\n2:34:07 the name actually is reasoning or the first two characters of re reasoning\n2:34:13 followed by action okay so that''s where the react comes from so this is one of\n2:34:19 our react agent Loops or iterations we''re going to go and do another one so\n2:34:24 next step we have this information the LM is now provided with this information now we want to do a search for front row\n2:34:32 okay so we do that this is the reasoning step we per the action search front row\n2:34:38 okay tool search query front row observation this is the response front\n2:34:43 row is controlled by an apple remote or keyboard function keys all right cool so\n2:34:50 we know keyboard function keys are the other device that we were asking about up here so now we have all the\n2:34:58 information we need we can provide an answer to our user so we go through\n2:35:04 another iteration here reasoning and action our reasoning is I can now\n2:35:09 provide the answer of keyboard function keys to the user okay great so then we\n2:35:16 use the answer tool like Final Answer In more common tool agent use and the\n2:35:25 answer would be keyboard function keys which we then output to our user okay so\n2:35:32 that is the react Loop okay so looking at this how where are we actually calling\n2:35:40 an llm and what and in what way are we actually calling llm\n2:35:46 so we have our reasoning step our LM is generating the text here right so LM is\n2:35:52 generating okay what should I do then our LM is going to generate input\n2:35:59 parameters to our action step here that will th those input parameters and and\n2:36:05 the tool being used will be taken by our code logic our agent executor logic and\n2:36:10 they''ll be used to execute some code in which we will get an output that output\n2:36:15 might be taken directly to our observation or our llm might take that output and then generate an observation\n2:36:22 based on that it depends on how you''ve implemented everything so our LM could\n2:36:29 potentially being be being used at every single step there and of course that\n2:36:35 will repeat through every iteration so we have further iterations down here so\n2:36:41 you''re potentially using LM multiple times throughout this whole process which of course in terms of latency and\n2:36:47 token cost it does mean that you''re going to be paying more for an agent\n2:36:53 than you are with just a sun LM but that that is of course expected because you have all of these different things going\n2:36:59 on but the idea is that what you can get out of an agent is of course much better\n2:37:05 than what you can get out of an LM alone so when we''re looking at all of this all\n2:37:11 of this iterative Chain of Thought and Tool use all this needs to be controlled\n2:37:17 by what we call the agent executor okay which is our code logic which is hitting our llm processing its outputs and\n2:37:25 repeating that process until we get to our answer so breaking that part down what does it actually look like it looks\n2:37:32 kind of like this so we have our user input goes into our llm okay and then we move on to the\n2:37:39 reasoning and action steps is the action the answer if it is the answer so as we\n2:37:47 saw here where is the answer if the action is the answer so true we\n2:37:53 would just go straight to our outputs otherwise we''re going to use our select tool agent executor is going to handle\n2:37:59 all this it''s going to execute our tool and then from that we get our you know\n2:38:04 three reasoning action observation inputs and outputs and then we''re feeding all that information back into\n2:38:11 our llm okay in which case we go back through that Loop so we could be looping\n2:38:16 for a little while until we get to that final but okay so let''s go across to the\n2:38:22 code when be going into the agent executor notebook we''ll open that up in\n2:38:27 coab and we''ll go ahead and just install our prerequisites nothing different here is\n2:38:34 just L chain L Smith optionally as before again optionally line chain API\n2:38:40 key if you do want to use l Smith okay and then we''ll come down to our first\n2:38:47 section where it''s going to define a few quick tools I''m not necessarily going to\n2:38:52 go through these because we''ve already covered them in the agent introduction\n2:38:58 but very quickly Lang chain core tool is we''re just importing this tool decorator which transforms each of our functions\n2:39:05 here into what we would call a structured tool object this thing here\n2:39:12 okay which we can see just having a quick look here and then if we want to we can extract all of the sort of key\n2:39:19 information from that structure tool using these parameters here or attributes so name description AR\n2:39:25 schemer model Json streer which give us essentially how the llm should use our\n2:39:32 function okay so I''m going to keep pushing through that now very quickly\n2:39:40 again we did cover this in the intro video so I don''t want to necessar go over again into much detail but our\n2:39:48 agent EX future logic is going to need this part so we''re going to be getting a\n2:39:53 string from our llm we''re going to be loading that into to a dictionary object and we''re going to be using that to\n2:40:00 actually execute our tool as we do here using keyword\n2:40:05 arguments okay like that okay so with the tools out of the way let''s take a\n2:40:11 look at how we create our agent so when I say agent here I''m specifically\n2:40:16 talking about the part that is generating our reasoning St then generating which\n2:40:24 tool and what the input parameters to that tool will be then the rest of that\n2:40:29 is not actually covered by the agent okay the rest of that would be covered by the agent execution logic which would\n2:40:35 be taking the tool to be used the parameters executing the tool getting\n2:40:41 the response aka the observation and then iterating through that until the llm is satisfied and we have enough\n2:40:47 information to answer a question so looking at that our agent we look\n2:40:53 something like this it''s pretty simple so we have our input parameters including the chat history user query we\n2:41:00 have our input parameters including the chat history us query and actually would also have any intermediate STS that have\n2:41:07 happened in here as well we have our prompt template and then we have our llm binded with tools so let''s see how all\n2:41:15 this would look starting with we''ll Define our promp template searching look\n2:41:20 like this we have our system message your helpful assistant when answering these question you should use on to\n2:41:27 provide after using a tool tool outp will provide in the scratch Pad below okay which we naming here if you have an\n2:41:34 answer in scratch Pad you should not use any more tools and set answer directly to the user okay so we have that as our\n2:41:41 system message we could obviously modify that based on what we''re actually doing\n2:41:46 then following our system message we''re going to have our chat history so any previous interactions between the user\n2:41:52 and the AI then we have our current message from the user okay we should be\n2:41:57 fed into the input field there and then following this we have our agent stretch\n2:42:03 pad or the intermediate thoughts so this is where things like the llm deciding\n2:42:09 okay this is what I need to do this is how I''m going to do it AKA The Tool call and this is the observation that''s where\n2:42:15 all of that information will be going right so each of those to pass in as a\n2:42:21 message okay and the way that we look is that any tool call generation from the\n2:42:27 llm so when the llm is saying use this tool please that will be a assistant\n2:42:32 message and then the responses from our tool so the\n2:42:37 observations they will be returned as tool messages great so we''ll run that to\n2:42:44 Define our prompt template we''re going to Define our LM we''re going to be using\n2:42:49 J2 40 mini with a temperature of zero because we want less creativity here\n2:42:55 particularly when we''re doing tour calling there''s just no need for us to use a high temperature here so we need\n2:43:01 to enter our open ey API key which we would get from platform open ey.com we enter this then we''re going to continue\n2:43:09 and we''re just going to add tools to our LM here\n2:43:15 okay these and we''re going to bind them here then we have tool Choice any so\n2:43:22 tool Choice any we we''ll see in a moment I''ll go through this a little bit more in a second but that''s going to\n2:43:28 essentially force a tool call you can also put required which is actually a bit more uh it''s bit clearer but I''m\n2:43:35 using any here so I''ll stick with it so these are our tools we''re going through we have our inputs into the agent\n2:43:42 runable we have our prom template and then that will get fed into our llm so\n2:43:48 let''s run that now we would invoke the agent part of everything here with this okay so\n2:43:55 let''s see what it outputs this is important so I''m asking what is 10+ 10 obviously that should use the addition\n2:44:01 tool and we can actually see that happening so the agent message content is actually empty here this is where\n2:44:08 you''d usually get an answer but if we go and have a look we have additional keyword dos in there we have tool calls\n2:44:16 and then we have function arguments Okay so we''re calling a function Arguments for that function are this okay so we\n2:44:24 can see this is string again the way that we would pass that as we do Json loads and that becomes a dictionary and\n2:44:30 then we can see which function is being called and it is the add function and that is all we need in order to actually\n2:44:37 execute our function or our our tool okay we can see it''s a little more\n2:44:43 detail here now what do we do from here we''re going to map the to name to the\n2:44:49 tool function and then we''re just going to execute the tool function with the generated ARS I\n2:44:55 those I''ll also just point out quickly that here we are getting the dictionary directly which I think is coming from\n2:45:02 somewhere else in this which is prob which is here okay so even that step\n2:45:08 here where we''re passing this out we don''t necessarily need to do that because I think on the L chain side\n2:45:14 they''re doing it for us so we''re already getting that so Json loads we don''t\n2:45:20 necessarily need here okay so we''re just creating this tool name to function\n2:45:25 mapping dictionary here so we''re taking the well the tool names and we''re just mapping those back to our tool functions\n2:45:32 and this is coming from our tools list so that tools list that we defined here\n2:45:37 okay or can even just see quickly that that will include everything or each of\n2:45:43 the tools you define there okay that''s all it is now we''re going to execute\n2:45:49 using our name to Tool mapping okay so this here will get us the function so it will get us this\n2:45:56 function and then to that function we''re going to pass the arguments that we\n2:46:02 generated okay let''s see what it looks like all right so the response so the observation\n2:46:10 is 20 now we are going to feed that back\n2:46:15 into our llm using the tool message and we''re actually going to put a little bit of text around this to make it a little\n2:46:22 bit nice so we don''t necessarily need to do this to be completely honest we could\n2:46:27 just return the answer directly uh I don''t understand I don''t even think\n2:46:33 there would really be any difference so we we could do either in some cases that\n2:46:38 could be very useful in other cases like here it doesn''t really make too much difference particularly because we have\n2:46:44 this tool call ID and what this tool call ID is doing is it''s being used by AI is being read by the LM so that the\n2:46:52 LM knows that the response we got here is actually mapped back to the the tool\n2:47:01 execution that it''s identified here because you see that we have this ID right we have an ID here the LM is going\n2:47:08 to see the ID it''s going see the ID that we pass back in here and it''s going to\n2:47:13 see those two are connected so see okay this is the tool I called and this is the response I got from\n2:47:19 because of that you don''t necessarily need to say which tool you used here you can it it depends on what you''re\n2:47:27 doing okay so what do we get here we have okay just running everything again\n2:47:34 we''ve added our tool call so that''s the original AI message that includes okay user add tool and then we have the tool\n2:47:40 execution tool message which is the observation we map those to the agent\n2:47:46 scratch pad and then what do we get we have an AI message but the content is empty again which is interesting because\n2:47:53 we we said to our llm up here if you have an answer to the in the scratchpad\n2:47:59 you should not use any more tools and said answer directly to the user so why why is our\n2:48:06 llm not answering well the reason for that is down here we specify tool Choice\n2:48:15 equals any which again it''s the same tool Choice required which is telling\n2:48:22 the L land that it cannot actually answer directly it has to use a tool and\n2:48:28 I usually do this right I would usually put tool Choice equals any or required and for the LM to use a tool every\n2:48:36 single time so then the question is if it has to use a tool every time how does\n2:48:41 it answer our user well we''ll see in a moment first I just want to show you\n2:48:49 the two options essentially that we have the second is what I would usually use but let''s let''s start with the first so\n2:48:56 the first option is that we set tool Choice equal to Auto and this tells the Ln that it can either use a tool or it\n2:49:03 can answer the user directly using the the final answer or using that content\n2:49:09 field so if we run that like we''re specifying to choices Auto we run that\n2:49:15 let''s invoke okay initially you see ah wait there''s still no content that''s because\n2:49:21 we didn''t add anything into the agent scratch Pad here there''s no information right it''s all\n2:49:27 empty um actually it''s empty because sorry so here you have the chat history that''s empty we didn''t specify the agent\n2:49:36 scratch Cad and the reason that we can do that is because we''re using if you look here we''re using get so essentially\n2:49:42 it''s saying try and get agent scratch pad from this dictionary but if it hasn''t been provided we''re just going to\n2:49:48 give an empty list so that''s what that''s why we don''t need to specify it here but\n2:49:54 that means that oh okay the the agent doesn''t actually know anything here it hasn''t used a tool yet so we''re going to\n2:50:01 just go through our iteration again right so we''re going to get our tool output we''re going to use that to create\n2:50:07 the tool message and then we''re going to add our tool call from the AI and the\n2:50:14 observation we''re going to pass those to the agent scratch pad and this time we see we run that okay now we get the\n2:50:22 content okay so now it''s not calling you see here there''s no to call or anything going\n2:50:28 on we just get content so that is this is the standard\n2:50:34 way of doing or building a tool calling agent the other option which I mentioned\n2:50:40 this is what I would usually go with so number two here I would usually create a\n2:50:45 final answer tool so why would we even do that why would we\n2:50:53 create a final answer tool rather than just you know this method is actually perfectly you know it works so why would\n2:50:59 we not just use this there are a few reasons the main ones are that with\n2:51:05 option two where we''re forcing tool calling this removes possibility of an\n2:51:11 agent using that content field directly and the reason at least the reason I\n2:51:17 found this good when building agents in the past is that occasionally when you do want to use a tool it''s actually\n2:51:22 going to go with the content field and it can get quite annoying and and use the content field quite frequently when\n2:51:29 you actually do want it to be using one of the tools and this is particularly\n2:51:36 noticeable with smaller models with bigger models it''s not as common\n2:51:41 although does so happen now the second thing that I quite like about using a\n2:51:47 tool as your final answer is that you can enforce a\n2:51:52 structured output in your answer so this is something we''re stting I think the first yes the first line chain example\n2:52:01 where we were using the structured output tool of Lang chain and what that\n2:52:06 actually is the structured outputs feature of Lang chain it''s actually just a tool call right so it''s forcing a tool\n2:52:13 call from your LM it''s just abstracted away so you don''t realize that that''s what it''s doing but that is what it''s doing\n2:52:20 so I find that structured outputs are very useful particularly when you have a\n2:52:26 lot of code around your agent so when that output needs to go Downstream into\n2:52:32 some logic that can be very useful because you can you have a reliable\n2:52:39 output format that you know is going to be output and it''s also incredibly useful if you have multiple outputs or\n2:52:47 multiple fields that you need to generate for so those can be very useful\n2:52:53 now to implement this so to implement option two we need to create a final answer tool we as with our other tools\n2:53:02 we''re actually going to description and you can or you cannot do this so you can\n2:53:08 you can also just return non and actually just use the generated\n2:53:14 action as the essentially what you''re going to send out of your agent\n2:53:19 execution logic or you can actually just execute the tool and just pass that\n2:53:24 information directly through perhaps in some cases you might have some additional postprocessing for your final\n2:53:31 answer maybe you do some checks to make sure it hasn''t said anything weird you could add that in this tool\n2:53:37 here but yeah in in this case we''re just trying to pass those through directly\n2:53:43 so let''s run this we''ve added where are we\n2:53:48 Final Answer we''ve added the final answer tool to our named tool mapping so our agent can now use it we redefine our\n2:53:56 agent setting tool choice to any because we''re forcing the tool Choice here and let''s go with what is 10 + 10 see what\n2:54:04 happens okay we get this right we can also one thing nice thing here is that\n2:54:10 we don''t need to check is out up in the content field or is it in the tool course field we know it''s going to be in\n2:54:15 the tool course field because we''re forcing that tool use quite nice so okay we know we''re using the ad tool and\n2:54:22 these are the arguments great we go or go through our process again we''re going\n2:54:27 to create our tool message and then we''re going to add those messages into our scratch pad or intermediate sets and\n2:54:34 then we can see again ah okay content field is empty that is expected we we''re\n2:54:41 forcing tool users no way that this can be this can be or have anything inside\n2:54:46 it but then if we come down here to our to calls nice final answer arbs answer\n2:54:53 10 + 10 = 20 all right we also have this tools used where''s tools used coming\n2:55:00 from okay while I mentioned before that you can add additional things or or\n2:55:06 outputs when you''re using this tool use for your final answer so if you just\n2:55:11 come up here to here you can see that I asked the llm to use that Tool''s use\n2:55:18 field which I defined here it''s a list of strings use this to tell me what tools you used in your answer right so\n2:55:25 I''m getting the normal answer but I''m also getting this information as well which is kind of nice so that''s where\n2:55:30 that is coming from see that okay so we have our actual answer here and then we\n2:55:36 just have some additional information okay and we''ve also defined a type here it''s just a list of strings which is\n2:55:41 really nice it''s giving us a lot of control over what we''re outputting which is perfect that''s you know when you''re\n2:55:47 building with agents the biggest problem in most cases\n2:55:52 is control of your llm so here we''re getting a honestly pretty unbelievable\n2:56:01 amount of control over what our LM is going to be doing which is perfect for\n2:56:06 when you''re building in the real world so this is everything we need this\n2:56:13 is our answer and we would of course be passing that Downstream into whatever log\n2:56:19 our AI application would be using okay so maybe that goes directly to a front\n2:56:25 end and we''re displaying this as our answer and we''re maybe providing some information about okay where did this\n2:56:31 answer come from or maybe there''s some additional steps Downstream where we''re\n2:56:36 actually doing some more processing or Transformations but yeah we have that that''s great now everything we''ve just\n2:56:44 done here we''ve been executing everything one by one and that''s to help us understand what process we go through\n2:56:53 when we''re building an agent executor but we''re not going to want to\n2:56:59 do that all the time are we most of the time we probably want to abstract all this away and that''s what we''re going to\n2:57:05 do now so we''re going to build essentially everything we''ve just taken\n2:57:11 we''re going to abstra take that and Abstract it away into a custom agent\n2:57:16 executor class so let''s have a quick look at what we''re doing here although it''s it''s literally just what we we just\n2:57:23 did okay so custom maor executor we initialize it we set this m\n2:57:29 Max iterations I''ll talk about this in a moment we initialize it that is going to set out chat history to just being empty\n2:57:38 okay it''s a new agent there should be no chat history in this case then we actually Define our agent right so that\n2:57:44 poted logic that is going to be taking out inputs and generating what to do next AKA what tool call to do okay and\n2:57:52 we set everything as attributes of our class and then we''re going to Define an\n2:57:58 invoke method this invoke method is going to take an input which just a\n2:58:03 string so it''s going to be our message from the user and what it''s going to do is it''s\n2:58:09 going to iterate through essentially everything we just did okay until we hit\n2:58:15 the The Final Answer tool Okay so well what does that mean we have our\n2:58:21 tool call right which is we''re just invoking our agent right so it''s going to generate what tool to use and what\n2:58:28 parameters should go into that okay and that''s a that''s an AI message so we would append that to our\n2:58:36 agent stretch pad and then we''re going to use the information from our tool call so the name of the tool and the ARs\n2:58:42 and also the ID we''re going to use all of that information to execute our tool\n2:58:49 and then provide the observation back to our llm okay so we execute our tool here\n2:58:55 we then format the tool output into a tool message see here that I''m just\n2:59:01 using the the output directly I''m not adding that additional information there\n2:59:06 we need do need to always pass in the tool call ID so that our LM knows which\n2:59:12 output is mapped to which tool I didn''t mention this before in in this video at\n2:59:17 least but that is that''s important when we have multiple toour calls happening in parallel because that can happen when\n2:59:23 we have multiple toour calls happening in parallel let''s say we have 10 tool calls all those responses might come\n2:59:28 back at different times so then the order of those can get messed up so we\n2:59:34 wouldn''t necessarily always see that it''s a AI message beginning a tool call followed\n2:59:41 by the answer to that tool call instead it might be AI message followed by like 10 different tool call responses so you\n2:59:49 need to have those IDs in there okay so then we pass our tool output back to our\n2:59:57 agent scratch pad or intermediate steps I''m sing a print in here so that we can see what''s happening whilst everything\n3:00:03 is running then we increment this count number we''ll talk about that in a moment\n3:00:08 so com past that we say okay if the tool name here is final answer that means we\n3:00:15 should stop okay so so once we get the final answer that means we can actually\n3:00:21 extract our final answer from the the final tool call okay and in this case\n3:00:26 I''m going to say that we''re going to extract the answer from the tool call or\n3:00:33 the the observation we''re going to extract the answer that was generated we''re going to pass that into our chat\n3:00:40 history so we''re going to have our user message is the one the user came up with followed by our answer which is just the\n3:00:47 the natur answer field and that''s going to be an AI message but then we''re actually going to be including all of\n3:00:53 the information so this is the the answer natural language answer and also\n3:00:59 the tools used output we''re going to be feeding all of that out to some\n3:01:04 Downstream process as preferred so we have that now one thing that can happen if\n3:01:12 we''re not careful is that our agent executor might may run many many times\n3:01:19 and particularly if we''ve done something wrong in our logic as we''re building these things it can happen that maybe\n3:01:26 we''ve not connected the observation back up into our agent executor logic and in\n3:01:33 that case what we might see is our agent executor runs again and again and again and I mean that''s fine we''re going to\n3:01:38 stop it but if we don''t realize straight away and we''re doing a lot of llm cords\n3:01:44 that can get quite expensive quite quickly so what we can do is we can set a limit right so that''s\n3:01:50 what we''ve done up here with this Max iterations we said okay if we go past three max iterations by default I''m\n3:01:56 going to say stop all right so that''s that''s why we have the count here while\n3:02:01 count is less than the max iterations we''re going to keep going once we hit the number of Max iterations we stop\n3:02:08 okay so the while loop will will just stop looping okay so it just protects Us\n3:02:14 in case of that and it also potentially maybe it''s Point your agent might be\n3:02:19 doing too much to answer a question so this will force it to stop and just provide an answer although if that does\n3:02:26 happen I just realize there''s a bit of a fault in the logic here if that does happen we wouldn''t necessarily have the\n3:02:33 answer here right so we would probably want to handle that nicely but in this\n3:02:39 scenario a very simple use case we''re not going to see that happening so we\n3:02:44 initialize our custom agent executor and then we invoke\n3:02:50 it okay and let''s see what happens all right there we go so that just wrapped\n3:02:56 everything into a single single invoke so everything is handled for us uh we\n3:03:03 could say okay what is 10 you know we can modify that and say 7.4 for example\n3:03:12 and that we''ll go through we''ll use the multiply tool instead and then we''ll come back to the final answer again okay\n3:03:18 so we can see that with this custom agent executor we''ve built an agent and\n3:03:24 we have a lot more control over everything that is going on in here one thing that we would probably need to add\n3:03:34 in this scenario is right now I''m assuming that only one tool call will happen at once it''s also why I''m asking\n3:03:39 here I''m not asking a complicated question because I don''t want it to go and try and execute multiple tool Calles\n3:03:46 at once uh which which can happen so let''s just try\n3:03:51 this okay so this is actually completely fine so this did just execute it one after the other so you can see that when\n3:03:59 asking this more complicated question it first did the exponentiate tool followed\n3:04:05 by the ad tool and then they actually gave us our final answer which is cool also told us we use both of those tools\n3:04:12 which it did but one thing that we should just be aware of is that from\n3:04:18 open AI open AI can actually execute multiple tool calls in parallel so by\n3:04:24 specifying that we''re just using this zero here we''re actually assuming that we''re only ever going to be calling one\n3:04:31 tool at any one time which is not always going to be the case so you would probably need to add a little bit of exual logic there in case of scenarios\n3:04:38 if you''re building an an agent that is likely to be running parallel to calls\n3:04:44 but yeah you can see here actually it''s completely fine so it''s running one after the other okay so with that we\n3:04:50 built our agent executor I know there''s a lot to that and of course you can just\n3:04:56 use the very abstract agent executor in L chain but I think it''s very good to\n3:05:01 understand what is actually going on to build our own agent executor in this case and it sets you up nicely for\n3:05:08 building more complicated or use case specific agent logic as\n3:05:14 well so that is it for this chapter in this chapter we''re going to\n3:05:20 be taking a look at line chains expression language we''ll be looking at the runnables the serializable and\n3:05:27 parallel of those the runable pass through and essentially how we use l\n3:05:33 cell in its full capacity now to do that well what I want to do is actually start\n3:05:40 by looking at the traditional approach to building chains in L chain so to do\n3:05:47 do that we''re going to go over to the ELO chapter and open that Cur up okay so\n3:05:54 let''s come down we''ll do the prerequisites as before nothing measure in here the one thing that is new is Doc\n3:06:01 array because later on as you see we''re going to be using this as an example of\n3:06:08 the parallel capabilities in L cell if you want to use Lim Smith you just need\n3:06:14 to add in your lime train API key okay and then let''s okay so now let''s dive into the\n3:06:20 traditional approach to chains in line chain so the LM chain I think is\n3:06:28 probably one of the first things introduced in line chain if I''m not wrong this take it to prompt and feeds\n3:06:33 into an l and that that''s it it you can also you can add like output passing to\n3:06:40 that as well but that''s optional and I don''t think we''re going to cover here so\n3:06:47 what that might look like is we have for example this promp template here give me a small report on topic okay so that\n3:06:54 would be our prompt template we set up as we usually do with the prom templates\n3:07:01 as we''ve seen before we then Define our LM need our\n3:07:07 open a key for this which as usual we would get from platform.\n3:07:13 open.com then we go ahead I''m just just showing you that you can Ino the LM there then we go ahead\n3:07:20 actually Define a output POS so we do do this I wasn''t sure we did but we would\n3:07:26 then Define our LM chain like this okay so LM chain we adding our prompt adding\n3:07:32 our LM adding our alasa okay this is the traditional\n3:07:39 approach so I would then say Okay retrieve Org the generation and what''s going to do it''s going to give me a\n3:07:45 little report back on on rag okay t a moment but you can see that\n3:07:51 that''s what we get here we can format out nicely as we\n3:07:56 usually do and we get okay look we get a nice little report however the LM chain\n3:08:02 is one it''s quite restrictive right we have to have like particular parameters that have been predefined as being\n3:08:09 usable which is you know restrictive and it''s also been deprecated so you know\n3:08:16 this isn''t the standard way of doing this anymore but we can still use it however the preferred method to building\n3:08:23 this and building anything else really or chains in general in L chain is using El cell right and it''s super simple\n3:08:30 right so we just actually take the prompt lemon Apple P that we had before and then we just chain them together\n3:08:36 with these pipe operators so the pipe operator here is saying take what is output from here and input it into here\n3:08:43 take wi''s output from here and input it into here it''s all it does super simple\n3:08:48 so put those together and we invoke it in the same way and we''ll get the same\n3:08:54 output okay and that''s what we get there is actually a slight difference on what\n3:09:00 we''re getting out from there you can see here we got actually a dictionary but\n3:09:06 that is pretty much the same okay so we get that and as before we can display\n3:09:12 that in markdown with this okay so we saw just now that we have this pipe\n3:09:18 operator here it''s not really\n3:09:23 standard P python syntax to use this or at least it''s definitely not common it''s\n3:09:29 it''s it''s an aberration of the intended use of python I think but anyway it\n3:09:36 does it looks cool and when you understand it I kind of get why they do\n3:09:42 because it make it does make things quite simple in comparison to what it could be otherwise so I kind get it it''s\n3:09:48 a little bit weird but it''s what they''re doing and I''m teaching that so that''s\n3:09:53 what we''re going to learn so what is that pipe operator\n3:09:59 actually doing well it''s as I mentioned it''s\n3:10:04 taking the output from this putting it as input into into what is ever under right but how does that actually work\n3:10:12 well let''s actually implement it ourselves without line chain so we''re going to create this class called\n3:10:18 runnable this class when we initialize it it''s going to take a function okay so this is literally a python function it''s\n3:10:24 going to take that and it''s going to essentially turn it into what we would\n3:10:30 call a runnable in line chain and what does that actually mean well it doesn''t really mean anything it just means that\n3:10:38 when you use run the invoke method on it it''s going to call that function in the\n3:10:43 way that you would have done otherwise all right so using just function you know brackets open parameters brackets\n3:10:50 closed it''s going to do that but it''s also going to add this method this all method now this all method in typical\n3:10:59 python syntax now this all method is essentially going to take your runnable\n3:11:06 function the one that you initialize with and it''s also going to take an other function okay this other function\n3:11:13 is actually going to be a runnable I believe yes it''s going to be runnable just like this and what it''s going to do\n3:11:20 is it''s going to run this runnable based on the output of your current runable\n3:11:28 okay that''s what this or is going to do seems a bit weird maybe but I''ll explain\n3:11:34 in a moment we''ll see why that works so I''m going to chain a few functions\n3:11:39 together using this or method so first we''re just going to turn\n3:11:45 them all into runnables Okay so these are normal functions as you can see normal python functions we then turn\n3:11:51 them into this runnable using our runnable class then look what we can do right so\n3:11:58 we we''re going to create a chain that is going to be our runnable\n3:12:04 chained with another runnable chained with another runnable okay let''s see what happens so we''re going to invoke\n3:12:11 that chain of runnables with three so what is this going to do\n3:12:16 okay we start with five we''re going to add five to three so we''ll get eight\n3:12:22 then we''re going to subtract five from8 to give us three again and then we''re\n3:12:29 going to multiply three by five to give us 15 and we can inval that and we get\n3:12:38 15 okay pretty cool so that is interesting how does that relate to the\n3:12:44 pipe operator well that pipe operator in Python is actually a\n3:12:51 shortcut for the or method so what we just implemented is the pipe operator so\n3:12:56 we can actually run that now with the pipe operator here and we''ll get the same get 15 right so that''s that''s what\n3:13:03 line chain is doing like under the hood that is what that pipe operator is it''s\n3:13:08 just chaining together these multiple runnables as we''d call them using their own internal or operator okay which is\n3:13:17 cool I I I will give them that it''s kind of a cool way of doing this creative I wouldn''t have thought about it\n3:13:24 myself so yeah that is a pipe operator then we have these runnable things okay\n3:13:31 so this is a this is different to the runable I just defined here this is we Define this ourselves it''s not a lang\n3:13:37 chain thing we didn''t get this from Lang chain Instead This runnable Lambda\n3:13:44 object here that is actually exactly the same as what we just defined\n3:13:49 all right so what we did here this runnable this runnable Lambda is the same thing but in Lang\n3:13:58 chain okay so if we use that okay we use that to now Define three runnables from\n3:14:05 the functions that we defined earlier we can actually pair those together now using the the pipe operator you could\n3:14:12 also pair them together if you want with the or operator right\n3:14:18 so we could do what we did earlier we can invoke that okay or as we were doing\n3:14:25 originally we use pipe operator exactly the same so this runnable Lambda from line chain is just\n3:14:31 what we what we just built with the runable cool so we have that now let''s\n3:14:37 try and do something a little more interesting we''re going to generate a report and we''re going to try and edit that report using this this\n3:14:43 functionality okay so give me a small report about topic okay we''ll Z through\n3:14:48 here we''re going to get our report on\n3:14:54 AI okay so we have this you can see that AI is mentioned many times in\n3:15:00 here then we''re going to take a very simple function right so I''m extract\n3:15:07 fact this is basically going to take uh what is it see taking the\n3:15:14 first okay so we''re actually trying to remove the introduction here I''m not sure if this actually will work as\n3:15:20 expected but it''s it''s fine try it anyway but then more importantly we''re\n3:15:28 going to replace this word okay so we''re going to replace an old word with a new word our old word going to be Ai and the\n3:15:34 word is going to be Skynet okay so we can wrap both of these functions as\n3:15:40 runable lambas okay we can add those as additional steps inside our entire chain\n3:15:46 all right so we''re going to extract try and remove the introduction although I think it needs a bit more processing\n3:15:53 than just splitting here and then we''re going to replace the word we need that actually to be AI run that run\n3:16:01 this okay so now we get artificial intelligence Skynet refers to the\n3:16:07 simulation of human intelligent process by machines uh we have narrow Skynet weak\n3:16:12 Skynet and strong Skynet applications of Skynet Skynet Technologies is being applied in\n3:16:18 numerous Fields including all these things scary despite potential sky that poses\n3:16:24 several challenges systems can perpetrate exist and biases it ra significant privacy\n3:16:31 concerns it can be exploited for malicious purposes okay so we have all\n3:16:37 these you know it''s just a silly little example we can see also the introduction didn''t work here the reason for that is\n3:16:43 because our introduction includes multiple new lines here so I would actually if I want to remove the\n3:16:50 introduction we should remove it from here I think and this is a I I would\n3:16:56 never actually recommend you do that uh because it''s not it''s not very flexible\n3:17:01 it''s not very robust but just so I show you that that is actually working so\n3:17:07 this extract fact runnable right so now we''re essentially just removing the\n3:17:14 introduction right why what do we want to do that I don''t know but it''s there just so you can see that we can have\n3:17:20 multiple of these runnable operations running and they can be whatever you want them to be okay it is worth knowing\n3:17:28 that the inputs to our functions here were all single arguments okay if you\n3:17:35 have function that is accepting multiple arguments you can do that the way that I would probably do it or you can do it in\n3:17:42 multiple ways one of the ways that you can do that is actually write your function to except for arguments but\n3:17:49 actually do them through a single argument so just like a single like X which would be like a dictionary or something and then just unpack them\n3:17:56 within the function and and use them as needed that''s just yeah that''s one way you can do it now we also have these\n3:18:02 different uh runnable objects that we can use so here we have runnable parallel and runnable pass through kind\n3:18:10 of self-explanatory to some degree so let me let just go through those so runable parallel allow you to run\n3:18:17 multiple runnable instances in parallel runnable pass through May was less\n3:18:24 self-explanatory allows us to pass a variable through to the next runnable without modifying it okay so let''s see\n3:18:31 how they would work so we''re going to come down here and we''re going to set these two dock arrays obiously these two\n3:18:38 sources of information and we''re going to need our\n3:18:43 LM to pull information from both of these sources of information in parallel which is going to look like this so we\n3:18:49 have these two sources of information Vector store a vector store B this is\n3:18:55 our dock array a and dock array B these are both going to be fed in as\n3:19:00 context into our prompt then our LM is going to use all of that to answer the\n3:19:06 question okay so to actually Implement that we have our we need an embedding\n3:19:12 model so he open our embeddings we have our vetur a a vector B they''re not you\n3:19:18 know real vectors they''re not full-on vectors SS here we''re just passing in a very small amount of information to both\n3:19:26 so we''re saying okay we''re going to create an inmemory vect S using these\n3:19:31 two bits of information so when say half the information is here this would be an irrelevant piece of information then we\n3:19:37 had the relevant information which is deep seek re3 was released in December 2024 okay then we''re going to have some\n3:19:45 other information in our other Vector sore again irrelevant piece here and\n3:19:50 relevant piece here okay the Deep seek V3 LM is a mixure of experts model with\n3:19:56 671 billion parameters at its largest okay so based on that we''re also going\n3:20:03 to build this prompt string so we''re going to pass in both of those contexts into our prompt then I''m going to ask a\n3:20:10 question we don''t actually need we don''t need that bit and actually we don''t even\n3:20:15 need that bit what am I doing so we just need this so we have the both the contexts there and we would run them\n3:20:22 through our prompt template okay so we have our system promp template which is\n3:20:27 this and then we''re just going to have okay our question is going to go into here as a user message cool so we have that and then\n3:20:36 let me make this easier to read we''re going to convert both those\n3:20:41 STS to retrievers which just means we can retrieve stuff from them and we''re going to use this runnable parallel to\n3:20:49 run both of these in parallel right so these are being both being run in\n3:20:55 parallel but then we''re also running our question in parallel because this needs to be essentially passed through this\n3:21:02 component without us modifying anything so when we look at this here it''s almost\n3:21:07 like okay the this section here would be our runable parallel and these are being\n3:21:14 running parallel but also our query is being passed through so it''s almost like\n3:21:19 there''s another line there which is our runable pass through okay so that''s what we''re doing here these running in\n3:21:25 parallel one of them is a pass through I need\n3:21:30 to run here I just realized here we''re using\n3:21:35 the uh deprecated embeddings just switch it to this so L chain open\n3:21:42 AI we run that run this run that and now\n3:21:47 this is set up okay so we then put our initial so this\n3:21:56 using our runable parallel and runnable pass through that is our initial step we\n3:22:01 then have our prompt LM now pass which would being chained together with usual\n3:22:07 you know the usual type operator okay and now we''re going to invoke question what architecture does\n3:22:13 the mod deep seek release in December use okay okay so for the elm to answer\n3:22:19 this question it''s here to need to tell us what it needs the information about the Deep seek model that was released in\n3:22:24 December which we have specified in one half uh here and then it also needs to\n3:22:31 know what architecture that model uses which is defined in the other half over\n3:22:37 here okay so let''s run this okay there we go deep SE V3 model\n3:22:44 released in December 2024 is a mix experts model with 671 billion\n3:22:49 parameters okay so mixture of experts and this many parameters pretty cool so\n3:22:55 we''ve put together our pipeline using elol using the pipe operator the\n3:23:01 runnables specifically we''ve looked at the runable parallel runable pass through and also the runable lampas so\n3:23:08 that''s it for this chapter on lell and we''ll move on to the next one in this\n3:23:14 chapter we''re going to cover streaming and async in Lang chain now both using\n3:23:19 async code and using streaming are incredibly important components of I\n3:23:27 think almost any conversational chat interface or at least any good\n3:23:32 conversational chat interface for async if your application is not async and\n3:23:39 you''re spending a load of time in your API or whatever else waiting for llm\n3:23:45 calls because a lot of those are behind apis you are waiting and your\n3:23:50 application is doing nothing because you''ve written synchronous code and that\n3:23:55 well there are many problems with that mainly it doesn''t scale so asyn code\n3:24:00 generally performs much better and especially for AI where a lot of the\n3:24:05 time we''re kind of waiting for API calls so asyn is incredibly important for that\n3:24:10 for streaming now streaming is slightly different thing so let''s say I want to\n3:24:16 to tell me a story okay I''m using gbt 4 here it''s a\n3:24:22 bit slower so we can achieve string we can see that token by token this text is being produced and sent to us now this\n3:24:29 is not just a visual thing this is the LM when it is generating tokens or words\n3:24:38 it is generating them one by one and and that''s because these llms literally\n3:24:43 generate tokens one by one so they''re looking at all of the previous tokens in order to generate the next one and then\n3:24:48 generate next one generate next one that''s how they work so when we are\n3:24:54 implementing streaming we''re getting that feed of tokens directly from the LM\n3:24:59 through to our you know our back end or our front end that is what we see when when we see that token by token\n3:25:06 interface right so that''s one thing what one other thing that I can do that let\n3:25:12 me switch across to 40 is I can say okay we just got this story I''m going to\n3:25:18 ask are there any standard\n3:25:25 storytelling techniques to follow used above please\n3:25:32 use search okay so look we we get this very\n3:25:41 briefly there we saw that it was searching the web and the way it''s not because we told it okay we told the llm\n3:25:48 to use the search tool but then the llm output some tokens to say use the search\n3:25:55 tool that is going to use a Search tool and it also would have output the token saying what that search query would have\n3:26:02 been although we didn''t see it there but what the chat GPT interface is\n3:26:08 doing there so it received those tokens saying hey I''m going to use a Search tool it didn''t just send us those tokens\n3:26:15 like it does with the tokens here instead it used those tokens to show us\n3:26:21 that searching the web little text box so streaming is not just the streaming\n3:26:28 of these direct tokens it''s also the streaming of these intermediate steps\n3:26:34 that the llm may be thinking through which is particularly important when it\n3:26:40 comes to agents and agentic interfaces so it''s also a feature thing right\n3:26:45 streaming does doesn''t just look nice is also a feature then finally of course\n3:26:51 when we''re looking at this okay let''s say we go back to\n3:26:57 GT4 and I say okay use all of this\n3:27:04 information to generate a long story for\n3:27:10 me right and okay we are getting the first token now we know something is\n3:27:16 happening and we need start reading now imagine if we were not streaming anything here and we''re just waiting\n3:27:23 right we''re still waiting now we''re still waiting and we wouldn''t see anything we''re just like oh it''s just\n3:27:28 blank or maybe there''s a little loading spinner so we''d still be waiting and even now we''re still\n3:27:39 waiting right this is an extreme example but can you imagine just waiting\n3:27:45 for so long and not seeing anything as a user right now just now we would have\n3:27:51 got our answer if we were not streaming I mean that that would be painful as a\n3:27:56 user you you not want to wait especially in a chat interface you don''t want to wait that\n3:28:01 long it''s okay with okay for example deep research takes a long time to process but you know it''s going to take\n3:28:08 a long time to process and it''s a different user case right you''re getting a report this is a chat interface and\n3:28:15 yes most messages are not going to take that long to generate we''re also\n3:28:21 probably not going to be using GPT 4 depending on I don''t know maybe some people still do but in some scenarios\n3:28:29 it''s painful to need to wait that long okay and it''s also the same for agents it''s nice when you''re using agents again\n3:28:36 update on okay we''re using this tool it''s using this tool this is how it''s using them perplexity for example have a\n3:28:42 very nice example of this so okay what what''s this open I founder joins morati\n3:28:49 sub let''s see right so we see this is really nice it''s we''re using Pro search it''s searching for news sharing with the\n3:28:55 results like we''re getting all this information as we''re waiting which is really cool and it helps to understand\n3:29:02 what is actually happening right it''s not needed in all use cases but it''s super nice to have those intermediate\n3:29:09 steps right so then we''re not waiting and then I think this bit probably also streamed but it was just super fast so I\n3:29:15 I didn''t see it but that''s pretty cool so streaming is pretty important let''s\n3:29:22 dive into our example okay we''ll open that in cab enough we out so starting\n3:29:27 with the prerequisites same as always Lang chain optionally L Smith we''ll also\n3:29:34 enter our L chain API key if you''d like to use l Smith we''ll also enter our openi API key so that is platform.\n3:29:42 open.com and then as usual we can just invoke our l m right so we have that\n3:29:48 it''s working now let''s see how we would stream with a stream okay so whenever a\n3:29:56 method so stream is actually a method as well we could use that but it''s not acing right so whenever we see a method\n3:30:02 in line chain that has a prefixed onto what would be another method that''s like\n3:30:08 the async version of this so we can actually stream using async\n3:30:16 super easily using just LM a stream okay\n3:30:21 now this is just a an example in to be completely honest you probably will not\n3:30:27 be able to use this in an actual application but it''s just an example and we''re going to see how we would use this\n3:30:34 or how we would stream asynchronously in an application further down in this\n3:30:40 notebook so starting with this you can see here that we''re getting these tokens\n3:30:45 right we''re just appending it to token here we don''t actually need to do that I don''t think we''re using this but maybe\n3:30:51 we yeah we do it here it''s fine so we''re just pending the tokens as they come\n3:30:56 back from our LM pending it to this we''ll see what that is in a moment and\n3:31:02 then I''m just printing the token content right so the content of the\n3:31:07 token so in this case that would be l in this case it would be LP it would be Sans for so on and so on so you can see\n3:31:14 for the most part it''s it''s tends to be Word level but it can also be subword level as you see scent iment is one word\n3:31:23 of course so you know they get broken up in in various ways then adding this pipe character\n3:31:30 onto the end here so we can see okay where are our individual tokens then we\n3:31:36 also have flush so flush uh you can actually turn this off and it''s still going to stream you''re still going to\n3:31:41 see everything which going to be a bit more you can see it''s kind of a it''s like bit by bit when we use flush it\n3:31:49 forces the console to update what is being shown to us immediately all right\n3:31:54 so we get a much smoother um when we''re looking at this it''s much smoother versus when flush is\n3:32:02 not set true so yeah when you''re printing that is good to do just so you can see you don''t necessarily need to\n3:32:08 okay now we added all those tokens to the tokens list so we can have a look at each individual object that was returned\n3:32:15 turn to us right and this is interesting so we see that we have the AI message chunk right that''s an object and then\n3:32:22 you have the content the first one''s actually empty second one has that n for NLP and\n3:32:30 yeah I mean that''s all we rarely need to know they''re very simple objects but they''re actually quite\n3:32:35 useful because uh just look at this right so we can add each one of our AI\n3:32:40 message chunks right let''s see what that does it doesn''t create a list it creates this right so we still just have one AI\n3:32:48 message chunk uh but it''s combined the content within those AI message chunks\n3:32:55 which is kind of cool right so for example like we could remove\n3:33:01 these right and then we just see NLP so that''s kind of nice little feature there\n3:33:06 I do I actually quite like that but uh you do need to just be a little bit careful because obviously you can do\n3:33:13 that the wrong way and you''re going to get like a I don''t know all that is some weird token salad so yeah you need to\n3:33:21 just make sure you are going to be merging those into correct order unless you I don''t know unless you''re doing\n3:33:28 something weird Okay cool so streaming that that was streaming from a LM let''s\n3:33:34 have look at streaming with agents so we it gets a bit more complicated to be\n3:33:41 completely honest but we also need to things are going to get a bit more\n3:33:46 complicated so that we can implement this in for example an API right so is\n3:33:52 it''s kind of like a necessary thing in any case so to just very quickly we''re\n3:33:58 going to construct our agent executor like we did in the agent execution chapter and for that for the agent\n3:34:06 executor we''re going to need tools chat prompt template llm agent and the agent H itself okay very quickly I''m not going\n3:34:13 to go through these uh in detail we just def find our tools have ADD multiply exponentiate subtract and Final Answer\n3:34:20 tool merge those into a single list of tools then we have our prompt template\n3:34:26 again same as before we just have system message we have chat history we have you query and then we have the agent scratch\n3:34:34 pad for those intermediate sets then we Define our agent using L cell L cell\n3:34:40 works quite well with both streaming and async by the way it supports both out of\n3:34:46 the box which is nice so we Define our agent then coming down here we''re going\n3:34:54 to create the agan ice fter this is the same as before right so there''s nothing\n3:34:59 new in here I don''t think so just initialize our agent things there then\n3:35:05 it''s you know We''re looping through looping through yeah nothing\n3:35:11 nothing new there so we''re just executing invoking our agent seeing if\n3:35:17 there''s a tool call uh this is slightly we could shift this to before or after it doesn''t actually matter that\n3:35:23 much so we''re checking if it''s final answer if not we continue X to our tools\n3:35:29 and so on Okay cool so then we can invoke\n3:35:35 that okay we go what is 10 + 10 there we go right so we have our\n3:35:42 agent executor it is working now now when we are running our agent executor\n3:35:51 with every new query if we''re putting this into an API we''re probably going to need to\n3:35:57 provide it with a a fresh callback Handler okay so this is the corat Handler that''s going to handle taking\n3:36:04 the tokens that are being generated by a LMO agent and giving them to some other\n3:36:10 piece of code like for example the the streaming response for a API\n3:36:16 and our Corbat Handler is going to put those tokens in a queue in our case and\n3:36:22 then our for example the streaming object is going to pick them up from the queue and put them wherever they need to\n3:36:28 be so to allow us to do that with every\n3:36:33 new query or is needing to initialize everything when we actually initialize\n3:36:39 our agent we can add a configural field to our llm okay so we set the configural\n3:36:45 Fields here oh also one thing is that way we set streaming equal to true that''s very manting but just so you see\n3:36:52 that there we do do that so we add some configurable fields to our LM which means we can basically pass an object in\n3:37:00 for these on every new invocation so we set our configurable\n3:37:05 field it''s going to be called callbacks and we we just add a description right there''s nothing more to it so this will\n3:37:12 now allow us to provide that field when we''re invoking our agent okay now we\n3:37:21 need to Define our callback Handler and as I mentioned what is basically going\n3:37:26 to be happening is this callback Handler is going to be passing tokens into our a\n3:37:32 sync IO Q object and then we''re going to be picking them up from the que\n3:37:37 elsewhere okay so we can call it a q callback Handler okay and that is\n3:37:42 inhering from the async Callback Handler cuz we want all this to be done asynchronously because we''re we''re\n3:37:47 thinking here about okay how do we Implement all this stuff within apis and actual real world code and we we do want\n3:37:55 to be doing all this in aing so let me execute that and I''ll just explain a little bit of what we''re looking at so\n3:38:01 we have the initialization right it''s nothing nothing specific here we just\n3:38:07 what we really want to be doing is we want to be setting our Q object assigning that to the class attributes\n3:38:14 and then there''s also this Final Answer scene which we''re setting to fults so\n3:38:19 what we''re going to be using that for is we our llm will be streaming tokens\n3:38:26 towards whilst it''s using its tool calling and we might not want to display those immediately or we remember to\n3:38:32 display them in a different way so by setting this Final Answer scene to\n3:38:37 false whilst our LM is outputting those tool tokens we can handle them in a\n3:38:44 different way and then as soon as we see that it''s done with the tool calls and it''s on to the final answer which is\n3:38:49 actually another tool call but once we see that it''s on to the final answer tool call we can set this true and then\n3:38:56 we can start processing our tokens in a you know different way essentially okay so we have that then we have this AER\n3:39:04 method this is required for any async generator object\n3:39:11 so what that is going to be doing is going to iterating through right it''s a generator it''s going to be going\n3:39:16 iterating through and it''s going saying okay if our queue is empty right this is the que that we set up here if it''s\n3:39:22 empty wait a moment right we use the Sleep Method here and this is an async\n3:39:28 Sleep Method this is super important we''re using we are waiting for an asynchronous sleep all right so whilst\n3:39:35 we''re whilst we''re waiting for that 0.1 seconds our our code can be doing other things\n3:39:42 right that that is important if we if we use I think the standard is time dos sleep that is not asynchronous and so it\n3:39:50 will actually block the thread for that 0. one seconds so we don''t want that to happen generally our Q should probably\n3:39:57 not be empty that frequently given how quickly uh tokens are going to be added to the queue so the only way that this\n3:40:05 would potentially be empty is maybe our LM stops maybe there''s like a connection\n3:40:11 Interruption for it you know a brief second or something and no tokens are added so in that case we don''t actually\n3:40:17 do anything we don''t keep the checking the queue we just wait a moment okay and then we check again now if it was empty\n3:40:26 we wait and then we continue onto the next iteration otherwise it probably won''t be empty we get whatever is from\n3:40:33 our inside our queue we get that out pull it out then we say Okay if that\n3:40:40 token is a done token we''re going to return so we''re going to stop this\n3:40:45 generator right we''re finished otherwise if it''s something else we''re going to yield that token\n3:40:52 which means we''re we''re returning that token but then we''re continuing through that loop again\n3:40:57 right so that is our generator logic then we have some other methods here\n3:41:04 these are L these are line chain specific okay we have on LM new token\n3:41:10 and we have on LM end starting with on LM new token this is basically when an\n3:41:15 LM returns a token to us line chain is going to run or execute this method okay\n3:41:22 this is the method that will be called what this is going to do is it''s going to go into the keyword arguments and\n3:41:28 it''s going to get the chunk object so this is coming Fromm if there is something in that chunk it''s going to\n3:41:36 check for a final answer tool call First okay so we get our tool calls and we say\n3:41:43 if the name within our ch chunk right probably this will be emptying most of\n3:41:48 the tokens we return right so you remember before when we''re looking at the chunks here this is what we''re\n3:41:54 looking at right the content for us is actually always going to be empty and instead we''re actually going to get the\n3:41:59 additional keyword objects here and inside there we''re going to have our tool calling our tool calls as we s in\n3:42:06 the the previous videos right so that''s what we''re extracting we''re extracting that information that''s why we''re going\n3:42:12 additional keyword ARS right and get those tool the tool call information\n3:42:18 right or it will be nonone right so if if it is nonone I don''t think it ever\n3:42:24 would be none to be honest it would be strange if it''s none I think that means something would be wrong okay so here\n3:42:29 we''re using the wars operator so the wars operator what it''s doing here is whil we''re checking the if logic here\n3:42:38 whilst we do that it''s also assigning whatever is inside this it''s assigning\n3:42:43 over to Tool Calles and then with the if we''re checking whether tool cause is\n3:42:49 something or nonone right because we''re using get here so if if this get\n3:42:54 operation fails and there is no tool calls this object here will be equal to\n3:42:59 none which gets assigned to Tool calls here and then this this if none will\n3:43:05 return false and this logic will not run okay and it will just continue if this\n3:43:11 is true so if there is something returned here we''re going to check if that''s something returned is using the\n3:43:16 function name or tool name final answer if it is we''re going to set that final answer see equal to True otherwise we''re\n3:43:24 just going to add our chunk into Q okay we use put no weight here because we''re\n3:43:30 we''re using async otherwise if you were not using async I think you might just put weight or maybe even put put no okay\n3:43:39 you you use put if it''s a synchronous code but I I don''t think I''ve ever implemented a synchronous so it would\n3:43:46 actually just be put no weight for Asing okay and then return so we have\n3:43:52 that then we have on llm end okay so this is when line chain sees that the\n3:43:59 llm has returned or indicated that it is finished with the response line chain\n3:44:06 will call this so you you have to be aware that this\n3:44:12 will happen multiple times during an agent execution because if you think within our agent executor we''re hitting\n3:44:21 the LM multiple times we have that first step where it''s deciding oh I''m going to use the add tool or the multiply tool\n3:44:28 and then that response gets back towards we execute that tool and then we pass\n3:44:33 the output from that tool and all the original user query in the chat history pass that back to our LM again all right\n3:44:39 so that''s another call to our LM that''s going to come back it''s going to finish it''s going to give us something else\n3:44:45 right so there''s multiple llm cods happening throughout our agent execution\n3:44:50 logic so this on LM call will actually get called at the end of every single one of those llm calls now if we get to\n3:44:59 the end of our llm call and it was just a it was a tool invocation so we had the you know it\n3:45:05 called the ad tool we don''t want to put the done token into our Cube because\n3:45:13 when the done token is added to our Cube we''re going to stop iterating\n3:45:19 okay instead if it was just a tool call we''re going to say step end right and\n3:45:24 we''ll actually get this token back so this is useful on for example the front\n3:45:29 end you could have okay I''ve I''ve used the ad tool the these are the parameters\n3:45:35 and it''s the end of the step so you could have that your tool callers being used on some front end and then as soon\n3:45:42 as it sees step end it knows okay we''re done with here was a response right and and it can just show you that and we''re\n3:45:49 going to use that we''ll see that soon but let''s say we get to the final an tool we''re on the final answer tool and\n3:45:55 then we get this signal that the llm has finished then we need to stop iterating\n3:46:02 otherwise our our streaming generator is just going to keep going forever right nothing''s going to stop it or maybe it\n3:46:09 will time out I don''t think it will though so at that point we need to send okay\n3:46:15 stop right we need to say we''re done and then that will that will come back to here to our a iterator and to our asnc\n3:46:23 iterator and it will return and stop the generator okay so that''s the core logic\n3:46:31 that we have inside there I know there''s a lot going on there it''s but we need all of this so it''s important to be\n3:46:38 aware of it okay so now let''s see how we might actually call our agent with all\n3:46:45 of this streaming uh in this way so we''re going to initialize our queue\n3:46:52 we''re going to use that to initialize a streamer okay using the the custom streamer that we just sell custom\n3:46:57 callback Handler whatever you want to call it okay then I''m going to define a function so this is an asynchronous\n3:47:04 function it has to be if if we''re using async and what it''s going to do is it''s going to call our agent with a config\n3:47:12 here and we''re going to pass it that call the the Callback which is the streamer right note here I''m not calling\n3:47:19 the agent executor I''m just calling the agent right so the uh if we come back up\n3:47:24 here we''re calling this all right so that''s not going to include all tool execution logic and\n3:47:30 importantly we''re calling the agent with the config that uses callbacks right so\n3:47:37 this this configurable field here from our LM is actually being fed through it propagates through to our agent object\n3:47:43 as well to the runable so realizable all right so that''s what we''re executing here we see agent with\n3:47:49 config and we''re passing in those callbacks which is just one actually okay so that sets up our agent and then\n3:47:57 we invoke it with a stram okay like we did before and we''re just going to return everything so let''s uh run that\n3:48:05 okay and we see all the token or the chunk objects that are being returned and this is useful to understand what\n3:48:12 we''re actually doing up here right so when we''re doing this chunk\n3:48:17 message additional C keyword arguments right we can see that in here so this would be the chunk message object we get\n3:48:24 the additional keyword objects go into tool calls and we get the information here so we have the ID for that tool\n3:48:30 call as we saw in the previous chapters then we have our function right so the\n3:48:37 function includes the name right so we know what tool we''re calling from this first chunk but we don''t know the\n3:48:43 arguments right those arguments are going to be streamed to us so we can see them begin to come through in the next\n3:48:50 chunk so the next chunk is just it''s just a first token for for the ad\n3:48:55 function right and we can see these all come together over multiple steps and we\n3:49:01 actually get all of our arguments okay that''s pretty cool\n3:49:07 so actually one thing I would like to show you here as well so if we just do token\n3:49:13 equals token sorry and we\n3:49:20 do tokens. pen\n3:49:26 token okay we have all of our tokens in here now right you see that they''re all\n3:49:31 AI message chunks so we can actually add those together right so let''s we''ll go with\n3:49:37 these here and based on these we''re going to get all the arguments okay so this is kind of interesting so it''s one\n3:49:45 until I think like the second to last\n3:49:51 maybe right so we have these and actually we just want to add those\n3:49:56 together so I''m going to go with tokens one I''m just going to go\n3:50:04 four uh four token in we''re going to go from\n3:50:10 the second onwards I''m going to TK plus\n3:50:16 token right and let''s see what TK looks like at the end here\n3:50:23 TK okay so now you see I kind of merged with all those um arguments here sorry\n3:50:30 plus equal okay so run that and you can see here that it''s merged those arguments it\n3:50:36 didn''t get all of them so I kind of missed some at the end there but it''s merging them right so we can see that that logic where it''s you know before it\n3:50:43 was adding the content from various trunks it also does the same for the\n3:50:49 other parameters within your trunk object which is is I I think it''s pretty cool you can see here the name wasn''t\n3:50:55 included that''s because we started on token one or on token zero where the name was so if we actually started from\n3:51:02 token zero and let''s just let''s just pull them in there right so from one\n3:51:10 onwards we''re going to get a complete AI message chunk which includes the name\n3:51:16 here and all of those arguments and you you''ll see also here right populate everything which is pretty\n3:51:23 cool okay so we have that now based on this we''re going to want to modify our\n3:51:29 custom agent executor because we''re streaming everything right so we want to add\n3:51:35 streaming inside our agent executor which we''re doing here right so this is\n3:51:40 async death stream and we''re sharing async for token in the a string okay so\n3:51:47 this is like the very first instance if output is none we''re just going to be adding our token so the the chunk sorry\n3:51:56 to our output like the first token becomes our output otherwise we''re just\n3:52:02 appending our tokens to the output okay if the token content is empty which it\n3:52:08 should be right because we''re using tool cores all the time we''re just going to print content okay I just added these as\n3:52:15 so we see like print everything I just want to want to be able to see that I wouldn''t expect this to run because\n3:52:22 we''re saying it has to use tool calling okay so within our agent if we come up\n3:52:28 to here we said tool Choice any so it''s been forced to use tool calling so it should never really be returning\n3:52:34 anything inside the content field but just in case it''s there right so we''ll we''ll see if that is actually true then\n3:52:40 we''re just getting out our tool CES information okay from our trunk and we''re going to say okay if there''s\n3:52:46 something in there we''re going to print what is in there okay and then we''re going to extract our tool name if there\n3:52:51 is some if there is a tool name I''m going to show you the tool name then we''re going to go to ORS and\n3:52:57 if the ORS are not empty we''re going to see what we get in there okay and then\n3:53:03 from all of this we''re actually going to we merge all of it into our AI message right because we''re merging everything\n3:53:09 as we''re going through we''re merging everything into outputs as I showed you before okay cool and then we just\n3:53:14 awaiting our stream that will like kick it off okay and then we do the the standard agent execut stuff again here\n3:53:21 right so we''re just pulling out tool name Tool logs tool call ID and then we''re using all that to execute our tool\n3:53:27 here and then we''re creating a new tool message and passing that back in and then also here I move the break for The\n3:53:35 Final Answer into the final step so that is our custom Asian executor with streaming and let''s see what let''s see\n3:53:42 what it does okay St for b equal true so we see all those print\n3:53:49 statements okay so you can kind of see it''s a little bit messy but you can see\n3:53:55 we have tool calls that had some stuff inside it had add here and what we''re\n3:54:01 printing out here is we''re printing out the full AI message chunk with tool calls and then I''m just printing out\n3:54:06 okay what are we actually pulling out from from that so these are actually coming from the same thing okay and then\n3:54:12 the same here right so we''re looking at full message and then we''re looking okay we''re getting this argument out from it\n3:54:19 okay so we can see everything that is being pulled out you know chunk by chunk\n3:54:25 or token by to token and that''s it okay so we could just get everything like that however right so I''m I''m printing\n3:54:32 everything so we can see that it''s streaming what if I don''t print okay so we''re setting the bo or by default the\n3:54:39 both is equal to false here so what happens if we invoke now plus\n3:54:47 C okay cool we got\n3:54:54 nothing so the reason we got nothing is because we''re not\n3:54:59 printing but we don''t if you are if you''re building an an API for example\n3:55:06 you''re you''re pulling your tokens through you can''t print them to your\n3:55:14 like like a front end or or print them as to the output of your API printing\n3:55:20 goes to your terminal right your console window it doesn''t go anywhere else instead what we want to do is we\n3:55:28 actually want to get those tokens out right but if but how do we do that all\n3:55:33 right so we we printed them but another place that those tokens are is in our que all right because we set them up to\n3:55:41 go to the que so we can actually pull them out of\n3:55:47 our queue whilst our agent executor is running and then we can do whatever we\n3:55:52 want with them because our code is async so it can be doing multiple things at the same time so whilst our code is\n3:55:58 running the agent executor whilst that is happening our code can also be pulling out from our\n3:56:05 queue tokens that are in there and sending them to like an API for example\n3:56:12 right or whatever Downstream you you have so let''s see what that looks like\n3:56:17 we start by just initializing our que initializing our streamer with that que then we create a task so this is\n3:56:24 basically saying okay I I want to run this but don''t run it right now I''m not ready yet the reason that I say I''m not\n3:56:31 ready yet is because I also want to Define here my async Loop which is going\n3:56:37 to be printing those tokens right but this is async right so we we set this up\n3:56:43 this is like get ready to run this because it is async this is running right this is just running like it there\n3:56:50 it''s already running so we get this we continue we continue this none of this is actually executed\n3:56:57 yet right only here when we await the task that we set up here only then does\n3:57:04 our agent executor run and our async object here\n3:57:11 begin getting tokens right and here again printing but I don''t need to print\n3:57:16 I could I could have like a let''s say where this is within an API or\n3:57:22 something let''s say I''m I''m saying okay send token to\n3:57:29 XYZ token right that''s sending up token somewhere or if we''re maybe we''re yielding this to our some sort streamer\n3:57:37 object within our API right we can do whatever we want with those tokens okay I''m just printing them because I want to\n3:57:44 see them okay but just important here is that we''re not printing them within our\n3:57:50 agent executor we''re printing them outside the agent executor we''ve got them out and we can put them wherever we\n3:57:56 want which is perfect when you''re building an actual sort real world use KS we using an API or something else\n3:58:02 okay so let''s run that let''s see what we get look at that we get all of the\n3:58:07 information we could need and a little bit more right because now we''re using the agent executor and now we can also\n3:58:15 see oh we have this step end right so I know all I I know just from looking at this right this is my first tool use so\n3:58:24 what tool is it let''s have a look it''s the add tool and then we have these arguments I can then pass them right\n3:58:31 Downstream then we have the next tool use which is here down here so we can\n3:58:38 then pass them in the way that we like so that''s pretty cool\n3:58:45 let''s I mean let''s see right so we''re getting those fers out can we can we do\n3:58:50 something with them before I before I print them and show them yes let''s see\n3:58:55 okay so we''re now modifying our our Loop here same stuff right we''re still\n3:59:01 initializing our queue initializing our streamer initializing our task okay and we''re still doing this aing for token\n3:59:08 streamer okay but then we''re doing stuff with our tokens so I''m saying okay if if we''re on\n3:59:15 stream end I''m not actually going to print stream end I''m going to print new line okay otherwise if we''re getting a tool\n3:59:23 call here we''re going to say if that tool call is the tool name I am going to\n3:59:29 Sprint calling tool name okay if it''s the arguments I''m going to print the\n3:59:35 tool argument and I''m going to end up with nothing so that we don''t go to a new line so we''re actually going to be\n3:59:41 streaming everything okay so let''s just see what this looks\n3:59:47 like oh my bad I just added\n3:59:52 that okay you see that so it go it goes very\n3:59:58 fast so it''s kind of hard to see it I''m going to slow it down so you can see so you can see that we as soon as we get\n4:00:04 the toour name we stream that we''re calling the add tool then we stream token by token the actual Arguments for\n4:00:11 that tool then for the next one again we do the same we''re calling this tool name then we''re streaming token by token\n4:00:18 again we''re processing everything Downstream from outside of the agent\n4:00:24 executor and this is an essential thing to be able to do when we''re actually implementing streaming and acing and\n4:00:32 everything else in an actual application so I know that''s a lot but it''s\n4:00:39 important so that is it for our chapter on streaming and Asing\n4:00:44 I hope this all been useful thanks now we''re on to the final Capstone chapter\n4:00:50 we''re going to be taking everything that we''ve learned so far and using it to build a actual chat application now the\n4:00:59 chat application is what you can see right now and we can go into this and ask some pretty interesting questions\n4:01:05 and because it''s an agent because as iess is tools it will be able to answer them for us so we''ll see inside our\n4:01:12 application that we can ask questions s that require tool use such as this and\n4:01:17 because of the streaming that we''ve implemented we can see all this information real time so we can see that sub API tool is being used these are the\n4:01:24 queries we saw all that was in parallel as well so each one of those tools were\n4:01:29 being used in parallel we modified the code a little bit to enable that and we\n4:01:35 see that we have the answer we can also see the structured output being used here so we can see our answer followed\n4:01:41 by the tools used here and then we could ask followup questions as well because this is conversational so we say how is\n4:01:48 the weather in each of those\n4:02:00 cities okay that''s pretty cool so this is what we''re going to be building we\n4:02:05 are of course going to be focusing on the API the back end I''m not front end engineer so I can''t take you through\n4:02:11 that but the code is there so for those of you that do want to go through the front end code you can of course go and\n4:02:17 do that but we''ll be focusing on how we build the API that powers all of this\n4:02:23 using of course everything that we''ve learned so far so let''s jump into it the first thing we going to want to do is\n4:02:29 clone this repo so we''ll copy this URL this is repo orelio Labs line chain\n4:02:36 course and you''ll just clone your repo like so I''ve already done this so I''m\n4:02:42 not going to do it again instead I''ll just navigate to the line chain course repo now there''s a few\n4:02:50 setup things that you do need to do all of those can be found in the read me so\n4:02:56 we just open a new tab here and I''ll open the read me okay so\n4:03:02 this explains everything we need we have if you were running this locally already you will have seen this or you will have\n4:03:09 already done all of this but for those of you that haven''t we go through quickly now so you will need to install\n4:03:17 the UV Library so this is how we manage our pyth environment our packages we use\n4:03:24 UV on Mac you would install it like so if you''re on Windows or Linux just\n4:03:31 double check how you''d install over here once you have installed this you then go\n4:03:37 to install python so UV python install then we want to to create our VM our\n4:03:45 virtual environment using that version of python so the VM\n4:03:52 here then as you can see here we need to activate that virtual environment which\n4:03:57 I did miss from here so let me quickly add that so you just run that for me I''m\n4:04:03 using fish so I just add fish onto the end there but if you''re using bash or zsh I think you can you can just run\n4:04:09 that directly and then finally we need to sync I install all of our packages using UV\n4:04:17 sync and you see that will install everything for you great so we have that\n4:04:24 and we can go ahead and actually open cursor or vs code and then we should\n4:04:31 find ourselves within cursor or vs code so in here you''ll find a few things that\n4:04:39 we will need so first is environment variables so we can come over to here\n4:04:45 and we have open AI API key larning chain API key and ser API API key create\n4:04:50 a copy of this and you would make this your EMV file or if you want to run it\n4:04:57 with Source you can well I like to use mac. EnV when I''m on Mac and I just add\n4:05:03 export onto the start there and then enter my API Keys now I actually already\n4:05:09 have these in this local. EMV file which over in my terminal I''ll just activate with Source\n4:05:17 again like that now we''ll need that when we are running our API and application\n4:05:23 later but for now let''s just focus on understanding what the API actually\n4:05:29 looks like so navigating into the 09 Capstone chapter we''ll find a few things\n4:05:36 what we''re going to focus on is the API here and we have a couple of notebooks that help us just understand okay what\n4:05:44 are we actually doing here so let me give you a quick overview of the API\n4:05:49 first so the API we''re using fast API for this we have a few functions in here\n4:05:54 the one we''ll start with is this okay so this is our post Endo for invoke and\n4:06:01 this essentially sends something to our llm and begins a streaming response so\n4:06:07 we can go ahead and actually start the API and we can just see what this looks like so we''ll go into chapter 09 caps\n4:06:14 there and API after setting our environment variables here and we just want to do UV\n4:06:20 run uicorn main colon app reload we don''t need to reload but if we''re\n4:06:26 modifying the code that can be useful okay and we can see that our API is now running on Local Host Port\n4:06:34 8000 and if we go to our browser we can actually open the dots for our API so we\n4:06:42 go to 8,000 slash dos okay we just see that we have that single invoke method\n4:06:48 it stripes the content and it gives us a small amount of information there now we could try\n4:06:55 out here so if we say say hello we can\n4:07:00 run that and we''ll see that we get a response we get this okay now the thing\n4:07:09 that we''re missing here is that this is actually being streamed back to us okay so this is not a just a direct response\n4:07:16 this is a stream to see that we''re going to navigate over to here to this streaming test notebook and we''ll run\n4:07:25 this so we are using request here we are not just doing a you know the standard\n4:07:32 post request because we want to stream the output and then print the output as we are receiving them okay so that''s why\n4:07:40 this looks a little more complicated than just a typical request. post or request. getet so what we''re doing here\n4:07:48 is we''re starting our session which is our our post request and then we''re just\n4:07:53 iterating through the content as we receive it from that request when we\n4:07:59 receive a token because sometimes this might be non we print that okay and we\n4:08:04 have that flush equals TRS we have used in the past so let''s define that and\n4:08:10 then let''s just ask a simple question what is 5 + 5\n4:08:16 okay and we we saw that that was it was pretty quick so it generated this response first and then it went ahead\n4:08:23 and actually continued streaming with all of this okay and we can see that there\n4:08:29 these special tokens are being provided this is to help the front end basically\n4:08:35 decide okay what should go where so here where we''re showing these multiple steps\n4:08:43 of tool use and the parameters the way the front end is deciding how to display\n4:08:48 those is it''s just it''s being provided the single stream but it has the SE tokens has a SE has se name then it has\n4:08:57 the parameters followed by the sort of ending of the step token and it''s looking at each one of these and then\n4:09:03 the one step name that it treats differently is where it will see The Final Answer step name when it sees the\n4:09:10 final answer step name rather than displaying this tool interface it instead begins streaming the tokens\n4:09:16 directly at like typical chat interface and if we look at what we actually get\n4:09:22 in our final answer it''s not just the answer itself right so we have the answer here this is streamed into that\n4:09:31 typical chat output but then we also have tools used and then this is added\n4:09:36 into the little boxes that we have below the chat here so there''s quite a lot\n4:09:42 going on just within this little stream now we can try with some other questions\n4:09:47 here so we can say okay tell me about the latest news in the world you can see that there''s a little bit of a wait here\n4:09:53 whilst it''s waiting to get the response and then yeah it''s streaming a lot of stuff quite quickly okay so there''s a\n4:10:00 lot coming through here okay and then we can ask other questions like okay this\n4:10:05 one here how called is in Osa right now is five mtip by five right so these two\n4:10:10 are going to be executed in parallel and then it will after it has the answers for those the agent will use the another\n4:10:18 multiply tool to multiply those two values together and all of that will get streamed okay and then as we saw earlier\n4:10:26 we have the what is the current Daye and time in these places same thing so three questions three questions here what is\n4:10:33 the current date and time in Dubai what is the current date and time in Tokyo and what is the current date and time in Berlin those three questions get\n4:10:41 executed in parallel against St I search at all and then all answers get returned\n4:10:47 within that final answer okay so that is how our API is working now let''s dive a\n4:10:55 little bit into the code and understand how it is working so there are a lot of\n4:11:01 important things here there''s some complexity but at the same time we''ve tried to make this as simple as possible\n4:11:07 as well so let''s just fast API syntax here with the app post invoke so our\n4:11:13 invoke endpoint we consume some some content which just a string and then if you remember from the agent execut a\n4:11:21 deep dive which is what we''ve implemented here or a modified version of that we have to initialize our asyn q\n4:11:30 and our streamer which is the Q coreback Handler which I believe is exactly the same as what we defined in that earlier\n4:11:37 chapter there''s no differences there so we Define that and then we return and\n4:11:43 this streaming response object right again this is a fast API thing this is so that you are streaming a response\n4:11:50 that streaming response has a few attributes here which again are fast API\n4:11:55 things or just generic API things so some headers giving instructions to the\n4:12:01 API and then the media type here which is text event stream you can also use I\n4:12:06 think it''s text plane possibly as well but I believe this standard here would\n4:12:11 be to use event screen and then the more important part for us is this token\n4:12:17 generator okay so what is this token generator well it is this function that\n4:12:23 we defined up here now if you again if you remember that earlier chapter at the\n4:12:28 end of the chapter we set up a a for Loop where we were printing out\n4:12:34 different tokens in various formats so we kind of pro postprocessing them before deciding how to display them\n4:12:42 that''s exactly what doing here so in this block here We''re looping through\n4:12:50 every token that we''re receiving from our streamer We''re looping through and we''re just saying okay if this is the\n4:12:57 end of a step we''re going to yield this end of Step token which we we saw here\n4:13:03 okay so it''s this end of end of St token there otherwise if this is a tool call\n4:13:10 so again we''ve got that W operator here so what we''re doing is saying okay get the tool calls out from our current\n4:13:18 message if there is something there so if this is not nonone we''re going to execute what inside here and what is\n4:13:25 being executed inside here is we''re checking for the tool name if we have the tool name we return this okay so we\n4:13:32 have the start step token the start of Step name token the tool name or set\n4:13:39 name whichever those you want to call it and then the end of the set name token\n4:13:45 okay and then this of course comes through to the front end like that okay that''s what we have there\n4:13:52 otherwise we should only be seeing the tool name returned as part of first token for every step after that it\n4:13:59 should just be tool arguments so in this case we say okay if we have those tool\n4:14:05 or function arguments we''re going to just return them directly so then that is the part that would stream all of\n4:14:11 this here okay like these would be individual tokens right for example\n4:14:16 right so we might have the open curly brackets followed by query could be a token latest could be a token world\n4:14:24 could be a token news could be a token Etc okay so that is what is happening there this should not get executed but\n4:14:31 we have a we just handle that just in case so we have any issues with tokens\n4:14:37 being returned there we''re just going to print as error and we''re going to continue with the streaming but that\n4:14:43 should not really be happening cool so that is our like token streaming Loop\n4:14:51 now the way that we are picking up tokens from our stream object here is of\n4:14:56 course through our agent execution logic which is happening in parallel okay so\n4:15:02 all of this is asynchronous we have this async definition here so all of this is happening asynchronously so what has\n4:15:09 happened here is here we have created a task which is the agent ex you to invoke\n4:15:16 and we passing our content we passing that streamer which we''re going to be pulling tokens from and we also set\n4:15:21 Theos to true we can actually remove that but that would just allow us to see\n4:15:27 additional output in our terminal window if we want it I don''t think there''s\n4:15:33 anything particularly interesting to look at in there but particularly if you are debugging that can be useful so we\n4:15:41 create our task here but this does not begin the task right this is a asyn iio\n4:15:47 create task but this does not begin until we await it down here so what is\n4:15:53 happening here is essentially this code here is still being run and like a we''re\n4:16:00 in an asynchronous Loop here but then we await this task as soon as we await this task tokens will start being placed\n4:16:07 within our que which then get picked up by the streamer object here so then this\n4:16:13 begins receiving tokens I know asyn code is always a little bit more confusing\n4:16:21 given the strange order of things but that is essentially what is happening\n4:16:26 you can imagine all this is essentially being executed all at the same time so\n4:16:31 we have that is there anything else to go through here I don''t think so it''s all sort of boiler plates stuff for fast\n4:16:37 API rather than the actual AI code itself so we have that that''s our\n4:16:43 streaming function now let''s have a look at the agent code itself okay so agent code where would\n4:16:50 that be so we''re using this agent executor invoke and we''re importing this\n4:16:56 from the agent file so we can have a look in here for this now you can see\n4:17:02 straight away we''re pulling in our API Keys here just yeah make sure that you\n4:17:07 do have those now all of our C okay this is what we''ve seen before\n4:17:13 in that agent execut to Deep dive chapter this is all practically the same\n4:17:20 so we have our LM we''ve set those configurable fields as we did in the\n4:17:26 earlier chapters that configurable field is for our callbacks we have our prompt this has been modified a little bit so\n4:17:35 essentially just telling it okay make sure you use the tools provided we say You must use the final\n4:17:41 answer tool to provide a final answer to the user and one thing that I added that I notice every now and again so I have explicitly\n4:17:48 said Ed to answer to users current question not pre-used questions so I\n4:17:54 found with this setup it will occasionally if I just have a little bit of small talk with the agent and\n4:18:01 beforehand I was asking questions about okay like what was the weather in this place or that place the agent will kind\n4:18:07 of hang on to those previous questions and try and use a tool again to answer and that is just something that you can\n4:18:13 more or less prompt out of it okay so we have that this is all exactly the same\n4:18:19 as before okay so we have our chat history to make this conversational we have our human message and then our\n4:18:25 agent scratchpad so that agent can think through multiple tool use messages great\n4:18:31 so we also have the article class so this is to process results from Sur\n4:18:39 API we have our Ser API function here I will talk about that a little more in a\n4:18:44 moment because this is also a little bit different to what we covered before what we covered before with C API if you\n4:18:51 remember was synchronous because we''re using the Ser API client directly or the\n4:18:57 Ser API tool directly from line chain and because we want everything to be\n4:19:02 asynchronous we have had to recreate that tool in a asynchronous fashion\n4:19:09 which we''ll talk about a little bit later but for now let''s move on from that we see our final answer being used\n4:19:18 here so this is I think we defined the exact same thing before probably in that\n4:19:23 deep dive chapter again where we have just the answer and the tools that have been used great so we have that one thing\n4:19:31 that is a little different here is when we are defining our name to Tool\n4:19:38 function so this takes a tool name and it Maps it to a tool to function when we\n4:19:45 have synchronous tools we actually use tool Funk here okay so rather than tool\n4:19:52 cartin it would be tool Funk however we are using a synchronous\n4:19:58 tools and so this is actually tool co-routine and this is why this is why\n4:20:04 if you if you come up here I''ve made every single tool asynchronous now that is not really NE\n4:20:12 for a tool like final answer because there is no there''s no API calls happening an API call is a very typical\n4:20:19 scenario where you do want to use async because if you make an API call with a synchronous function your code is just\n4:20:26 going to be waiting for the response from the API while the API is processing\n4:20:32 and doing whatever it''s doing so that is an ideal scenario where you would want to use async because rather\n4:20:39 than your code just waiting for the response from the API it can instead go\n4:20:44 and do something else whilst it''s waiting right so that''s an ideal scenario where you''d use async which is\n4:20:50 why we would use it for example with a Ser API tool here but for final answer\n4:20:55 and for all of these calculator tools that we built there''s actually no need\n4:21:02 to have these as async because our code is just running through its executing\n4:21:07 this code there''s no waiting involved so it doesn''t necessarily make sense have these a synchronous however by making\n4:21:15 them asynchronous it means that I can do tool care routine for all of them rather than saying oh if this tool is\n4:21:22 synchronous use tool. Funk whereas if this one is async use tool. cartin so\n4:21:28 just simplifies the code for us a lot more but yeah not directly necessary but\n4:21:34 it does help us write cleaner code here this is also true later on because we actually have\n4:21:40 to await our tool code which we can see over here right so we\n4:21:47 have to await those tool calls that would get Messier if we were using the\n4:21:53 like some sync tools some async tools so we have that we have our Q callback\n4:21:59 Handler this is again that''s the same as before so I''m not going to go through\n4:22:04 I''m not going to go through that we covered that in the earlier Deep dive chapter we have our execute tool\n4:22:09 function here again that is a synchronous this just helps us you know clean up code a little bit this would I\n4:22:16 think in the Deep dive chapter we had this directly placed within our agent\n4:22:21 executor function and you can do that it''s fine it''s just a bit cleaner to kind of pull this out and we can also\n4:22:28 add more type annotations here which I like so execute tool expects us to\n4:22:33 provide an AI message which includes a tool call within it and it will return\n4:22:38 as a tool message okay agent exor this\n4:22:43 is all the same as before and we''re actually not even using verose here so we could fully remove it but I I will\n4:22:49 leave it of course if you would like to use that you can just add a ifos and then log or print some stuff where you\n4:22:57 need it okay so what do we have in here we have our streaming function so this\n4:23:03 is what actually calls our agent right so we have a query this will\n4:23:10 call our agent just here and we could even make this a little clearer so for example this could be\n4:23:17 configured agent because this is this is not the response this is a configured\n4:23:23 agent so I think this is may be a lot clearer so we are configuring our agent with our callbacks okay which is just\n4:23:30 our streamer then we''re iterating through the tokens are returned by our\n4:23:35 agent using a stream here okay and as we are iterating through this because we\n4:23:42 pass our streamer to the Callback here what that is going to do is every single\n4:23:48 token that our agent returns is going to get processed through our Q callback\n4:23:56 Handler here okay so this on LM new token on LMN these are going to get\n4:24:03 executed and then all of those tokens you can see here I''ll pass to our Q okay\n4:24:09 then we come up here and we have this a it so that this aor method here is used\n4:24:15 by our generator over in our API is used by this token\n4:24:21 generator to pick up from the queue the tokens that have been put in the queue\n4:24:28 by these other methods here okay so it''s putting tokens into the queue and\n4:24:34 pulling them out with this okay so that is just happening in\n4:24:39 parallel as well as this code is running here now the reason that we extract the\n4:24:45 tokens out here is that we want to pull out our tokens and we append them all to\n4:24:50 our outputs now those outputs that becomes a list of AI messages which are\n4:24:57 essentially the AI telling as what tool to use and what parameters to pass to each one of those tools this is very\n4:25:05 similar to what we covered in that deep dive chapter but the one thing that I have modified here is I''ve enabled us to\n4:25:12 use parallel tool calls so that is what we see here with this these four lines\n4:25:20 of code we''re saying okay if our tool call includes an ID that means we have a\n4:25:25 new tool call or a new AI message so what we do is we append that AI message\n4:25:32 which is the AI message chunk to our outputs and then following that if we\n4:25:37 don''t get an ID that means we''re getting the tool arguments so following that we''re just\n4:25:42 adding our AI message chunk to the most recent AI message Chunk from our outputs\n4:25:49 okay so what that will do is it it will create that list of AI messages would be\n4:25:56 like AI message one and then this will just append everything to that AI\n4:26:02 message one then we''ll get our next AI message chunk this will then just append\n4:26:08 everything to that until we get a complete AI message and so on and so on\n4:26:15 okay so what we do here is here we''ve collected all our AI message chunk\n4:26:21 objects then finally what we do is just transform all those AI message chunk objects into actual AI message objects\n4:26:28 and then return them from our function which we then receive over here so into the tool cuse variable okay now this is\n4:26:37 very similar to The Deep dive chapter again we''re going through that that count that Loop where we have a Max\n4:26:44 iterations at which point we will just stop but until then we continue\n4:26:49 iterating through and making more tool calls executing those tool calls and so on so what what is going on here let''s\n4:26:57 see so we got our tool calls there''s going to be a list of AI message objects then what we do with those AI\n4:27:04 message objects is we pass them to this ex cuute tool function if you remember what is that that is this function here\n4:27:13 so we pass each AI message individually to this function and that will execute\n4:27:19 the tool force and then return us that observation from the tool okay so that is what you see\n4:27:28 happening here but this is an async method so typically what you''d have to\n4:27:33 do is you''d have to do await X you tool and we could do that so we could do a\n4:27:39 okay let me let me make this a little bigger for us okay and so what we could do for example\n4:27:45 which might be a bit clearer is you could do tool OBS equals an empty list\n4:27:51 and what you can do is you can say for Tool call oops in tool calls the\n4:27:58 tool observation is we''re going to append execute tool call which would\n4:28:04 have to be in a weit so we'' actually put your weight in there and what this would do is actually the exact same thing as\n4:28:10 what we''re doing here the difference being that we''re doing this tool by Tool\n4:28:16 okay so we are we''re executing async here but we''re doing them sequentially\n4:28:23 whereas what we can do which is better is we can use asyn I gather so what this\n4:28:28 does is gathers all those Co routines and then we await them all at the same\n4:28:33 time to run them all asynchronously they all begin at the same time or almost\n4:28:38 exactly at the same time and we get those responses kind of in parallel but of course it''s\n4:28:44 saying so it''s not fully in parallel but practically in parallel cool so we have\n4:28:51 that and then that okay we get all of our tool observations from that so that''s all of our tool messages and then\n4:28:57 one interesting thing here is if we let''s say we have all of our AI messages\n4:29:04 of all of our tool cores and we just append all of those to our agent scratch Pad right so let''s say here we''re just\n4:29:11 like oh okay scratch Pad extend and then we would just have\n4:29:17 okay we'' have our tool calls and then we do agent stretch PCT\n4:29:23 send tool OBS all right so what what is happening here is this would essentially\n4:29:28 give us something that looks like this so we have our AI message say I''m just\n4:29:37 going to put okay we''ll just put tool call IDs in here to simplify a little bit\n4:29:42 this would be tool call ID a then we would have ai message tool call ID B\n4:29:50 then we''d have tool message let''s just remove this content\n4:29:55 field I don''t want that and Tool message tool call ID B right so it would look\n4:30:02 something like this so the the order is the tool message is not following the AI\n4:30:07 message which you would think okay we have this tool qual ID that''s probably fine actually when we''re running this if\n4:30:14 you add these to your agent scratch pad in this order what you''ll see is your response\n4:30:20 just hangs like nothing nothing happens when you come through to your second uh\n4:30:25 iteration of your agent call so actually what you need to do is these need to be\n4:30:30 sorted so that they are actually in order and it doesn''t actually doesn''t\n4:30:36 necessarily matter which order in terms of like a or b or c or whatever you use so you could have this order we have ai\n4:30:42 message tool message AI message tool message just as long as you have your tool call IDs are both together or you\n4:30:49 could know invert this for example right so you could have this right and that that will work\n4:30:55 as well it''s essentially just as long as you have your AI message followed by your tool message and both of those are\n4:31:02 sharing that tool call ID you need to make sure you have that order okay so\n4:31:08 that of course would not happen if we do this and instead what we need to do is\n4:31:14 something like this okay so I made this a lot easier to read okay so we''re\n4:31:20 taking the tool call ID we are pointing it to the tool observation and we''re\n4:31:26 doing that for every tool call and to Observation within like a zip of those\n4:31:31 okay then what we''re saying is for each tool call within our tool calls we are\n4:31:37 extending our agent scratch pad with that tool call followed by by the tool\n4:31:43 observation message which is the tool message so this would be our this is the AI message and that is\n4:31:50 the tool messages down there okay so that is what it''s happening and that is how we get this correct order which will\n4:31:58 run otherwise things will not run so that''s important to be aware of okay now\n4:32:05 we''re we''re almost done I know there''s we just been through quite a lot so we continue we incre increment our count as\n4:32:11 we were doing before then we need to check for the final answer tool okay and because we''re running these tools in parallel okay\n4:32:18 because we''re allowing multiple tool calls in one step we can''t just look at the most recent tool and look if it is\n4:32:25 it has the name Final Answer instead we need to iterate through all of our tool calls and check if any of them have the\n4:32:30 name final answer if they do we say okay we extract that final answer call we\n4:32:35 extract the final answer as well so this is the direct text content and we say\n4:32:40 okay we have found found the final answer so this will we set to True okay which should happen every time but let''s\n4:32:47 say if our agent gets stuck in a loop of calling multiple tools this might not\n4:32:53 happen before we break based on the max iterations here so we might end up\n4:32:59 breaking based on Max iterations rather than we found a final answer okay so\n4:33:04 that can happen so anyway if we find that final answer we break out of this\n4:33:09 for Loop here and then of course we do need to break out of our wow Loop which is here so we say if we found the final\n4:33:16 answer break okay cool so we have that\n4:33:22 finally after all of that so this is our you know we''ve executed our tool our\n4:33:27 agent steps and iterations has process we''ve been through those finally we come\n4:33:34 down to here where we say okay we''re going to add that final output to our chat history so this is just going to be\n4:33:41 the text content right so this here get direct answer but then what we do is we\n4:33:49 return the full final answer call the full final answer call is basically this here right so this answer and tools used\n4:33:57 but of course populated so we''re saying here that if we have a final answer okay\n4:34:03 if we have that we''re going to return the final answer call which was generated by our llm otherwise we''re\n4:34:09 going to return this one so this is in the scenario that maybe the agent got caught in a loop and just kept iterating\n4:34:16 if that happens we''ll say it will come back with okay no answer found and it will just return okay we didn''t use any\n4:34:23 tools which is not technically true but it''s this is like a exception handling\n4:34:29 event so it ideally it shouldn''t happen but it''s not really a big deal if we''re\n4:34:35 saying okay there were no tools use in my opinion anyway cool so we have all of\n4:34:41 that and yeah we just we initialize our agent executor and then I mean that that is\n4:34:48 our agent execution code the one last thing we want to go through is the Ser API tool which we will do in a moment\n4:34:56 okay so Ser API let''s see what let''s see\n4:35:01 how we build our Ser API tool okay so we''ll start with the synchronous Ser API\n4:35:10 now the reason we''re starting with this is that it''s actually it''s just a bit simpler so I''ll show you this quickly\n4:35:16 before we move on to the async implementation which is what we''re using within our app so we want to get our set\n4:35:23 API API key so I''ll run that and we just enter it at the top\n4:35:29 there and this will R so we''re going to use the sub API SDK first we''re\n4:35:36 importing Google search and these are the input prameters so we have our API key we''re using we say want use Google\n4:35:42 we our question is so query so Q for query we''re searching for the latest\n4:35:47 news in the world it will return quite a lot of stuff you can see there''s a ton of stuff in there right\n4:35:56 now what we want is contained within this organic results key so we can run\n4:36:02 that and we''ll see K is talking about you various things pretty recent stuff\n4:36:09 at the moment so we can tell okay that is that is in fact working now this is\n4:36:14 quite messy so what I would like to do first is just clean that up a little bit so we Define this article base model\n4:36:21 which is pantic and we''re saying okay from a set of results okay so we''re\n4:36:27 going to iterate through each of these we''re going to extract the title source\n4:36:32 link and the snippet so you can see title source link and snippet here\n4:36:42 okay so that''s all usedful we''ll run that and what we do is we go through\n4:36:48 each of the results in organic results and we just load them into our article using this class method here and then we\n4:36:55 can see okay let''s have a look at what those look like it''s much nicer okay we\n4:37:02 get this nicely formatted object here cool that''s great now all of this what\n4:37:10 we just did here so this is using sub apis SDK which is great super easy to\n4:37:15 use the problem is that they don''t offer a async SDK which is a shame but it''s\n4:37:22 not that hard for us to set up ourselves so typically with a synchronous requests\n4:37:29 what we can use is the aiio HTTP Library it''s well it''s you can see what we''re\n4:37:35 doing here so this is equivalent to requests Dot get okay that''s essentially\n4:37:43 what we''re doing here and the equivalent is literally this okay so this is the\n4:37:50 equivalent using requests that we are running here but we''re using asyn Code\n4:37:55 so we''re using AI Hep client session and then session. getet okay with this async\n4:38:03 width here and then we just await our response so this is all yeah this is what we do rather than this to make our\n4:38:11 code async so it''s really simple and then the output that we get is exactly the same\n4:38:16 right so we still get this exact same output so that means of course that we can use that articles method like this\n4:38:25 in the exact same way and we get we get the same result there''s no need to make this article from sub API result asnc\n4:38:34 because again like this this bit of code here is fully local it''s just our python\n4:38:39 running everything so this does not need to be async okay and we can see that we\n4:38:45 get literally the exact same result there so with that we have everything\n4:38:51 that we would need to build a fully asynchronous Sur API tool which is exactly what we do here for Lang chain\n4:38:58 so we import those tools and I mean there''s nothing is there anything different here no this is exactly what\n4:39:04 we we just said but I will run this because I would like to show you very quickly this okay so this is how we were\n4:39:12 initially calling our Tools in previous chapters because we we were okay mostly\n4:39:18 with using the the synchronous tools however you can see that the funk here\n4:39:26 is just empty right so if I do type just a non-type that is\n4:39:31 because well this is an async function okay it''s an async tool sorry so it was\n4:39:40 defined with async here what happens when you do that is you get\n4:39:45 this Co routine object so rather than Funk which is it isn''t here you get that\n4:39:52 cartine if we then modified this which would be kind of okay let''s just remove\n4:39:58 all the ayns here and the await if we modify that like so and then\n4:40:06 we look at the set API structure tool we go across we see that we now get that\n4:40:12 funk okay so that is that is just the difference between an async structured\n4:40:17 tool versus a sync structured tool we of course on\n4:40:22 async okay now we have K again so important to be aware of that and of\n4:40:29 course we we run using the sub API care\n4:40:34 routine so that is that''s how we build the sub API tool\n4:40:41 uh there''s nothing I mean that is exactly what we did here so I don''t need to I don''t think we need to go through that any further so yeah I think that is\n4:40:50 basically all of our code behind this API with all of that we can then go\n4:40:55 ahead so we have our API running already let''s go ahead and actually run also our\n4:41:02 front end so we''re going to go to documents orelo line chain course and\n4:41:07 then we want to go to Chapters 09 Capstone app and you will need to have npm\n4:41:14 installed so to do that what do we do we can take a look at this answer for example this is probably what I would\n4:41:21 recommend okay so I would run Brew install node followed by Brew install mpm if you''re on Mac of course it''s\n4:41:28 different if you''re on Linux or Windows once you have those you can do npm install and this will just install all\n4:41:34 of the oop sorry mpm install and this would just install all of the node\n4:41:40 packages that we need and then we can just run npm run Dev okay and now we\n4:41:48 have our app running on locost 3000 so we can come over to here open\n4:41:53 that up and we have our application can ignore this so in here we can begin just\n4:42:00 asking questions okay so we can start with quick question what is 5 +\n4:42:07 5 and you see so we have our streaming happening here it said the agent wants to use ad tool and these are the input\n4:42:14 parameters to the ad tool and then we get the streamed response so this is the\n4:42:20 final answer tool where we''re outputting that answer key and value and then here we''re outputting that tools used key and\n4:42:27 value which is just an array of the tools being used which just functions add so we have that then let''s ask\n4:42:35 another question this time we''ll trigger Ser API with tell me about the latest news in the world\n4:42:42 okay so we can see that''s using C API and a query is latest world\n4:42:48 news and then it comes down here and we actually get some citations here which is kind of cool so you can also come\n4:42:55 through to here okay and it teses through to here so that''s pretty cool unfortunately I\n4:43:01 just lost my chat so fine let me I can ask that\n4:43:07 question again\n4:43:16 okay we can see that to us set API there now let''s continue with the next question from our notebook which is how\n4:43:23 cold is in I like right now what is five M by five what do you get when multiplying those two numbers together\n4:43:30 I''m just going to modify that to say in Celsius so that I can understand thank\n4:43:36 you okay so for this one we can see what did we get so we got current temperature\n4:43:41 in ow we got multiply 5 by five which our second question and then we also got\n4:43:49 subtract interesting that I I don''t know why it did that it''s kind of weird so it\n4:43:54 it decided to use oh ah okay so this is\n4:44:00 okay so then here it was okay that kind of makes sense does\n4:44:05 that make sense roughly okay so I think the the conversion for Fahrenheits Celsius is\n4:44:11 say like subtract 32 okay yes so to go from Fahrenheit to\n4:44:18 Celsius you are doing basically Fahrenheit minus 32 and then you''re\n4:44:23 multiplying by this number here which the iume the AI did\n4:44:28 not oh it roughly did okay so subtracting 36 like 32 would have given us four and it gave us approximately two\n4:44:36 so if you think okay multiply by this it''s practically multiplying by 0.5 five\n4:44:41 so halfing the value and that would give us roughly 2 so that''s what this was\n4:44:47 doing here kind of interesting Okay cool so we''ve gone through we have seen how\n4:44:54 to build a fully fledged chat application using what\n4:44:59 we''ve learned throughout a course and we''ve built quite a lot if you think about this application you''re getting\n4:45:07 the real time updates on what tools are being used the parameters being input to those tools and then that is all being\n4:45:13 returned in a streamed output and even in a structured output for your final\n4:45:18 answer including the answer and the tools that we use so of course you know what we built here is fairly limited but\n4:45:26 it''s super easy to extend this like you could maybe something that you might want to go and do is take what we''ve\n4:45:33 built here like Fork this application and just go and add different tools to it and see what happens because this is\n4:45:40 very extensible you can do a lot with it but yeah that is the end of the course\n4:45:46 of course this is just the beginning of whatever it is you''re wanting to learn\n4:45:52 or build with AI treat this as the beginning and just go out and find all\n4:45:58 the other cool interesting stuff that you can go and build so I hope this course has been useful\n4:46:06 informative and gives you an advantage in whatever it is is you''re going out of this build so thank you very much for\n4:46:13 watching and taking the course and sticking through right to the end I know it''s pretty long so I appreciate it a\n4:46:21 lot and I hope you get a lot out of it thanks bye"',
  '{"channel": "James Briggs", "video_id": "Cyv-dgv80kE", "duration": "4:46:46", "level": "ADVANCED", "application": "LangChain", "topics": ["LangChain v0.3", "LLM Basics", "LCEL", "Prompts", "Chat Memory", "Agents", "Agent Executor", "Streaming", "Async", "LangSmith", "Capstone Project"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Cyv-dgv80kE',
  3,
  'LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]',
  '### Ran Playwright code await page.evaluate(''() => {\n  const segments = document.querySelectorAll(\''ytd-transcript-segment-renderer\'');\n  let transcript = [];\n  segments.forEach(segment => {\n    c...',
  '### Ran Playwright code
await page.evaluate(''() => {\n  const segments = document.querySelectorAll(\''ytd-transcript-segment-renderer\'');\n  let transcript = [];\n  segments.forEach(segment => {\n    const timestamp = segment.querySelector(\''.segment-timestamp\'')?.textContent?.trim();\n    const text = segment.querySelector(\''.segment-text\'')?.textContent?.trim();\n    if (timestamp && text) {\n      transcript.push(`${timestamp} ${text}`);\n    }\n  });\n  return transcript.join(\''\\n\'');\n}'');

### New console messages
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204 was preloaded using li...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204?conn2 was preloaded us...
- [WARNING] The resource https://i.ytimg.com/generate_204 was preloaded using link preload but not use...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204 was preloaded using li...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204?conn2 was preloaded us...
- [WARNING] The resource https://i.ytimg.com/generate_204 was preloaded using link preload but not use...

### Page state
- Page URL: https://www.youtube.com/watch?v=Cyv-dgv80kE
- Page Title: LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3] - YouTube
- Page Snapshot:
```yaml
- <changed> generic [ref=e27]:
  - ref=e28 [unchanged]
  - ref=e29 [unchanged]
  - button "1 Minutes 53 Seconds of 4 Hours 46 Minutes 46 Seconds" [ref=e16785] [cursor=pointer]: 1:53 / 4:46:46
  - ref=e288 [unchanged]
```',
  '{"channel": "James Briggs", "video_id": "Cyv-dgv80kE", "duration": "4:46:46", "level": "ADVANCED", "application": "LangChain", "topics": ["LangChain v0.3", "LLM Basics", "LCEL", "Prompts", "Chat Memory", "Agents", "Agent Executor", "Streaming", "Async", "LangSmith", "Capstone Project"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=ZaPbP9DwBOE',
  1,
  'Don''t learn AI Agents without Learning these Fundamentals',
  '0:00 A lot has been going on with AI over the past few years. Prompt engineering, context, windows, tokens, embeddings,\n0:06 rag, vector DB, MCPS, agents, lang chain, langraph, claude, Gemini, and\n0...',
  '0:00 A lot has been going on with AI over the past few years. Prompt engineering, context, windows, tokens, embeddings,\n0:06 rag, vector DB, MCPS, agents, lang chain, langraph, claude, Gemini, and\n0:12 more. If you felt left out, this is the only video you''ll need to watch to catch up. In this video, we assume you know\n0:18 absolutely nothing and try to explain all of these concept through a single project so that by the end of it, you go\n0:24 from zero to gaining an overall understanding of everything that''s going on with AI. We''ll start with AI\n0:30 fundamentals, then move on to rag, vector DB, lang chain, langraph, MCP,\n0:35 prompt engineering, and finally put it all together with a complete system. Let''s start with the basics. When you\n0:40 ask an AI model a question, it''s typically answered by a subset of AI called large language models. Large\n0:46 language models have gotten popular right around when Chachib was released in late 2022 when we started to see\n0:52 language models get larger in size because of their obvious benefits in performance. So let''s dig a bit deeper\n0:58 to understand how large language models are able to process requests that we send. Popular LLMs like OpenASGPT,\n1:06 Enthropics Claude, and Google''s Gemini are all transformer models that are trained on large sets of data. The size\n1:13 of training tokens can go up to tens of trillions of tokens that are used to train these models. And the training\n1:19 data includes data from thousands of different domains like healthcare, law, coding, science, and more. But when we\n1:25 work in TechCorb, the 500 GB of data that we have aren''t part of the training data that was used to train the model,\n1:32 which means that in order for us to use the LLMs to ask questions about the TechCorp''s internal documents, we need\n1:38 the ability to pass in data to the LLM. One of the ways that we can pass the data into the model is by adding them to\n1:45 the conversation history functions like a short-term memory where during the duration of the conversation, all of\n1:51 this context is kept in memory. And this memory is called the context window. Context windows are measured in tokens\n1:58 which is roughly 3/4 of a word for English text. The context window is typically limited in size and the upper\n2:05 limit varies depending on the model. Some models like XAI GO 4 have 256,000\n2:11 tokens whereas Enthropics Cloud Opus 4 has 200,000 tokens and Google''s Gemini\n2:16 2.5 Pro has 1 million tokens. So as you can see the total upper bound for how\n2:22 much context can be stored for each model can vary. While the context window plays an important role in storing them\n2:28 in memory, there are practical limitations in how LLM treats what''s inside the context window. For example,\n2:34 if I asked you to memorize the pi digits 3.141592653589791\n2:42 and asked you to recite it, some of you might have a hard time committing that many numbers all at once, which is\n2:47 similar to how LLM''s context window works. So therein lies the current limitations in LLM. How much context can\n2:55 it hold in a given time? This can vary depending on model to model. For example, a lot of nano, mini, and flash\n3:02 models can have very small context windows in the size of 2,000 to 4,000 tokens, which amounts to about 1,500 to\n3:09 3,000 words. Conversely, bigger models like GPT4.1 and Gemini 2.5 Pro offer\n3:16 context windows up to 1 million tokens, which is equivalent to roughly 7,500\n3:21 words or 50,000 lines of code. So, as you can see, choosing the right model\n3:26 for the task can be very important. For example, if you downloaded a novel in a\n3:31 txt format and you wanted to change the script, choosing a model that offers a large context window would be best.\n3:38 Conversely, if you are working on a small document and require very low latency, meaning faster responses, using\n3:45 flash and nano variants would be best. Here''s another angle to look at when it comes to memory in LLMs. Let''s say I ask\n3:52 you this question. Sally and Bob own an apple farm. Sally has 14 apples. Apples\n3:58 are often red. 12 is a nice number. Bob has no red apple, but he has two green\n4:03 apples. Green apples often taste bad. How many apples do they all have? This\n4:08 might require you to think about the problem a little bit to get to the final answer, which is 16. That''s because the\n4:14 context here includes information that is completely irrelevant to the question, which is to count how many\n4:20 apples they have in total. The fact that apples are red or green or how it tastes\n4:25 have nothing to do with the total number of apples that they have because they either have the apple or they don''t. Now\n4:31 that we have a grasp on what context window provides, Techorp''s 500 GB of documents, this creates an immediate\n4:38 problem. Even the largest context window, like Gemini 2.5 Pro''s 1 million tokens, can hold only about 50 files of\n4:46 typical business documents all at once. We need our AI model to understand all\n4:51 500 gigabytes, but it can only see a tiny fraction at a given moment. This is\n4:56 where embedding comes in, and they''re absolutely crucial to understand. Embeddings transform the way we think\n5:02 about information. Instead of storing text as words, we convert meaning into numbers. The sentence employee vacation\n5:10 policy and staff time off guidelines use completely different words, but they mean essentially the same thing.\n5:17 Embeddings capture that semantic similarity. And here''s how it works. An embedding model takes a text and\n5:23 converts it into a vector. Typically, 1536 numbers that represent the meaning.\n5:29 Similar concepts end up with similar number patterns like vacation and holiday will have vectors that are\n5:35 mathematically close to each other. For TechCorb, this means that we can find relevant documents based on what someone\n5:42 means, not just the exact word that they''ve used. When an employee asks, \"Can I wear jeans to work?\" Our system\n5:48 will find the dress code policy, even if it never mentions the word jeans specifically.\n5:54 Now that we understand how LLMs and embeddings work, we will need a system that ties everything together. In our\n6:01 case, Tech Cororb needs a chatbot where customers can ask questions about the company policy, product information, and\n6:07 support issues. The chatbot needs to remember conversation history, access the company knowledge base, and handle\n6:14 complex multi-step interactions. Your first instinct might be to use OpenAI''s SDK to build a quick chat interface. But\n6:22 you quickly realize that there are massive missing pieces. Storing chat messages, maintaining conversation\n6:27 context, connecting to Tech Corp''s internal knowledge base, and handling the possibility that the company might\n6:33 switch from OpenAI to Anthropic or Google in the future. And now what seemed like a simple project becomes a\n6:39 massive undertaking. While you can write your own implementation to connect them, there''s already a wellestablished\n6:45 abstraction layer called langchain. Langchain is an abstraction layer that helps you build AI agents with minimal\n6:52 code. It addresses all those pain points using pre-built components and standardized interfaces. But first,\n6:58 let''s understand the crucial difference between an LLM and an agent. When you use large language models like GBT,\n7:04 Claude and Gemini directly, you''re using them as static brain that can answer question based on their training data.\n7:11 An agent on the other hand has autonomy, memory, and tools to perform whatever task it thinks that is necessary to\n7:18 complete your request. For TechCorp''s customer support scenario, imagine a customer asks, \"What''s your company''s\n7:24 policy on refunding my product that arrived damaged?\" An agent will self-determine how it should answer that\n7:30 request based autonomously instead of traditional software that requires conditional statement that determines\n7:36 how a program should execute. Langchain comes with extensive pre-built components that handle the heavy lifting\n7:42 for Techorp Chatbot. Langchain chat models provide direct access to LLM\n7:48 providers. Instead of writing custom API integration code, you can set up OpenAI with open bracket model equals GPT3\n7:55 turbo. So if the requirements change to use enthropic instead, you simply change one line LLM equals chat enthropic open\n8:02 bracket model equals claw 3 sonnet. This same pattern applies to every other capability techp needs. Memory\n8:09 management uses memory saver to automatically store and retrieve chat history, which means there''s no need to\n8:15 build your own database schema or session management. Vector database integration works through standardized\n8:21 interfaces. Whether you choose Pine Cone or Chroma DB, Langchain provides consistent APIs and we''ll go through\n8:27 what a vector database is in the next couple chapters. For text embedding, it uses OpenAI embeddings or similar\n8:34 components to convert Tech Corp''s document into vector representation. The embedding process becomes a single\n8:40 function call instead of managing API connections and data transformations manually. Finally, tool integration\n8:46 allows the agent to access external system. So if you need to query Tech Corp''s customer database, you can simply\n8:53 create a tool that the agent can call when it determines customer specific information is needed. Without lang\n8:59 chain, you would need to build all of this infrastructure yourself. API management for multiple LLM providers,\n9:05 vector databases, SDKs, embedding pipelines, semantic search logic, state\n9:10 management, memory system, and tool routing. The complexity grows exponentially. Lang chain''s component\n9:16 library includes modules like chat anthropic API connections. Chromad vector database operations, OpenAI\n9:23 embeddings for texttoveector conversion, memory saver for chat history management, custom tool definitions for\n9:29 external system integrations. The agent orchestrates these components based on the conversation context. So as we''re\n9:36 talking about tech corp depending on what the question is asked the agent will now use the given tools like vector\n9:42 databases as well as the context it built from conversation memory and the system prompt written in API layer\n9:48 autonomously handle your request and you can extend the agents abilities beyond this example by using other pre-built\n9:55 tools that lang chain offers like custom database access web search local file\n10:00 system access and more. Now that we covered the conceptual elements of lang chain, let''s look at how it looks like\n10:06 on a practical level. We can look over at this lab specifically geared towards how to use lang chain. All right, let''s\n10:12 start with the labs. In this lab, we''re going to explore how to make your very first AI API calls. The mission here is\n10:18 to take you from absolute zero to being able to connect, call, and understand responses from OpenAI''s APIs in just a\n10:26 few progressive steps. We begin by verifying our environment. In this step, we''re asked to activate the virtual\n10:32 environment. Check that Python is installed. Ensure the OpenAI library is available and confirm that our API keys\n10:39 are set. This is important because without this foundation, nothing else will work. Once the verification runs\n10:45 successfully, the lab will confirm that the environment is ready. Next, we take a moment to understand what OpenAI is.\n10:52 Here we''re introduced to the company behind chatbt and their family of AI models including GBT4, GBT4.1 Mini and\n11:00 GBT 3.5. The narration highlights that we''ll be working with the Python OpenAI library which acts as a bridge between\n11:07 our code and OpenAI server. With that context set, we move into task one. In this task, we''re asked to open up a\n11:14 Python script and complete the missing imports. Specifically, we need to import the OpenAI library and the OS library.\n11:21 After completing these lines, we run the script to make sure that the libraries are properly installed and ready to use.\n11:27 If everything is correct, the program confirms that the import worked. From here, we transition into authentication\n11:33 and client setup. Here, the lab explains the importance of an API client, an API key, and the base URL. The API key works\n11:41 like a password that identifies us and grants access, while the base URL defines the server location where\n11:47 requests are sent. This prepares us for task two. In task two, we open another\n11:52 Python script and are asked to initialize the client by plugging in the correct environment variables. This\n11:58 involves making sure we pass the OpenAI API key and OpenAI API base. Once those\n12:03 values are filled in, we run the script to verify the client has been properly initialized. If done correctly, the\n12:10 script confirms the connection to OpenAIS servers. Once the setup is complete, we move on to the heart of the\n12:15 lab, making an API call. Before jumping into it, we learn what chat completions are. This is OpenAI''s conversational API\n12:22 where we send messages and receive messages just like a chat. Lab explains the three roles in a conversation.\n12:29 System, user, and assistant, and how the request format looks like in Python. That takes us into task three. Here we\n12:36 open the script, uncomment the lines that define the model, role, and content, and then configure it. So the\n12:42 AI introduces itself. Once we run the script, if all is correct, the AI should respond back with an introduction. This\n12:49 is the first live call to the model. Next, we''re guided into understanding the structure of the response object.\n12:54 The lab breaks down the response path, showing how we drill down into the response, choices, message content to\n13:00 extract the actual text returned by the AI. Although the response object contains other fields like usage,\n13:06 statistics, and timestamps, most of the time what we really need is the content field. That brings us to task four,\n13:13 where we''re asked to update the script to extract the AI''s response using the exact path. Running the script here\n13:19 confirms that we can successfully capture and display the text that the AI returns. Once we''ve mastered making\n13:25 calls and extracting responses, the lab shifts gears to tokens and costs. We\n13:30 learned that tokens are the pieces of text used by the model that every request consumes tokens. Prom tokens are\n13:37 what we send in. Completion tokens are what the AI sends back and total tokens are the sums of both. Importantly,\n13:43 output tokens are more expensive than input tokens. So being concise can save money. Finally, in task five, we''re\n13:50 asked to extract the token usage values, prompt completion, and total tokens from the response. The script is already set\n13:57 up to calculate costs. So once we complete the extraction and run it, we can see exactly how much the API call\n14:03 costs. The lab wraps up by congratulating us. At this point, we verified our environment, connected to\n14:09 OpenAI, made real API calls, extracted responses, and calculated costs. The key\n14:14 takeaway is remembering how to navigate the response object with response.content.\n14:21 Some of the finer points and details like exploring usage fields or playing with different models are left for you\n14:27 to explore yourself. But by now, you should have a solid foundation for working with AI APIs and be ready for\n14:33 what comes next in the upcoming labs. All right, let''s start with the labs. In this lab, we''re going to explore Lang\n14:40 Chain and understand how it makes working with multiple AI provider simpler and faster. The key idea here is\n14:46 that instead of being locked into one provider''s SDK and rewriting code whenever you switch, Langchain offers\n14:53 one interface that works everywhere. With it, you can move from OpenAI to Google''s Gemini or XAI''s Gro by changing\n15:00 just a single word. We begin the environment verification. In this step, we''re asked to run a script that checks\n15:06 whether Langchain and its dependencies are installed, validates our API keys and our base URL, and confirms that we\n15:13 have access to different model providers. Once this check passes, we''re ready to start experimenting. The first\n15:20 test compares the traditional OpenAI SDK approach with Lang. We have to write 10 or more lines of boilerplate code just\n15:27 to make an API call. If we want to switch to another provider, we''d have to rewrite all of it. With Langchain, the\n15:33 same logic is cut down to just three lines. And switching providers is as simple as changing the model name. In\n15:39 this task, we''re asked to complete both versions in a script and then run them side by side.\n15:46 This is where we really see the 70% reduction in code. The second task\n15:51 demonstrates multimodel support. Here we''re asked to configure three providers. Open GPT4, Google''s Gemini,\n15:58 and XAS Gro, all with the same class and structure. Once configured, we can run the exact same prompt through all of\n16:05 them and compare their responses. This is especially powerful when you need to do AB test or balance cost because you\n16:11 can evaluate multiple models instantly without changing your code structure. In the third task, we''re introduced to\n16:17 prompt templates. Instead of writing separate hard-coded prompts for every variations, we create one reusable\n16:23 template with placeholders. Then we can fill in the variables dynamically just like fstrings in Python. This eliminates\n16:30 the nightmare of maintaining hundreds of slightly different prompt files. After completing the template, we test it with\n16:37 multiple inputs to see how the same structure generates varied responses. The fourth task takes a step further by\n16:44 introducing output parsers. Often AI responses are just free text, but what\n16:49 our code really needs are structured objects. Here we''re asked to add parsers that can transform responses into lists\n16:56 or JSON objects. In this way, instead of dealing with unstructured sentences, we\n17:01 can access clean Python lists or dictionaries that our application can use directly.\n17:07 Finally, we reach task five, which is all about chain composition. Langchain allows us to connect components together\n17:13 with pipe operator. Just like Unix pipes, instead of writing multiple variables for each step, creating a\n17:20 prompt, sending it to the model, getting a response, and parsing the result, we simply chain everything together. With\n17:26 one line, we can link prompts, models, and parsers, and then invoke the chain to get the structure output. It''s a much\n17:33 cleaner and more scalable way to build AI pipelines. By the end of this lab, we''ve learned how a lane chain reduces\n17:39 boiler plate, enables multimodel flexibility, creates reusable templates, parses structured outputs, and ties\n17:46 everything together with elegant chaining. Some of the finer details like experimenting with more complex parser\n17:52 setups or chaining additional steps are left for you to explore on your own.\n17:57 Now, we come to the technique, but it''s not a technique in the sense of building lane chain application like we just did.\n18:03 No, we''re talking about a technique that involves how you send your prompt to the agent that we just built. In other\n18:09 words, prompt engineering. When you send a prompt as an input to TechCorps AI\n18:14 document assistant that we just built, the quality of your prompt directly impacts the quality of responses you\n18:21 receive. While AI agents can certainly handle wide range of prompts, understanding prompt techniques help you\n18:27 communicate more effectively with TechCorp system. For example, if you prompt the agent with this question,\n18:33 \"What is the policy?\" It can pull a lot of details that are irrelevant. Sending a more specific prompt like, \"What''s the\n18:39 company''s remote work policy for international employees?\" will lead to a more accurate result from the agent. And\n18:46 the same thing applies to role definition when you''re describing the role of the agent. For example, you\n18:51 might descriptively write out a detailed prompt like, \"You are a tech customer support expert.\" When you are asked\n18:57 about the company''s policy, you are to always respond with bullet points for easier readability. As you can see,\n19:04 being able to control the agents behavior can directly benefit from a well-written prompt. This type of\n19:09 technique is referred to as prompt engineering. And there are different prompt techniques like zeroot, oneshot,\n19:15 fshot, and chain of thought prompting have its own use case for the task. For example, zerootshot prompting means that\n19:22 we are asking AI to perform a task without providing any examples. So if\n19:27 you send a prompt, write a data privacy policy for our European customers, you''re essentially relying entirely on\n19:34 the AI''s existing knowledge base to write the data policy document. Since within the prompt, we''re not giving any\n19:40 examples of what they are. Oneshot and few shot prompting is similar to zerootshot but in this case we''re\n19:46 providing examples of how the agent should respond directly within the prompt. For example, you might say\n19:53 here''s how we format our policy documents. Now write a data privacy policy following the same structure\n19:59 because you provided a template. The AI follows your specific formatting and style preferences more consistently. And\n20:06 conversely, fusart learning is the act of learning from the LLM side where even though the LLM might not have seen the\n20:12 exact training data for how to process your unique request, it''s able to demonstrate the ability to fulfill your\n20:19 request from similar examples provided. And finally, chain of thought prompting is a style of prompting where you\n20:25 provide the model with a trail of steps to think through how to solve specific problems. For example, instead of\n20:32 prompting the AI agent with fix our data retention policy, you might instead use\n20:37 chain of thought prompting to say here''s how you fix data retention policy. Review current GDPR requirements for\n20:43 data retention periods. Then analyze our existing policy for specific gaps. Then\n20:49 research industry best practices for similar companies. And finally, draft specific recommendations within\n20:55 implementation steps. Now fix our customer policy. As you can see, providing how LLM should go through in\n21:02 breaking down a specific request for how data retention policy should be fixed gives an exact blueprint for how the LLM\n21:09 should then go and fix the customer policy, which in this case, we''re not explicitly telling the agent how to fix\n21:15 the policy for that, but it gives the reasoning steps for the model to fix accordingly. So, in this lab, we''re\n21:22 going to master prompt engineering using lane chain. The main problem being addressed here is that AI can sometimes\n21:29 give vague or inconsistent responses or not follow instructions properly. The\n21:34 solution is to use structured prompting techniques, zero shot, one shot, viewshot, and chain of thought. Each of\n21:40 which controls the AI''s behavior in a different way. We begin by verifying the environment. The provided script checks\n21:46 that lang chain and its OpenAI integrations are installed, confirms that the API key and base URL are set\n21:53 and ensures that prompt template utilities are available. Once this verification passes, we''re ready to move\n21:58 into tasks. The first task introduces zero prompting. In this exercise, we''re asked to compare what happens when we\n22:05 provide a vague instruction versus when we write a very specific prompt. For example, simply asking the AI to write a\n22:12 policy results in a long generic essay. But when we specify write a 200word GDPR\n22:19 compliant privacy policy for European customer with 30-day retention period, the response is focused, useful, and\n22:26 aligned to the constraints. This demonstrates why being specific is crucial in zero shot prompts. The second\n22:32 task moves us to oneot prompting. Here we provide one example for the AI to\n22:37 follow almost like showing a single template. For example, if we gave the AI one refund policy example with five\n22:44 structured sections, we can then ask it to produce a remote work policy and it will replicate the same style in\n22:50 structure. This shows how one example can set the tone and ensure consistency\n22:55 across many outputs. Next, in task three, we expand on this with few shop\n23:00 prompting. Instead of one example, we provide multiple examples so the AI can learn not only the format but also the\n23:08 tone, patterns, and style. For example, giving three examples of emphatic\n23:13 support replies teaches the model how to handle customer support issues consistently. Once the examples are in\n23:19 place, the AI can generate new responses that follow the same tone and structure,\n23:24 making it especially powerful for use cases like customer service. In task 4,\n23:29 we''re introduced to chain of thought prompting. This technique encourages the AI to show its reasoning step by step.\n23:36 Instead of vague oneline answer, the AI breaks a problem into steps and works\n23:41 through it systematically. This results in clear or more reliable and more accurate outputs, particularly for\n23:48 complex reasoning tasks. Finally, task 5 brings all of these techniques together in a head-to-head comparison. We run the\n23:55 same problem through zero shot, one shot, few shot and chain of thought prompts to see the difference. Each\n24:01 approach has its strength. Zero shot is quick. One shot ensures formatting. Few shot enforces tone and consistency and\n24:08 chain of thought excels at detailed reasoning. The outcome shows that choosing the right technique can\n24:13 dramatically improve results depending on the task. By end of this lab, we not only learned what each prompt method is,\n24:20 but we''ve also seen them in action. The key takeaway is that the right technique can make your prompts 10 times more\n24:26 effective. Some of these exercises are left for you to explore and refine. But now you have the foundation to decide\n24:32 whether you need speed, structure, style, or reasoning in your AI responses. That wraps up this narration.\n24:39 And with it, you''re now ready to move on to the next lab onto vector databases and semantic search.\n24:45 Let''s do a quick recap of what we just built. We learned about what LLM is and how LLMs use what''s inside the context\n24:52 window. After learning about LLMs, we wanted to solve Tech Corp''s business requirements of searching for 500 GB\n24:59 worth of data. In order to do that, we determined that embedding is a good way to search a massive set of documents.\n25:06 After that, we went over Langchain and what functions they serve, which is that they allow us to easily build Gentic\n25:12 application like Tech Corps chatbot. So now that we have the lang chain application, we need to be able to\n25:17 search through these large sets of documents. Let''s say inside the 500 GB of documents, your company has a\n25:24 document called employee handbook that covers policies like time off, dress code, and equipment use. Employees might\n25:31 ask terms like vacation policy, but miss time off guidelines. While these are\n25:36 common questions that people would typically ask, building a database around this requirement can be tricky.\n25:42 In a conventional approach where data is stored in a structured database like SQL, you typically need to do some\n25:48 amount of similarity search like select all from documents content like vacation\n25:54 or vacation policy with a wild card before and after to look for details about questions on holiday. To expand\n26:00 your result set, you might increase the scope by adding VA or VAC space P.\n26:05 However, the drawback to this approach is that it puts the onus on the person searching for the data to get the search\n26:12 term formatted correctly. But what if there was a different way to store the data? What if instead of storing them by\n26:18 the value, we store the meaning of those words? This way, when you search the database by sending the question itself\n26:24 of can I request time off on a holiday based on the meaning of those words contained in the question, the database\n26:30 returns only relevant data back. This is a spirit of what vector databases tries\n26:35 to address storing data by the embedding. So essentially instead of searching by value, we can now search by\n26:42 meaning. Popular implementation of vector databases include pine cone and chroma. These platforms are designed to\n26:49 handle embeddings at scale and provide efficient retrieval based on semantic similarity. And these are also great use\n26:56 cases for prototyping something quick. While conceptually this seems straightforward, there''s a bit of an\n27:02 overhead in setting this up. And you might be asking, well, can we just throw the employee handbook into the database\n27:08 like we just did for SQL database? Not quite. And here''s why. With SQL database, the burden is put on the user\n27:15 searching to structure the database. But with vector databases, the burden is put on you who is setting up the database\n27:21 since you are trying to make it easier for someone searching for the data. And you can imagine why a method like this\n27:27 is becoming extremely popular when paired with large language models in AI since you don''t have to train separately\n27:33 on how LLM should search your database. Instead, the LLM can freely search based\n27:39 on meaning and have the confidence that your database will return relevant data it needs. So let''s explore some of the\n27:45 key concepts behind what goes into setting up a basic vector database. Let''s start with embedding. Embedding is\n27:52 really the key concept that makes the medium go from value to meaning. In SQL, we store the values contained in the\n27:59 employee handbook as a straightup value. But in a vector database, you need to do some extra work up front to convert the\n28:06 value into semantic meanings. And these meanings are stored in what''s called embeddings. For example, the words\n28:12 holiday and vacation should semantically share a similar space since the meaning of those words are close to each other.\n28:18 So before the sentence employee shall not request time off on holidays in the document is added to the database. The\n28:25 system runs through an embedding model and the embedding model converts that sentence into a long vector of numbers\n28:32 and when you search the database you are actually comparing this exact vector. That way when someone later asks can I\n28:38 take vacation during a holiday even though the phrasing is a little bit different the database can still service\n28:44 the request. And this is the fundamental shift. Instead of searching by exact wording, we''re now searching by meaning.\n28:50 Another important concept is dimensionality. And you might be asking, why do I have to worry about\n28:56 dimensionality? Can''t I just throw the words into embedding and store it into the database? There''s one more aspect in\n29:02 embedding that you need to think about, and that''s dimensionality. Typically, a word doesn''t just have one meaning to\n29:08 learn from. For example, the word vacation can have different semantics depending on the context that is used\n29:14 in. and capturing all those intricacies like tone, formality, and other features\n29:19 can give richness to those words. Typically, dimensions we use today are 1536 dimensions, which is a good mix of\n29:27 not having too much burden in size, but also giving enough context to allow for depth in each search. Once the embedding\n29:34 is stored with proper dimension, there are two other major angles that we need to consider when we''re working with\n29:40 vector databases. And this is the retrieval side. Meaning now that we store the meaning of those words, we\n29:46 have to take on the burden of the retrieval side of embeddings. Since we are not doing searches like we did in\n29:52 SQL with a wear query, we need to make a decision on what would technically be counted as a much and by how much. This\n30:00 is done by looking at scoring and chunk overlap. And if you''re at this point wondering, this seems like a lot of\n30:05 tweaking just to use vector database. And that''s the serious trade-off you ought to consider when using a vector\n30:11 database, which is that while a properly set up vector database makes searching so much more flexible, getting the\n30:17 vector database properly configured often adds complexity up front. So with that in mind, scoring is a threshold you\n30:24 set to how similar the results need to be to be considered a proper match. For example, the word Florida might have\n30:30 some similarity to the word vacation since it''s often where people go for vacation. But asking the question, can I\n30:37 take my company laptop to Florida is very different than does my company allow vacation to Florida? Since one is\n30:44 asking about a policy in IT jurisdiction and the other is about vacation policy.\n30:50 So setting up a score threshold based on the question can help you limit those low similarities to count as a match.\n30:57 Okay, there''s one final angle which is chunk overlap. So in SQL, we''re used to\n31:02 storing things rowby row, but in vector databases, things look a little bit different. When we''re storing values in\n31:08 vector databases, they''re often chunked going into the database. So when we chunk down an entire employee handbook\n31:14 into chunks, it''s possible that the meaning gets chunked with it. That''s why we allow chunk overlap so that the\n31:21 context spills over to leave enough margin for the search to work properly.\n31:26 In this app, we''re going to build a semantic search engine step by step. The story begins with TechDoc Inc. where\n31:32 users search through documentation 10,000 times a day. But more than half of those searches fail. Why? Because\n31:39 traditional keyword search can''t connect reset password with password recovery process. Our mission is to fix that by\n31:47 building a search system that understands meaning, not just words. We begin with the environment setup. In\n31:53 this step, we''re asked to install the libraries that make vector search possible. Sentence transformers for\n31:58 embeddings. lang chain for orchestration, chromadb for vector database and few utility libraries like\n32:04 numpy. Once installed, we verify the setup using a provided script. If everything checks out, we''re ready to\n32:10 move forward. Next, we take a moment to understand embeddings. These are the backbone of semantic search. Instead of\n32:16 treating texts as words, embedding converts text into numerical vectors. Similar meanings end up close to each\n32:23 other in this mathematical space. That means forgot my password in account\n32:28 recovery looks very different in words but almost identical in vector 4. This is a magic that allows our search engine\n32:35 to succeed where keyword search fails. That takes us into task number one where we put embedding into action. We open\n32:42 the script, initialize the mini LM model, encode both queries and documents and then calculate similarity using\n32:49 cosign similarity. Running the script demonstrates how a search for forgot password successfully matches password\n32:56 recovery, showing semantic understanding in real time. Once we understand embeddings, we move to document\n33:01 chunking. Large documents can''t be embedded all at once. So, we need to split them into smaller chunks, but if\n33:08 we cut too bluntly, we lose context. That''s why overlapping chunks are important. They preserve meanings across\n33:15 boundaries. For example, setting a chunk size of 500 characters with 100 characters overlap can improve retrieval\n33:22 accuracy by almost 40%. Lang chain helps us do this intelligently. In task number\n33:27 two, we put this into practice by editing a script to import lane chain''s recursive characters text splitter and\n33:33 set the chunking parameters. Running the script confirms that our documents are now split into overlapping pieces ready\n33:40 for storage. The next concept we explore is vector stores. Embeddings alones are just numbers. We need a system to store\n33:47 and search through them efficiently. That''s where Chromma comes in. It''s a production ready vector database that\n33:53 can handle millions of embeddings, perform similarity search in milliseconds, and support metadata\n33:58 filtering. In task number three, we''re asked to create a vector store using Chroma DB. We import the necessary\n34:04 classes, configure the embedding model, and then run the script. Once confirmed, we have a working vector store that can\n34:11 accept documents and retrieve them semantically. Finally, we bring everything together with semantic\n34:16 search. Here we implement the full pipeline, convert the user query into an embedding, search the Chromma store,\n34:24 retrieve the most relevant document chunks, and return them to the user. For example, a query like work from home\n34:30 policy will now correctly surface remote work guidelines. In task 4, we configure\n34:36 the query, set the number of top results to return, and establish a threshold for\n34:41 similarity scores. Running the script validates our search engine end to end. The lab closes with a recap. We started\n34:48 with a broken keyword search system where 60% of the search failed. Along the way, we learned about embeddings,\n34:54 smart document chunking, vector stores, and semantic search. By the end, we built a productionready search engine\n35:01 with 95% success rate. Some of the deeper experiments like adjusting chunk sizes, testing different embedding\n35:08 models and or adding metadata filters are left for you to explore on your own.\n35:14 So is it possible that instead of searching through the entire 500 GB of documents, AI assistant can fit them\n35:20 into their context window and generate output. This is called rag or retrieval augmented generation. Let''s say your\n35:27 company used the AI assistant to ask this question. What''s our remote work policy for international employees? In\n35:34 order to understand how rag works, we need to break them into three simple steps. Retrieval, augmented, and\n35:40 generation. Starting with retrieval, just like how we convert the document into vector embeddings to store them\n35:47 inside the database, we do the exact same step for the question that reads, \"What''s our remote work policy for\n35:53 international employees?\" Once the word embedding for this question is generated, the embedding for that\n35:58 question is compared against embeddings of the documents. This type of search is called semantic search where instead of\n36:05 searching by the static keywords to find relevant contents, the meaning and the context of the query is used to match\n36:12 against the existing data set. Moving on to augmentation in rag refers to the process where the retrieved data is\n36:18 injected into the prompt at runtime. And you might think why is this all that special? Typically, AI assistants rely\n36:25 on what they learned during pre-training, which is static knowledge that can become outdated. Instead, our\n36:31 goal here is to have the AI assistant rely on up-to-date information stored in the vector database. In the case of RAG,\n36:38 the semantic search result pends to the prompt that essentially serves as an augmented knowledge. So, for your\n36:44 company, the AI assistant is given details from company''s documents that are real, up-to-date, and private data\n36:51 set. And all this can occur without needing to fine-tune or modify the large language model with custom data. The\n36:58 final step of rag is generation. This is a step where AI assistant generates the\n37:03 response given the semantic relevant data retrieved from the vector database. So the initial prompt that says what''s\n37:10 our remote work policy for international employees? The AI assistant will now demonstrate its understanding of your\n37:16 company''s knowledge base by using the documents that relate to remote work and policy. And since the initial prompt\n37:23 specifies a criteria of international employees, the generation step will use its own reasoning to wrestle with the\n37:30 data provided to best answer the question. Now, RAG is a very powerful system that can instantly improve the\n37:36 depth of knowledge beyond its training data. But just like any other system, learning how to calibrate is an acquired\n37:43 skill that needs to be learned to get the best results. Setting up a rag system will look different from one\n37:49 system to another because it heavily depends on the data set that you''re trying to store. For example, legal\n37:55 documents will require different chunking strategies than a customer support transcript document. This is\n38:01 because legal documents often have long structured paragraph that need to be preserved and intact. While\n38:06 conversational transcript can be just fine with sentence level chunking with high overlap to preserve context. In\n38:14 this lab, we''re taking our semantic search system to the next level by adding AI power generation. Up until\n38:20 now, we''ve been able to find relevant documents with high accuracy. For example, matching remote work policy\n38:27 when someone searches work from home. But the CEO wants more. Instead of retrieving a document, the system should\n38:33 actually answer the user''s questions directly. something like yes, you can work three days from home, not just\n38:39 showing a PDF. We begin the environment setup. In this step, we''re asked to activate the Python environment and\n38:45 install the key libraries. These include Chroma DB for vector storage, sentence transformers for embeddings within lane\n38:52 chain with integrations for OpenAI and hugging face. Once installed, we verify everything using the provided script to\n38:59 ensure the rack framework is ready. Next, we move into task number one, setting up the vector store. Here we\n39:05 initialize a chromabb client. Create our get collection named tech corp brag and\n39:11 configure the embedding model all mini6v2. This is where we get our system of\n39:17 memory. A place where all of our company documents will be stored as vectors so that we can search them semantically. In\n39:25 task number two, the focus shifts to document processing and chunking. Unlike our earlier lab where we split text into\n39:32 fixed-size character chunks, here we upgrade to paragraph-based chunking with smart overlaps. The goal is to preserve\n39:40 meaning so that each chunk contains complete thoughts. This is crucial for RAG because when AI generates answers,\n39:47 the quality depends on having coherent chunks of context. From there, we go to task number three,\n39:54 LLM integration. This is where we connect OpenA model GPD4.1 Mini. The API\n40:00 key and base are already preconfigured for us. We just need to set the generation parameters like temperature,\n40:06 max tokens, and top P values. Once integrated, we can test simple text generation before layering on retrieval\n40:12 and augmented steps. Task number four introduces prompt engineering for rag. We''re asked to build a structured prompt\n40:19 template that always ensures context is included. The system prompt makes it clear that answers must come only from\n40:25 the retrieved documents. If the information isn''t in context, the AI must respond with, \"I don''t have that\n40:31 information in the provided documents.\" This keeps our answers factual and prevents hallucinations. Finally, we\n40:38 reach task number five, the complete rag pipeline. Here we wire everything\n40:43 together. The flow is embed the user query, search Chromad, retrieve the top\n40:49 three chunks, build a contextaware prompt, and generate an answer using LLM. The final touch is a source\n40:55 attribution. Every answer points back to the document it was derived from. This\n41:01 transforms the system into a full production ready Q&A engine. At the end,\n41:06 we celebrate rag mastery. What started as simple document search has evolved into a powerful system that retrieves,\n41:13 augments, and generates answers. This is the same architecture that powers tools like catchupt, claude, and gemini. Some\n41:21 parts like experimenting with different trunk strategy, refining prompts are left for you to explore yourself. But by\n41:27 now you''ve built a complete RAG system that answers questions with context,\n41:32 accuracy, and confidence. Now that we covered the conceptual elements of RAG, let''s look at how it\n41:39 looks on a practical level. To better understand this, we can look over at this lab specifically geared towards how\n41:45 to use Rag. Now we covered the basic concepts of simple chat application that\n41:50 allows us to chat with documents using vector databases and rag. Most business cases in the real world may be slightly\n41:58 more complicated. For example, in tech corp''s case, the business requirement might extend to more complex\n42:04 requirements like being able to connect the agent to human resource management system to pull employee documents to\n42:11 cross reference and make personalized responses. However, lang chain has limitations. When business requirements\n42:17 become more complex like multi-step workflows, conditional branching or iterative processes, you need something\n42:23 more sophisticated for better orchestration. That''s where langraph becomes essential. Langraph extends lane\n42:30 chain to handle more complex multi-step workflows that go beyond simple question and answer interactions. For example, if\n42:37 a customer asks, I need to understand our data privacy policy for EU customers. Since we assume that inside\n42:44 the 500 GB of database, it contains details about EU specific regulations,\n42:49 we need to create a system that can analyze Tech Corp''s data privacy policies for EU customers, ensuring\n42:55 compliance with GDPR, local regulations, and company standards. While in a traditional software development, you\n43:01 need to write code that can sequentially and conditionally call different sections of the code to process this\n43:07 request. With lang graph, this becomes a graph where each node handles a specific responsibility. For example, node one,\n43:15 search and gather privacy policy documents. Node two, extract and clean document content. Node three, evaluate\n43:22 GDPR compliance using LLM analysis. Node four, cross reference the local EU\n43:28 regulations. And node five, identify compliance gaps and generate recommendation. A node is an individual\n43:34 unit of computation. So think of a function that you can call. Once you have all the nodes created in langraph,\n43:41 you will then need to connect them and this connection is called an edge. Edges in langraph define execution flow. For\n43:48 example, after node one gathers documents, the edge routes to node two for content extraction. And after node 3\n43:56 evaluates compliance, a conditional edge either routes to node four for additional analysis or jumps to node\n44:03 five for report generation. And one final concept to keep in mind beyond nodes and edges is shared state between\n44:10 each node. This is possible by using state graph that essentially stores information throughout the entire\n44:17 workflow. For example, class compliance state type dictionary topic string\n44:23 documents list of string current documents optional string compliance score optional integer gaps list of\n44:30 string recommendation list of string can be used for nodes we identified before. As the workflow progresses, each node\n44:37 updates relevant state variables. Node one populates documents with found policy files. Node two processes\n44:44 individual documents and updates current document. Node 3 calculates compliance score. Node 4 identifies gaps. Node 5\n44:52 generates recommendation. The state graph orchestrates execution based on configured flow. If node 3 determines\n44:59 compliance score is below 75% the conditional edge routes back to node one to gather additional documents. If the\n45:06 score exceeds 75% execution proceeds to node 5 for final report generation. As\n45:13 you can see this creates powerful capabilities loops for iterative analysis conditional branching on\n45:18 intermediate results persistent state that maintains context across the entire workflow. So for Tech Corps compliance\n45:26 assistant, Langraph is an essential tool for workflow automation. All right,\n45:31 let''s start with the labs. In this lab, we''re diving into Langraph, a framework designed for building stateful\n45:37 multi-step AI workflows. Unlike simple chains, Langraph gives us specific control over how data moves, letting us\n45:44 create branching logic, loops, and decision points. By the end of this journey, we''ll have built a complete\n45:50 research assistant that can use multiple tools intelligently. We begin with environment setup. In this setup, we\n45:57 activate the Python virtual environment and install the required libraries. Langraph itself, Langchain, and OpenAI\n46:04 integration. Once everything is installed, we run a verification script to make sure our setup is ready. With\n46:10 the environment ready, we start small. Task number one introduces us to essential import. We bring in state\n46:17 graph end and type dict to define the data that flows through the workflow. Then we add a simple state field for\n46:24 messages. This is the foundation. State graph holds the workflow and marks\n46:29 completion and the state holds shared data. In task number two, we create our\n46:34 first nodes. Nodes are just Python functions that take state as inputs and\n46:39 return partial updates. In this case, we define a greeting node and an enhancement node. Once connected, one\n46:46 node outputs a basic greeting and the next node improves it with a bit of a flare. This demonstrates how state\n46:52 accumulates step by step. Pass number three is about edges. The connections between nodes. Here we use add nodes and\n46:59 add edges to wire greeting nodes to the enhancement node. With that, we built our first mini workflow. Data flows from\n47:06 one function to another. The state updates along the way. In task number four, we take it further with a\n47:13 multi-step flow. We add new nodes like a draft step and a review step. Connect them and see how the data moves through\n47:19 multiple stages. Each step preserves states, adds detail and passes it on.\n47:25 This mimics real world pipelines where content is outlined, drafted and polished. Pass number five introduces\n47:32 conditional routing. Instead of a fixed flow, the system now decides dynamically. For example, if the query\n47:39 is short, it routes one way. If detailed, it routes another. The router inspects the state and returns the next\n47:46 node name, making workflows flexible and adaptive. Then comes task number six,\n47:51 tool integration. Here we add a calculator tool. The router checks if the query is math related. If so, it\n47:58 routes to the calculator node which computes the answer. This is our first glimpse at how Langraph lets us\n48:04 integrate specialized tools directly into workflows. Finally, task number seven puts everything together into\n48:11 research agent. We combine the calculator with a web search tool like duck.go. Depending on the query, the\n48:18 system decides whether to perform a calculation, run a web search, or handle a text normally. This is a dynamic tool\n48:25 orchestration, the foundation of modern AI agents. By the end of this lab, we''ve\n48:30 gone from simple imports to a fully functional research assistant. We''ve seen how to build nodes, connect them,\n48:37 design multi-step flows, and add routing logic and integrate tools. Some of the deeper experiments like chaining more\n48:43 advanced tools or refining the router logic are left for you to explore on your own.\n48:51 Now that we covered lang chain and langraph and understood how techp business requirements can be met by\n48:57 leveraging pre-built tools that it offers, there''s one final piece that''s been popular since Anthropics released\n49:03 back in November 2022 called MCP or model context protocol. Techorps AI\n49:09 document assistant is working well for internal knowledge base, but employees might now need to access external\n49:16 systems like customer database, support systems, inventory management software, and other thirdparty APIs. And writing\n49:23 custom integrations to all these API connections will take a huge amount of time. MCP functions like an API, but\n49:31 with crucial differences that make it perfect for AI agents. Traditional APIs\n49:36 expose endpoints that require you to understand implementation details leading to rigid integrations tied to\n49:42 specific systems. MCP doesn''t just expose tools. It provides self-describing interfaces that AI\n49:49 agents can understand and use autonomously. The key advantage here is that unlike traditional APIs, MCP puts\n49:57 the burden on the AI agent rather than the developer. So when you start an MCP\n50:02 server, an instance starts and establish a connection with your AI agent. For example, the Techorps document assistant\n50:09 might easily have these MCP servers to enable powerful integration. Let''s say customer database MCP. When someone\n50:16 asks, \"What''s the status of the order 1 2 3 4?\" The AI uses an MCP to query\n50:22 TechCorb''s order management system, retrieves the current status, and provides a complete response. The same\n50:28 logic applies for support tickets, inventory databases and notification system mentioned earlier where we can\n50:34 simply plug into existing integrations of MCP servers to allow the agent to now\n50:40 extend its capabilities. For example, we can create a very simple MCP server code\n50:45 that looks something like this. Here we have fast MCP customer DB that starts the MCP server with the name customer DB\n50:53 at MCB tool that exposes a function to MCP clients. the AI can call like an API\n50:59 function parameters and return types that tells the MCP client what inputs are required and what type of output to\n51:06 expect. Finally, customers variable that is a fake database in this case stored in memory, but in your company''s case,\n51:13 you can connect this to a SQL database or MongoDB or any other custom database you might hold customer information on.\n51:19 Now, looking at this code might confuse you on what MCP really is, since it''s typically being talked about as a simple\n51:26 plugand play, and you''re right to think that way. The difference here is that this MCP server code that''s written only\n51:33 needs to be written once, and it doesn''t necessarily have to be you. In other words, a community of MCP developers\n51:40 might have written custom MCP servers for other popular tools like GitHub, GitLabs, or SQL databases, and you can\n51:46 simply use them directly on your agent without having to write the code yourself. That''s where the power of MCP\n51:53 really comes from. In this lab, we''re going deeper into MCP, model context\n51:58 protocol, and learning how to extend Langraph with external tools. Think of MCP as a universal port like USB that\n52:06 allows AI system to connect to any tool, database or API in a standardized way.\n52:11 With it, our langraph agents can go beyond built-in logic and integrate external services seamlessly. We begin\n52:18 with the environment step. In this step, we activate our virtual environment and install the key packages. Lang graph for\n52:26 the workflow framework, Langchain for the core abstractions, and Langchain OpenAI for the model integration. The\n52:32 setup also prepares for fast MCP, the framework we''ll use to build MCP servers. Once installed, we verify by\n52:40 learning the provided script, ensuring everything is ready for MCP development. Next, we get a conceptual overview of\n52:46 the MCP architecture. Here the lab explains that the MCP protocol acts as a\n52:52 bridge between an AI assistant built with Langraph and external tools. The\n52:57 flow works like this. The MCP server exposes tools and schemas. Langraph integrates with them and queries are\n53:04 routed intelligently. The naming convention MCP server tool ensures\n53:10 clarity when multiple tools are involved. A helpful analogy is comparing MCP to USB devices. A protocol is a\n53:17 port. The server is a device. The tools are its functions. And Langraph is a computer that uses them. That brings us\n53:24 to task number one, MCP basics. Here we''re asked to create our very first MCP\n53:30 server. The task involves initializing a server called calculator, defining a function as a tool with at MCP.tool\n53:37 decorator and running it with the SCDIO transport. This shows how simple it is to expose a structured function as an\n53:45 external tool. that langraph can later consume. In task number two, we integrate MCP with langraph. The\n53:52 challenge here is to connect the calculator server to an agent. This involves configuring the client fetching\n53:58 tools from the server create react agent that can decide when to call the calculator selected when needed. Next,\n54:06 task number three scales things up with multiple MCP servers. Instead of just a calculator, we add another server, in\n54:13 this case, a weather service. Now, Langraph orchestrates between both. The system retrieves available tools,\n54:19 creates an agent with access to both servers, and intelligently routes queries. If a user asks a math question,\n54:26 the calculator responds. If they ask about the weather, the weather tool responds. This is where we see the true\n54:32 power of MCP. Multiple servers are working together under a unified AI agent. The lab wraps up by celebrating\n54:39 MCP mastery. By now, we''ve created MCP servers, integrated them with ElangRaph,\n54:45 and orchestrated multiple tools. The key takeaways are that MCP is universal. It\n54:50 can connect any tool to any AI. Routing is what gives it power. The design is\n54:55 extendable, so we can add servers anytime. Some deeper explorations like exposing databases, APIs or file systems\n55:03 through MCP are left for you to explore on your own. That concludes this narration. Next, we''ll continue the\n55:10 journey by experimenting with resource exposure, human in the loop approval flows, and eventually deploying\n55:15 production ready MCP packages. Now that we have put all these pieces together like context windows, vector\n55:22 databases, lang chain, langraph, MCP, and prompt engineering, Techorp is now\n55:28 able to do complex document search that went from manual searching that could have taken up to 30 minutes to now less\n55:35 than 30 seconds using our AI agent. And we also have a higher accuracy using\n55:40 contextaware semantic search like using rag. And finally, the chat application UI allows users to have more\n55:47 satisfaction in working with a tool that can help keep track of conversation history and better intuition overall.\n55:54 And the availability for this is 24/7 as long as the application is running. And\n55:59 this is just the beginning. Imagine layering on predictive analytics, proactive compliance agents, and\n56:05 workflow automation that doesn''t just answer questions, but actively solves problems before employees can even ask.\n56:12 The shift from static documents to living intelligent system marks a turning point not just for Tech Corp,\n56:18 but for how every other business can unlock a full value of its knowledge using agents.\n0:00 A lot has been going on with AI over the past few years. Prompt engineering, context, windows, tokens, embeddings,\n0:06 rag, vector DB, MCPS, agents, lang chain, langraph, claude, Gemini, and\n0:12 more. If you felt left out, this is the only video you''ll need to watch to catch up. In this video, we assume you know\n0:18 absolutely nothing and try to explain all of these concept through a single project so that by the end of it, you go\n0:24 from zero to gaining an overall understanding of everything that''s going on with AI. We''ll start with AI\n0:30 fundamentals, then move on to rag, vector DB, lang chain, langraph, MCP,\n0:35 prompt engineering, and finally put it all together with a complete system. Let''s start with the basics. When you\n0:40 ask an AI model a question, it''s typically answered by a subset of AI called large language models. Large\n0:46 language models have gotten popular right around when Chachib was released in late 2022 when we started to see\n0:52 language models get larger in size because of their obvious benefits in performance. So let''s dig a bit deeper\n0:58 to understand how large language models are able to process requests that we send. Popular LLMs like OpenASGPT,\n1:06 Enthropics Claude, and Google''s Gemini are all transformer models that are trained on large sets of data. The size\n1:13 of training tokens can go up to tens of trillions of tokens that are used to train these models. And the training\n1:19 data includes data from thousands of different domains like healthcare, law, coding, science, and more. But when we\n1:25 work in TechCorb, the 500 GB of data that we have aren''t part of the training data that was used to train the model,\n1:32 which means that in order for us to use the LLMs to ask questions about the TechCorp''s internal documents, we need\n1:38 the ability to pass in data to the LLM. One of the ways that we can pass the data into the model is by adding them to\n1:45 the conversation history functions like a short-term memory where during the duration of the conversation, all of\n1:51 this context is kept in memory. And this memory is called the context window. Context windows are measured in tokens\n1:58 which is roughly 3/4 of a word for English text. The context window is typically limited in size and the upper\n2:05 limit varies depending on the model. Some models like XAI GO 4 have 256,000\n2:11 tokens whereas Enthropics Cloud Opus 4 has 200,000 tokens and Google''s Gemini\n2:16 2.5 Pro has 1 million tokens. So as you can see the total upper bound for how\n2:22 much context can be stored for each model can vary. While the context window plays an important role in storing them\n2:28 in memory, there are practical limitations in how LLM treats what''s inside the context window. For example,\n2:34 if I asked you to memorize the pi digits 3.141592653589791\n2:42 and asked you to recite it, some of you might have a hard time committing that many numbers all at once, which is\n2:47 similar to how LLM''s context window works. So therein lies the current limitations in LLM. How much context can\n2:55 it hold in a given time? This can vary depending on model to model. For example, a lot of nano, mini, and flash\n3:02 models can have very small context windows in the size of 2,000 to 4,000 tokens, which amounts to about 1,500 to\n3:09 3,000 words. Conversely, bigger models like GPT4.1 and Gemini 2.5 Pro offer\n3:16 context windows up to 1 million tokens, which is equivalent to roughly 7,500\n3:21 words or 50,000 lines of code. So, as you can see, choosing the right model\n3:26 for the task can be very important. For example, if you downloaded a novel in a\n3:31 txt format and you wanted to change the script, choosing a model that offers a large context window would be best.\n3:38 Conversely, if you are working on a small document and require very low latency, meaning faster responses, using\n3:45 flash and nano variants would be best. Here''s another angle to look at when it comes to memory in LLMs. Let''s say I ask\n3:52 you this question. Sally and Bob own an apple farm. Sally has 14 apples. Apples\n3:58 are often red. 12 is a nice number. Bob has no red apple, but he has two green\n4:03 apples. Green apples often taste bad. How many apples do they all have? This\n4:08 might require you to think about the problem a little bit to get to the final answer, which is 16. That''s because the\n4:14 context here includes information that is completely irrelevant to the question, which is to count how many\n4:20 apples they have in total. The fact that apples are red or green or how it tastes\n4:25 have nothing to do with the total number of apples that they have because they either have the apple or they don''t. Now\n4:31 that we have a grasp on what context window provides, Techorp''s 500 GB of documents, this creates an immediate\n4:38 problem. Even the largest context window, like Gemini 2.5 Pro''s 1 million tokens, can hold only about 50 files of\n4:46 typical business documents all at once. We need our AI model to understand all\n4:51 500 gigabytes, but it can only see a tiny fraction at a given moment. This is\n4:56 where embedding comes in, and they''re absolutely crucial to understand. Embeddings transform the way we think\n5:02 about information. Instead of storing text as words, we convert meaning into numbers. The sentence employee vacation\n5:10 policy and staff time off guidelines use completely different words, but they mean essentially the same thing.\n5:17 Embeddings capture that semantic similarity. And here''s how it works. An embedding model takes a text and\n5:23 converts it into a vector. Typically, 1536 numbers that represent the meaning.\n5:29 Similar concepts end up with similar number patterns like vacation and holiday will have vectors that are\n5:35 mathematically close to each other. For TechCorb, this means that we can find relevant documents based on what someone\n5:42 means, not just the exact word that they''ve used. When an employee asks, \"Can I wear jeans to work?\" Our system\n5:48 will find the dress code policy, even if it never mentions the word jeans specifically.\n5:54 Now that we understand how LLMs and embeddings work, we will need a system that ties everything together. In our\n6:01 case, Tech Cororb needs a chatbot where customers can ask questions about the company policy, product information, and\n6:07 support issues. The chatbot needs to remember conversation history, access the company knowledge base, and handle\n6:14 complex multi-step interactions. Your first instinct might be to use OpenAI''s SDK to build a quick chat interface. But\n6:22 you quickly realize that there are massive missing pieces. Storing chat messages, maintaining conversation\n6:27 context, connecting to Tech Corp''s internal knowledge base, and handling the possibility that the company might\n6:33 switch from OpenAI to Anthropic or Google in the future. And now what seemed like a simple project becomes a\n6:39 massive undertaking. While you can write your own implementation to connect them, there''s already a wellestablished\n6:45 abstraction layer called langchain. Langchain is an abstraction layer that helps you build AI agents with minimal\n6:52 code. It addresses all those pain points using pre-built components and standardized interfaces. But first,\n6:58 let''s understand the crucial difference between an LLM and an agent. When you use large language models like GBT,\n7:04 Claude and Gemini directly, you''re using them as static brain that can answer question based on their training data.\n7:11 An agent on the other hand has autonomy, memory, and tools to perform whatever task it thinks that is necessary to\n7:18 complete your request. For TechCorp''s customer support scenario, imagine a customer asks, \"What''s your company''s\n7:24 policy on refunding my product that arrived damaged?\" An agent will self-determine how it should answer that\n7:30 request based autonomously instead of traditional software that requires conditional statement that determines\n7:36 how a program should execute. Langchain comes with extensive pre-built components that handle the heavy lifting\n7:42 for Techorp Chatbot. Langchain chat models provide direct access to LLM\n7:48 providers. Instead of writing custom API integration code, you can set up OpenAI with open bracket model equals GPT3\n7:55 turbo. So if the requirements change to use enthropic instead, you simply change one line LLM equals chat enthropic open\n8:02 bracket model equals claw 3 sonnet. This same pattern applies to every other capability techp needs. Memory\n8:09 management uses memory saver to automatically store and retrieve chat history, which means there''s no need to\n8:15 build your own database schema or session management. Vector database integration works through standardized\n8:21 interfaces. Whether you choose Pine Cone or Chroma DB, Langchain provides consistent APIs and we''ll go through\n8:27 what a vector database is in the next couple chapters. For text embedding, it uses OpenAI embeddings or similar\n8:34 components to convert Tech Corp''s document into vector representation. The embedding process becomes a single\n8:40 function call instead of managing API connections and data transformations manually. Finally, tool integration\n8:46 allows the agent to access external system. So if you need to query Tech Corp''s customer database, you can simply\n8:53 create a tool that the agent can call when it determines customer specific information is needed. Without lang\n8:59 chain, you would need to build all of this infrastructure yourself. API management for multiple LLM providers,\n9:05 vector databases, SDKs, embedding pipelines, semantic search logic, state\n9:10 management, memory system, and tool routing. The complexity grows exponentially. Lang chain''s component\n9:16 library includes modules like chat anthropic API connections. Chromad vector database operations, OpenAI\n9:23 embeddings for texttoveector conversion, memory saver for chat history management, custom tool definitions for\n9:29 external system integrations. The agent orchestrates these components based on the conversation context. So as we''re\n9:36 talking about tech corp depending on what the question is asked the agent will now use the given tools like vector\n9:42 databases as well as the context it built from conversation memory and the system prompt written in API layer\n9:48 autonomously handle your request and you can extend the agents abilities beyond this example by using other pre-built\n9:55 tools that lang chain offers like custom database access web search local file\n10:00 system access and more. Now that we covered the conceptual elements of lang chain, let''s look at how it looks like\n10:06 on a practical level. We can look over at this lab specifically geared towards how to use lang chain. All right, let''s\n10:12 start with the labs. In this lab, we''re going to explore how to make your very first AI API calls. The mission here is\n10:18 to take you from absolute zero to being able to connect, call, and understand responses from OpenAI''s APIs in just a\n10:26 few progressive steps. We begin by verifying our environment. In this step, we''re asked to activate the virtual\n10:32 environment. Check that Python is installed. Ensure the OpenAI library is available and confirm that our API keys\n10:39 are set. This is important because without this foundation, nothing else will work. Once the verification runs\n10:45 successfully, the lab will confirm that the environment is ready. Next, we take a moment to understand what OpenAI is.\n10:52 Here we''re introduced to the company behind chatbt and their family of AI models including GBT4, GBT4.1 Mini and\n11:00 GBT 3.5. The narration highlights that we''ll be working with the Python OpenAI library which acts as a bridge between\n11:07 our code and OpenAI server. With that context set, we move into task one. In this task, we''re asked to open up a\n11:14 Python script and complete the missing imports. Specifically, we need to import the OpenAI library and the OS library.\n11:21 After completing these lines, we run the script to make sure that the libraries are properly installed and ready to use.\n11:27 If everything is correct, the program confirms that the import worked. From here, we transition into authentication\n11:33 and client setup. Here, the lab explains the importance of an API client, an API key, and the base URL. The API key works\n11:41 like a password that identifies us and grants access, while the base URL defines the server location where\n11:47 requests are sent. This prepares us for task two. In task two, we open another\n11:52 Python script and are asked to initialize the client by plugging in the correct environment variables. This\n11:58 involves making sure we pass the OpenAI API key and OpenAI API base. Once those\n12:03 values are filled in, we run the script to verify the client has been properly initialized. If done correctly, the\n12:10 script confirms the connection to OpenAIS servers. Once the setup is complete, we move on to the heart of the\n12:15 lab, making an API call. Before jumping into it, we learn what chat completions are. This is OpenAI''s conversational API\n12:22 where we send messages and receive messages just like a chat. Lab explains the three roles in a conversation.\n12:29 System, user, and assistant, and how the request format looks like in Python. That takes us into task three. Here we\n12:36 open the script, uncomment the lines that define the model, role, and content, and then configure it. So the\n12:42 AI introduces itself. Once we run the script, if all is correct, the AI should respond back with an introduction. This\n12:49 is the first live call to the model. Next, we''re guided into understanding the structure of the response object.\n12:54 The lab breaks down the response path, showing how we drill down into the response, choices, message content to\n13:00 extract the actual text returned by the AI. Although the response object contains other fields like usage,\n13:06 statistics, and timestamps, most of the time what we really need is the content field. That brings us to task four,\n13:13 where we''re asked to update the script to extract the AI''s response using the exact path. Running the script here\n13:19 confirms that we can successfully capture and display the text that the AI returns. Once we''ve mastered making\n13:25 calls and extracting responses, the lab shifts gears to tokens and costs. We\n13:30 learned that tokens are the pieces of text used by the model that every request consumes tokens. Prom tokens are\n13:37 what we send in. Completion tokens are what the AI sends back and total tokens are the sums of both. Importantly,\n13:43 output tokens are more expensive than input tokens. So being concise can save money. Finally, in task five, we''re\n13:50 asked to extract the token usage values, prompt completion, and total tokens from the response. The script is already set\n13:57 up to calculate costs. So once we complete the extraction and run it, we can see exactly how much the API call\n14:03 costs. The lab wraps up by congratulating us. At this point, we verified our environment, connected to\n14:09 OpenAI, made real API calls, extracted responses, and calculated costs. The key\n14:14 takeaway is remembering how to navigate the response object with response.content.\n14:21 Some of the finer points and details like exploring usage fields or playing with different models are left for you\n14:27 to explore yourself. But by now, you should have a solid foundation for working with AI APIs and be ready for\n14:33 what comes next in the upcoming labs. All right, let''s start with the labs. In this lab, we''re going to explore Lang\n14:40 Chain and understand how it makes working with multiple AI provider simpler and faster. The key idea here is\n14:46 that instead of being locked into one provider''s SDK and rewriting code whenever you switch, Langchain offers\n14:53 one interface that works everywhere. With it, you can move from OpenAI to Google''s Gemini or XAI''s Gro by changing\n15:00 just a single word. We begin the environment verification. In this step, we''re asked to run a script that checks\n15:06 whether Langchain and its dependencies are installed, validates our API keys and our base URL, and confirms that we\n15:13 have access to different model providers. Once this check passes, we''re ready to start experimenting. The first\n15:20 test compares the traditional OpenAI SDK approach with Lang. We have to write 10 or more lines of boilerplate code just\n15:27 to make an API call. If we want to switch to another provider, we''d have to rewrite all of it. With Langchain, the\n15:33 same logic is cut down to just three lines. And switching providers is as simple as changing the model name. In\n15:39 this task, we''re asked to complete both versions in a script and then run them side by side.\n15:46 This is where we really see the 70% reduction in code. The second task\n15:51 demonstrates multimodel support. Here we''re asked to configure three providers. Open GPT4, Google''s Gemini,\n15:58 and XAS Gro, all with the same class and structure. Once configured, we can run the exact same prompt through all of\n16:05 them and compare their responses. This is especially powerful when you need to do AB test or balance cost because you\n16:11 can evaluate multiple models instantly without changing your code structure. In the third task, we''re introduced to\n16:17 prompt templates. Instead of writing separate hard-coded prompts for every variations, we create one reusable\n16:23 template with placeholders. Then we can fill in the variables dynamically just like fstrings in Python. This eliminates\n16:30 the nightmare of maintaining hundreds of slightly different prompt files. After completing the template, we test it with\n16:37 multiple inputs to see how the same structure generates varied responses. The fourth task takes a step further by\n16:44 introducing output parsers. Often AI responses are just free text, but what\n16:49 our code really needs are structured objects. Here we''re asked to add parsers that can transform responses into lists\n16:56 or JSON objects. In this way, instead of dealing with unstructured sentences, we\n17:01 can access clean Python lists or dictionaries that our application can use directly.\n17:07 Finally, we reach task five, which is all about chain composition. Langchain allows us to connect components together\n17:13 with pipe operator. Just like Unix pipes, instead of writing multiple variables for each step, creating a\n17:20 prompt, sending it to the model, getting a response, and parsing the result, we simply chain everything together. With\n17:26 one line, we can link prompts, models, and parsers, and then invoke the chain to get the structure output. It''s a much\n17:33 cleaner and more scalable way to build AI pipelines. By the end of this lab, we''ve learned how a lane chain reduces\n17:39 boiler plate, enables multimodel flexibility, creates reusable templates, parses structured outputs, and ties\n17:46 everything together with elegant chaining. Some of the finer details like experimenting with more complex parser\n17:52 setups or chaining additional steps are left for you to explore on your own.\n17:57 Now, we come to the technique, but it''s not a technique in the sense of building lane chain application like we just did.\n18:03 No, we''re talking about a technique that involves how you send your prompt to the agent that we just built. In other\n18:09 words, prompt engineering. When you send a prompt as an input to TechCorps AI\n18:14 document assistant that we just built, the quality of your prompt directly impacts the quality of responses you\n18:21 receive. While AI agents can certainly handle wide range of prompts, understanding prompt techniques help you\n18:27 communicate more effectively with TechCorp system. For example, if you prompt the agent with this question,\n18:33 \"What is the policy?\" It can pull a lot of details that are irrelevant. Sending a more specific prompt like, \"What''s the\n18:39 company''s remote work policy for international employees?\" will lead to a more accurate result from the agent. And\n18:46 the same thing applies to role definition when you''re describing the role of the agent. For example, you\n18:51 might descriptively write out a detailed prompt like, \"You are a tech customer support expert.\" When you are asked\n18:57 about the company''s policy, you are to always respond with bullet points for easier readability. As you can see,\n19:04 being able to control the agents behavior can directly benefit from a well-written prompt. This type of\n19:09 technique is referred to as prompt engineering. And there are different prompt techniques like zeroot, oneshot,\n19:15 fshot, and chain of thought prompting have its own use case for the task. For example, zerootshot prompting means that\n19:22 we are asking AI to perform a task without providing any examples. So if\n19:27 you send a prompt, write a data privacy policy for our European customers, you''re essentially relying entirely on\n19:34 the AI''s existing knowledge base to write the data policy document. Since within the prompt, we''re not giving any\n19:40 examples of what they are. Oneshot and few shot prompting is similar to zerootshot but in this case we''re\n19:46 providing examples of how the agent should respond directly within the prompt. For example, you might say\n19:53 here''s how we format our policy documents. Now write a data privacy policy following the same structure\n19:59 because you provided a template. The AI follows your specific formatting and style preferences more consistently. And\n20:06 conversely, fusart learning is the act of learning from the LLM side where even though the LLM might not have seen the\n20:12 exact training data for how to process your unique request, it''s able to demonstrate the ability to fulfill your\n20:19 request from similar examples provided. And finally, chain of thought prompting is a style of prompting where you\n20:25 provide the model with a trail of steps to think through how to solve specific problems. For example, instead of\n20:32 prompting the AI agent with fix our data retention policy, you might instead use\n20:37 chain of thought prompting to say here''s how you fix data retention policy. Review current GDPR requirements for\n20:43 data retention periods. Then analyze our existing policy for specific gaps. Then\n20:49 research industry best practices for similar companies. And finally, draft specific recommendations within\n20:55 implementation steps. Now fix our customer policy. As you can see, providing how LLM should go through in\n21:02 breaking down a specific request for how data retention policy should be fixed gives an exact blueprint for how the LLM\n21:09 should then go and fix the customer policy, which in this case, we''re not explicitly telling the agent how to fix\n21:15 the policy for that, but it gives the reasoning steps for the model to fix accordingly. So, in this lab, we''re\n21:22 going to master prompt engineering using lane chain. The main problem being addressed here is that AI can sometimes\n21:29 give vague or inconsistent responses or not follow instructions properly. The\n21:34 solution is to use structured prompting techniques, zero shot, one shot, viewshot, and chain of thought. Each of\n21:40 which controls the AI''s behavior in a different way. We begin by verifying the environment. The provided script checks\n21:46 that lang chain and its OpenAI integrations are installed, confirms that the API key and base URL are set\n21:53 and ensures that prompt template utilities are available. Once this verification passes, we''re ready to move\n21:58 into tasks. The first task introduces zero prompting. In this exercise, we''re asked to compare what happens when we\n22:05 provide a vague instruction versus when we write a very specific prompt. For example, simply asking the AI to write a\n22:12 policy results in a long generic essay. But when we specify write a 200word GDPR\n22:19 compliant privacy policy for European customer with 30-day retention period, the response is focused, useful, and\n22:26 aligned to the constraints. This demonstrates why being specific is crucial in zero shot prompts. The second\n22:32 task moves us to oneot prompting. Here we provide one example for the AI to\n22:37 follow almost like showing a single template. For example, if we gave the AI one refund policy example with five\n22:44 structured sections, we can then ask it to produce a remote work policy and it will replicate the same style in\n22:50 structure. This shows how one example can set the tone and ensure consistency\n22:55 across many outputs. Next, in task three, we expand on this with few shop\n23:00 prompting. Instead of one example, we provide multiple examples so the AI can learn not only the format but also the\n23:08 tone, patterns, and style. For example, giving three examples of emphatic\n23:13 support replies teaches the model how to handle customer support issues consistently. Once the examples are in\n23:19 place, the AI can generate new responses that follow the same tone and structure,\n23:24 making it especially powerful for use cases like customer service. In task 4,\n23:29 we''re introduced to chain of thought prompting. This technique encourages the AI to show its reasoning step by step.\n23:36 Instead of vague oneline answer, the AI breaks a problem into steps and works\n23:41 through it systematically. This results in clear or more reliable and more accurate outputs, particularly for\n23:48 complex reasoning tasks. Finally, task 5 brings all of these techniques together in a head-to-head comparison. We run the\n23:55 same problem through zero shot, one shot, few shot and chain of thought prompts to see the difference. Each\n24:01 approach has its strength. Zero shot is quick. One shot ensures formatting. Few shot enforces tone and consistency and\n24:08 chain of thought excels at detailed reasoning. The outcome shows that choosing the right technique can\n24:13 dramatically improve results depending on the task. By end of this lab, we not only learned what each prompt method is,\n24:20 but we''ve also seen them in action. The key takeaway is that the right technique can make your prompts 10 times more\n24:26 effective. Some of these exercises are left for you to explore and refine. But now you have the foundation to decide\n24:32 whether you need speed, structure, style, or reasoning in your AI responses. That wraps up this narration.\n24:39 And with it, you''re now ready to move on to the next lab onto vector databases and semantic search.\n24:45 Let''s do a quick recap of what we just built. We learned about what LLM is and how LLMs use what''s inside the context\n24:52 window. After learning about LLMs, we wanted to solve Tech Corp''s business requirements of searching for 500 GB\n24:59 worth of data. In order to do that, we determined that embedding is a good way to search a massive set of documents.\n25:06 After that, we went over Langchain and what functions they serve, which is that they allow us to easily build Gentic\n25:12 application like Tech Corps chatbot. So now that we have the lang chain application, we need to be able to\n25:17 search through these large sets of documents. Let''s say inside the 500 GB of documents, your company has a\n25:24 document called employee handbook that covers policies like time off, dress code, and equipment use. Employees might\n25:31 ask terms like vacation policy, but miss time off guidelines. While these are\n25:36 common questions that people would typically ask, building a database around this requirement can be tricky.\n25:42 In a conventional approach where data is stored in a structured database like SQL, you typically need to do some\n25:48 amount of similarity search like select all from documents content like vacation\n25:54 or vacation policy with a wild card before and after to look for details about questions on holiday. To expand\n26:00 your result set, you might increase the scope by adding VA or VAC space P.\n26:05 However, the drawback to this approach is that it puts the onus on the person searching for the data to get the search\n26:12 term formatted correctly. But what if there was a different way to store the data? What if instead of storing them by\n26:18 the value, we store the meaning of those words? This way, when you search the database by sending the question itself\n26:24 of can I request time off on a holiday based on the meaning of those words contained in the question, the database\n26:30 returns only relevant data back. This is a spirit of what vector databases tries\n26:35 to address storing data by the embedding. So essentially instead of searching by value, we can now search by\n26:42 meaning. Popular implementation of vector databases include pine cone and chroma. These platforms are designed to\n26:49 handle embeddings at scale and provide efficient retrieval based on semantic similarity. And these are also great use\n26:56 cases for prototyping something quick. While conceptually this seems straightforward, there''s a bit of an\n27:02 overhead in setting this up. And you might be asking, well, can we just throw the employee handbook into the database\n27:08 like we just did for SQL database? Not quite. And here''s why. With SQL database, the burden is put on the user\n27:15 searching to structure the database. But with vector databases, the burden is put on you who is setting up the database\n27:21 since you are trying to make it easier for someone searching for the data. And you can imagine why a method like this\n27:27 is becoming extremely popular when paired with large language models in AI since you don''t have to train separately\n27:33 on how LLM should search your database. Instead, the LLM can freely search based\n27:39 on meaning and have the confidence that your database will return relevant data it needs. So let''s explore some of the\n27:45 key concepts behind what goes into setting up a basic vector database. Let''s start with embedding. Embedding is\n27:52 really the key concept that makes the medium go from value to meaning. In SQL, we store the values contained in the\n27:59 employee handbook as a straightup value. But in a vector database, you need to do some extra work up front to convert the\n28:06 value into semantic meanings. And these meanings are stored in what''s called embeddings. For example, the words\n28:12 holiday and vacation should semantically share a similar space since the meaning of those words are close to each other.\n28:18 So before the sentence employee shall not request time off on holidays in the document is added to the database. The\n28:25 system runs through an embedding model and the embedding model converts that sentence into a long vector of numbers\n28:32 and when you search the database you are actually comparing this exact vector. That way when someone later asks can I\n28:38 take vacation during a holiday even though the phrasing is a little bit different the database can still service\n28:44 the request. And this is the fundamental shift. Instead of searching by exact wording, we''re now searching by meaning.\n28:50 Another important concept is dimensionality. And you might be asking, why do I have to worry about\n28:56 dimensionality? Can''t I just throw the words into embedding and store it into the database? There''s one more aspect in\n29:02 embedding that you need to think about, and that''s dimensionality. Typically, a word doesn''t just have one meaning to\n29:08 learn from. For example, the word vacation can have different semantics depending on the context that is used\n29:14 in. and capturing all those intricacies like tone, formality, and other features\n29:19 can give richness to those words. Typically, dimensions we use today are 1536 dimensions, which is a good mix of\n29:27 not having too much burden in size, but also giving enough context to allow for depth in each search. Once the embedding\n29:34 is stored with proper dimension, there are two other major angles that we need to consider when we''re working with\n29:40 vector databases. And this is the retrieval side. Meaning now that we store the meaning of those words, we\n29:46 have to take on the burden of the retrieval side of embeddings. Since we are not doing searches like we did in\n29:52 SQL with a wear query, we need to make a decision on what would technically be counted as a much and by how much. This\n30:00 is done by looking at scoring and chunk overlap. And if you''re at this point wondering, this seems like a lot of\n30:05 tweaking just to use vector database. And that''s the serious trade-off you ought to consider when using a vector\n30:11 database, which is that while a properly set up vector database makes searching so much more flexible, getting the\n30:17 vector database properly configured often adds complexity up front. So with that in mind, scoring is a threshold you\n30:24 set to how similar the results need to be to be considered a proper match. For example, the word Florida might have\n30:30 some similarity to the word vacation since it''s often where people go for vacation. But asking the question, can I\n30:37 take my company laptop to Florida is very different than does my company allow vacation to Florida? Since one is\n30:44 asking about a policy in IT jurisdiction and the other is about vacation policy.\n30:50 So setting up a score threshold based on the question can help you limit those low similarities to count as a match.\n30:57 Okay, there''s one final angle which is chunk overlap. So in SQL, we''re used to\n31:02 storing things rowby row, but in vector databases, things look a little bit different. When we''re storing values in\n31:08 vector databases, they''re often chunked going into the database. So when we chunk down an entire employee handbook\n31:14 into chunks, it''s possible that the meaning gets chunked with it. That''s why we allow chunk overlap so that the\n31:21 context spills over to leave enough margin for the search to work properly.\n31:26 In this app, we''re going to build a semantic search engine step by step. The story begins with TechDoc Inc. where\n31:32 users search through documentation 10,000 times a day. But more than half of those searches fail. Why? Because\n31:39 traditional keyword search can''t connect reset password with password recovery process. Our mission is to fix that by\n31:47 building a search system that understands meaning, not just words. We begin with the environment setup. In\n31:53 this step, we''re asked to install the libraries that make vector search possible. Sentence transformers for\n31:58 embeddings. lang chain for orchestration, chromadb for vector database and few utility libraries like\n32:04 numpy. Once installed, we verify the setup using a provided script. If everything checks out, we''re ready to\n32:10 move forward. Next, we take a moment to understand embeddings. These are the backbone of semantic search. Instead of\n32:16 treating texts as words, embedding converts text into numerical vectors. Similar meanings end up close to each\n32:23 other in this mathematical space. That means forgot my password in account\n32:28 recovery looks very different in words but almost identical in vector 4. This is a magic that allows our search engine\n32:35 to succeed where keyword search fails. That takes us into task number one where we put embedding into action. We open\n32:42 the script, initialize the mini LM model, encode both queries and documents and then calculate similarity using\n32:49 cosign similarity. Running the script demonstrates how a search for forgot password successfully matches password\n32:56 recovery, showing semantic understanding in real time. Once we understand embeddings, we move to document\n33:01 chunking. Large documents can''t be embedded all at once. So, we need to split them into smaller chunks, but if\n33:08 we cut too bluntly, we lose context. That''s why overlapping chunks are important. They preserve meanings across\n33:15 boundaries. For example, setting a chunk size of 500 characters with 100 characters overlap can improve retrieval\n33:22 accuracy by almost 40%. Lang chain helps us do this intelligently. In task number\n33:27 two, we put this into practice by editing a script to import lane chain''s recursive characters text splitter and\n33:33 set the chunking parameters. Running the script confirms that our documents are now split into overlapping pieces ready\n33:40 for storage. The next concept we explore is vector stores. Embeddings alones are just numbers. We need a system to store\n33:47 and search through them efficiently. That''s where Chromma comes in. It''s a production ready vector database that\n33:53 can handle millions of embeddings, perform similarity search in milliseconds, and support metadata\n33:58 filtering. In task number three, we''re asked to create a vector store using Chroma DB. We import the necessary\n34:04 classes, configure the embedding model, and then run the script. Once confirmed, we have a working vector store that can\n34:11 accept documents and retrieve them semantically. Finally, we bring everything together with semantic\n34:16 search. Here we implement the full pipeline, convert the user query into an embedding, search the Chromma store,\n34:24 retrieve the most relevant document chunks, and return them to the user. For example, a query like work from home\n34:30 policy will now correctly surface remote work guidelines. In task 4, we configure\n34:36 the query, set the number of top results to return, and establish a threshold for\n34:41 similarity scores. Running the script validates our search engine end to end. The lab closes with a recap. We started\n34:48 with a broken keyword search system where 60% of the search failed. Along the way, we learned about embeddings,\n34:54 smart document chunking, vector stores, and semantic search. By the end, we built a productionready search engine\n35:01 with 95% success rate. Some of the deeper experiments like adjusting chunk sizes, testing different embedding\n35:08 models and or adding metadata filters are left for you to explore on your own.\n35:14 So is it possible that instead of searching through the entire 500 GB of documents, AI assistant can fit them\n35:20 into their context window and generate output. This is called rag or retrieval augmented generation. Let''s say your\n35:27 company used the AI assistant to ask this question. What''s our remote work policy for international employees? In\n35:34 order to understand how rag works, we need to break them into three simple steps. Retrieval, augmented, and\n35:40 generation. Starting with retrieval, just like how we convert the document into vector embeddings to store them\n35:47 inside the database, we do the exact same step for the question that reads, \"What''s our remote work policy for\n35:53 international employees?\" Once the word embedding for this question is generated, the embedding for that\n35:58 question is compared against embeddings of the documents. This type of search is called semantic search where instead of\n36:05 searching by the static keywords to find relevant contents, the meaning and the context of the query is used to match\n36:12 against the existing data set. Moving on to augmentation in rag refers to the process where the retrieved data is\n36:18 injected into the prompt at runtime. And you might think why is this all that special? Typically, AI assistants rely\n36:25 on what they learned during pre-training, which is static knowledge that can become outdated. Instead, our\n36:31 goal here is to have the AI assistant rely on up-to-date information stored in the vector database. In the case of RAG,\n36:38 the semantic search result pends to the prompt that essentially serves as an augmented knowledge. So, for your\n36:44 company, the AI assistant is given details from company''s documents that are real, up-to-date, and private data\n36:51 set. And all this can occur without needing to fine-tune or modify the large language model with custom data. The\n36:58 final step of rag is generation. This is a step where AI assistant generates the\n37:03 response given the semantic relevant data retrieved from the vector database. So the initial prompt that says what''s\n37:10 our remote work policy for international employees? The AI assistant will now demonstrate its understanding of your\n37:16 company''s knowledge base by using the documents that relate to remote work and policy. And since the initial prompt\n37:23 specifies a criteria of international employees, the generation step will use its own reasoning to wrestle with the\n37:30 data provided to best answer the question. Now, RAG is a very powerful system that can instantly improve the\n37:36 depth of knowledge beyond its training data. But just like any other system, learning how to calibrate is an acquired\n37:43 skill that needs to be learned to get the best results. Setting up a rag system will look different from one\n37:49 system to another because it heavily depends on the data set that you''re trying to store. For example, legal\n37:55 documents will require different chunking strategies than a customer support transcript document. This is\n38:01 because legal documents often have long structured paragraph that need to be preserved and intact. While\n38:06 conversational transcript can be just fine with sentence level chunking with high overlap to preserve context. In\n38:14 this lab, we''re taking our semantic search system to the next level by adding AI power generation. Up until\n38:20 now, we''ve been able to find relevant documents with high accuracy. For example, matching remote work policy\n38:27 when someone searches work from home. But the CEO wants more. Instead of retrieving a document, the system should\n38:33 actually answer the user''s questions directly. something like yes, you can work three days from home, not just\n38:39 showing a PDF. We begin the environment setup. In this step, we''re asked to activate the Python environment and\n38:45 install the key libraries. These include Chroma DB for vector storage, sentence transformers for embeddings within lane\n38:52 chain with integrations for OpenAI and hugging face. Once installed, we verify everything using the provided script to\n38:59 ensure the rack framework is ready. Next, we move into task number one, setting up the vector store. Here we\n39:05 initialize a chromabb client. Create our get collection named tech corp brag and\n39:11 configure the embedding model all mini6v2. This is where we get our system of\n39:17 memory. A place where all of our company documents will be stored as vectors so that we can search them semantically. In\n39:25 task number two, the focus shifts to document processing and chunking. Unlike our earlier lab where we split text into\n39:32 fixed-size character chunks, here we upgrade to paragraph-based chunking with smart overlaps. The goal is to preserve\n39:40 meaning so that each chunk contains complete thoughts. This is crucial for RAG because when AI generates answers,\n39:47 the quality depends on having coherent chunks of context. From there, we go to task number three,\n39:54 LLM integration. This is where we connect OpenA model GPD4.1 Mini. The API\n40:00 key and base are already preconfigured for us. We just need to set the generation parameters like temperature,\n40:06 max tokens, and top P values. Once integrated, we can test simple text generation before layering on retrieval\n40:12 and augmented steps. Task number four introduces prompt engineering for rag. We''re asked to build a structured prompt\n40:19 template that always ensures context is included. The system prompt makes it clear that answers must come only from\n40:25 the retrieved documents. If the information isn''t in context, the AI must respond with, \"I don''t have that\n40:31 information in the provided documents.\" This keeps our answers factual and prevents hallucinations. Finally, we\n40:38 reach task number five, the complete rag pipeline. Here we wire everything\n40:43 together. The flow is embed the user query, search Chromad, retrieve the top\n40:49 three chunks, build a contextaware prompt, and generate an answer using LLM. The final touch is a source\n40:55 attribution. Every answer points back to the document it was derived from. This\n41:01 transforms the system into a full production ready Q&A engine. At the end,\n41:06 we celebrate rag mastery. What started as simple document search has evolved into a powerful system that retrieves,\n41:13 augments, and generates answers. This is the same architecture that powers tools like catchupt, claude, and gemini. Some\n41:21 parts like experimenting with different trunk strategy, refining prompts are left for you to explore yourself. But by\n41:27 now you''ve built a complete RAG system that answers questions with context,\n41:32 accuracy, and confidence. Now that we covered the conceptual elements of RAG, let''s look at how it\n41:39 looks on a practical level. To better understand this, we can look over at this lab specifically geared towards how\n41:45 to use Rag. Now we covered the basic concepts of simple chat application that\n41:50 allows us to chat with documents using vector databases and rag. Most business cases in the real world may be slightly\n41:58 more complicated. For example, in tech corp''s case, the business requirement might extend to more complex\n42:04 requirements like being able to connect the agent to human resource management system to pull employee documents to\n42:11 cross reference and make personalized responses. However, lang chain has limitations. When business requirements\n42:17 become more complex like multi-step workflows, conditional branching or iterative processes, you need something\n42:23 more sophisticated for better orchestration. That''s where langraph becomes essential. Langraph extends lane\n42:30 chain to handle more complex multi-step workflows that go beyond simple question and answer interactions. For example, if\n42:37 a customer asks, I need to understand our data privacy policy for EU customers. Since we assume that inside\n42:44 the 500 GB of database, it contains details about EU specific regulations,\n42:49 we need to create a system that can analyze Tech Corp''s data privacy policies for EU customers, ensuring\n42:55 compliance with GDPR, local regulations, and company standards. While in a traditional software development, you\n43:01 need to write code that can sequentially and conditionally call different sections of the code to process this\n43:07 request. With lang graph, this becomes a graph where each node handles a specific responsibility. For example, node one,\n43:15 search and gather privacy policy documents. Node two, extract and clean document content. Node three, evaluate\n43:22 GDPR compliance using LLM analysis. Node four, cross reference the local EU\n43:28 regulations. And node five, identify compliance gaps and generate recommendation. A node is an individual\n43:34 unit of computation. So think of a function that you can call. Once you have all the nodes created in langraph,\n43:41 you will then need to connect them and this connection is called an edge. Edges in langraph define execution flow. For\n43:48 example, after node one gathers documents, the edge routes to node two for content extraction. And after node 3\n43:56 evaluates compliance, a conditional edge either routes to node four for additional analysis or jumps to node\n44:03 five for report generation. And one final concept to keep in mind beyond nodes and edges is shared state between\n44:10 each node. This is possible by using state graph that essentially stores information throughout the entire\n44:17 workflow. For example, class compliance state type dictionary topic string\n44:23 documents list of string current documents optional string compliance score optional integer gaps list of\n44:30 string recommendation list of string can be used for nodes we identified before. As the workflow progresses, each node\n44:37 updates relevant state variables. Node one populates documents with found policy files. Node two processes\n44:44 individual documents and updates current document. Node 3 calculates compliance score. Node 4 identifies gaps. Node 5\n44:52 generates recommendation. The state graph orchestrates execution based on configured flow. If node 3 determines\n44:59 compliance score is below 75% the conditional edge routes back to node one to gather additional documents. If the\n45:06 score exceeds 75% execution proceeds to node 5 for final report generation. As\n45:13 you can see this creates powerful capabilities loops for iterative analysis conditional branching on\n45:18 intermediate results persistent state that maintains context across the entire workflow. So for Tech Corps compliance\n45:26 assistant, Langraph is an essential tool for workflow automation. All right,\n45:31 let''s start with the labs. In this lab, we''re diving into Langraph, a framework designed for building stateful\n45:37 multi-step AI workflows. Unlike simple chains, Langraph gives us specific control over how data moves, letting us\n45:44 create branching logic, loops, and decision points. By the end of this journey, we''ll have built a complete\n45:50 research assistant that can use multiple tools intelligently. We begin with environment setup. In this setup, we\n45:57 activate the Python virtual environment and install the required libraries. Langraph itself, Langchain, and OpenAI\n46:04 integration. Once everything is installed, we run a verification script to make sure our setup is ready. With\n46:10 the environment ready, we start small. Task number one introduces us to essential import. We bring in state\n46:17 graph end and type dict to define the data that flows through the workflow. Then we add a simple state field for\n46:24 messages. This is the foundation. State graph holds the workflow and marks\n46:29 completion and the state holds shared data. In task number two, we create our\n46:34 first nodes. Nodes are just Python functions that take state as inputs and\n46:39 return partial updates. In this case, we define a greeting node and an enhancement node. Once connected, one\n46:46 node outputs a basic greeting and the next node improves it with a bit of a flare. This demonstrates how state\n46:52 accumulates step by step. Pass number three is about edges. The connections between nodes. Here we use add nodes and\n46:59 add edges to wire greeting nodes to the enhancement node. With that, we built our first mini workflow. Data flows from\n47:06 one function to another. The state updates along the way. In task number four, we take it further with a\n47:13 multi-step flow. We add new nodes like a draft step and a review step. Connect them and see how the data moves through\n47:19 multiple stages. Each step preserves states, adds detail and passes it on.\n47:25 This mimics real world pipelines where content is outlined, drafted and polished. Pass number five introduces\n47:32 conditional routing. Instead of a fixed flow, the system now decides dynamically. For example, if the query\n47:39 is short, it routes one way. If detailed, it routes another. The router inspects the state and returns the next\n47:46 node name, making workflows flexible and adaptive. Then comes task number six,\n47:51 tool integration. Here we add a calculator tool. The router checks if the query is math related. If so, it\n47:58 routes to the calculator node which computes the answer. This is our first glimpse at how Langraph lets us\n48:04 integrate specialized tools directly into workflows. Finally, task number seven puts everything together into\n48:11 research agent. We combine the calculator with a web search tool like duck.go. Depending on the query, the\n48:18 system decides whether to perform a calculation, run a web search, or handle a text normally. This is a dynamic tool\n48:25 orchestration, the foundation of modern AI agents. By the end of this lab, we''ve\n48:30 gone from simple imports to a fully functional research assistant. We''ve seen how to build nodes, connect them,\n48:37 design multi-step flows, and add routing logic and integrate tools. Some of the deeper experiments like chaining more\n48:43 advanced tools or refining the router logic are left for you to explore on your own.\n48:51 Now that we covered lang chain and langraph and understood how techp business requirements can be met by\n48:57 leveraging pre-built tools that it offers, there''s one final piece that''s been popular since Anthropics released\n49:03 back in November 2022 called MCP or model context protocol. Techorps AI\n49:09 document assistant is working well for internal knowledge base, but employees might now need to access external\n49:16 systems like customer database, support systems, inventory management software, and other thirdparty APIs. And writing\n49:23 custom integrations to all these API connections will take a huge amount of time. MCP functions like an API, but\n49:31 with crucial differences that make it perfect for AI agents. Traditional APIs\n49:36 expose endpoints that require you to understand implementation details leading to rigid integrations tied to\n49:42 specific systems. MCP doesn''t just expose tools. It provides self-describing interfaces that AI\n49:49 agents can understand and use autonomously. The key advantage here is that unlike traditional APIs, MCP puts\n49:57 the burden on the AI agent rather than the developer. So when you start an MCP\n50:02 server, an instance starts and establish a connection with your AI agent. For example, the Techorps document assistant\n50:09 might easily have these MCP servers to enable powerful integration. Let''s say customer database MCP. When someone\n50:16 asks, \"What''s the status of the order 1 2 3 4?\" The AI uses an MCP to query\n50:22 TechCorb''s order management system, retrieves the current status, and provides a complete response. The same\n50:28 logic applies for support tickets, inventory databases and notification system mentioned earlier where we can\n50:34 simply plug into existing integrations of MCP servers to allow the agent to now\n50:40 extend its capabilities. For example, we can create a very simple MCP server code\n50:45 that looks something like this. Here we have fast MCP customer DB that starts the MCP server with the name customer DB\n50:53 at MCB tool that exposes a function to MCP clients. the AI can call like an API\n50:59 function parameters and return types that tells the MCP client what inputs are required and what type of output to\n51:06 expect. Finally, customers variable that is a fake database in this case stored in memory, but in your company''s case,\n51:13 you can connect this to a SQL database or MongoDB or any other custom database you might hold customer information on.\n51:19 Now, looking at this code might confuse you on what MCP really is, since it''s typically being talked about as a simple\n51:26 plugand play, and you''re right to think that way. The difference here is that this MCP server code that''s written only\n51:33 needs to be written once, and it doesn''t necessarily have to be you. In other words, a community of MCP developers\n51:40 might have written custom MCP servers for other popular tools like GitHub, GitLabs, or SQL databases, and you can\n51:46 simply use them directly on your agent without having to write the code yourself. That''s where the power of MCP\n51:53 really comes from. In this lab, we''re going deeper into MCP, model context\n51:58 protocol, and learning how to extend Langraph with external tools. Think of MCP as a universal port like USB that\n52:06 allows AI system to connect to any tool, database or API in a standardized way.\n52:11 With it, our langraph agents can go beyond built-in logic and integrate external services seamlessly. We begin\n52:18 with the environment step. In this step, we activate our virtual environment and install the key packages. Lang graph for\n52:26 the workflow framework, Langchain for the core abstractions, and Langchain OpenAI for the model integration. The\n52:32 setup also prepares for fast MCP, the framework we''ll use to build MCP servers. Once installed, we verify by\n52:40 learning the provided script, ensuring everything is ready for MCP development. Next, we get a conceptual overview of\n52:46 the MCP architecture. Here the lab explains that the MCP protocol acts as a\n52:52 bridge between an AI assistant built with Langraph and external tools. The\n52:57 flow works like this. The MCP server exposes tools and schemas. Langraph integrates with them and queries are\n53:04 routed intelligently. The naming convention MCP server tool ensures\n53:10 clarity when multiple tools are involved. A helpful analogy is comparing MCP to USB devices. A protocol is a\n53:17 port. The server is a device. The tools are its functions. And Langraph is a computer that uses them. That brings us\n53:24 to task number one, MCP basics. Here we''re asked to create our very first MCP\n53:30 server. The task involves initializing a server called calculator, defining a function as a tool with at MCP.tool\n53:37 decorator and running it with the SCDIO transport. This shows how simple it is to expose a structured function as an\n53:45 external tool. that langraph can later consume. In task number two, we integrate MCP with langraph. The\n53:52 challenge here is to connect the calculator server to an agent. This involves configuring the client fetching\n53:58 tools from the server create react agent that can decide when to call the calculator selected when needed. Next,\n54:06 task number three scales things up with multiple MCP servers. Instead of just a calculator, we add another server, in\n54:13 this case, a weather service. Now, Langraph orchestrates between both. The system retrieves available tools,\n54:19 creates an agent with access to both servers, and intelligently routes queries. If a user asks a math question,\n54:26 the calculator responds. If they ask about the weather, the weather tool responds. This is where we see the true\n54:32 power of MCP. Multiple servers are working together under a unified AI agent. The lab wraps up by celebrating\n54:39 MCP mastery. By now, we''ve created MCP servers, integrated them with ElangRaph,\n54:45 and orchestrated multiple tools. The key takeaways are that MCP is universal. It\n54:50 can connect any tool to any AI. Routing is what gives it power. The design is\n54:55 extendable, so we can add servers anytime. Some deeper explorations like exposing databases, APIs or file systems\n55:03 through MCP are left for you to explore on your own. That concludes this narration. Next, we''ll continue the\n55:10 journey by experimenting with resource exposure, human in the loop approval flows, and eventually deploying\n55:15 production ready MCP packages. Now that we have put all these pieces together like context windows, vector\n55:22 databases, lang chain, langraph, MCP, and prompt engineering, Techorp is now\n55:28 able to do complex document search that went from manual searching that could have taken up to 30 minutes to now less\n55:35 than 30 seconds using our AI agent. And we also have a higher accuracy using\n55:40 contextaware semantic search like using rag. And finally, the chat application UI allows users to have more\n55:47 satisfaction in working with a tool that can help keep track of conversation history and better intuition overall.\n55:54 And the availability for this is 24/7 as long as the application is running. And\n55:59 this is just the beginning. Imagine layering on predictive analytics, proactive compliance agents, and\n56:05 workflow automation that doesn''t just answer questions, but actively solves problems before employees can even ask.\n56:12 The shift from static documents to living intelligent system marks a turning point not just for Tech Corp,\n56:18 but for how every other business can unlock a full value of its knowledge using agents.',
  '{"channel": "KodeKloud", "video_id": "ZaPbP9DwBOE", "duration": "56:39", "level": "BEGINNER", "application": "LangChain, LangGraph, MCP", "topics": ["AI Agents", "LLMs", "Tokens", "Embeddings", "Context Windows", "LangChain", "Prompt Engineering", "Vector Databases", "ChromaDB", "Pinecone", "RAG", "LangGraph", "MCP Model Context Protocol", "OpenAI API"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  1,
  'LangChain v0.3  Getting Started',
  '0:00 Okay, so moving on to our next chapter, getting started with a line chain. In this chapter, we''re going to be 0:05 introducing a line chain by building a simple LM powered assistant that will do...',
  '0:00 Okay, so moving on to our next chapter, getting started with a line chain. In this chapter, we''re going to be
0:05 introducing a line chain by building a simple LM powered assistant that will do
0:11 various things for us. It will multimodal generating some text, generating images, generate some
0:18 structured outputs. It will do a few things. Now, to get started, we will go over to the course repo. All of the
0:25 code, all the chapters are in here. There are two ways of running this either locally or in Google Collab. We
0:32 would recommend running in Google Collab because it''s just a lot simpler with environments, but you can also run it
0:38 locally. And actually for the capstone, we will be running it locally. There''s
0:43 no way of us doing that in Collab. So if you would like to run everything locally, I''ll show you how quickly. Now,
0:50 if you would like to run in collab, which I would recommend at least for the the first notebook chapters, just skip
0:58 ahead. There will be chapter points in the timeline of the video. So, for
1:04 running running it locally, we just come down to here. So, this actually tells you everything that you need. So, you
1:11 will need to install UV, right? So this is the package manager that we recommend
1:17 like the Python and package management library. You don''t need to use UV, you
1:22 know, it''s it''s up to you. UV is is very simple. It works really well. So I would
1:28 recommend that. So you would install it with this command here. This is on Mac, so it will be different otherwise if you
1:36 are on Windows or otherwise you can uh look at the installation guide there and it''ll tell you what to do. And so before
1:42 we actually do this, what I will do is go ahead and just clone this
1:48 repo. So we''ll come into here. I''m going to create like a temp directory for me
1:53 cuz I already have the chain course in there. And what I''m going to do is just get clone line chain course. Okay. So
2:01 you will also need to install git if you don''t have that. Okay. So we have that.
2:08 Then what we''ll do is copy this. Okay. So this will install Python 3.12.7 for
2:13 us with this command. Then this will create a new VM within that or using
2:21 Python 3.12.7 that we''ve installed. And then UV sync will actually be looking at
2:28 the pi projectl file. That''s like the uh the package installation for the repo',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  2,
  'LangChain v0.3  Getting Started',
  '2:33 and using that to install everything that we need. Now we should actually make sure that we are within the line 2:40 chain course directory and then yes we can run those three and there we go. So...',
  '2:33 and using that to install everything that we need. Now we should actually make sure that we are within the line
2:40 chain course directory and then yes we can run those three and there we go. So everything
2:47 should install with that. Now if you are in cursor you can just do cursor dot or
2:56 we can run code. If VSS code I''ll just be running this and then I''ve opened up
3:03 the course. Now within that course you''ll have your notebooks and then you just run through these making sure you
3:09 select your kernel Python environment and making sure you''re using the correct VN from here. So that should pop up
3:16 already as this VM bin Python and you''ll click that and then you can run through.
3:22 When you are running locally don''t run these you don''t need to. You''ve already installed everything. So you don''t this
3:28 specifically is for collab. So that is running things locally. Now let''s have a
3:33 look at running things in collab. So for running everything in collab, we have
3:40 our notebooks in here. We click through and then we have each of the chapters through here. So starting with the first
3:47 chapter, the introduction, which is where we are now. So what you can do to open this in
3:53 collab is either just click this collab button here or if you really want to uh
4:00 for example maybe this it is not loading for you. What you can do is you can copy
4:05 the URL at the top here you can go over to collab you can go to
4:11 open GitHub and then just paste that in there and press
4:16 enter. And there we go. We have our notebook. Okay. So, we''re in now. Uh
4:23 what we will do first is just install the prerequisites. So, we have line chain. I just load line chain packages
4:30 here. LChain core line chain openai because we''re using open AI and line
4:36 chain community which is needed for running uh what we''re running. Okay. So, that has installed everything for us. So
4:43 we can move on to our first step which is initializing our LM. So we''re going
4:50 to be using GT40 mini which is like a small but fast but also cheaper model.
4:58 Uh that is also very good from OpenAI. So what we need to do here is get an API
5:04 key. Okay. So for getting that API key we''re going to go to OpenAI''s website.
5:10 And you can see here that we''re opening platform.openai.com. openai.com and then we''re going to go into settings,
5:15 organization, API keys. So, you can copy that or just click it from here. Okay. So, I''m going',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  3,
  'LangChain v0.3  Getting Started',
  '5:22 to go ahead and create a new secret key. Actually, just in case you''re kind of 5:28 looking for where this is, it''s settings, organization, API keys again. Okay. Create a new API key. I''m going t...',
  '5:22 to go ahead and create a new secret key. Actually, just in case you''re kind of
5:28 looking for where this is, it''s settings, organization, API keys again. Okay. Create a new API key. I''m going to
5:35 call it line train course. I''ll just put it under semantic
5:41 router. That''s just my organization. You you put it wherever you want it to be.
5:46 And then you would copy your API key. You can see mine here. I''m obviously
5:51 going to revert that before you see this, but you can try and use it if you really like. So, I''m going to copy that.
5:56 And I''m going to place it into this little box here. You could also just place it put your uh full API key in
6:04 here. It''s up to you. But this little box just makes things easier. Now that
6:10 what we''ve basically done there is just passed in our API key. We''re setting our open model GT40 mini. And what we''re
6:17 going to be doing now is essentially just connecting and setting up our LLM parameters with line chain. So we run
6:26 that. We say okay we''re using a GP4 mini and we''re also setting ourselves up to
6:32 use two different LM here or two of the same LM with slightly different
6:37 settings. So the first of those is an LLM with a temperature setting of zero. The temperature setting basically
6:44 controls almost the randomness of the output of your LLM. And the way that it
6:51 works is when an LLM is predicting the sort of next token or next word in
6:59 sequence, it''ll provide a probability actually for all of the tokens within the LM''s knowledge base or what the LM
7:05 has been trained on. So what we do when we set a temperature of zero is we say you are going to give us the token with
7:14 highest probability according to you. Okay. Whereas when we set a temperature
7:20 of 0.9 what we''re saying is okay there''s actually an increased probability of you
7:26 giving us a token that according to your generated output is not the token with
7:32 the highest probability according to the LM. But what that tends to do is give us
7:37 more sort of creative outputs. So that''s what the temperature does. So we are creating a normal LM and then a more
7:45 creative LM with this. So what are we going to be building? We''re going to be
7:50 taking a article draft. So like a draft article uh from the Aurelio learning
7:58 page and we''re going to be using line chain to generate various themes that we might um find helpful as we''re you know',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  4,
  'LangChain v0.3  Getting Started',
  '8:06 we have this article draft and we''re editing it and just kind of like finalizing it. So what are those going 8:11 to be? You can see them here. We have the title for the article description and S...',
  '8:06 we have this article draft and we''re editing it and just kind of like finalizing it. So what are those going
8:11 to be? You can see them here. We have the title for the article description and SEO friendly description
8:18 specifically. The third one, we''re going to be getting the LM to provide us advice on an existing paragraph and
8:25 essentially writing a new paragraph for us from that existing paragraph. And what it''s going to do, this is a
8:30 structured output part, is going to write a new version of that paragraph for us. And it''s going to give us advice
8:36 on where we can improve our writing. Then we''re going to generate a thumbnail or hero image for our article. So, you
8:44 know, nice image that you would put at the top. So here we''re just going to input our article. You can you can put
8:51 something else in here if you like. Essentially this is just a big article
8:56 that''s written a little while back on agents. And now we can go ahead and
9:02 start preparing our prompts which are essentially the instructions for our LLM.
9:07 So, line chain comes with a lot of different uh like utilities for prompts
9:13 and we''re going to dive into them in a lot more detail, but I do want to just give you uh the essentials now just so
9:19 you can understand what we''re looking at at least conceptually. So, prompts for chat agents are at a minimum broken up
9:26 into three components. Those are the system prompt. This provides instructions to our LM on how it should
9:32 behave, what its objective is, and how it should go about achieving that objective. Generally, system prompts are
9:39 going to be a bit longer than what we have here depending on the use case. Then we have our user prompts. So, these
9:45 are user written messages. Uh, usually sometimes we might want to pre-populate those if we want to encourage a
9:52 particular type of um conversational patterns from our agent. But for the
9:58 most part, yes, these are going to be user generated. Then we have our AI prompts. So these are of course AI
10:05 generated. And again, in some cases, we might want to generate those ourselves
10:10 beforehand or within a conversation if we have a particular reason for doing so. But for the most part, you can
10:17 assume that these are actually user and AI generated. Now, line chain provides
10:22 us with templates for each one of these prompt types. Let''s go ahead and have a',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  5,
  'LangChain v0.3  Getting Started',
  '10:29 look at what these look like within line chain. So, to begin, we are looking at 10:35 this one. So, we have our system message prompt template and human messages, the 10:41 the user that we saw...',
  '10:29 look at what these look like within line chain. So, to begin, we are looking at
10:35 this one. So, we have our system message prompt template and human messages, the
10:41 the user that we saw before. So, we have these two system prompt, keeping it quite simple here. You are a system that
10:48 helps generate article titles, right? So our first component where we want to generate his article title. So we''re
10:54 telling the AI, you know, that''s what we want it to do. And then here, right? So
11:00 here we''re actually providing kind of like a template for a
11:05 user input. So yes, as I mentioned, user input can
11:12 be um it can be fully generated by a user. It might be kind of not generated
11:18 by user. We might be setting up a conversation beforehand which then a user would later use or in this scenario
11:25 we''re actually creating a template and the what the user will provide us will
11:31 actually just be inserted here inside article and that''s why we have this import variables. So, what this is going
11:40 to do is okay, we have all of these instructions around here. They''re all going to be provided to OpenAI as if it
11:46 is the user saying this. Um, but it will actually just be this here that a user
11:52 will be providing. Okay. And we might want to also format this a little nicer. It kind of depends. This will work as it
11:58 is, but we can also put, you know, something like this to make it a little bit clearer to the LM. Okay. what is the
12:06 article and where are the prompts? So, we have that and you can see in this
12:12 scenario there''s not that much difference between what the system prompt and the user prompt is doing. And this is it''s a particular scenario. It
12:19 varies when you get into the more conversational stuff as we will do later. Uh you''ll see that the user
12:25 prompt is generally more fully userenerated or mostly user generated.
12:32 And much of these types of instructions we might actually be putting into the system prompt it varies and we''ll see
12:39 throughout the course many different ways of using these different types of prompts in various different
12:45 places. Then you''ll see here so I just want to show you how this is working. We
12:51 can use this format method on our user prompt here to actually insert something
12:57 within the uh article input here. So we''re going to go use prompt format and',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  6,
  'LangChain v0.3  Getting Started',
  '13:03 then we pass in something for article. Okay. And we can also maybe format this a little nicer but I''ll just show you 13:09 this for now. So we have our human message and then inside content this...',
  '13:03 then we pass in something for article. Okay. And we can also maybe format this a little nicer but I''ll just show you
13:09 this for now. So we have our human message and then inside content this is the the text that we had. You can see
13:15 that we have all this right and this is what we wrote before. We wrote all this except from this part. We didn''t write
13:22 this. Instead of this we had article. All right. So let''s format this a little
13:28 nicer so we can see. Okay. So this is exactly what we wrote up here. Exactly
13:33 the same except from now we have test string instead of article. So later when
13:38 we insert our article it''s going to go inside there. That''s all it''s doing. It''s like it''s an it''s an string in
13:45 Python. Okay. And this is again this is one of the things where people might complain about lang chain. you know,
13:50 this sort of thing can be, you know, it seems excessive because you could just do this with me stringing, but there
13:56 are, as we''ll see later, particularly when you''re streaming, just really helpful features that come with using
14:03 line chains kind of builtin uh prompt templates or at least uh message objects
14:09 that we''ll see. So, you know, we we need to uh keep that in mind. Again, as
14:16 things get more complicated, line chain can be a bit more useful. So, chat prompt template. Uh, this is basically
14:23 just going to take what we have here, our system prompt, user prompts. You could also include some AI prompts in
14:29 there. And what it''s going to do is merge both of those and then when we do
14:36 format, what it''s going to do is put both of those together into a chat history. Okay. So, let''s see what that
14:42 looks like first uh in a more messy way. Okay. So you can see we have just the
14:49 content right so it doesn''t include the whole you know before we had human message we''re not include we''re not
14:55 seeing anything like that here instead we''re just seeing the string so now let''s switch back to
15:02 print and we can see that what we have is our system message here is just
15:07 prefixed with this system and then we have human and it''s prefixed by human and then it continues right so that''s
15:13 that''s all it''s doing it''s just kind of merging those in some sort of chat log we could also put in like AI messages
15:18 and they would appear in there as well. Okay, so we have that. Now that is our
15:23 prompt template. Let''s put that together with an LLM to create what would be in',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  7,
  'LangChain v0.3  Getting Started',
  '15:29 the past line chain be called an LLM chain. Uh now we wouldn''t necessarily call it an LLM chain because we''re not 15:35 using the LLM chain abstraction. It''s not super important. If that doesn''t...',
  '15:29 the past line chain be called an LLM chain. Uh now we wouldn''t necessarily call it an LLM chain because we''re not
15:35 using the LLM chain abstraction. It''s not super important. If that doesn''t make sense, we will go into it in more
15:41 detail later particularly in the in the L cell chapter.
15:46 So what this chain will do you think line chain is just chains we''re chaining
15:52 together these multiple components it will perform the steps prompt formatting so that''s what I just showed you lm
16:01 generation so sending our prompt to open AAI getting a response and getting that
16:06 output so you can also add another step here if you want to format that in a particular way we''re going to be
16:13 outputting that in a particular format so that we feed it into the next step more easily. But there are also things
16:18 called output passes which pass your output in a more dynamic or complicated
16:25 way depending on what you''re doing. So this is our first look at Els. I don''t
16:30 want us to focus too much on the syntax here because we will be doing that later. But I do want you to just
16:36 understand what is actually happening here and logically what are we writing.
16:43 So all we really need to know right now is we define our inputs with the first
16:49 dictionary segment here. Right? So this is a you know our inputs which we have
16:55 defined already. Okay. So if we come up to our
17:01 uh user prompt here we said the input variable is our article right and we might have also added input variables to
17:06 the system prompt here as well. In that case, you know, let''s say we had you are an AI assistant
17:14 called name, right? That helps generate article
17:19 titles. In this scenario, we might have an input variables name here, right? And
17:27 then what we would have to do down here is we would also have to pass that
17:33 in, right? So it also we would have article but we would also have name. So
17:40 basically we just need to make sure that in here we''re including the variables that we have defined as input variables
17:47 for our our first prompts. Okay. So we can actually go ahead and let''s add that
17:52 uh so we can see it in action. So we''ll run this again and just include that or
17:59 or reinitialize our first prompt. So we see that and if we just have a look at
18:05 what that means for this format function here it means we''ll also need to pass in a name. Okay we call it Joe. Okay so Joe',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  8,
  'LangChain v0.3  Getting Started',
  '18:13 the AI right so you are an AI system called Joe now. Okay so we have Joe our 18:19 AI that is going to be fed in through these input variables. Then we have this pipe operator. The pipe operator...',
  '18:13 the AI right so you are an AI system called Joe now. Okay so we have Joe our
18:19 AI that is going to be fed in through these input variables. Then we have this pipe operator. The pipe operator is
18:25 basically saying whatever is to the left of the pipe operator, which in this case would be this, is going to go into
18:33 whatever is on the right of the pipe operator. It''s that simple. Uh again, we''ll we''ll dive into this and kind of
18:39 break it apart in the LSL chapter, but for now, that''s all we need to know. So, this is going to go into our first
18:45 prompt that is going to format everything. It''s going to add the name and the article that we provided into
18:51 our first prompt. And it''s going to output that, right? Okay, it''s going to output that. We have our pipe operate
18:56 here. So the output of this is going to go into the input of our next step. It''s our creative LM. Then that is going to
19:05 generate some tokens. It''s going to generate our output. That output is going to be an AI message. And as you
19:12 saw before, if I take this bit out, within those message objects, we
19:19 have this content field. Okay. So we are actually going to extract the content
19:24 field out from our AI message to just get the content and that is what we do
19:30 here. So we get the AI message out from ILM and then we''re extracting the content from that AI message object and
19:36 we''re going to passing it into a dictionary that just contains article title like so. Okay, we don''t need to do
19:42 that. We can just get the AI message directly. I just want to show you how we are using this sort of chain in Els. So
19:52 once we have set up our chain, we then call it or execute it using the invoke
19:58 method. Into that we will need to pass in those variables. So we have our article already, but we also gave our AI
20:04 a name now. So let''s add that and we''ll run this. Okay, so Joe has generated us a
20:13 article title. Unlocking the future, the rise of neurosymbolic AI agents. Cool.
20:19 much better name than what I gave the article, which was AI agents are neurosymbolic systems. No, I don''t think
20:26 I did too bad. Okay, so we have that. Now, let''s continue. And what we''re
20:33 going to be doing is building more of these types of LM chain pipelines where
20:39 we''re feeding in some prompts. We''re generating something, getting something, and and doing something with it.',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  9,
  'LangChain v0.3  Getting Started',
  '20:45 So, as mentioned, we have the title. We''re now moving on to the description. So, we''re going to generate a 20:50 description. So, we have our human message prompt template. So, this is actually...',
  '20:45 So, as mentioned, we have the title. We''re now moving on to the description. So, we''re going to generate a
20:50 description. So, we have our human message prompt template. So, this is actually going to go into a similar format as before. We
20:59 probably also want to redefine this because I think I''m using the same system message there. So, let''s uh let''s
21:06 go ahead and do modify that. Or what we could also do is let''s
21:12 just remove the name now because I''ve shown you that. So, what we could do is
21:18 you''re an AI assistant that helps build good articles, right? Build good
21:25 articles. And we could just use this as our, you know, generic system prompt. Now, so let''s say that''s our new system
21:32 prompt. Now, we have our user prompt. You''re tasked with creating a description for the article. The article is here for you to examine article. Here
21:39 is the article title. Okay. So, we need the article title now as well in our input variables. And then we''re going to
21:45 output an SEOfriendly article description. And we''re just saying, you know, just to be certain here, do not
21:50 output anything other than the description. So, you know, sometimes an LLM might say, hey, look, this is what
21:56 I''ve generated for you. The reason I think this is good is because so on and so on and so on, right? If you''re
22:01 programmatically taking some output from an LLM, you don''t want all of that fluff around what the LM has generated. You
22:08 just want exactly what you''ve asked it for, okay? because otherwise you need to pass out of code and it can get messy
22:14 and also just far less reliable. So we''re just saying don''t output anything
22:19 else. Then we''re putting all of these together. So system prompt and a second user prompt. This one here putting those
22:26 together into a new chat prompt template. And then we''re going to feed all that in to another cell chain as we
22:33 have here to well to generate our our description. So let''s go ahead. We
22:38 invoke that as before. or we''ll just make sure we add in the article title that we got from before and let''s see
22:45 what we get. Okay, so we have this explore the transformative potential of neuros symbolic AI agents in a little
22:52 bit long to be honest, but yeah, you can see what it''s doing here, right? And of course, we could then go in. We see this
22:58 is kind of too long like SEO friendly description. Not not really. So, we can',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  10,
  'LangChain v0.3  Getting Started',
  '23:05 modify this. Output the SEO friendly description. Um, make sure we don''t 23:12 exceed. Let me put that on a new line. Make sure we don''t exceed say 200 23:18 characters or maybe it''s even less f...',
  '23:05 modify this. Output the SEO friendly description. Um, make sure we don''t
23:12 exceed. Let me put that on a new line. Make sure we don''t exceed say 200
23:18 characters or maybe it''s even less for SEO. I don''t I don''t have a clue. I''ll just say 120 characters. Do not output
23:24 anything other than the description. Right. So we could just you know go back modify our prompting see what that
23:30 generates again. Okay. So much shorter probably too short now but that''s fine.
23:35 Cool. So we have that. We have a summary process that now you know in this dictionary formula that we have here.
23:42 Cool. Now the third step, we want to consume that first article variable with
23:49 our full article and we''re going to generate a few different output fields.
23:55 So for this, we''re going to be using the structured output feature. So let''s
24:01 scroll down. We''ll see what that is or what that looks like. So structured
24:06 output is essentially we''re forcing the alignment like it has to output a dictionary with these you know
24:13 particular fields. Okay and we can modify this quite a bit but in this
24:19 scenario what I want to do is I want there to be an original paragraph right so I just want it to regenerate the
24:24 original paragraph cuz I''m lazy and I don''t want to extract it out. Then I want to get the new edited paragraph.
24:31 This is the ln generated improved paragraph. And then we want to get some feedback because we we don''t want to
24:37 just automate ourselves. We want to augment ourselves and get better with AI
24:43 rather than just being like ah you do you do this. So that''s what we do here. And you can see that here we''re using
24:50 this pyantic object. And what pyantic allows us to do is define these particular fields. And it also allows us
24:56 to assign these descriptions to a field. And line chain is actually going to go ahead read all of this, right? Even
25:03 reads. So, for example, we could put integer here and we could actually get a numeric score for our paragraph, right?
25:11 We can try that, right? So, let''s uh let''s let''s just try that quickly. I''ll show you. Um, so numeric numeric score.
25:18 In fact, let''s even just ignore let''s not put anything here. So, I''m going to put constructive feedback on the
25:25 original paragraph, but I just put in here. So, let''s see what happens. Okay. So we have that and what I''m going to do',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  11,
  'LangChain v0.3  Getting Started',
  '25:31 is I''m going to get our creative LM. I''m going to use this with structured output method and that''s actually going to 25:37 modify that LM class. Create a new LM class that forces that LM to use...',
  '25:31 is I''m going to get our creative LM. I''m going to use this with structured output method and that''s actually going to
25:37 modify that LM class. Create a new LM class that forces that LM to use this
25:42 structure for the output. Right? So passing in paragraph into here using this we''re creating this new structured
25:49 LM. So let''s run that and see what happens. Okay. So we''re going to modify
25:55 our chain accordingly. Maybe what I can do is also just remove this bit for now.
26:02 So we can just see what the structured LLM outputs directly. And let''s
26:08 see. Okay, so now you can see that we actually have that paragraph object,
26:14 right? The one we defined up here, which is kind of cool. And then in there we have the original paragraph, right? So
26:20 this is where this is coming from. I definitely remember writing something
26:25 that looks a lot like that. So I think that is uh correct. We have the edited paragraph. So this is okay what it
26:30 thinks is better. And then interestingly the feedback is three which is weird
26:37 right because uh here we said the constructive feedback on the original paragraph. But what we''re doing when we
26:43 use this with structured output what lang chain is doing is essentially performing a tool call to open AAI and
26:50 what a tool call can do is force a particular structure in the output of an LM. So when we say feedback has to be an
26:58 integer, no matter what we put here, it''s going to give us an integer because how do you provide constructive feedback
27:04 with an integer? It doesn''t really make sense. But because we''ve set that limitation, that restriction here, that
27:12 is what it does. It just gives us the uh a numeric value. So I''m going to shift that to string and then let''s rerun
27:19 this. See what we get. Okay, we should now see that we actually do get constructive feedback. All right. So
27:25 yeah, you can see it''s quite quite long. So the original paragraph effectively communicates limitations with neuro AI
27:31 systems in performing certain tasks. However, it could benefit from slightly improved clarity and conciseness. For
27:38 example, the phrase was becoming clear can be made more direct by changing it to became evident. Well, yeah, true.
27:46 Thank you very much. So yeah, now we actually get that that feedback which is is pretty nice. Now, let''s add in this
27:54 final step to our chain. Okay. And it''s just going to pull out our paragraph object here and',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  12,
  'LangChain v0.3  Getting Started',
  '28:01 extracting to a dictionary. We don''t necessarily need to do this. Honestly, I actually kind of prefer it within this paragraph object, but just so we can see 28:10 how we would pass things on th...',
  '28:01 extracting to a dictionary. We don''t necessarily need to do this. Honestly, I actually kind of prefer it within this paragraph object, but just so we can see
28:10 how we would pass things on the other side of the chain. Okay. So, now we can
28:15 see we''ve extracted that out. Cool. So we have all of that interesting feedback
28:23 again. But let''s leave it there for the text part of this. Now let''s have a look
28:29 at the sort of multimodal features that we can work with. So this is, you know,
28:34 maybe one of those things that''s kind of seems a bit more abstracted, a little bit complicated where it maybe could be
28:40 improved. But, you know, we''re not going to really be focusing too much on the multi mode stuff. We''ll sub be focusing
28:45 on language. But I did want to just show you very quickly. So we want this article to look better. Okay, we want to
28:54 generate a prompt based on the article itself that we can then pass to Darly
29:02 the the image generation model from OpenAI that will then generate an image like a like a thumbnail image for us.
29:09 Okay. So the first step of that is we''re actually going to get an LM to generate
29:14 that. Right. So we have our prompt that we''re going to use for that. So I''m going to say generate a prompt with less
29:19 than 500 characters to uh generate an image based on the following article.
29:25 Okay. So that''s our prompt. You know, super simple. Uh we''re using the generic prompt template here. You can use that.
29:32 You can use user uh prompt template. It''s up to you. This is just like the generic prompt template. Then what we''re
29:39 going to be doing is based on what this outputs, we''re then going to feed that
29:44 in to this generate and display image function via the image prompt parameter
29:50 that is going to use the Darly API wrapper from Langchain. It''s going to run that image prompt and we''re going to
29:57 get a URL out from that essentially and then we''re going to read that using SK
30:02 image here. Right? All right. So, it''s going to read that image URL. Going to get the image data and then we''re just going to display it. Okay. So, pretty
30:11 straightforward. Now, again, this is a cell thing here that we''re doing. We
30:17 have this runnable lambda thing. When we''re running functions within LEL, we
30:23 need to wrap them within this runnable lambda. I, you know, I don''t want to go too much into what this is doing here',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  13,
  'LangChain v0.3  Getting Started',
  '30:30 because we do cover in the LE cell chapter. But it''s just you know all you really need to know is we have a custom 30:36 function wrap it in runnable lambda and then what we get from that we can...',
  '30:30 because we do cover in the LE cell chapter. But it''s just you know all you really need to know is we have a custom
30:36 function wrap it in runnable lambda and then what we get from that we can use
30:41 within this here right the the lolt syntax. So what are we doing here? Let''s
30:47 figure this out. We are taking our original that image prompt that we defined just up here right input
30:53 variable to that is article. Okay we have our article data being input here.
31:00 Feeding that into our prompt. From there, we get our message that we then feed into our LM. From the LM, it''s
31:07 going to generate us a like an image prompt. Uh like a prompt for generating our image for this article. We can even
31:15 let''s uh let''s print that out so that we can see what it generates because I''m also kind of curious. Okay. So, we''ll
31:22 just run that and then let''s see. It will feed in that content into our
31:28 runnable, which is basically this function here. And we''ll see what it generates. Okay. Don''t expect anything
31:35 amazing from Darly. It''s not it''s not the best to be honest, but at least we
31:41 see how to use it. Okay. So, we can see the prompt that was used here. Create an
31:46 image that visually represents a concept of muros symbolic agents. It depicts a futuristic interface where large know
31:53 interacts with traditional code symbolizing integration of oh my gosh uh
31:58 something computation include elements like a brain to represent neuronet networks gears or circuits or symbolic
32:06 logic and a web of connections illustrating vast use cases of AI
32:11 agents. Oh my gosh look at all that big prompt. Then we get this. So,
32:17 you know, Darly is interesting. I would say we could even take this. Let''s just see what that comes up with in something
32:24 like midjourney. You can see these way cooler images that we get from just
32:29 another image generation model. Far better, but pretty cool honestly. So, in terms of generation images, the phrasing
32:37 the the prompt itself is actually pretty good. The image, you know, could be better, but that''s it. Right. So with
32:45 all of that, we''ve seen a little introduction to what we might building with ling chain. So that''s it for our
32:51 introduction chapter. As I mentioned, we don''t want to go too much into what each of these things is doing. I just really
32:59 want to focus on okay, this is kind of how we''re building something with line',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=i2jGTcJsDPM',
  14,
  'LangChain v0.3  Getting Started',
  '33:04 chain. This is the overall flow. uh but we don''t really want to be focusing too 33:10 much on okay what exactly LSL is doing or what exactly uh you know this prompt 33:16 thing is that we''re set...',
  '33:04 chain. This is the overall flow. uh but we don''t really want to be focusing too
33:10 much on okay what exactly LSL is doing or what exactly uh you know this prompt
33:16 thing is that we''re setting up. We''re going to be focusing much more on all of those things and much more in the
33:23 upcoming chapters. So for now we''ve just seen a little bit of what we can build
33:28 before diving in in more detail.',
  '{"channel": "James Briggs", "video_id": "i2jGTcJsDPM", "duration": "33:47", "level": "BEGINNER", "application": "LangChain", "topics": ["Local Setup", "Colab Setup", "OpenAI LLMs", "LLM Prompting", "Temperature", "Prompt Templates", "LCEL Chains", "Structured Outputs", "Pydantic", "Image Generation", "DALL-E"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=lG7Uxts9SXs',
  1,
  'LangChain Crash Course for Beginners',
  '0:00 Lang chain is a framework designed to simplify the creation of applications using large language models it makes it\n0:06 easy to connect AI models with a bunch of different data sources so you c...',
  '0:00 Lang chain is a framework designed to simplify the creation of applications using large language models it makes it\n0:06 easy to connect AI models with a bunch of different data sources so you can create customized NLP applications\n0:12 Rashad Kumar created this Lang sync course for beginners he is an experienced engineer and a great teacher\n0:19 let''s learn about what langchin is sulang 10 is an open source framework\n0:25 that allows developers working with AI to combine large language models like\n0:30 gpt4 with external sources of computation and data the framework is\n0:36 currently offered in Python in JavaScript well typescript to be specific and you can combine large\n0:44 language models like gpt4 from open AI or hugging phase to your own application\n0:50 so it''s an open source framework that allows you to build you know AI llm applications allows you to connect a\n0:57 large language model like tpt4 to your own sources of data and we are not\n1:02 talking about you know pasting a snippet of text into chat GPT prompt we''re\n1:07 talking about referencing an entire database filled with your own data so it could be you know a book that''s in PDF\n1:15 format that you have converted into the right format for these llms to use which\n1:21 are known as Vector databases and not only that once you get all this information you need you can have leg\n1:28 chain to perform a certain Action For You by integrating external apis so\n1:33 let''s say you want to send an email at the end of you know whatever task you did with your given data set and this is\n1:41 where the kind of the main Concepts come into play for the Lang chain framework\n1:47 so I built this diagram to better you know kind of understand the concepts so\n1:52 you have three main kind of Concepts you have components chains and agents So\n2:00 within components you know we have llm wrappers that allow us to connect to a\n2:06 large language model like gpt4 or hugging face then we have prompt templates\n2:13 prompt templates allows us to avoid having to hard code text which is the\n2:19 input to LLS and then we have indexes that allows us\n2:24 to extract the relevant information for the other labs the second concept is change the chains\n2:32 allow us to combine multiple components which are these here\n2:37 together to solve a specific task and build an entire application\n2:44 and finally we have the agents that allows llm to interact with its\n2:50 environment and any of the external apis remember how I talked about the task you\n2:56 want to perform after you have retrieved the information there is a lot to unpack\n3:01 in Lang chain and new stuff is being added every day but on a high level this\n3:06 is what the framework looks like but I have built you know kind of a demo app\n3:11 as you know projects are the way that all of this information basically sticks\n3:18 talking about requirements for this course so you will need python installed and specifically version 3.8 or higher\n3:26 and pip which is python package manager a code editor so I''ll be using visual\n3:33 studio code but you can choose whatever code editor of your choices\n3:38 and also an open AI account since we''ll be using the open AIS llm today to build\n3:44 our link chain applications I''ll be using a Windows machine so all of the commands you''ll see in the terminal will\n3:50 be for Windows users but they are quite similar on Mac OS or Linux systems so\n3:56 let''s start with the first thing which is you''ll need an openai account and in\n4:03 order to sign up for an opening account you can go to openai.com and click on\n4:08 login this will take you to the login screen I already have an account signed up with\n4:15 my Google account so I''ll go ahead and log in the reason why we need openai is we will be using open AIS llm and we\n4:23 need an API key so if you click on your user account on the top right hand corner you can click on view API Keys as\n4:30 you can see I have generated a few of them in your case you''ll not see any API keys so you can click on create new API\n4:38 key and this will give you a new openai API key now remember to save that safely\n4:46 somewhere because as you can see you can''t reveal the API key again so once\n4:51 you create a new one it''ll be only revealed one time so save that and we''ll be using it later as an environment\n4:58 variable in our code so now let me open up my terminal here and what I''m going\n5:04 to do is create the project directory where our code will reside so I want to\n5:10 make sure I''m in the right directory on my computer here which is GitHub and\n5:16 I''ll create a new directory by typing in the command mkdir and we''ll call this\n5:21 Lang chain Dash llm-app now let''s change our directory to our project directory\n5:28 here and let me open it up in Visual Studio code which is the editor of my\n5:34 choice again you can use any code editor that you like okay now that we have of\n5:39 the project directory open in Visual Studio code I''m just going to open up a terminal in my visual studio code here\n5:46 what I want to do now is create a virtual environment so we''ll be using\n5:51 Python and we''ll be creating a virtual environment and you can do that by typing python Dash mvnv and then dot VNV\n6:00 so this is the command and then dot VNV is the directory where the virtual\n6:05 environment will exist and as you can see on the right hand side where my project directory is open we have a\n6:13 folder now called dot V EnV and once we have that prepared we''ll need to\n6:19 activate this virtual environment and you can do that on Windows by typing in\n6:24 E and V scripts and then activate.ps1 which is a Powershell\n6:29 script that will activate our virtual environment as you can see there is a green virtual environment text in the\n6:36 front of the prompt so this means the my virtual X environment has been active and now we''ll use pip which is a python\n6:44 package manager to install the required packages that we''ll be using today so I''m gonna do pip install and then Lang\n6:50 chain openai streamlit and also python dot EnV so Lang chain allows us to you\n6:57 know work with Lang chain using python open AI since we''ll be using open Ai\n7:02 zlnm and then streamlit allows us to build interface for python applications\n7:08 and you''ll be seeing it how streamlit makes it so easy to build interfaces and then python.env allows us to use dot EnV\n7:16 file which is where our openai API key will reside safely as you know\n7:21 environment variables in our python code hit enter okay so after some time all the packages\n7:29 should be installed and you can see my terminal is giving me a warning that a new version of pip is available so if\n7:36 you get same burning you can either upgrade it or you can ignore the warning for now I''ll just hit clear so that my\n7:42 terminal has a clear screen but also I''ll close it for now what I want you to do is now create a main dot Pi file so\n7:50 now we have main.pi we are python code for the site so let''s start by importing\n7:57 Lang chain on the top and we''ll be using llms I want to use open AI again I''m\n8:03 using open AI I know it will cost some money and I''ll show you in my openai\n8:08 dashboard how much of the API calls cost but it''s in cents but it is the best one if you want to use some other ones like\n8:16 the open source hugging phase llm models you can do that too Lang chin supports\n8:21 it but right now I''m happy with openai and also what I want to do is use dot\n8:27 EnV the python.env package that we installed to load our environment\n8:32 variables and we can initiate that by typing in load.env now I can go ahead\n8:38 and create a DOT EnV file and save my open AI underscore API underscore key as\n8:46 an environment variable here and this is where you will paste the SK Dash key that was created in the openai dashboard\n8:53 so let me do that and I don''t want to reveal my open AI API key okay so I\n8:59 copied my API key from my open AI account and pasted it in EnV file here\n9:05 so I''ll close that now what I want to do with the first sample application here is generate pet names so let''s say I\n9:14 have a pet dog and I want to generate some cool names for it and maybe we''ll add few parameters where people can\n9:22 select what kind of pet it is and maybe color so let''s to start with that\n9:27 function so you can define a function in Python by typing in Def and then we''ll call this generate underscore pet\n9:34 underscore name and now we''ll be using llm from our Lang chain library and as I\n9:41 said I''ll be using openai today so this has few properties one of them is\n9:46 temperature now what temperature means is how creative you want your model to\n9:52 be so if the temperature is set to let''s say 0 it means it is very safe and it is\n9:58 not taking any bets or risks but if it is set to 1 it will be very creative and\n10:04 will take risks and also might generate wrong output but it is very creative at\n10:09 the same time so I tend to set my temperature to be 0.5 or 0.6 so that you\n10:16 know it can get a little bit of creative so let''s set that by typing in temperature\n10:22 and now what I want to do is use this llm to create cool names for my pet\n10:28 which is in my case a DOT so I''ll type something like I have a dog pad and I want cool names for it suggest me five\n10:36 cool names so that''s what our luncheon app is gonna be it''ll suggest five cool names for your pet so let me type that\n10:43 out as a prompt okay so I have my prompt ready and this function will return the\n10:49 name and now what we can do is if name is equal to main which is you know\n10:55 boilerplate python code I wanted to print whatever that function\n11:01 generates so generate pet name will be printed in our console output so let''s\n11:06 give this a try by opening up the terminal here and typing in Python main\n11:12 dot py so as you can see it gave me five names for my pet dog Apollo Blaze\n11:20 Hershey Kona and Maverick which are pretty good names and as you can see I\n11:25 ended up setting the temperature to 0.7 so it''s getting a little bit of creative again you can you know test this out by\n11:33 toggling this between 0 to 1 and see what temperature suits your needs but\n11:40 for me yeah 0.5 to 0.7 anything between that is good since I need my llm to be a\n11:46 bit creative so we just introduced one component of Lang chain which is llm the\n11:52 next thing that I want to introduce you to is promptemplate so prompt templates\n11:58 make it easy to generate these problems so you don''t have to keep asking openai\n12:05 a different prompt every time right so we want to repurpose this so that people on the internet might be able to\n12:11 generate pet names so maybe we will create you know imagine that you want to create a web app where people can\n12:17 comment and read pet names we want to repurpose this prompt and also we don''t want to hard code dog and if we want to\n12:24 have pet color as an option we don''t want to hard code that so we want the ability to repurpose our llm prompt for\n12:34 different kind of animals and different kind of colors and the way we can do that is by using prompt templates so\n12:41 prompt template name let''s just call it that and in Lang chain it''s called\n12:47 prompt template and we will also have to import it from Lang change so going to\n12:52 the top let''s import prompt templates okay so you''re using Lang chin prompts\n12:57 and importing prompt template now you can see the squiggly line underneath it has gone so let''s give it to let''s give\n13:05 it an input variables so input variables are the parameters that\n13:11 can be dynamic so in our case it will be animal type right so animal underscore\n13:17 type and now we''ll also have to add that as a parameter to our python function\n13:22 there we go so animal type is the input variable and the template that our\n13:28 prompt has is same as this so I''ll copy this and instead of a dog pet I will use\n13:36 the input variable here which is animal underscore type so now you can imagine you can say hey I have a cat and some\n13:45 other person comes and says hey I have a cow pet and I want a cool name for it suggest me five cool names so that is\n13:52 what prompt templates allows you to do and now we''ll have to also get rid of this and use chains as a concept so\n14:00 that''s import chain from Lang chain from Lang chain dot chains import llm chain\n14:08 what llm chain allows us to do is put these individual components of Lang\n14:14 chain together so llm and Prime template in our case so llm chain llm is equal to\n14:20 llm in our case because we named it and prompt is equal to prompt template so\n14:26 I''ll just copy this so prompt template name and instead of name let''s call this\n14:31 name chain right since this is an llm chain and instead of returning name let''s create a response here\n14:40 and that response basically will be name underscore chain and we''ll be using the\n14:46 animal type parameter here right which is basically whatever the animal type\n14:51 the person specifies and will be returning the response here so response\n14:58 will be whatever this chain gives us the output has right now let''s try instead\n15:05 of dog let''s try cat right so we are using parameters to print five cool pet\n15:11 names using our name underscore chain which is the llm chain using openai and\n15:18 using this prompt template hit Ctrl s and going back to my terminal here\n15:24 let''s run python main.py so now you can see we are getting a Json\n15:30 response with animal type which is cat and the text that we got is so these are\n15:37 the names that we got one is mochi or Moki nacho Pebbles tiger and whiskers so\n15:46 we got five names for our animal type cat similarly you can try cow here hit\n15:52 Ctrl s and run the python file again now it says animal type was cow and the\n15:58 text response is where our cow pet names are so one is hambone Daisy moo Moody\n16:06 Milky Way and give her hugs awesome so the other parameter that I want to add\n16:11 to our pet''s name generator is the pet color because I think that is an\n16:17 important aspect when you name your pet right so pet color and so we''ll add pet\n16:23 color as a parameter to our generate pet names function but also we''ll have to add it as an input variable in our\n16:30 prompt template so let''s add fat color over here there we go and now we''ll also\n16:36 have to change the template itself so I have an animal type pet and I want a\n16:42 cool name for it and let''s add it is whatever the color is so pet color so maybe it''s black in color suggest me\n16:50 five cool names for my pet there we go so that is our new prompt\n16:56 hit control s and in the name chain we''ll also have to add the pet color\n17:01 here so pet underscore color and that will be equal to whatever the pad color\n17:07 the person picks or says so now we can\n17:12 run this by saying cow and our cow color is black so let''s let''s try that out\n17:18 toggle back my terminal here and type in Python main.pi and you can see so we got\n17:24 animal type cow pet color is black and we received text response with those\n17:30 five names so one is Shadow second is midnight uh Starlight and we have Raven\n17:36 awesome so our pet''s name generator is working as expected maybe I want to\n17:42 publish this as web app later right and that''s where streamlib comes in streamlit will build us a web interface\n17:49 and we don''t have to do much we can use our python file here to build that\n17:55 beautiful interface and then people can come in and select whatever pet kind\n18:00 they have and whatever pet color they have and it would output those five\n18:06 names utilizing the Lang chin app we built so in order to do that what I I\n18:11 want to do is instead of having all of this code in main.pi I want to create\n18:18 another file called Lang chain underscore Helper and this is where all\n18:24 our Lang chain code will go so I''m going to go into main.pi Ctrl a to select all\n18:31 the code and paste it in the langchin helper file here and then we can clear\n18:37 the main.pi so our main.pi is blank and I have moved all my code to Lang chain\n18:43 underscore helper.py hit Ctrl s so make sure you have saved that and in main.pi\n18:49 what I want to do is input our Lang chain helper Library so we can do that\n18:55 by doing Simple import statement on the top so I''m importing Lang chain helper as lch just short form so that I''ll be\n19:04 able to call our generate pet name function by just using lch dot right\n19:10 also remember we did pip install streamlit in the beginning so we''ll be using that here too and I''ll be calling\n19:18 it throughout the python code AS SD which is just short for streamlit so in order to create our streamlit app you\n19:25 can use different text types and you can also use markdown which streamlit will\n19:31 render but one of the main things is having a title for our web interface and\n19:37 you can do that by doing St dot title and we''ll call this pet''s name\n19:43 generator right hit save and now I''ll just show you how to run a streamlined app where\n19:50 you can do that is open the terminal and type in streamlit run main.py hit enter\n19:56 and let me open my browser on Port 8501\n20:02 there we go so as you can see right out of the box we have this interface that was built using streamlit\n20:09 and again if you haven''t heard about streamlit it''s an amazing tool you can go to streamlit.io and go through their\n20:16 documentation on how to even make your web app better since I''ll be using some\n20:22 basic components from streamlit to display our pet''s name generator beautifully so let''s get back to our app\n20:31 here so back in our code editor I''ll hit Ctrl C in my terminal to stop the\n20:36 streamlit app and bring my terminal down and now we need some variables and Logic\n20:43 for the ability for users to pick their pets and the pet color so one of them is\n20:49 the animal type right whether it''s dog cat or a cow will give a sidebar selection for our users so SD dot\n20:58 sidebar dot select box will allow you to do that and you can input what the\n21:04 question is so what is your pet question mark and then you can include the\n21:11 options so it will be a drop down where people can select cat right dog and cow\n21:19 and maybe a hen right so think of all the pets then people that people can\n21:24 have maybe hamster is more popular I guess so cat dog cow hamster and then\n21:29 you can just keep going so that is the animal type and I can show you how this looks on our streamlit apps so streamlit\n21:35 space run space so you can see that and I can zoom in a\n21:41 little bit on the left hand side we have a sidebar now and you can select what kind of pet you have so what is your pet\n21:48 the next logic that I want to build is another option to select the color of\n21:54 your pet but I want it in a way that once you have selected the pad type so\n21:59 animal type right if it''s cat it should say what color is your cat and we can do\n22:04 that by if statements so if animal underscore type is cat right I want pet\n22:11 color which is another variable we pass to our generate pet name function here\n22:18 you can say pet color is equal to and then we use the select box component\n22:24 from streamlit to ask what color is your\n22:29 cap now I feel like there can be different variations so you can''t just\n22:34 put in black blue white orange you know since with cows and even cats\n22:42 and dogs you can have multiple colored pets right like a white dog with black\n22:47 spots on it so we can''t have a select box let''s just keep this as a text to you and I just thought of that as I was\n22:55 building this right so instead of a select box we have a sidebar with a text\n23:00 area that asks for what color is your cat and I also want to maybe have a\n23:08 limit of Maximum characters that people can put into this because remember we are calling the open AI API and the API\n23:15 calls depend on the amount of information you are sending in the prompt template so if our prompt gets\n23:22 bigger we''ll be charged more so in order to limit that let''s have a Max character\n23:28 property here again this is available on Shameless documentation and we''ll use\n23:34 the label ER so the label is what color is your cat and the maximum characters that users will be allowed to put in is\n23:40 15 and we can hit save what you can do is copy this over for dog cow so I''ll\n23:48 put dog here and what color is your dog or dog for cow it will be what color is\n23:54 your cow and then I think we''re left with one which is for hamster again I''ll\n23:59 just copy this and paste for hamster okay there is an efficient way to do\n24:06 this but I''m just gonna copy the code that I already have go back to my browser here refresh my streamlit page\n24:13 and now you can see if we select dog it will say what color is your dog and you see the text area which has a limit of\n24:20 15 characters similarly if you select the cow you can see it asks\n24:26 what color is your cup so both of the parameters have been set right now what\n24:32 I want to do is send this information to our Lang chain helper right because this\n24:38 is where it will generate those names and give it back to us so let''s do that\n24:44 so after we have set the pet color right because that''s the last question\n24:49 we ask our users what we want to do is have a variable here called response\n24:56 and response is equal to LC Edge which stands which is just short for Lang\n25:02 chain helper here and the function in the Lang chain helper is generate pet\n25:08 name so I''ll copy that over so you do lch dot generate pet name so we are accessing\n25:15 that function animal type was the first parameter that we need again I''m using\n25:21 animal type as a variable here maybe we can say user\n25:27 underscore animal type and I''ll have to change that over here\n25:33 over here over here and over here again just so that you''re not confused so\n25:40 two parameters animal type and pet color and then I''m using the user animal type as a variable on our main.pi so user\n25:48 underscore animal underscore type and the second parameter is pet color again\n25:53 you can do the same here so user underscore pet underscore color and you''ll have to update all of these here\n26:00 just to avoid confusion so we are passing these variables that the user\n26:06 said so user will say I have a dog and its pet color is white and we are\n26:12 passing those to our generate pet name function and then we''ll just write that\n26:19 as a text field so our text Will field will just reply with response so let''s\n26:25 save that now let''s go back to our browser here hit refresh\n26:31 now let''s select dog and type in the color black\n26:37 and you can hit Control Plus enter to apply and you can see we got a response\n26:45 with the five pet names right and\n26:51 what you can do to display this beautifully is set an output key right\n26:59 so let''s go back to our Lang chain helper here and in the name underscore chain We''ll add a third property called\n27:06 output key right and the output key is pet underscore name so basically instead\n27:13 of giving us a text output it will associate those five names that it\n27:18 generated to this output key and we can access this in our main.pi so instead of\n27:25 just returning the entire response so the whole text here see how it looks\n27:30 weird we''ll just we''ll just access the names that it generated and we can do\n27:36 that by doing response and then accessing that underscore name which was the Kiwi set\n27:44 so hit Ctrl s go back to our browser window and click refresh this time let''s\n27:49 go with the cat which is white hit Control Plus enter to apply and you can see it displays the text now\n27:58 better right it looks beautiful and we have the recommendations here as snowy\n28:04 marshmallow cotton pull blizzard let''s go over the brown hamster so hamster\n28:10 and brown Coco mocha Chestnut caramel biscuit\n28:16 love those names so now as you can see we have a streamlit app and we are using Lang chain to generate\n28:23 five cool pet names for the pets that we might have and we saw how you can use\n28:30 another lamp prom templates and the chain which are three main components of\n28:38 Lang chain but now the important one that''s left is Agents right so agents\n28:45 allow llms to interact with the environment so think of apis or things\n28:51 you want to do after Gathering the information so going over the Lang chain documentation about agents the core idea\n28:58 of Agents is to use an llm to choose a sequence of actions to take in Chains a\n29:05 sequence of actions is hard coded in code whereas in agents a language model is\n29:12 used as a reasoning engine to determine which actions to take and in which order\n29:18 and there are several key components Langton provides a few different types of agents to get started even then you\n29:25 will likely want to customize those agents depending on the personality of the agent and the background context you\n29:32 are giving to the agent and then there are tools so tools are functions that an\n29:37 agent calls there are two important considerations giving the agent access to the right tools and describing the\n29:45 tools in a way that is most helpful to the agent so let''s test it out so we\n29:52 already have a pet''s name generator thing that''s working for us right gives\n29:57 us a name for our pet now let me create another function here\n30:03 which will name Lang chain underscore agent and before we can interact with\n30:10 the agent we have to import the Lang chain Agents from the framework so you\n30:15 can do that by adding these three import statements on top so we are importing\n30:20 tools we are also importing the initialization of the agent and the\n30:26 agent type so coming back to our function here so first we''ll Define the llm that we want to use and I still want\n30:33 to use the openai llm and the temperature will set it to 0.5 here and\n30:39 then we can load some tools that will perform the given action so\n30:45 there are various tools that are available and again you can go through the availability of tools or the list of\n30:53 tools on the link chain documentation but I''ll be using Wikipedia which will\n30:59 be the first tool I want to use and I''ll get to it why I want to use Wikipedia and the other one is llm matte because I\n31:06 want to perform some matte and this is to just showcase what agents can do right and then the llm that we''ll be\n31:14 using is defined here which is the open AI so llm is equal to llm right and now\n31:20 we''ll have to initiate the agent so agent and to initialize its initialize\n31:27 underscore agent and here you specify the tools that will\n31:32 be providing it which is stored right here which is Wikipedia and lmat the llm\n31:38 we want to use right and the agent type so one of the agent\n31:43 types that''s available in the quick start guide for langchain is the react\n31:49 and you can go to the agent types documentation here so zero shot react is\n31:55 the one that I''ll be using decision uses react framework to determine which tool\n32:00 to use based solely on the tools description so heading over to our code\n32:06 and the way you define that agent type is by setting it here and we''ll set the\n32:13 verbose flag to True which means it''ll show us the reasoning that''ll happen in\n32:19 our console so that''s the agent we want and we''ll create a result here where we\n32:26 run the agent and now you can specify the tasks so you want to perform through\n32:32 this agent so since our app is solely based on pets let''s ask it what is the\n32:38 average age of a dog and I''ll ask it to do some math\n32:44 and that is the reason why I loaded the llm math tool here multiply the age by\n32:50 three and at the end we''ll print result so that looks good and I will change\n32:55 this so I''ll comment this out instead we''ll print whatever this generates so\n33:01 Lang chain agent right hit save and now we can run this and just to demonstrate\n33:07 it I''ll not be linking this to our streamlit app which was the web interface I''ll just run the\n33:14 Lang chain underscore helper python file just to Showcase you how agent works so\n33:20 before I do that I have to make sure that Wikipedia is installed through pip\n33:25 so pip install Wikipedia will install that python Library so now if I run the\n33:31 langchin helper file we''ll see the agent in action okay so you''ll see that it\n33:38 finished the chain and the answer was the average of a dog is 45 years when\n33:43 multiplied by three but the final answer that it got was 15 right so the average\n33:49 age of the dog is 15 and then it multiplied by 3 which is 45. so you can\n33:55 see that it was able to grab the information from Wikipedia which is 15\n34:01 as the average age of a dog and it was also able to perform the math and get to\n34:08 this right and now since we set the verbose flag to true you can see the\n34:13 reasoning that went into it right and I''ll increase my terminal and with no size here and get rid of the file\n34:21 explorer on the right so you can see I need to find out the average age of a\n34:26 dog action is Wikipedia action input is averages of talk and this is the\n34:32 observation that it found right so it did scan few pages on Wikipedia\n34:38 thought I now know the average age of a dog and the age of the oldest dog right and then action is calculator where it''s\n34:46 trying to multiply 15 which is the average age by three because that''s what we asked it to do awesome so that''s how\n34:53 the agents work and I believe we have kind of covered almost all components\n34:59 within the langchin framework the only thing that''s left is indexes right so\n35:05 what are indexes basically as you can see we are still working with the open\n35:12 AI llm but we are also not providing any of the custom knowledge right so we are\n35:18 still relying on open Ai and the information that they have gathered but\n35:23 with langchain you can also provide your own knowledge or knowledge base on which you can ask llm to do certain actions so\n35:32 think of a PDF file or even URLs that you can script or maybe you have a large\n35:40 PDF file with a lot of text and maybe you want to run an llm AI chat bot for\n35:46 your own document so you can do that with the help of language in the next project that I want to showcase you will\n35:52 exactly do that will take a long YouTube video so think of a podcast which is\n35:58 hours long or a long YouTube video right so what I have here is\n36:04 the Microsoft CEO certain dealer full interview on recode but it''s 51 minutes\n36:09 long and what I want to do with Lang chain is the ability to ask questions to\n36:16 this video so the context that the llm would have is strictly of that video and\n36:22 I''ll be using few libraries like YouTube transcript which basically converts\n36:28 whatever URL we provide for a YouTube video and gets its transcript right so\n36:33 let''s build this YouTube assistant now I''m going to show you how you can create this assistant that can answer questions\n36:40 about a specific YouTube video so coming back to the concept of indexes I touched\n36:47 briefly on it but we also saw it in the Lang chain diagram but we know that these large language models become\n36:54 really powerful when you combine them with your own data and your own data in this scenario will be the YouTube\n37:00 transcript that we are going to download automatically but you can basically replace that transcript with any\n37:07 information in this approach so it could be a PDF it could be blog post URL right\n37:13 so what Langton offers is document loaders and I can quickly show you the YouTube\n37:21 transcript one so this is the YouTube transcript and basically it allows you\n37:26 to get the transcript which will be the text version of the YouTube video right\n37:31 but there are several other document loaders that you can see on the left hand side right so you can bring in an\n37:38 S3 file you could bring an Azure blob storage file you could do Hacker News\n37:44 posts or articles right so these are some of the document loaders that are\n37:50 supported by linkchin as of now and we''ll be using text Splitters and Vector\n37:55 stores so we are going to use these three components to load our YouTube video transcript split it into smaller\n38:02 chunks and then store it as Vector stores so you can think of these as little helper tools that will make it\n38:09 easy for us to load the transcript which might be thousands of lines of text so\n38:16 to get us started what I have already done is created a YouTube assistant\n38:21 directory so not be using the pets generator directory that we had and what\n38:26 I have done is pretty similar to the pet''s name generator right so I have main.pi which will hold our streamlit\n38:33 interface and then the langchin helper will have the length chain components\n38:39 and I''ve also created a virtual environment and installed all the\n38:44 necessary packages which is link chain openai YouTube transcript also I''ve I\n38:50 went ahead and created dot EnV file which holds my openai API key so pretty\n38:56 similar to the pet''s name generator and now we can start with the lag chain\n39:01 helper first so the first thing that we are going to import is the YouTube loader that we saw right which is a\n39:09 document loader so from langtin dot document loaders we are importing that YouTube loader and the second important\n39:15 thing we need is the text splitter so as I showcased that the video that I have\n39:21 is 51 minutes long you could also pick up a podcast like Lex and they have\n39:29 podcasts that are three hours long and which means you''ll have thousands of\n39:35 lines and that is where we''ll use the the text splitter to break down those\n39:41 huge transcripts into smaller chunks and I''ll show you how and for the rest of the inputs we are gonna input the lag\n39:49 chain components like the llm which will be open AI prompt template and llm chain\n39:54 the other thing coming back to indexes we''ll be using Vector stores so I''ll be using the phase\n40:02 library and I''ll quickly show you what the face library is phase is a library by meta or\n40:11 Facebook for efficient similarity search and you might have heard of other Vector\n40:17 stores or databases like Pinecone or vv8 right but I''ll be using phase for this\n40:25 project so let''s start with writing some code so I''ve done all the necessary\n40:30 inputs here the only input that''s left is the dot EnV which will load our\n40:37 environment variables and I''ll initiate dot EnV here also since I''ll be using\n40:44 openai embeddings so we''ll initiate that to here and I forgot to import those so\n40:51 I''ll import the open aim bearings and now we can create our first function to\n40:57 create a function we know that in Python it''s deaf and let''s name this function\n41:02 that will be be creating a vector DB\n41:08 create Vector DB from YouTube so that''s a pretty big function name right but I\n41:14 want to specify what we are doing and we''ll be using phase here also for the\n41:20 parameter let''s give this a required parameter which is the video URL right\n41:25 so we''ll be pasting this video URL in our streamlit interface and that''s what\n41:30 we''ll be using and this will be a string right so the first thing we want to do\n41:35 is load the YouTube video from the URL right so we''ll use loader which we\n41:42 imported on the top so YouTube loader Dot from YouTube URL and we''ll pass the\n41:50 video URL parameter here after we have loaded the YouTube video I want to save\n41:55 this into the transcript variable so we''ll create transcript here and we''ll\n42:00 just do loader dot load and this should give us the transcript now we''ll be\n42:07 using text splitter and I''ll specifically tell you why so text\n42:12 splitter and we imported it here as recursive character text splitter you\n42:18 can specify few parameters when using this so the first one is chunk size\n42:24 which will set to 1000 and chunk overlap so chunk size is how much each chunk\n42:31 will contain so for me it will be 1000 right and then overlap is once it has\n42:37 created those individual docs from the long transcript it''ll have an overlap in\n42:43 every document so document one the last hundred words would also be included in\n42:48 the document twos first hundred words right so that is what overlap is and now we''ll save them into a docs\n42:56 variable so text underscore splitter not split documents as the function and\n43:03 we''ll provide the transcript that we had loaded from the YouTube url there we go\n43:08 okay now let''s also initiate the phase so\n43:14 phase Dot from documents and we will be using docs which we stored here right\n43:22 docs and we''ll be using the open Ai embeddings and we''ll return this DB\n43:29 okay so now on to the explanation why we have to split the text so basically what we\n43:37 are doing at the text splitter is we have taken over thousands of lines and\n43:43 split up the documents so it has taken very large transcript over and split it\n43:49 up into chunks of 1000 so that is the first step now you might wonder right so we can''t just provide thousand lines to\n43:57 the open AI API remember there is a token size or a limit on how much\n44:04 information you can send to open AIS API and that is why we have split the amount\n44:10 of context we''ll be sending for for a YouTube transcript right because the model that I''ll be using is the text\n44:18 DaVinci 003 and as you can see it can only take\n44:24 4097 tokens so I cannot send the entire transcript to open aiz Ai and that is\n44:32 why we''ll be splitting it and storing it into Vector stores again this is quite technical I''ll not\n44:40 go into much detail but vectors basically are a numerical representation\n44:45 of the text we just created here right so the core responsibility of this\n44:52 function is to load the transcript right take all the text that''s in the\n44:57 transcript split it into smaller chunks and then save those chunks as Vector\n45:03 stores again we can''t just provide all of these Vector stores to the open AI\n45:08 right we can''t just send over the 10 000 or maybe even 50 chunks that we have\n45:15 created of smaller text that''s where we''ll use phase to do a similarity\n45:20 search right and that''s what the next function will be and before I write that\n45:25 next function we''ll see if this works so video underscore URL so I''m gonna hard\n45:32 code the video URL that we have for the podcast and see if we get the smaller\n45:39 chunk documents right so let''s print this function at the end\n45:47 hit save and we''ll open the terminal make sure your virtual environment is\n45:53 activated and you have installed the required packages again all of this will\n45:58 be available on GitHub for reference later but let''s run the line chain helper python file again it''ll take some\n46:06 time to do the computation I missed to write print so we''ll have to\n46:12 print this whatever this function returns which should be the database that we created right so let''s run it\n46:20 again and this time we should get the vector stores that were created and so\n46:25 instead of DB if I return docs you''ll see those chunks\n46:32 so if I expand my terminal here you can see we have quite a few text here but\n46:39 here are the docs right so you can see that there''s a document and then it\n46:46 starts with the content and you''ll see multiple document chunks so these are\n46:51 the chunks that we created from the larger transcript so this is one right this is the second one\n46:58 and so on I know the formatting is weird so you can''t really tell where the new\n47:04 document starts but yeah this is all the chunks that we have awesome so our\n47:09 function to create the vector DB from YouTube url is working as expected so\n47:14 I''ll get rid of this print statement and full return DB here now for the next\n47:19 function which is going to be getting off the response on our query we have to\n47:26 ask this YouTube video right so let''s create that function we''ll name it get\n47:32 underscore response from query again pretty self-explanatory name for the\n47:39 function itself and we''ll pass few parameters to this function one is DB the important one will be query which\n47:47 will be the question that the user asks and K which is another argument that\n47:52 I''ll go over this is used for the similarity search that will do so keep in mind the amount of tokens that the\n48:00 text DaVinci 3 Model can take right so keep that in mind it''s 4097 so I''ll just\n48:06 add a comment here saying text DaVinci can handle 2097 tokens right now in\n48:15 order to do a similarity search we''ll save that into a docs variable within this function so DB\n48:22 is what we''ll use we''ll perform a similarity search on the DB which is the database we created in the previous\n48:29 function so gb.similarity underscore search and the search will be basically\n48:35 the query so the first thing I want to do with this function is basically search the query relevant documents so\n48:42 let''s say in this podcast they talk about a ransomware somewhere so right\n48:48 here they talk about ransomware right and if I want to ask a question saying what did they talk about ransomware so\n48:54 my query is just about ransomware that that they talked about in the podcast so\n49:00 it will only search the document that has details about ransomware so we''ll\n49:06 not send the entire documents that were created but just the one that is\n49:12 relevant to the query that the user made I hope that makes sense and this is also\n49:17 where we''ll pass the K as argument and I''ll tell you what K is so remember that\n49:25 we can have 4097 tokens but our chunk size is 1000.\n49:31 so that means we can kind of send four documents right because each document is\n49:39 a size of thousand so let''s set that value to four okay so we''ll be sending\n49:44 four relevant docs based on the query that the user made now I''ll create another variable called docs page\n49:51 content and what we''ll basically do is join those four docs that we''ll be sending\n50:03 okay so we got those four docs and we are joining them to create one dock\n50:08 because the Toca limit is 4097 and here we''ll almost have 4 000 tokens being\n50:15 sent to the text DaVinci 3 mod awesome now let''s work with the llm right so\n50:23 pretty similar to what we did with the pet''s name generator we''ll initiate the llm to be open Ai and\n50:31 as I said the model that I''ll be using is text DaVinci 3 so let me go to the\n50:37 open AIS documentation copy this model name come back here and paste it and\n50:42 there is some white space at the end so we''ll get rid of that and the second thing we did with the pets name\n50:48 generator was prompt right so prompt templates is the is another main\n50:53 component of Lang chain so we''ll use that and this is variable define what the prompt should be for the open AI llm\n51:02 so the first thing would be to specify the input variables right so the first\n51:08 one is question or query right so whatever the question is being asked by\n51:14 the user in Docs so docs is basically the similarity\n51:19 search we did there we go now the template that we''ll be using is a prompt\n51:25 that I''ve created here so I''m gonna copy this really quick since it''s a long prompt okay so I''ve copied the prompt\n51:32 basically it says you''re a helpful YouTube assistant that can answer questions about videos based on the\n51:39 videos transcript right answer the following question and this is where the input variable goes\n51:46 whatever the question the user is asking by searching the following video transcript which is the docs right so\n51:53 docs is basically the similarity search we did only use factual information from the\n51:58 transcript to answer the question if you feel like you don''t have enough information simply say I don''t know\n52:04 right because we don''t want the AI or the llm to hallucinate your answer should be needed so that is basically\n52:11 the prompt that we''ll be using to answer questions and now we''ll be using another\n52:17 main component which is chain within the Lang chain so let''s create an llm chain\n52:23 where llm is equal to llm because we specified it here that will be using\n52:29 openai model text DaVinci 3 and prompt is equal to prompt which we specified\n52:36 here using prompt template okay now we just have to learn the response so\n52:43 I''ll create a variable call response it will do chain dot run which will basically run our chain since we had\n52:49 question as the input variable here we''ll say that question is equal to\n52:56 query because that''s what we were referring to it on the previous function and\n53:03 Docs is equal to docs page underscore content\n53:09 remember because we joined all the four documents because K is set to 4 to\n53:17 be one doc because we can we have the ability to send four thousand tokens and\n53:22 then response is equal to response dot replace and this is just some\n53:29 formatting that we have to do because if you remember in the pet''s name generated to the response we were\n53:35 getting was in one line and it included new line characters so we''ll replace that with\n53:42 some white space and we''ll return response okay so now we can test this\n53:50 out as it is in the console by hard coding the question and the URL which we\n53:56 already did so let''s get ready for that but also build the interface because it''ll be really quick with streamlit so\n54:03 coming over to our main dot Pi let''s do some inputs on the top so pretty similar\n54:08 to what we did in our pets named generator so streamlit I''m importing it as St and the langchain helper where are\n54:17 all of the Lang chain code is and I''m also importing text wrap basically it\n54:22 gives you the ability to wrap text so that you''re not you don''t have to scroll the page the title of this page will be\n54:31 YouTube assistant right so YouTube assistant and now on the sidebar we can have those\n54:39 parameters that we need from the user with sidebar I want to create a farm so\n54:45 we have a submit button at the end so SD dot form is how you do that\n54:52 and you also have to specify a key so key is my form again this is all\n54:59 streamlit stuff and let me know in the comments if I should create a course on streamlit on how to build you know cool\n55:05 python interfaces I love this tool because I don''t have to care about building a front end and the first\n55:12 parameter we had in our length chain helper was the YouTube url right so\n55:18 we''ll save that as video URL so YouTube url is equal to SD dot sidebar text\n55:27 as we used in the pet''s name and we''ll just say that the label is what is the\n55:34 YouTube video URL and we''ll give a Max to maximum character limit of 50 because I don''t think a video URL can exceed 50\n55:42 characters uh the other parameter we had was the question that the user can ask and we''ll save it as query here so St\n55:49 dot sidebar dot text underscore area right and then the label will be asked\n55:57 me about the video so again you can have a limit here right so maybe you can only\n56:05 ask questions that are not long enough so we''ll set max characters 50 here too\n56:11 and also set the key to query here okay and at the last since I created this as\n56:18 a form we''ll give it a submit button and the label here will be submit now\n56:25 so if Kiri which is the question the user can ask and YouTube url exist right\n56:32 what I want to do is basically run this function to give us the answer right so\n56:40 we''ll be as we are already importing the link chain helper on the top as LS lch\n56:46 so that''s what we''ll be using here so DB which is the database will be equal to\n56:53 so remember we have to pass the video URL uh to the create Vector DB function\n56:59 to create a new Vector database based on the transcript that we got so DB is\n57:05 equal to lch which helps which is basically that we are accessing this python file and then the create Vector\n57:12 DB from YouTube url function and we''ll pass the YouTube url as the parameter because remember we\n57:19 just need the video URL here response comma docs is equal to and now we''ll get\n57:28 a response which we can do by running this function which is get response from\n57:33 query and remember the parameters that will be passing so lch dot getresponse\n57:40 from query the first one is DB which we just created right and query is the\n57:46 question that I will be asking so um right here whatever the user asks will be the query\n57:53 so I am missing a comma here as I''m going through my code so I''ll add that and now we''ll save that response in our\n58:02 interface with streamlit so let me create a sub header here which will say answer right and below that we''ll have\n58:10 St dot text and we''ll wrap that text\n58:15 and this is where the text wrap library is being used you''ll you''ll see this in the interface once I run it so text wrap\n58:22 dot fill and whatever the response we get from the length chain function\n58:30 you can also set the width of this text area to be 80 let''s go with 80 and see\n58:36 how that looks and that is basically it so two parameters for necessary one is the\n58:42 YouTube video URL and the question that the user asks right and we are passing so if the both of those parameters exist\n58:49 first we are creating the database from the YouTube video URL and then we''re getting the response based on the\n58:56 question that the user asked using the llm so now we can run our streamlit app\n59:02 after saving the file so if I scroll down to the bottom here for my terminal\n59:07 expand this and run stream lit run main.pi\n59:13 hit enter it should load our web interface for our\n59:18 streamlit app awesome on the left hand side you can see we need to provide a YouTube video URL so\n59:26 I''ll just go ahead copy this interview video URL paste it here\n59:32 ask me about the video so let''s say what did they talk about rent somewhere is\n59:42 what I want to know and hit submit okay so we have got some errors saying input variables let''s go\n59:51 to our terminal and see if we have any logging okay so I found the error I was just\n59:59 missing S I thought I typed it right so instead of input variable it needs to\n1:00:05 be input variables and we''ll Ctrl C to stop our streamlit app and do streamlit\n1:00:13 run run main.pi again so after adding the S hit enter and now\n1:00:20 we need the same exact information so copy this\n1:00:25 and copy the question so the YouTube video URL and what did they talk about\n1:00:31 ransomware hit submit there we go we got our answer so it says\n1:00:39 they discussed how ransomware is difficult to track due to zero day exploits and how Microsoft is making it\n1:00:45 a mission to help with secure cloud backup for Enterprises better tracking of zero day exploits and helping with\n1:00:52 enforcements they also discussed the importance of public-private Partnerships in order to prioritize\n1:00:57 cyber security and create new standards such as those for nist so\n1:01:02 remember our prompt I asked it to be as detailed as possible also say I don''t know if it doesn''t know\n1:01:09 what the answer is based on the transcript we provided and not to hallucinate so I think this is a pretty\n1:01:15 good answer um that we got out of this 52 minute\n1:01:20 video again you can pick a longer video and ask about anything specifically\n1:01:27 longer from podcasts right maybe the video is four to five hour Longs and you\n1:01:32 need to know a specific detail I think that''s where this tool or the app we\n1:01:38 build can be really handy right but yeah so we learned a lot about\n1:01:44 Lang chain today specifically the main three main components which is llm so\n1:01:52 any of the large language models that you can use like open AI or hugging face prompt templates right\n1:01:59 and chains so how you can combine these components into chains to perform the\n1:02:05 required task and agents right remember in the pets generator we talked a little\n1:02:11 bit about agents and how they have reasoning behind the\n1:02:16 tasks that they perform because we try to calculate average age of a dog and\n1:02:22 also multiply it by three so it used Wikipedia and llm math to get those\n1:02:27 answers but also we learned a bit about indexing and Vector stores so how you\n1:02:33 can split large documents into smaller chunks and store it as Vector which is basically you know\n1:02:39 numerical representation of the documents that we created and then\n1:02:46 passing those on to the llm since there are certain limits of how much context\n1:02:52 you can send to the API but yeah one other thing I would like to mention is\n1:02:58 if you are planning to make these apps public remember we were storing our\n1:03:04 environment variables in dot EnV file and you might be wondering every ship I\n1:03:10 also created an openai API key like how much all of this is going to cost so\n1:03:16 I''ll go into my dashboard in into billing to see how much did it cost me\n1:03:21 to you know basically kind of build this course out so you can see um\n1:03:28 10 cents and 30 cents so very close to\n1:03:33 less than a dollar like half of a Dollar close to 50 cents is what it costed me\n1:03:39 to make all of these queries to the openai llm the thing I was gonna\n1:03:45 recommend if you want to publish this app so that the public can use it is to have a field here uh you know with\n1:03:54 the sidebar saying open AI API key so that the users have to submit their\n1:04:00 openai API key with their app so you can have a text field here saying hey what\n1:04:06 is your open AI API key just so that you know you are not being charged and you can make that as a secret field so that\n1:04:14 the key is not displayed in the interface but you can use that key to make these queries you will just have to\n1:04:21 pass it in the Lang chain helper so whatever the variable name you decide\n1:04:27 maybe like open AI API key which you''ll get the value from our streamlit\n1:04:34 interface you can pass that right here when you initiate the large language\n1:04:40 model so you''ll specify openai API key as a\n1:04:45 parameter here and the value of that key which will be the variable you decide so\n1:04:51 yeah that''s pretty much it for this course again we learned quite a bit about the langchin framework\n1:04:58 specifically in Python uh you know the models prompts indexes chains and agents\n1:05:03 or the five main Concepts within nag chain that I wanted to cover again I\n1:05:09 hope this helps you understand the framework itself and how you can utilize this information to build something\n1:05:16 really cool with the power of llms but if you would like to see a streamlit\n1:05:21 course again let me know in the comments but I hope you find this course helpful I''ll see you in the next one peace\n0:00 Lang chain is a framework designed to simplify the creation of applications using large language models it makes it\n0:06 easy to connect AI models with a bunch of different data sources so you can create customized NLP applications\n0:12 Rashad Kumar created this Lang sync course for beginners he is an experienced engineer and a great teacher\n0:19 let''s learn about what langchin is sulang 10 is an open source framework\n0:25 that allows developers working with AI to combine large language models like\n0:30 gpt4 with external sources of computation and data the framework is\n0:36 currently offered in Python in JavaScript well typescript to be specific and you can combine large\n0:44 language models like gpt4 from open AI or hugging phase to your own application\n0:50 so it''s an open source framework that allows you to build you know AI llm applications allows you to connect a\n0:57 large language model like tpt4 to your own sources of data and we are not\n1:02 talking about you know pasting a snippet of text into chat GPT prompt we''re\n1:07 talking about referencing an entire database filled with your own data so it could be you know a book that''s in PDF\n1:15 format that you have converted into the right format for these llms to use which\n1:21 are known as Vector databases and not only that once you get all this information you need you can have leg\n1:28 chain to perform a certain Action For You by integrating external apis so\n1:33 let''s say you want to send an email at the end of you know whatever task you did with your given data set and this is\n1:41 where the kind of the main Concepts come into play for the Lang chain framework\n1:47 so I built this diagram to better you know kind of understand the concepts so\n1:52 you have three main kind of Concepts you have components chains and agents So\n2:00 within components you know we have llm wrappers that allow us to connect to a\n2:06 large language model like gpt4 or hugging face then we have prompt templates\n2:13 prompt templates allows us to avoid having to hard code text which is the\n2:19 input to LLS and then we have indexes that allows us\n2:24 to extract the relevant information for the other labs the second concept is change the chains\n2:32 allow us to combine multiple components which are these here\n2:37 together to solve a specific task and build an entire application\n2:44 and finally we have the agents that allows llm to interact with its\n2:50 environment and any of the external apis remember how I talked about the task you\n2:56 want to perform after you have retrieved the information there is a lot to unpack\n3:01 in Lang chain and new stuff is being added every day but on a high level this\n3:06 is what the framework looks like but I have built you know kind of a demo app\n3:11 as you know projects are the way that all of this information basically sticks\n3:18 talking about requirements for this course so you will need python installed and specifically version 3.8 or higher\n3:26 and pip which is python package manager a code editor so I''ll be using visual\n3:33 studio code but you can choose whatever code editor of your choices\n3:38 and also an open AI account since we''ll be using the open AIS llm today to build\n3:44 our link chain applications I''ll be using a Windows machine so all of the commands you''ll see in the terminal will\n3:50 be for Windows users but they are quite similar on Mac OS or Linux systems so\n3:56 let''s start with the first thing which is you''ll need an openai account and in\n4:03 order to sign up for an opening account you can go to openai.com and click on\n4:08 login this will take you to the login screen I already have an account signed up with\n4:15 my Google account so I''ll go ahead and log in the reason why we need openai is we will be using open AIS llm and we\n4:23 need an API key so if you click on your user account on the top right hand corner you can click on view API Keys as\n4:30 you can see I have generated a few of them in your case you''ll not see any API keys so you can click on create new API\n4:38 key and this will give you a new openai API key now remember to save that safely\n4:46 somewhere because as you can see you can''t reveal the API key again so once\n4:51 you create a new one it''ll be only revealed one time so save that and we''ll be using it later as an environment\n4:58 variable in our code so now let me open up my terminal here and what I''m going\n5:04 to do is create the project directory where our code will reside so I want to\n5:10 make sure I''m in the right directory on my computer here which is GitHub and\n5:16 I''ll create a new directory by typing in the command mkdir and we''ll call this\n5:21 Lang chain Dash llm-app now let''s change our directory to our project directory\n5:28 here and let me open it up in Visual Studio code which is the editor of my\n5:34 choice again you can use any code editor that you like okay now that we have of\n5:39 the project directory open in Visual Studio code I''m just going to open up a terminal in my visual studio code here\n5:46 what I want to do now is create a virtual environment so we''ll be using\n5:51 Python and we''ll be creating a virtual environment and you can do that by typing python Dash mvnv and then dot VNV\n6:00 so this is the command and then dot VNV is the directory where the virtual\n6:05 environment will exist and as you can see on the right hand side where my project directory is open we have a\n6:13 folder now called dot V EnV and once we have that prepared we''ll need to\n6:19 activate this virtual environment and you can do that on Windows by typing in\n6:24 E and V scripts and then activate.ps1 which is a Powershell\n6:29 script that will activate our virtual environment as you can see there is a green virtual environment text in the\n6:36 front of the prompt so this means the my virtual X environment has been active and now we''ll use pip which is a python\n6:44 package manager to install the required packages that we''ll be using today so I''m gonna do pip install and then Lang\n6:50 chain openai streamlit and also python dot EnV so Lang chain allows us to you\n6:57 know work with Lang chain using python open AI since we''ll be using open Ai\n7:02 zlnm and then streamlit allows us to build interface for python applications\n7:08 and you''ll be seeing it how streamlit makes it so easy to build interfaces and then python.env allows us to use dot EnV\n7:16 file which is where our openai API key will reside safely as you know\n7:21 environment variables in our python code hit enter okay so after some time all the packages\n7:29 should be installed and you can see my terminal is giving me a warning that a new version of pip is available so if\n7:36 you get same burning you can either upgrade it or you can ignore the warning for now I''ll just hit clear so that my\n7:42 terminal has a clear screen but also I''ll close it for now what I want you to do is now create a main dot Pi file so\n7:50 now we have main.pi we are python code for the site so let''s start by importing\n7:57 Lang chain on the top and we''ll be using llms I want to use open AI again I''m\n8:03 using open AI I know it will cost some money and I''ll show you in my openai\n8:08 dashboard how much of the API calls cost but it''s in cents but it is the best one if you want to use some other ones like\n8:16 the open source hugging phase llm models you can do that too Lang chin supports\n8:21 it but right now I''m happy with openai and also what I want to do is use dot\n8:27 EnV the python.env package that we installed to load our environment\n8:32 variables and we can initiate that by typing in load.env now I can go ahead\n8:38 and create a DOT EnV file and save my open AI underscore API underscore key as\n8:46 an environment variable here and this is where you will paste the SK Dash key that was created in the openai dashboard\n8:53 so let me do that and I don''t want to reveal my open AI API key okay so I\n8:59 copied my API key from my open AI account and pasted it in EnV file here\n9:05 so I''ll close that now what I want to do with the first sample application here is generate pet names so let''s say I\n9:14 have a pet dog and I want to generate some cool names for it and maybe we''ll add few parameters where people can\n9:22 select what kind of pet it is and maybe color so let''s to start with that\n9:27 function so you can define a function in Python by typing in Def and then we''ll call this generate underscore pet\n9:34 underscore name and now we''ll be using llm from our Lang chain library and as I\n9:41 said I''ll be using openai today so this has few properties one of them is\n9:46 temperature now what temperature means is how creative you want your model to\n9:52 be so if the temperature is set to let''s say 0 it means it is very safe and it is\n9:58 not taking any bets or risks but if it is set to 1 it will be very creative and\n10:04 will take risks and also might generate wrong output but it is very creative at\n10:09 the same time so I tend to set my temperature to be 0.5 or 0.6 so that you\n10:16 know it can get a little bit of creative so let''s set that by typing in temperature\n10:22 and now what I want to do is use this llm to create cool names for my pet\n10:28 which is in my case a DOT so I''ll type something like I have a dog pad and I want cool names for it suggest me five\n10:36 cool names so that''s what our luncheon app is gonna be it''ll suggest five cool names for your pet so let me type that\n10:43 out as a prompt okay so I have my prompt ready and this function will return the\n10:49 name and now what we can do is if name is equal to main which is you know\n10:55 boilerplate python code I wanted to print whatever that function\n11:01 generates so generate pet name will be printed in our console output so let''s\n11:06 give this a try by opening up the terminal here and typing in Python main\n11:12 dot py so as you can see it gave me five names for my pet dog Apollo Blaze\n11:20 Hershey Kona and Maverick which are pretty good names and as you can see I\n11:25 ended up setting the temperature to 0.7 so it''s getting a little bit of creative again you can you know test this out by\n11:33 toggling this between 0 to 1 and see what temperature suits your needs but\n11:40 for me yeah 0.5 to 0.7 anything between that is good since I need my llm to be a\n11:46 bit creative so we just introduced one component of Lang chain which is llm the\n11:52 next thing that I want to introduce you to is promptemplate so prompt templates\n11:58 make it easy to generate these problems so you don''t have to keep asking openai\n12:05 a different prompt every time right so we want to repurpose this so that people on the internet might be able to\n12:11 generate pet names so maybe we will create you know imagine that you want to create a web app where people can\n12:17 comment and read pet names we want to repurpose this prompt and also we don''t want to hard code dog and if we want to\n12:24 have pet color as an option we don''t want to hard code that so we want the ability to repurpose our llm prompt for\n12:34 different kind of animals and different kind of colors and the way we can do that is by using prompt templates so\n12:41 prompt template name let''s just call it that and in Lang chain it''s called\n12:47 prompt template and we will also have to import it from Lang change so going to\n12:52 the top let''s import prompt templates okay so you''re using Lang chin prompts\n12:57 and importing prompt template now you can see the squiggly line underneath it has gone so let''s give it to let''s give\n13:05 it an input variables so input variables are the parameters that\n13:11 can be dynamic so in our case it will be animal type right so animal underscore\n13:17 type and now we''ll also have to add that as a parameter to our python function\n13:22 there we go so animal type is the input variable and the template that our\n13:28 prompt has is same as this so I''ll copy this and instead of a dog pet I will use\n13:36 the input variable here which is animal underscore type so now you can imagine you can say hey I have a cat and some\n13:45 other person comes and says hey I have a cow pet and I want a cool name for it suggest me five cool names so that is\n13:52 what prompt templates allows you to do and now we''ll have to also get rid of this and use chains as a concept so\n14:00 that''s import chain from Lang chain from Lang chain dot chains import llm chain\n14:08 what llm chain allows us to do is put these individual components of Lang\n14:14 chain together so llm and Prime template in our case so llm chain llm is equal to\n14:20 llm in our case because we named it and prompt is equal to prompt template so\n14:26 I''ll just copy this so prompt template name and instead of name let''s call this\n14:31 name chain right since this is an llm chain and instead of returning name let''s create a response here\n14:40 and that response basically will be name underscore chain and we''ll be using the\n14:46 animal type parameter here right which is basically whatever the animal type\n14:51 the person specifies and will be returning the response here so response\n14:58 will be whatever this chain gives us the output has right now let''s try instead\n15:05 of dog let''s try cat right so we are using parameters to print five cool pet\n15:11 names using our name underscore chain which is the llm chain using openai and\n15:18 using this prompt template hit Ctrl s and going back to my terminal here\n15:24 let''s run python main.py so now you can see we are getting a Json\n15:30 response with animal type which is cat and the text that we got is so these are\n15:37 the names that we got one is mochi or Moki nacho Pebbles tiger and whiskers so\n15:46 we got five names for our animal type cat similarly you can try cow here hit\n15:52 Ctrl s and run the python file again now it says animal type was cow and the\n15:58 text response is where our cow pet names are so one is hambone Daisy moo Moody\n16:06 Milky Way and give her hugs awesome so the other parameter that I want to add\n16:11 to our pet''s name generator is the pet color because I think that is an\n16:17 important aspect when you name your pet right so pet color and so we''ll add pet\n16:23 color as a parameter to our generate pet names function but also we''ll have to add it as an input variable in our\n16:30 prompt template so let''s add fat color over here there we go and now we''ll also\n16:36 have to change the template itself so I have an animal type pet and I want a\n16:42 cool name for it and let''s add it is whatever the color is so pet color so maybe it''s black in color suggest me\n16:50 five cool names for my pet there we go so that is our new prompt\n16:56 hit control s and in the name chain we''ll also have to add the pet color\n17:01 here so pet underscore color and that will be equal to whatever the pad color\n17:07 the person picks or says so now we can\n17:12 run this by saying cow and our cow color is black so let''s let''s try that out\n17:18 toggle back my terminal here and type in Python main.pi and you can see so we got\n17:24 animal type cow pet color is black and we received text response with those\n17:30 five names so one is Shadow second is midnight uh Starlight and we have Raven\n17:36 awesome so our pet''s name generator is working as expected maybe I want to\n17:42 publish this as web app later right and that''s where streamlib comes in streamlit will build us a web interface\n17:49 and we don''t have to do much we can use our python file here to build that\n17:55 beautiful interface and then people can come in and select whatever pet kind\n18:00 they have and whatever pet color they have and it would output those five\n18:06 names utilizing the Lang chin app we built so in order to do that what I I\n18:11 want to do is instead of having all of this code in main.pi I want to create\n18:18 another file called Lang chain underscore Helper and this is where all\n18:24 our Lang chain code will go so I''m going to go into main.pi Ctrl a to select all\n18:31 the code and paste it in the langchin helper file here and then we can clear\n18:37 the main.pi so our main.pi is blank and I have moved all my code to Lang chain\n18:43 underscore helper.py hit Ctrl s so make sure you have saved that and in main.pi\n18:49 what I want to do is input our Lang chain helper Library so we can do that\n18:55 by doing Simple import statement on the top so I''m importing Lang chain helper as lch just short form so that I''ll be\n19:04 able to call our generate pet name function by just using lch dot right\n19:10 also remember we did pip install streamlit in the beginning so we''ll be using that here too and I''ll be calling\n19:18 it throughout the python code AS SD which is just short for streamlit so in order to create our streamlit app you\n19:25 can use different text types and you can also use markdown which streamlit will\n19:31 render but one of the main things is having a title for our web interface and\n19:37 you can do that by doing St dot title and we''ll call this pet''s name\n19:43 generator right hit save and now I''ll just show you how to run a streamlined app where\n19:50 you can do that is open the terminal and type in streamlit run main.py hit enter\n19:56 and let me open my browser on Port 8501\n20:02 there we go so as you can see right out of the box we have this interface that was built using streamlit\n20:09 and again if you haven''t heard about streamlit it''s an amazing tool you can go to streamlit.io and go through their\n20:16 documentation on how to even make your web app better since I''ll be using some\n20:22 basic components from streamlit to display our pet''s name generator beautifully so let''s get back to our app\n20:31 here so back in our code editor I''ll hit Ctrl C in my terminal to stop the\n20:36 streamlit app and bring my terminal down and now we need some variables and Logic\n20:43 for the ability for users to pick their pets and the pet color so one of them is\n20:49 the animal type right whether it''s dog cat or a cow will give a sidebar selection for our users so SD dot\n20:58 sidebar dot select box will allow you to do that and you can input what the\n21:04 question is so what is your pet question mark and then you can include the\n21:11 options so it will be a drop down where people can select cat right dog and cow\n21:19 and maybe a hen right so think of all the pets then people that people can\n21:24 have maybe hamster is more popular I guess so cat dog cow hamster and then\n21:29 you can just keep going so that is the animal type and I can show you how this looks on our streamlit apps so streamlit\n21:35 space run space so you can see that and I can zoom in a\n21:41 little bit on the left hand side we have a sidebar now and you can select what kind of pet you have so what is your pet\n21:48 the next logic that I want to build is another option to select the color of\n21:54 your pet but I want it in a way that once you have selected the pad type so\n21:59 animal type right if it''s cat it should say what color is your cat and we can do\n22:04 that by if statements so if animal underscore type is cat right I want pet\n22:11 color which is another variable we pass to our generate pet name function here\n22:18 you can say pet color is equal to and then we use the select box component\n22:24 from streamlit to ask what color is your\n22:29 cap now I feel like there can be different variations so you can''t just\n22:34 put in black blue white orange you know since with cows and even cats\n22:42 and dogs you can have multiple colored pets right like a white dog with black\n22:47 spots on it so we can''t have a select box let''s just keep this as a text to you and I just thought of that as I was\n22:55 building this right so instead of a select box we have a sidebar with a text\n23:00 area that asks for what color is your cat and I also want to maybe have a\n23:08 limit of Maximum characters that people can put into this because remember we are calling the open AI API and the API\n23:15 calls depend on the amount of information you are sending in the prompt template so if our prompt gets\n23:22 bigger we''ll be charged more so in order to limit that let''s have a Max character\n23:28 property here again this is available on Shameless documentation and we''ll use\n23:34 the label ER so the label is what color is your cat and the maximum characters that users will be allowed to put in is\n23:40 15 and we can hit save what you can do is copy this over for dog cow so I''ll\n23:48 put dog here and what color is your dog or dog for cow it will be what color is\n23:54 your cow and then I think we''re left with one which is for hamster again I''ll\n23:59 just copy this and paste for hamster okay there is an efficient way to do\n24:06 this but I''m just gonna copy the code that I already have go back to my browser here refresh my streamlit page\n24:13 and now you can see if we select dog it will say what color is your dog and you see the text area which has a limit of\n24:20 15 characters similarly if you select the cow you can see it asks\n24:26 what color is your cup so both of the parameters have been set right now what\n24:32 I want to do is send this information to our Lang chain helper right because this\n24:38 is where it will generate those names and give it back to us so let''s do that\n24:44 so after we have set the pet color right because that''s the last question\n24:49 we ask our users what we want to do is have a variable here called response\n24:56 and response is equal to LC Edge which stands which is just short for Lang\n25:02 chain helper here and the function in the Lang chain helper is generate pet\n25:08 name so I''ll copy that over so you do lch dot generate pet name so we are accessing\n25:15 that function animal type was the first parameter that we need again I''m using\n25:21 animal type as a variable here maybe we can say user\n25:27 underscore animal type and I''ll have to change that over here\n25:33 over here over here and over here again just so that you''re not confused so\n25:40 two parameters animal type and pet color and then I''m using the user animal type as a variable on our main.pi so user\n25:48 underscore animal underscore type and the second parameter is pet color again\n25:53 you can do the same here so user underscore pet underscore color and you''ll have to update all of these here\n26:00 just to avoid confusion so we are passing these variables that the user\n26:06 said so user will say I have a dog and its pet color is white and we are\n26:12 passing those to our generate pet name function and then we''ll just write that\n26:19 as a text field so our text Will field will just reply with response so let''s\n26:25 save that now let''s go back to our browser here hit refresh\n26:31 now let''s select dog and type in the color black\n26:37 and you can hit Control Plus enter to apply and you can see we got a response\n26:45 with the five pet names right and\n26:51 what you can do to display this beautifully is set an output key right\n26:59 so let''s go back to our Lang chain helper here and in the name underscore chain We''ll add a third property called\n27:06 output key right and the output key is pet underscore name so basically instead\n27:13 of giving us a text output it will associate those five names that it\n27:18 generated to this output key and we can access this in our main.pi so instead of\n27:25 just returning the entire response so the whole text here see how it looks\n27:30 weird we''ll just we''ll just access the names that it generated and we can do\n27:36 that by doing response and then accessing that underscore name which was the Kiwi set\n27:44 so hit Ctrl s go back to our browser window and click refresh this time let''s\n27:49 go with the cat which is white hit Control Plus enter to apply and you can see it displays the text now\n27:58 better right it looks beautiful and we have the recommendations here as snowy\n28:04 marshmallow cotton pull blizzard let''s go over the brown hamster so hamster\n28:10 and brown Coco mocha Chestnut caramel biscuit\n28:16 love those names so now as you can see we have a streamlit app and we are using Lang chain to generate\n28:23 five cool pet names for the pets that we might have and we saw how you can use\n28:30 another lamp prom templates and the chain which are three main components of\n28:38 Lang chain but now the important one that''s left is Agents right so agents\n28:45 allow llms to interact with the environment so think of apis or things\n28:51 you want to do after Gathering the information so going over the Lang chain documentation about agents the core idea\n28:58 of Agents is to use an llm to choose a sequence of actions to take in Chains a\n29:05 sequence of actions is hard coded in code whereas in agents a language model is\n29:12 used as a reasoning engine to determine which actions to take and in which order\n29:18 and there are several key components Langton provides a few different types of agents to get started even then you\n29:25 will likely want to customize those agents depending on the personality of the agent and the background context you\n29:32 are giving to the agent and then there are tools so tools are functions that an\n29:37 agent calls there are two important considerations giving the agent access to the right tools and describing the\n29:45 tools in a way that is most helpful to the agent so let''s test it out so we\n29:52 already have a pet''s name generator thing that''s working for us right gives\n29:57 us a name for our pet now let me create another function here\n30:03 which will name Lang chain underscore agent and before we can interact with\n30:10 the agent we have to import the Lang chain Agents from the framework so you\n30:15 can do that by adding these three import statements on top so we are importing\n30:20 tools we are also importing the initialization of the agent and the\n30:26 agent type so coming back to our function here so first we''ll Define the llm that we want to use and I still want\n30:33 to use the openai llm and the temperature will set it to 0.5 here and\n30:39 then we can load some tools that will perform the given action so\n30:45 there are various tools that are available and again you can go through the availability of tools or the list of\n30:53 tools on the link chain documentation but I''ll be using Wikipedia which will\n30:59 be the first tool I want to use and I''ll get to it why I want to use Wikipedia and the other one is llm matte because I\n31:06 want to perform some matte and this is to just showcase what agents can do right and then the llm that we''ll be\n31:14 using is defined here which is the open AI so llm is equal to llm right and now\n31:20 we''ll have to initiate the agent so agent and to initialize its initialize\n31:27 underscore agent and here you specify the tools that will\n31:32 be providing it which is stored right here which is Wikipedia and lmat the llm\n31:38 we want to use right and the agent type so one of the agent\n31:43 types that''s available in the quick start guide for langchain is the react\n31:49 and you can go to the agent types documentation here so zero shot react is\n31:55 the one that I''ll be using decision uses react framework to determine which tool\n32:00 to use based solely on the tools description so heading over to our code\n32:06 and the way you define that agent type is by setting it here and we''ll set the\n32:13 verbose flag to True which means it''ll show us the reasoning that''ll happen in\n32:19 our console so that''s the agent we want and we''ll create a result here where we\n32:26 run the agent and now you can specify the tasks so you want to perform through\n32:32 this agent so since our app is solely based on pets let''s ask it what is the\n32:38 average age of a dog and I''ll ask it to do some math\n32:44 and that is the reason why I loaded the llm math tool here multiply the age by\n32:50 three and at the end we''ll print result so that looks good and I will change\n32:55 this so I''ll comment this out instead we''ll print whatever this generates so\n33:01 Lang chain agent right hit save and now we can run this and just to demonstrate\n33:07 it I''ll not be linking this to our streamlit app which was the web interface I''ll just run the\n33:14 Lang chain underscore helper python file just to Showcase you how agent works so\n33:20 before I do that I have to make sure that Wikipedia is installed through pip\n33:25 so pip install Wikipedia will install that python Library so now if I run the\n33:31 langchin helper file we''ll see the agent in action okay so you''ll see that it\n33:38 finished the chain and the answer was the average of a dog is 45 years when\n33:43 multiplied by three but the final answer that it got was 15 right so the average\n33:49 age of the dog is 15 and then it multiplied by 3 which is 45. so you can\n33:55 see that it was able to grab the information from Wikipedia which is 15\n34:01 as the average age of a dog and it was also able to perform the math and get to\n34:08 this right and now since we set the verbose flag to true you can see the\n34:13 reasoning that went into it right and I''ll increase my terminal and with no size here and get rid of the file\n34:21 explorer on the right so you can see I need to find out the average age of a\n34:26 dog action is Wikipedia action input is averages of talk and this is the\n34:32 observation that it found right so it did scan few pages on Wikipedia\n34:38 thought I now know the average age of a dog and the age of the oldest dog right and then action is calculator where it''s\n34:46 trying to multiply 15 which is the average age by three because that''s what we asked it to do awesome so that''s how\n34:53 the agents work and I believe we have kind of covered almost all components\n34:59 within the langchin framework the only thing that''s left is indexes right so\n35:05 what are indexes basically as you can see we are still working with the open\n35:12 AI llm but we are also not providing any of the custom knowledge right so we are\n35:18 still relying on open Ai and the information that they have gathered but\n35:23 with langchain you can also provide your own knowledge or knowledge base on which you can ask llm to do certain actions so\n35:32 think of a PDF file or even URLs that you can script or maybe you have a large\n35:40 PDF file with a lot of text and maybe you want to run an llm AI chat bot for\n35:46 your own document so you can do that with the help of language in the next project that I want to showcase you will\n35:52 exactly do that will take a long YouTube video so think of a podcast which is\n35:58 hours long or a long YouTube video right so what I have here is\n36:04 the Microsoft CEO certain dealer full interview on recode but it''s 51 minutes\n36:09 long and what I want to do with Lang chain is the ability to ask questions to\n36:16 this video so the context that the llm would have is strictly of that video and\n36:22 I''ll be using few libraries like YouTube transcript which basically converts\n36:28 whatever URL we provide for a YouTube video and gets its transcript right so\n36:33 let''s build this YouTube assistant now I''m going to show you how you can create this assistant that can answer questions\n36:40 about a specific YouTube video so coming back to the concept of indexes I touched\n36:47 briefly on it but we also saw it in the Lang chain diagram but we know that these large language models become\n36:54 really powerful when you combine them with your own data and your own data in this scenario will be the YouTube\n37:00 transcript that we are going to download automatically but you can basically replace that transcript with any\n37:07 information in this approach so it could be a PDF it could be blog post URL right\n37:13 so what Langton offers is document loaders and I can quickly show you the YouTube\n37:21 transcript one so this is the YouTube transcript and basically it allows you\n37:26 to get the transcript which will be the text version of the YouTube video right\n37:31 but there are several other document loaders that you can see on the left hand side right so you can bring in an\n37:38 S3 file you could bring an Azure blob storage file you could do Hacker News\n37:44 posts or articles right so these are some of the document loaders that are\n37:50 supported by linkchin as of now and we''ll be using text Splitters and Vector\n37:55 stores so we are going to use these three components to load our YouTube video transcript split it into smaller\n38:02 chunks and then store it as Vector stores so you can think of these as little helper tools that will make it\n38:09 easy for us to load the transcript which might be thousands of lines of text so\n38:16 to get us started what I have already done is created a YouTube assistant\n38:21 directory so not be using the pets generator directory that we had and what\n38:26 I have done is pretty similar to the pet''s name generator right so I have main.pi which will hold our streamlit\n38:33 interface and then the langchin helper will have the length chain components\n38:39 and I''ve also created a virtual environment and installed all the\n38:44 necessary packages which is link chain openai YouTube transcript also I''ve I\n38:50 went ahead and created dot EnV file which holds my openai API key so pretty\n38:56 similar to the pet''s name generator and now we can start with the lag chain\n39:01 helper first so the first thing that we are going to import is the YouTube loader that we saw right which is a\n39:09 document loader so from langtin dot document loaders we are importing that YouTube loader and the second important\n39:15 thing we need is the text splitter so as I showcased that the video that I have\n39:21 is 51 minutes long you could also pick up a podcast like Lex and they have\n39:29 podcasts that are three hours long and which means you''ll have thousands of\n39:35 lines and that is where we''ll use the the text splitter to break down those\n39:41 huge transcripts into smaller chunks and I''ll show you how and for the rest of the inputs we are gonna input the lag\n39:49 chain components like the llm which will be open AI prompt template and llm chain\n39:54 the other thing coming back to indexes we''ll be using Vector stores so I''ll be using the phase\n40:02 library and I''ll quickly show you what the face library is phase is a library by meta or\n40:11 Facebook for efficient similarity search and you might have heard of other Vector\n40:17 stores or databases like Pinecone or vv8 right but I''ll be using phase for this\n40:25 project so let''s start with writing some code so I''ve done all the necessary\n40:30 inputs here the only input that''s left is the dot EnV which will load our\n40:37 environment variables and I''ll initiate dot EnV here also since I''ll be using\n40:44 openai embeddings so we''ll initiate that to here and I forgot to import those so\n40:51 I''ll import the open aim bearings and now we can create our first function to\n40:57 create a function we know that in Python it''s deaf and let''s name this function\n41:02 that will be be creating a vector DB\n41:08 create Vector DB from YouTube so that''s a pretty big function name right but I\n41:14 want to specify what we are doing and we''ll be using phase here also for the\n41:20 parameter let''s give this a required parameter which is the video URL right\n41:25 so we''ll be pasting this video URL in our streamlit interface and that''s what\n41:30 we''ll be using and this will be a string right so the first thing we want to do\n41:35 is load the YouTube video from the URL right so we''ll use loader which we\n41:42 imported on the top so YouTube loader Dot from YouTube URL and we''ll pass the\n41:50 video URL parameter here after we have loaded the YouTube video I want to save\n41:55 this into the transcript variable so we''ll create transcript here and we''ll\n42:00 just do loader dot load and this should give us the transcript now we''ll be\n42:07 using text splitter and I''ll specifically tell you why so text\n42:12 splitter and we imported it here as recursive character text splitter you\n42:18 can specify few parameters when using this so the first one is chunk size\n42:24 which will set to 1000 and chunk overlap so chunk size is how much each chunk\n42:31 will contain so for me it will be 1000 right and then overlap is once it has\n42:37 created those individual docs from the long transcript it''ll have an overlap in\n42:43 every document so document one the last hundred words would also be included in\n42:48 the document twos first hundred words right so that is what overlap is and now we''ll save them into a docs\n42:56 variable so text underscore splitter not split documents as the function and\n43:03 we''ll provide the transcript that we had loaded from the YouTube url there we go\n43:08 okay now let''s also initiate the phase so\n43:14 phase Dot from documents and we will be using docs which we stored here right\n43:22 docs and we''ll be using the open Ai embeddings and we''ll return this DB\n43:29 okay so now on to the explanation why we have to split the text so basically what we\n43:37 are doing at the text splitter is we have taken over thousands of lines and\n43:43 split up the documents so it has taken very large transcript over and split it\n43:49 up into chunks of 1000 so that is the first step now you might wonder right so we can''t just provide thousand lines to\n43:57 the open AI API remember there is a token size or a limit on how much\n44:04 information you can send to open AIS API and that is why we have split the amount\n44:10 of context we''ll be sending for for a YouTube transcript right because the model that I''ll be using is the text\n44:18 DaVinci 003 and as you can see it can only take\n44:24 4097 tokens so I cannot send the entire transcript to open aiz Ai and that is\n44:32 why we''ll be splitting it and storing it into Vector stores again this is quite technical I''ll not\n44:40 go into much detail but vectors basically are a numerical representation\n44:45 of the text we just created here right so the core responsibility of this\n44:52 function is to load the transcript right take all the text that''s in the\n44:57 transcript split it into smaller chunks and then save those chunks as Vector\n45:03 stores again we can''t just provide all of these Vector stores to the open AI\n45:08 right we can''t just send over the 10 000 or maybe even 50 chunks that we have\n45:15 created of smaller text that''s where we''ll use phase to do a similarity\n45:20 search right and that''s what the next function will be and before I write that\n45:25 next function we''ll see if this works so video underscore URL so I''m gonna hard\n45:32 code the video URL that we have for the podcast and see if we get the smaller\n45:39 chunk documents right so let''s print this function at the end\n45:47 hit save and we''ll open the terminal make sure your virtual environment is\n45:53 activated and you have installed the required packages again all of this will\n45:58 be available on GitHub for reference later but let''s run the line chain helper python file again it''ll take some\n46:06 time to do the computation I missed to write print so we''ll have to\n46:12 print this whatever this function returns which should be the database that we created right so let''s run it\n46:20 again and this time we should get the vector stores that were created and so\n46:25 instead of DB if I return docs you''ll see those chunks\n46:32 so if I expand my terminal here you can see we have quite a few text here but\n46:39 here are the docs right so you can see that there''s a document and then it\n46:46 starts with the content and you''ll see multiple document chunks so these are\n46:51 the chunks that we created from the larger transcript so this is one right this is the second one\n46:58 and so on I know the formatting is weird so you can''t really tell where the new\n47:04 document starts but yeah this is all the chunks that we have awesome so our\n47:09 function to create the vector DB from YouTube url is working as expected so\n47:14 I''ll get rid of this print statement and full return DB here now for the next\n47:19 function which is going to be getting off the response on our query we have to\n47:26 ask this YouTube video right so let''s create that function we''ll name it get\n47:32 underscore response from query again pretty self-explanatory name for the\n47:39 function itself and we''ll pass few parameters to this function one is DB the important one will be query which\n47:47 will be the question that the user asks and K which is another argument that\n47:52 I''ll go over this is used for the similarity search that will do so keep in mind the amount of tokens that the\n48:00 text DaVinci 3 Model can take right so keep that in mind it''s 4097 so I''ll just\n48:06 add a comment here saying text DaVinci can handle 2097 tokens right now in\n48:15 order to do a similarity search we''ll save that into a docs variable within this function so DB\n48:22 is what we''ll use we''ll perform a similarity search on the DB which is the database we created in the previous\n48:29 function so gb.similarity underscore search and the search will be basically\n48:35 the query so the first thing I want to do with this function is basically search the query relevant documents so\n48:42 let''s say in this podcast they talk about a ransomware somewhere so right\n48:48 here they talk about ransomware right and if I want to ask a question saying what did they talk about ransomware so\n48:54 my query is just about ransomware that that they talked about in the podcast so\n49:00 it will only search the document that has details about ransomware so we''ll\n49:06 not send the entire documents that were created but just the one that is\n49:12 relevant to the query that the user made I hope that makes sense and this is also\n49:17 where we''ll pass the K as argument and I''ll tell you what K is so remember that\n49:25 we can have 4097 tokens but our chunk size is 1000.\n49:31 so that means we can kind of send four documents right because each document is\n49:39 a size of thousand so let''s set that value to four okay so we''ll be sending\n49:44 four relevant docs based on the query that the user made now I''ll create another variable called docs page\n49:51 content and what we''ll basically do is join those four docs that we''ll be sending\n50:03 okay so we got those four docs and we are joining them to create one dock\n50:08 because the Toca limit is 4097 and here we''ll almost have 4 000 tokens being\n50:15 sent to the text DaVinci 3 mod awesome now let''s work with the llm right so\n50:23 pretty similar to what we did with the pet''s name generator we''ll initiate the llm to be open Ai and\n50:31 as I said the model that I''ll be using is text DaVinci 3 so let me go to the\n50:37 open AIS documentation copy this model name come back here and paste it and\n50:42 there is some white space at the end so we''ll get rid of that and the second thing we did with the pets name\n50:48 generator was prompt right so prompt templates is the is another main\n50:53 component of Lang chain so we''ll use that and this is variable define what the prompt should be for the open AI llm\n51:02 so the first thing would be to specify the input variables right so the first\n51:08 one is question or query right so whatever the question is being asked by\n51:14 the user in Docs so docs is basically the similarity\n51:19 search we did there we go now the template that we''ll be using is a prompt\n51:25 that I''ve created here so I''m gonna copy this really quick since it''s a long prompt okay so I''ve copied the prompt\n51:32 basically it says you''re a helpful YouTube assistant that can answer questions about videos based on the\n51:39 videos transcript right answer the following question and this is where the input variable goes\n51:46 whatever the question the user is asking by searching the following video transcript which is the docs right so\n51:53 docs is basically the similarity search we did only use factual information from the\n51:58 transcript to answer the question if you feel like you don''t have enough information simply say I don''t know\n52:04 right because we don''t want the AI or the llm to hallucinate your answer should be needed so that is basically\n52:11 the prompt that we''ll be using to answer questions and now we''ll be using another\n52:17 main component which is chain within the Lang chain so let''s create an llm chain\n52:23 where llm is equal to llm because we specified it here that will be using\n52:29 openai model text DaVinci 3 and prompt is equal to prompt which we specified\n52:36 here using prompt template okay now we just have to learn the response so\n52:43 I''ll create a variable call response it will do chain dot run which will basically run our chain since we had\n52:49 question as the input variable here we''ll say that question is equal to\n52:56 query because that''s what we were referring to it on the previous function and\n53:03 Docs is equal to docs page underscore content\n53:09 remember because we joined all the four documents because K is set to 4 to\n53:17 be one doc because we can we have the ability to send four thousand tokens and\n53:22 then response is equal to response dot replace and this is just some\n53:29 formatting that we have to do because if you remember in the pet''s name generated to the response we were\n53:35 getting was in one line and it included new line characters so we''ll replace that with\n53:42 some white space and we''ll return response okay so now we can test this\n53:50 out as it is in the console by hard coding the question and the URL which we\n53:56 already did so let''s get ready for that but also build the interface because it''ll be really quick with streamlit so\n54:03 coming over to our main dot Pi let''s do some inputs on the top so pretty similar\n54:08 to what we did in our pets named generator so streamlit I''m importing it as St and the langchain helper where are\n54:17 all of the Lang chain code is and I''m also importing text wrap basically it\n54:22 gives you the ability to wrap text so that you''re not you don''t have to scroll the page the title of this page will be\n54:31 YouTube assistant right so YouTube assistant and now on the sidebar we can have those\n54:39 parameters that we need from the user with sidebar I want to create a farm so\n54:45 we have a submit button at the end so SD dot form is how you do that\n54:52 and you also have to specify a key so key is my form again this is all\n54:59 streamlit stuff and let me know in the comments if I should create a course on streamlit on how to build you know cool\n55:05 python interfaces I love this tool because I don''t have to care about building a front end and the first\n55:12 parameter we had in our length chain helper was the YouTube url right so\n55:18 we''ll save that as video URL so YouTube url is equal to SD dot sidebar text\n55:27 as we used in the pet''s name and we''ll just say that the label is what is the\n55:34 YouTube video URL and we''ll give a Max to maximum character limit of 50 because I don''t think a video URL can exceed 50\n55:42 characters uh the other parameter we had was the question that the user can ask and we''ll save it as query here so St\n55:49 dot sidebar dot text underscore area right and then the label will be asked\n55:57 me about the video so again you can have a limit here right so maybe you can only\n56:05 ask questions that are not long enough so we''ll set max characters 50 here too\n56:11 and also set the key to query here okay and at the last since I created this as\n56:18 a form we''ll give it a submit button and the label here will be submit now\n56:25 so if Kiri which is the question the user can ask and YouTube url exist right\n56:32 what I want to do is basically run this function to give us the answer right so\n56:40 we''ll be as we are already importing the link chain helper on the top as LS lch\n56:46 so that''s what we''ll be using here so DB which is the database will be equal to\n56:53 so remember we have to pass the video URL uh to the create Vector DB function\n56:59 to create a new Vector database based on the transcript that we got so DB is\n57:05 equal to lch which helps which is basically that we are accessing this python file and then the create Vector\n57:12 DB from YouTube url function and we''ll pass the YouTube url as the parameter because remember we\n57:19 just need the video URL here response comma docs is equal to and now we''ll get\n57:28 a response which we can do by running this function which is get response from\n57:33 query and remember the parameters that will be passing so lch dot getresponse\n57:40 from query the first one is DB which we just created right and query is the\n57:46 question that I will be asking so um right here whatever the user asks will be the query\n57:53 so I am missing a comma here as I''m going through my code so I''ll add that and now we''ll save that response in our\n58:02 interface with streamlit so let me create a sub header here which will say answer right and below that we''ll have\n58:10 St dot text and we''ll wrap that text\n58:15 and this is where the text wrap library is being used you''ll you''ll see this in the interface once I run it so text wrap\n58:22 dot fill and whatever the response we get from the length chain function\n58:30 you can also set the width of this text area to be 80 let''s go with 80 and see\n58:36 how that looks and that is basically it so two parameters for necessary one is the\n58:42 YouTube video URL and the question that the user asks right and we are passing so if the both of those parameters exist\n58:49 first we are creating the database from the YouTube video URL and then we''re getting the response based on the\n58:56 question that the user asked using the llm so now we can run our streamlit app\n59:02 after saving the file so if I scroll down to the bottom here for my terminal\n59:07 expand this and run stream lit run main.pi\n59:13 hit enter it should load our web interface for our\n59:18 streamlit app awesome on the left hand side you can see we need to provide a YouTube video URL so\n59:26 I''ll just go ahead copy this interview video URL paste it here\n59:32 ask me about the video so let''s say what did they talk about rent somewhere is\n59:42 what I want to know and hit submit okay so we have got some errors saying input variables let''s go\n59:51 to our terminal and see if we have any logging okay so I found the error I was just\n59:59 missing S I thought I typed it right so instead of input variable it needs to\n1:00:05 be input variables and we''ll Ctrl C to stop our streamlit app and do streamlit\n1:00:13 run run main.pi again so after adding the S hit enter and now\n1:00:20 we need the same exact information so copy this\n1:00:25 and copy the question so the YouTube video URL and what did they talk about\n1:00:31 ransomware hit submit there we go we got our answer so it says\n1:00:39 they discussed how ransomware is difficult to track due to zero day exploits and how Microsoft is making it\n1:00:45 a mission to help with secure cloud backup for Enterprises better tracking of zero day exploits and helping with\n1:00:52 enforcements they also discussed the importance of public-private Partnerships in order to prioritize\n1:00:57 cyber security and create new standards such as those for nist so\n1:01:02 remember our prompt I asked it to be as detailed as possible also say I don''t know if it doesn''t know\n1:01:09 what the answer is based on the transcript we provided and not to hallucinate so I think this is a pretty\n1:01:15 good answer um that we got out of this 52 minute\n1:01:20 video again you can pick a longer video and ask about anything specifically\n1:01:27 longer from podcasts right maybe the video is four to five hour Longs and you\n1:01:32 need to know a specific detail I think that''s where this tool or the app we\n1:01:38 build can be really handy right but yeah so we learned a lot about\n1:01:44 Lang chain today specifically the main three main components which is llm so\n1:01:52 any of the large language models that you can use like open AI or hugging face prompt templates right\n1:01:59 and chains so how you can combine these components into chains to perform the\n1:02:05 required task and agents right remember in the pets generator we talked a little\n1:02:11 bit about agents and how they have reasoning behind the\n1:02:16 tasks that they perform because we try to calculate average age of a dog and\n1:02:22 also multiply it by three so it used Wikipedia and llm math to get those\n1:02:27 answers but also we learned a bit about indexing and Vector stores so how you\n1:02:33 can split large documents into smaller chunks and store it as Vector which is basically you know\n1:02:39 numerical representation of the documents that we created and then\n1:02:46 passing those on to the llm since there are certain limits of how much context\n1:02:52 you can send to the API but yeah one other thing I would like to mention is\n1:02:58 if you are planning to make these apps public remember we were storing our\n1:03:04 environment variables in dot EnV file and you might be wondering every ship I\n1:03:10 also created an openai API key like how much all of this is going to cost so\n1:03:16 I''ll go into my dashboard in into billing to see how much did it cost me\n1:03:21 to you know basically kind of build this course out so you can see um\n1:03:28 10 cents and 30 cents so very close to\n1:03:33 less than a dollar like half of a Dollar close to 50 cents is what it costed me\n1:03:39 to make all of these queries to the openai llm the thing I was gonna\n1:03:45 recommend if you want to publish this app so that the public can use it is to have a field here uh you know with\n1:03:54 the sidebar saying open AI API key so that the users have to submit their\n1:04:00 openai API key with their app so you can have a text field here saying hey what\n1:04:06 is your open AI API key just so that you know you are not being charged and you can make that as a secret field so that\n1:04:14 the key is not displayed in the interface but you can use that key to make these queries you will just have to\n1:04:21 pass it in the Lang chain helper so whatever the variable name you decide\n1:04:27 maybe like open AI API key which you''ll get the value from our streamlit\n1:04:34 interface you can pass that right here when you initiate the large language\n1:04:40 model so you''ll specify openai API key as a\n1:04:45 parameter here and the value of that key which will be the variable you decide so\n1:04:51 yeah that''s pretty much it for this course again we learned quite a bit about the langchin framework\n1:04:58 specifically in Python uh you know the models prompts indexes chains and agents\n1:05:03 or the five main Concepts within nag chain that I wanted to cover again I\n1:05:09 hope this helps you understand the framework itself and how you can utilize this information to build something\n1:05:16 really cool with the power of llms but if you would like to see a streamlit\n1:05:21 course again let me know in the comments but I hope you find this course helpful I''ll see you in the next one peace',
  '{"channel": "freeCodeCamp.org", "video_id": "lG7Uxts9SXs", "duration": "1:05:29", "level": "BEGINNER", "application": "LangChain", "topics": ["LangChain", "LLM", "Prompt Templates", "Chains", "Agents", "Vector Stores", "Streamlit", "OpenAI API", "Pets Name Generator", "YouTube Assistant"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=yF9kGESAi3M',
  1,
  'LangChain Master Class For Beginners 2024 [+20 Examples, LangChain V0.2]',
  '0:00 hey guys welcome to this Lang chain master class for beginners in this video you''re going to learn everything you\n0:06 need to know about Lang chain so that you can start your own AI development...',
  '0:00 hey guys welcome to this Lang chain master class for beginners in this video you''re going to learn everything you\n0:06 need to know about Lang chain so that you can start your own AI development journey and by the end of this master\n0:12 class you''re going to learn everything that you need to know about Lang chain so that you can go off and create your own rag chat Bots create your own agents\n0:19 and tools and use Lang chain to automate task and even though there''s a ton of information in this tutorial I''ve done\n0:26 my best to make it as beginner friendly as possible you''ll see as we go through this tutorial that I''ve structured\n0:31 everything to start out with the absolute Basics so that we can build up a strong foundation and from there we''re\n0:37 going to add in more advanced features and complexities so that you can see what Lane chain is fully capable of and\n0:43 because I want you to get started building your own lane chain projects as fast as possible I''ve actually included over 20 different examples in this video\n0:50 so that you can just copy my code and start using it in your own projects and to make things even easier I have a link\n0:57 down the description below where you can download all the source code in this video completely for free and it would\n1:02 mean a lot to me if you could hit that like And subscribe button while you''re down there too also if you get stuck at all during this tutorial you''re in luck\n1:09 because I created a free school Community for AI developers just like you in the community you can go over there and ask questions get support join\n1:15 our weekly free coaching calls and we have over, 1500 different active members in the community so it''s a great place\n1:22 for you to meet like-minded AI developers on your development Journey so I definitely recommend checking out it''s completely for free and I have a\n1:29 link down the description below below so come over and join the party but enough of that let''s go ahead and dive into the rest of the\n1:35 video all right so let''s go ahead and cover the outline for this tutorial so that you can understand what you''re\n1:40 getting into and so that you can get the most out of this tutorial so to start off the first thing we''re going to do is\n1:45 set up the environment on your local computer so that you can run the over 20 different Lang chain examples that I''ve\n1:51 built for you guys from there we''re going to start diving deep into each of the core components of Lang chain so to\n1:56 start off we''re going to start working with chat models and this is basically how we''re going to start interfacing with you know open AI chat gbt we''re\n2:03 going to start working with CLA that''s all going to happen in the chat model section and what we''re going to do from there is we''re going to start working\n2:08 with prompt templates next and this is how we''re going to be able to format the inputs that we pass over to our chat\n2:14 models and this is really just helping us build up a good strong foundation so that eventually when we start trying to\n2:19 automate task using our chains which is my favorite part of this whole tutorial we''re going to be able to really\n2:25 understand how we can start you know automating task by putting together chat models or prompts and other tasks and\n2:31 really run them all together to automate your workflows from there we''re going to start working with rag now this is a\n2:36 huge section of this entire tutorial as you can see we have a ton of different examples in here and if you''ve heard of\n2:42 people chatting with their PDFs or documents this is what they were doing rag retrieval augmented generation so\n2:47 we''re going to do a huge Deep dive into this one and then finally what we''re going to do to wrap up this tutorial is we''re going to do a deep dive into\n2:54 agents and tools and as you can see we''re going to start off with the basics and then we''re going to do a deep dive into agents which are are basically just\n3:00 you know chat models however they can make decisions on their own and act it''s super cool how it works and then we''re\n3:05 going to do a deep dive into tools and tools are how we''re going to supercharge our agents to provide them more\n3:11 capabilities so as you can see we have a ton of information in this tutorial so let''s go ahead and start diving into the\n3:17 first section which is going to be setting up your local environment so that you can run all the different examples inside of this code base oh\n3:23 real quick I want to mention if you want to get the most out of this video to learn linkchain as quickly as possible\n3:28 here''s my recommendation first I recommend watching the video the whole way through on two times speed just so\n3:33 that you can understand all the highlevel Core Concepts of blank chain and then I recommend coming back through\n3:40 this video a second time and just skip to the part that you want to learn about for whatever project you''re building for\n3:45 example if you''re learning about rag I would definitely recommend just skipping down through the time sense below to\n3:50 watch rewatch the rag section and do a deep dive into the exact part that you want to learn about this is how I go\n3:56 about learning new Concepts so I just want to throw it out to you guys too so you can speed up your development journey and start gilding projects so\n4:02 all right enough of that let''s actually go in and start setting up your environment all right guys so it''s time\n4:08 for us to start working on setting up our local environment so that we can run all the different examples inside of\n4:13 this project now inside of the read me I have outlined all the different steps that you need to take in order to start\n4:19 running these and it''s actually super straightforward so let me just quickly walk you through it to start off we need to install Python and we need to install\n4:25 poetry python I''m sure you know what that is poetry basically if you haven''t heard of it before or it''s a dependency\n4:30 management tool for python so as you can see we have something called a p project. tommo and this includes all of\n4:37 the different dependencies that we need to install in order to run all the different examples that I''ve set up for\n4:43 you and poetry makes this super simple to do so what you''ll do go over to click this link right here and it will walk\n4:50 you through all the installation steps to install poetry on your local computer and once You'' have installed poetry\n4:55 you''ll be able to run commands like this so poetry to confirm that it''s working and you can see yep poetry is working\n5:01 it''s giving me back information and then from there what we can do is once you''re inside of your code base and remember\n5:07 all the source code for this project is completely for free just click the link down the description below and you can download it but once you''ve done that\n5:13 what you can do is type in poetry install D- noout and what it''ll do is\n5:18 it''ll go through and install all the different dependencies that we saw back over in our py project. Tomo file over\n5:26 here it''ll install it and what''s awesome is then we can eventually start to run commands like this poetry shell and what\n5:33 this will do is it''ll actually spin an interactive shell up that we can see right here it actually has the name of\n5:39 basically our project that we''re working on you can see right here it has our name of our project that we''re working on and we''ve created and we can actually\n5:45 start you know running Python and actually start calling all the different example code bases that we have all the\n5:51 different projects so you can see we can do something like python one. chat models and then we can start running you\n5:56 know all of our different code examples so if you got this far you are good to go go for the rest of the course when it comes to python now we just have a few\n6:03 more cleanup things that we need to do first off uh coming back to our read me is I mentioned that you need to update\n6:10 your environment variables so uh when you download the source code you will only see a EnV example file and this is\n6:18 where you''re going to instore your environment variables eventually what we''re going to do is you are going to\n6:24 rename this file to justv and what this will do is is it''ll\n6:29 become your environment variable folder so that whenever you go to run your you know start using open Ai and some of\n6:36 your other different projects they require an open AI key or a Google key or you know all these keys and that''s\n6:42 going to get stored to your environment variables I''ve already have mine set up and we''ll be walking through it later but that''s the second thing you need to\n6:48 do and actually in addition to that go over to open AI Google fir craw and\n6:53 actually start adding in those open ad keys but we''ll talk more about that later on okay all right enough of talking about environment if you''ve\n7:00 gotten this far everything should be working and we can now move on to actually start playing with the code and Diving deep into L Jing so we''re going\n7:06 to go over next and start working with chat models hey guys I meant to show you this quick tip that''s going to make coding inside a visual studio code and\n7:13 cursor with python so much easier and solve a big headache that you''ll probably have so if you head over to one\n7:18 of your python files you''ll notice that you have a bunch of squigglies inside of your project and the reason why is cuz\n7:25 Visual Studio code is not properly hooked up to the new python environment that you just created with all the\n7:31 proper dependencies so here''s how we fix that first you''re just going to open up a terminal down here and what you''ll do\n7:37 is you''ll type in poetry shell like we did earlier and this will actually access and you can see yep we''re in the\n7:42 right shell but the important part is is it gives us the location of where this basically all of our dependencies were\n7:49 installed so here''s where the magic happens you''ll come down here and click that you know interpreter path and\n7:54 you''ll say enter interpreter path and then you''ll just paste in the path you just copied\n7:59 and whenever you do that it''ll actually get rid of all the squigglies because now Visual Studio code or cursor is\n8:05 hooked up to your development environment with python so now every time you add a new dependency with poetry ad or whatever you do it will\n8:12 actually you know sync up and you won''t have all those random squiggly marks even though you have the Right Packages installed and just as a final note if\n8:18 you ever like well I want to not use this poetry shell environment anymore all you have to do is just type in the word exit and it''ll put you back to your\n8:25 base environment so yeah that''s a quick crash course on poetry and setting up visual studio code and after that let''s dive back into the\n8:32 video all right so it''s time for us to dive into our first core component of Lang chain which is going to be chat\n8:39 models now what the heck are chat models and what do they do well a chat model is\n8:44 basically Lang Chain''s way of making it super easy for us developers to talk to all the different large language models\n8:51 out there like chat BT claw Gemini and a bunch more they abstract away all the complexity and allow us to basically\n8:58 have conversation ations with these models hints chat and chat models it''s all about conversations so what is super\n9:05 nice about Lang chain if we go over to their documentation you can see they have a list of all the models that they\n9:12 currently support now it is very important to mention that for this whole tutorial we''re going to be using version\n9:18 0.2 of Lang chain this is the most upto-date one and a lot of the features and version 0.1 will soon be deprecated\n9:26 uh whenever they upgrade you know probably the next you know five six months from now but enough of that let''s dive back into chat models and talk\n9:32 about how we can use them so as you can see looking at chat models here''s a huge list of all the different models that we\n9:39 have access to some of these models are better at other things and what''s very interesting is chat models inside of\n9:45 Lang chain they provide different functionality such as tool calling outputs you know do they support\n9:51 outputting content in Json are they multimodal such as accepting images and audio so that''s what you can see at a\n9:58 high level going over here and what''s nice is if you want to work with any of these different models you can just\n10:03 click uh you know use poetry ad and you can type in the name of the package and you''ll add it to the environment that\n10:09 you just set up a few seconds ago so let''s go ahead and do a deeper dive and look at chat open AI because that''s the\n10:15 one we''re going to be using mostly inside of this tutorial so if we go over here to chat open aai you can do a\n10:21 deeper dive and look at some of the examples that they already have set up for you so you can see once again they\n10:26 recap what it''s capable of doing and then they walk walk you through Yep this is how you can start setting up your\n10:33 files to start using this new package to start chatting with open Ai and eventually down here they dive into\n10:39 showing you y this is how you can start actually using it updating the models and so forth but of there documentation\n10:45 let''s go ahead and actually head over to the example that I created for you guys where we''re going to start at the absolute Basics and work our way up so\n10:51 you can see what these chat models are capable of so let''s come over here start looking at the code and uh take it from\n10:57 here and before we dive into the code on this specific file I just want to give you a\n11:03 quick overview of what you can expect from each of these files so what I''ll do in each one of the examples that we''re\n11:08 going to run through I will try to provide documentation for you guys up top so you on your own can do a deeper\n11:14 dive into whatever concept we''re just learning for example we just talked about chat models and I showed you a\n11:19 link and I also did open AI chat models and we walked through another link so in all the files wherever I point out\n11:24 something you''ll be able to go ahead and click those links and a deeper dive on your own if you ever want to learn more about those topics but enough of that\n11:30 let''s actually go ahead and start talking about chat models on a basic\n11:35 level all right guys so let''s walk through the Three core steps that we need to take to start interacting with\n11:41 our chat models in this case our open AI chat model at a super basic level so the first thing that we''re going to do is\n11:47 load our environment variables and if you remember from the beginning we set up aemv file which stored all of our\n11:53 keys to all of the different platforms we were going to access in this case we''re trying to work with open AI so it''s important that we have an open AI\n12:00 key feel free if you haven''t set that up just head over to the open a website go create an account and you''ll actually be\n12:06 able to access your open AI key and bring it back and copy and paste it over here now once you have done that what''s\n12:12 nice is this load. EnV is going to add all those environment variables so that we can start accessing them in this file\n12:19 and you might be wondering like Brandon what the heck I don''t see us accessing the open AI key anywhere well if we\n12:25 actually Peak under the hood inside of the chat model which actually gets imported from linkchain open Ai and\n12:31 that''s actually if you head back over to those packages that we had set up earlier that''s where it was stored but if you actually hit command on your\n12:37 keyboard if you''re on Mac or control if you''re on Windows and actually click on it you can actually start looking at the\n12:43 source code under the hood and you can see hey in order to start using this chat model you need to have this API key\n12:50 and we''re actually going to start using it if you scroll down a little bit you can actually see under the hood it''s\n12:56 actually automatically grabbing this environment key here and unless you pass it in manually and just to like give you\n13:01 guys a full tool you could actually manually pass in your API key here it''s just not the safest manner because if\n13:07 you accidentally save your open a key to the public other people could actually grab your API key from GitHub and it''s\n13:14 not the most secure so that''s why you''re going to store everything in your environment variables file okay enough of that let''s keep chugging along to\n13:20 actually you know walk all the way through this example the next thing that we''re going to do is once we have created our chat model which in this\n13:26 case we''re going to say we''re using jet gbt 4.0 but we easily could have done something like chat gbt 4 we could have\n13:32 done something like chat gbt for uh you know 3.5 turbo we could have easily changed things up but once we''ve created\n13:38 that model we can actually St uh start now interacting with it and using it and the key lesson here is we''re going to\n13:45 interact with our models in pretty much everything in Lane chain using the do invoke property this is a function that\n13:52 really just whatever we''re using it triggers its core functionality so in this case we''re working with chat models so it''s going to go trigger off like hey\n13:59 open AI or Claud start processing the request I give you but when we''re using chains or rag or agents later on\n14:07 everything uses invoke so that''s a very key thing to keep in mind as you''re working with Lang chain all right so\n14:12 let''s just keep walking through what''s going on well in our case we''re telling our model hey go perform this query for\n14:18 me and we''re going to get back a result so let''s actually dive into these results so we can see what''s happening under the hood and the way we''re going\n14:23 to do that is we''re going to open up our terminal and let''s close it out and give you guys some more space but once again\n14:29 we''re going to use poetry shell and this is going to open up that interactive shell that we created earlier which uses\n14:35 you know our new Lang chain crash course environment what we can do is we can actually start calling this function so\n14:41 we''re just going to call it Python and this is our chat model so one Tab and it''ll go ahead and autocomplete for me\n14:47 and this is the first example so I''ll hit one and tab again and it''ll autocomplete now I can start running it\n14:52 so let''s actually go ahead and press enter and start looking what''s happening under the hood so here''s what''s actually\n14:59 super interesting so you can see we had two print statements one was for the full result and one was content only so\n15:04 for full results you can see that open AI under the hood gives us back a ton of information they give us back the\n15:11 content which is the exact answer we wanted 81 divid by 9 is 9 but then they give us this metadata which is like how\n15:17 many tokens did we use which you know basically why did we finish there''s a ton of information which run number was\n15:23 this basically there''s a ton of information that they give back to us 99% time you don''t care about it however\n15:29 I just want to show you that it is accessible if you ever need to use it so most of the time when you''re using you know these chat models the main thing\n15:36 that you want to grab is the content because the content is the example so whenever we grab the result and we\n15:42 access the content property we''ll get back the exact string that we usually want to you know show to our users or\n15:48 pull out and pass over to the next prompt in our you know our chat model so that''s under the hood how our first\n15:55 basic chat models are going to work so what I would like to do next is we''re going to go ahead and move over to the\n16:00 next example where we''re going to start actually showing you guys how to do a basic conversation using our chat models\n16:06 to where we can actually like you know pass in more information and actually having a full-on conversation let''s go and start working on this example\n16:13 now all right guys so welcome to the second example where we''re going to start focusing more on creating a\n16:18 conversation with our chat models now the three new important Concepts to know when going into this example are there\n16:25 are three different types of messages in our case that''s going to be a system message and a system message just sets\n16:31 like the broad context for the conversation so these types of messages are usually something like hey you are a\n16:37 professional accountant or hey you are a professional python software engineer help me write this code so that''s like\n16:43 just the broad what''s going on inside the conversation just the context and then from there there''s two different\n16:48 types of messages there are human messages which is us talking to the AI and then there''s AI messages which are\n16:54 the AI responding back to us so those are the three types of messages that you can have in a conversation with the AI\n17:01 okay cool now let''s dive into the rest of this code so you can kind of see what''s happening and this is just once again we''re building on our foundation\n17:07 so we''re just copying a lot of the code from the previous example and now we''re going to start building on top in this case we''re going to start creating a\n17:14 conversation so in our case you can see a conversation is nothing more than we have our messages list and our list is\n17:21 just going to store a combination and series of messages in our case we''re just GNA um start off with a system\n17:27 message and it''s important system messages best practice and I think it''s actually enforced a system message must\n17:32 come first and then from there you can alternate human AI human AI but system always comes first because remember this\n17:39 is the context for the conversation as a whole so in our case we''re going to say hey solve the math problem and then from\n17:45 there we can pass in a human message and what we would expect to get back is obviously an AI message so let''s\n17:51 actually kind of see how we can actually trigger this conversation so we can get a result so in this case you''ll see that\n17:57 we have a once again again we have our model our chat gbt model and we''re going to call that important function that we\n18:02 talked about in the last one which is invoke but this time we''re not going to pass in a you know a hardcoded string\n18:09 like we did over here where you can see we passed in a hardcoded string this time we''re actually going to pass in our entire message history and it''ll\n18:16 actually read through the entire conversation and then spit out a result if you worked with cat GPT it''s exactly like that if you''ve ever typed into like\n18:22 you know the chbt website it''s just like that okay cool so what we''re going to do is we''re going to go ahead I''m going to comment this out we''re going to run the\n18:29 code so we can actually see what answer we get so this time we''re going to do python this is you know still working\n18:34 with chat models and this is example two so you can actually see whenever we get an example back with working with chbt\n18:40 it says you know 81 divided by 9 is 9 so you know that''s exactly what we would expect to see okay cool but what''s nice\n18:46 is we can continue this example and actually have a full-blown conversation so I''ll show you what that looks like\n18:52 now so you can see in this second part we actually have continued on the conversation by adding in AI responses\n18:58 and then messages so this case we''ve added now like the response from the previous one and now we''re going to you\n19:04 know just continue adding human messages so I''m just going to run this so you can actually see what''s going to happen and this time it''s going to come back and\n19:10 give us you know you know 10times 5 so this is completely basic however here''s why chat conversations are super\n19:16 important in the real world when you''re building your conversations you know it''s very common for you to provide an\n19:22 example like hey chat gbt or AI model build me an email it returns a response\n19:28 and then you provide provide feedback cuz remember this is all about conversations and what''s going to happen is as you provide those you know\n19:34 feedback of like hey no make it less formal make it a bulleted list you know as you provide that feedback what''s nice\n19:39 is going forward in your conversation you can say okay great now do exactly what you did for that last email but now\n19:45 do it for this email and you know storing this message history like you have right here is how you''re going to\n19:50 be able to basically make your chat models have awareness in context of what''s good and what''s bad and what''s wrong so this is a very powerful tool\n19:57 and you actually use this a lot more whenever you know you''re working on bigger and larger projects but okay cool\n20:03 well now that we have that under the hood and we actually understand like just like basic conversations let''s keep it going and actually start working on\n20:10 actually exploring other alternatives for different chat models that we can use because right now we''ve been focusing on only open AI but let''s look\n20:16 at a few different examples of how we can use Lang chain chat models but with different you know llms so let''s go\n20:22 ahead and start working on that now all right guys so welcome to this third example where we''re going to start\n20:28 exploring different Alternatives of working with other models outside of open AI so in this case we''re going to\n20:35 explore Google''s Gemini models we''re going to look at anthropic or CLA and look at Open Eye because I just want to\n20:42 show you guys how easy Lang chain makes it to work with these different models so let''s scroll down so we can look and\n20:48 compare and contrast all the new code so what you can see up top this is exactly\n20:53 what we''ve been doing so far in all our examples we create our chat model once we have the model we we go off and\n20:59 invoke it and then we get back some sort of result well Lang chain abstracts all the complexity away and makes it super\n21:05 easy to do the same thing with our anthropic models and our Google Gemini models all we do is we instead of using\n21:12 the chat open AI model like we''ve been doing in the past we now just use chat anthropic and then we can pass in\n21:18 whatever specific model within Cloud that we want to use just like we did with chat gbt up here and then we''ll do\n21:24 the normal part where we just you know go off and invoke it with our messages and get back a result and is the exact\n21:29 same thing for Google''s giz models down here at the bottom so link chain makes it super easy to work with these\n21:35 different models and this is very important as you build larger projects because certain models are much better\n21:40 at you know at performing certain tasks some are cheaper some are faster so for different situations you need to use\n21:45 different models Lang chain makes it super easy to do and if you want to explore all the different models that\n21:50 each you know anthropic provides and Google provides I have links for you guys and just as a important reminder if\n21:56 you want to go off and explore all all the different chat models back over here in our first example back when we were\n22:02 working with a chat model documentation you could start searching through here so you can explore all the different models and all the different\n22:09 functionalities that these different models provide but okay cool enough of that let''s go ahead and start exploring our next example where we''re going to be\n22:16 diving in and actually having a conversation with our user through the terminal so let''s go and start working on this example\n22:22 now all right guys welcome to the fourth chat model example where we''re going to start actually having a realtime\n22:27 conversation with with our AI models this is going to feel just like the chat gbt website except it''s running locally\n22:33 on our computer so let''s walk through how we''re going to set this up so the important part per usual we''re going to create our chat model by loading all our\n22:39 environment variables and creating an instance of it now here''s where all the interesting part happens because we''re having a conversation we''re going to\n22:46 create a chat history list and this is going to store all our messages so as we ask questions we''re going to add\n22:52 messages to the chat history as the AI responds we''re going to add those messages to the chat history and we''re just going to continually keep add add\n22:58 in all of our messages to this list so here''s how that works in this code example so first off the first message\n23:04 we''re going to add is our system message because if you remember system messages are just general context for the\n23:09 conversation that''s about to happen so we''re going to create that system message and then we''re going to add it\n23:15 to our chat history by calling append on the chat history list cool so now here''s\n23:20 where all the core logic happens so this is where our chat Loop happens and basically here''s the core Loop we first\n23:27 ask our users hey what is the question or prompt that you have for the chat model we''re going to get back a query so\n23:33 this is whatever the user passed to us if the user gave us the keyword exit we''re just going to stop the\n23:38 conversation right then and there and we''re just going to print out the whole chat history however if they didn''t give us the keyword exit we''re just going to\n23:44 have a full-on conversation so what we''re going to do is we''re going to add the person''s query as a human message\n23:50 and we''re going to add it to our chat history then what we''re going to do is pass over that entire chat history list\n23:56 over to our model like you can see right here and then we''re going to invoke it so that model is going to then read all\n24:02 the messages in our chat history get a response and then we''re going to print back that response to our user and we''re\n24:09 also going to most importantly track that response by once again updating our chat history so we''re going to update\n24:15 our chat history with an AI message cuz you know this is the AI response so enough talk let''s go ahead and look at a\n24:21 code example so you guys can see this in action so I''m going to once again open up our terminal and I''m just going to\n24:26 call Python and then this is our chat model project and this is the fourth example so now we can actually start\n24:32 running it and per usual it''s now going to ask for query so I''m just going to ask it who are you so then what we''ll do\n24:39 is we''ll pass that question over to open AI open AI will generate a response and it actually prints it back to us so this\n24:45 is exactly you know if we''re talking to chat gbt on their website this is exactly what it feels like and we can\n24:50 actually ask questions about it because as a conversation we can refer to previous messages can you expand on that\n24:57 and what it''ll do is it''ll you know cuz it can refer back to the previous messages and actually provide additional\n25:02 context so this is super powerful and you''ll definitely be using this a lot more and finally we can pass in the word\n25:08 exit it''ll quit the conversation and what''s cool is you can see our entire message history here so you could save\n25:14 this off somewhere you could you know save it locally save it to the cloud do whatever you want so that whenever the user comes back in the future you can\n25:20 continue the conversation and we''ll actually do a much more deeper example later on where we actually save this to\n25:25 the cloud so you''ll learn about that in just a little bit all right cool enough of that let''s go ahead and start working on our fifth example which is exactly\n25:32 what I was talking about where we''re going to start saving our messages to the cloud so let''s go ahead and start working on this where we''re going to save all of our messages over to\n25:38 Firebase I think you''re really going to love this one all right let''s go ahead all right guys welcome to the\n25:44 fifth chat model example so this one is by far my favorite because you''re going to learn how to save all the messages\n25:49 you type locally over to the cloud and this case we''re going to be saving everything to Firebase so what I''ve done\n25:55 is I''ve actually typed out all the installation in steps for you guys because there''s there''s you know a little bit more background setup because\n26:02 we''re working with the cloud but once we knock out all the development setup in the cloud what we''ll do is just like run through the code so you can see how\n26:08 everything works but I think you''ll see that like all in all this is actually pretty straightforward the hard part is\n26:13 just setting everything up in the cloud so let''s go ahead and do that now and the first thing I do want to mention about the cloud is this is actually\n26:18 based on this Google fire store codebase example so you can see like this is exactly what we''re doing however uh I\n26:25 feel like they didn''t do the best job walking through how to get it set up and I struggled a ton the first time I did this so I just want to share all the\n26:31 lessons learned that I have done and and I''ve copied all my lessons learned here okay so the first thing that you need to\n26:37 do is create a Firebase account so just head over to you know console firebase.com and you''re going to create\n26:44 an account and once you''ve created an account you''re going to then go over here and create a project to so I''m not going to like walk through creating a\n26:50 project cuz it''s it''s super simple but just come over here click the drop- down menu and click create a project so\n26:55 that''s how you create a project once you''ve done that you''ll then need to create when you go over to the build tab\n27:01 there''s something called the firestore database and this is where we''re going to be storing all of our messages inside of our chat history except our chat\n27:08 history is now in the cloud so whenever you click fir store database if it''s the first time you''re using the project you''re going to want to turn this on so\n27:15 there''ll just be a turn on button here for you and you''ll want to click that and once you''ve done that and you run the code eventually all of your messages\n27:21 and user sessions will get saved up to the cloud and more on this later but I just want to go ahead and like paint a picture of where we''re going fantastic\n27:27 so once you''ve done that let''s heading back to our examples you''ll need to start copying some information about your Firebase project so what I mean by\n27:34 that is you need to start copying information such as the project ID so in our case what you''ll do come back over\n27:40 here you''re going to click the gear icon you''ll hit project settings and this is where you''ll find your project ID\n27:46 project number and everything that''s you know related to what you''re what you''re doing with the project you just created\n27:51 over here in Firebase Okay cool so that''s the easy part now this is where things get a little bit more complicated\n27:57 because we''re trying to work with the cloud so Google cloud and we''re trying to have our local computer communicate\n28:04 with the cloud so this is where we''re going to have to go a little bit deeper so the first thing we''re going to have to do is install the Google Cloud\n28:10 command line interface on our computer because what we''re trying to do is authenticate our local computer to make\n28:17 requests to the back end that''s all we''re trying to do so what you''ll do is first thing you''ll click this link over\n28:22 here and it will take you to this and this will basically just walk you through exactly what you need need to\n28:28 type to install the Google Cloud CLI it''s super straightforward you''ll just click download once you''ve downloaded it\n28:34 literally walks you through step by-step how to install it so that''s super super easy then once you''ve gone through and\n28:40 installed everything and initialized it the final part that we''re going to do is you need to authenticate your local\n28:46 Google Cloud CLI to your account so once again click this link it''ll take you over here and it will just walk you\n28:52 through how you can actually authenticate your local computer to Google Cloud cuz we''re just trying to create some default credal\n28:58 that way whenever our codee''s running it just can you know seamlessly make a call to the back end and the main things that\n29:04 you need to do first off just run Google Cloud off application default login a\n29:09 signin like normal Google Cloud signin screen will pop up you''ll just log in it''ll make a service account for you\n29:14 life fantastic and then you''ll just run you''ll run this basically this script as well to finally get everything working\n29:20 so I hope that''s enough I don''t want to go too deep just because I want to focus more on Lang chain but run through those\n29:25 steps you guys will be good to go and then you''ll find finally have everything working inside your code so all right\n29:31 enough of that here''s how we''re going to get things working over here so you''re now going to come in here uh CU you have your project ID that you just copied\n29:37 you''re then going to have a session ID so we''re just going to make a new one so we''re going to call this user session\n29:43 new on this is where all of our messages are going to be saved and then finally there''s a thing called a collection name\n29:48 this is heading back over to Firebase just the way our fir store saves data\n29:53 it''s in a collection document basically a database type so you can see collection document collection so it\n30:00 just keeps alternating so we''re going to store all of our messages in the chat history collection and each session is\n30:07 going to be a document and that document is just going to contain a list of messages so that''s what''s about to happen so what we need to do first off\n30:14 is we need to initialize our firestore client and what that''s going to do is allow us with our client to go off and\n30:21 make requests so that''s why we''re doing it and then what we''re going to do next is have something called firestore chat\n30:27 message history this is basically we''ll just click in here so you can see what''s going on but basically what we''re trying to do is in the past you and I were\n30:34 storing all of our messages in a list well Lang chain has provided a bunch of different options for us to store all of\n30:41 the different ways that we can have and store our messages so in this case we''re using fir store to save our messages\n30:48 there''s a few different examples inside of Lang chain where we can actually save our messages using I think the like file\n30:53 message history class where we can actually save all the messages locally on our computer so there''s a bunch of different options that I want you guys\n31:00 to be aware of and feel free to explore through the L chain documentation to see other options but just know at its core\n31:06 all it''s doing is just saving a list of messages you know human messages AI messages back and forth except now they''re just being saved off to\n31:12 somewhere else so that''s what''s happening under the hood but let''s keep going so in our case now that we have\n31:17 basically we''ve created our new chat history except our new chat history is up in the cloud so what we can do now is\n31:22 actually start running through the exact same Loop that we did last time which is where the human''s going to ask a question question and now what we''re\n31:29 going to do is just keep adding with our chat history we''re just going to add user messages or we''re going to add AI messages and we''re just going to go back\n31:34 and forth so enough talking let''s actually dive into the demo because this is super cool in my opinion so we''re going to go step by step because there''s\n31:40 a lot of cool ways to show this off so the first thing that we''re going to do let''s clear this out and we''re just going to run python so we''re in the chat\n31:47 models example where this is the fifth example now what this will do is it''ll start you know initializing firestore\n31:52 client it''ll start the chat message history and usually this takes a few seconds to get started and then now we\n31:58 can actually start chatting so it''s cool though uh and you''ll see this get populated in second our current chat history well because this is the first\n32:04 time I created that new session ID there''s no history of this chat in the cloud you know there''s no user session d\n32:11 new so there''s no messages to load but now I can actually start talking about it so I can say who is Sam Alman so\n32:20 it''ll go off and answer questions and you know so here''s here''s who Sam is but what''s super cool is if now if I come\n32:26 back over to the class we can actually see our messages being stored in real time you''ll notice they''re stored as\n32:31 bite strings and they''re not actual just like plain text messages that''s just totally fine that''s how Firebase and fire store are storing messages but\n32:38 let''s keep going does he have a brother so we can keep asking and then\n32:44 it''ll say yes he has a brother here''s his name and then what we can do is exit it so we''ll exit Okay fantastic all of\n32:51 our messages are cleared you can see over over here we actually still have additional messages but what''s nice is\n32:56 whenever we restart the same file with the same you know chat session it''ll\n33:02 actually say the current chat history and it''ll pull all of the different you know messages that we have previously\n33:08 sent to the AI so this is a super cool way and I hope you guys are like this is awesome this is a super cool way for us\n33:14 to continue conversations at a later date without having to completely refresh from start so I hope you guys\n33:20 thought this was awesome because we can still you know do additional stuff like who was I just talking about let me fix\n33:26 this oh sorry wrong button I can say who we just talking about and then because\n33:32 this is a chat history it can refer to the previous messages so yeah this is awesome so I hope you guys are pumped\n33:37 and that concludes the first module where we''re just run through all the different capabilities with working with\n33:43 chat models and from here we''re going to move on to prompt templates next so I do just want to point out before we keep going if you have any questions\n33:49 definitely hit that link down the description below for school go over there and ask any questions you have or hop on our weekly coaching calls would\n33:55 love to help you guys out but let''s keep chugging along and start moving over to prompt\n34:00 templates all right guys welcome to the second module in this Lang chain crash course this whole module is completely\n34:06 dedicated to prompt templates and I think you''ll find this is a super simple concept but it''s going to make our lives\n34:12 much easier as developers as we work with link chain so what the heck is a prompt template why does it matter how\n34:17 does it work with chat models those are the main things we''re going to be tackling in this module so the first thing is prompt templates the best way\n34:23 to think of them is exactly what the name says we are building up a prompt that''s our whole goal we''re trying to\n34:29 create a prompt but we''ve created some sort of template like this and it''s up to us to pass in values into that\n34:36 template so you know kind of think of like Fillin the blank is the best way to describe it so you know you could do\n34:41 generate three jokes about dogs or something like that is kind of what we''re what we''re shooting for here so\n34:47 enough like high level let''s actually dive into like what this actually looks like in action as like a quick overview\n34:54 and then after this we''ll dive into the code okay so like I said our whole goal is we''re striving to generate a prompt\n35:00 but we''re going to be passing in some variables to make this work so you know we''re going to be taking like a user\n35:05 response to help us really fill out this prompt and this is important because eventually these prompts are going to\n35:11 get passed over to our chat models to you know to to perform some action so in our case like this is a joke template So\n35:17 eventually this template once it gets populated it''s going to get passed over to you know chbt or Claude one of those\n35:23 models and it''s actually going to spit out some jokes for us so that''s just this prompt is going to help us structure The Prompt that we pass over\n35:29 so it''s super helpful we''ll dive into some more examples here in just a second but this is exactly how it works it''s up to you as a you know whenever you''re\n35:35 creating these prompts to basically create a string and inside that string it''s up to you to use these curly\n35:41 brackets to create variables these variables are later going to be populated with these values and what''s\n35:48 important to notice here is once we''ve created our prompt template we''re going to Define an input dictionary and this\n35:54 input dictionary is going to Define for each one of these variables the values that are going to replace them because\n35:59 eventually whenever we have our template pass in these inputs and call invoke because remember invoke is the super\n36:06 magic word for all of Lang chain what it''s going to do is it''s going to basically do some string interpolation and replace all these values so that we\n36:13 end up with an output that looks just like this obviously this was a super simple example but let''s just think like\n36:18 higher level like for a real world setting you could eventually be creating some sort of tool that helps developer\n36:24 automatically debug their code well in your case you would create a prompt template that''s like here is the user''s\n36:30 code and then you know you would have user''s code here''s the error they''re getting that would be the next variable\n36:37 please help them debug this and then basically you would pass in that entire prompt filled with the user''s code and\n36:42 error over to open Ai and then open AI would tell you oh it looks like here''s a bug here''s how it fix it so this is just\n36:48 a really nice way for us to create structured prompts that make it easy you know where we can do some prompt engineering and easily add in our users\n36:55 input to The Prompt templates that we''re so uh enough jargon let''s actually dive into an example and start walking\n37:01 through our first example working with prompt templates and I think this will make a ton of sense and it''ll be super easy to\n37:08 understand all right guys so let''s go ahead and dive into the code and this one''s going to be a super simple Code\n37:13 walkthrough there''s three parts to this and then I have one little extra part so you guys can see you know when not to\n37:19 use prompt templates but let''s walk through this part by part just so you understand everything at a basic level and then we''ll work our way up from\n37:24 there okay so if you remember our whole goal is we''re trying to create prompts that we could eventually pass over to\n37:30 our chat models so we''re focusing in on creating prompt templates that''s all we''re trying to do so let''s go part by\n37:36 part in this code and what we''re going to be trying to do is trying to relate it back to this outline that we have over here so the first thing we need to\n37:41 do is create a prompt template that stores our variables of things we want to replace so that''s exactly what we''re\n37:47 doing right here in this code we''re creating a template which is just a string and what we''re going to do is\n37:52 we''re going to call chat prompt template and like well what the heck is this well if you remember at high level we''re\n37:58 eventually trying to our end goal is we just talked about chat models that''s all we were just talking about in module one\n38:04 well what we''re trying to do is create prompts that we could eventually pass over to those chat models so that you\n38:09 know AI can basically answer whatever questions or ask that we''re putting together in our prompts so that''s our\n38:14 end goal so what this chat prompt template does what it''s trying to do is it converts our string into a template\n38:20 that makes it easy to work with to actually replace you know basically replace all these variables with these\n38:26 values so basically what''s happening under the hood so once we create this prompt template so once we have our\n38:31 template what we''re trying to do is we want to pass in an input dictionary that has all of our keys and values so that''s\n38:39 basically exactly what was happening right here and what we''re trying to do is once again we''re calling that magical\n38:44 word invoke so in this case what''s going to happen under the hood is we have this prompt template we''re going to call\n38:49 invoke with these specific values and all it''s going to do is it''s going to go through each variable and replace it\n38:55 with the appropriate value so let''s go ahead and run this code so you can see an action so in our case we''re going to\n39:00 do python now we''re in prompt templates and we''re going to run the first example so two one and what this will do is now\n39:07 when I run it what you''ll notice is it actually doesn''t spit out a string it actually spits out a human message so\n39:13 tell me a joke about cats so that''s pretty cool it actually filled in the values that we told it to and if you\n39:19 remember what it did is it spit out basically a messages array that we could eventually pass over to our chat models\n39:25 cuz that''s where we''re trying to go so that was super simple example one of the other things I want to show you in this is we can actually instead of just\n39:31 replacing one value let''s go ahead and look at part two what you''ll notice in this one is we can actually replace\n39:37 multiple values in a prompt template so this is the exact same exact same thing\n39:42 except this time what we''re doing is replacing multiple values so let''s go ahead and yeah we''re still calling the same chat prompt template from template\n39:49 so let''s go ahead and run the same example again just so you can see it in action so yeah running with multiple placeholders this time tell me a funny\n39:56 story about a panda that''s exactly what happens okay enough of the simple examples let''s actually get on to some\n40:01 more of the cooler parts which is part three so if you can uh if you''ve seen so far every time it''s created something it\n40:08 has basically been from a human message point of view but sometimes you actually want to do a little bit more you want to\n40:14 have a little bit more control so what you''ll notice in part three is we can actually start defining basically we can\n40:22 specify the types of messages we want to create and not only what type of message we want to create we can actually still\n40:28 you know replace all the variables in here and what you might notice and which I thought was weird the first time I used it I would have expected that you\n40:34 know what the heck is this like Tuple where the first part''s a simple uh you know the system and then the message\n40:41 then we have another human then message like why aren''t we just using like human messages and system messages well it all\n40:47 comes back to the way the chat prop template works is it just expects this tupal format where you define the\n40:53 message type first and then the content so just know if you want to basically use prompt templates and also Define the\n40:59 message type you have to go with this Tuple approach but yeah let''s go ahead and see an action so you can see exactly what it looks like so we''re running part\n41:06 three so we''re just going to run the code again so you can see now we have a list of messages and the first message\n41:12 in this array is going to be a system message and you know you''re a comedian who tells jokes about lawyers so that''s\n41:17 setting the context and then you can say tell me three jokes so we actually just replace the variables there too so yeah\n41:23 so once again we''ve created these messages and we could actually pass this message list over to our chat models and\n41:28 it would spit out those jokes about lawyers okay now I just want to show you this is just a lesson learn that I thought was pretty interesting so if we\n41:35 come down here to the final part what you will notice I''m just going to comment this out so you can see it so\n41:41 this code does work so you know how we just talked about tupal well anywhere\n41:46 you want to do string interpolation you know where we''re replacing values you have to use the tupal so you''ll notice\n41:52 we want to replace Topic in this system''s tupal so we have to put it in a tupal and then well for the human\n41:58 message we''re not doing any interpolation there so we''re just going to leave it how it is so this example would 100% work let''s clear it out and\n42:05 I''m just going to show you it works so this one works yep it looks just like we talked about in the first example well\n42:11 if you come down here to the final example that I have in this file you''ll notice I say this does not work and\n42:17 basically what we''re doing here is we''re trying to do interpolation on a human message so let me just show you what\n42:22 happens when you try and like if you don''t use a tupal you''ll basically get uh you''ll basically notice here tell me\n42:28 a joke and it never did the replacing part I ran into a lot of issues when I first started using Lane chain and doing\n42:34 prompt template Replacements and I was like why isn''t it replacing it and it''s like oh it has to be in this tupal\n42:39 format okay I hopefully um that''s enough prompt templates like Basics let''s actually start moving on to the next\n42:45 part where we''re going to start using actually start using these prompt templates with a chat model in our second example so let''s go ahead and\n42:50 start working on that now all right so welcome to the second prompt template example this one we''re\n42:57 actually going to tie together everything for module one which is creating our chat models and everything we just learned about prompt templates\n43:03 so let''s go ahead and merge both these together in the example so you''ll actually really understand the whole value of creating these different prompt\n43:09 templates okay so per usual like we did in all of module one we''re going to load our environment variables and we''re\n43:15 going to go ahead and create a chat model that''s using open AI in this case now let''s go part by part so you can\n43:20 actually see the exact same basically all the prompt templates that we created in demo part one we''re actually now just\n43:26 going to like not only create those prompt templates but we''re actually going to pass over those prompts to our\n43:32 model so let''s look at example one in depth and then we''ll just run it you know for everything else so per usual uh\n43:38 we''re going to create our template with the variable we want to replace once we have that string template we''re going to call chat prompt template and we''re\n43:44 going to make an actual template from that string just so it''s easy for Lang chain to actually start manipulating it\n43:51 and from there we''re going to do exactly what we did last time which is call invoke cuz that''s the magic word for our\n43:56 prompt templates to take in that string and actually start replacing all the values so once we call invoke we''re\n44:02 going to get back that prompt and what''s cool is we can now pass that prompt which is if you remember from the last\n44:08 one it''s now just going to be a message array that has all these values in here so you know it''ll say tummy joke about\n44:14 cat and that''ll be one of the messages one of a messages in our message history that we''re going to pass to our model I\n44:20 hope that makes sense um you whenever I run it it''ll actually make more sense so um but what you can see is we''re now going to pass that prompt over to our\n44:27 model and we''re going to tell our model invoke because we want our model to actually like perform actions on these\n44:33 messages and we''re going to print the result so you''ll start to see you know invoke is going to get called a lot as\n44:39 you start to work with Lane chain more and more and I''m not going to go too deep in part two and part three because we just copied and pasted over all the\n44:45 examples from the last time and now we''re just actually going to see let AI actually run basically run the request\n44:50 which is creating jokes about all these different prompts we''ve created so what we''re going to do call python example two and this is the second example for\n44:57 prompt templates with a chat model let''s go ahead and run that and this will take a few different examples seconds but you\n45:03 can see so prompt template prompt from a single template which was right here tell me a joke about cats so you know\n45:09 sure here''s a joke about cats for you why was the cat sitting on the computer because I wanted to keep an eye on the\n45:15 mouse so obviously they''re pretty corny and same with multiple placeholders so it actually told a short story about you\n45:21 know a short funny story about pandas so here''s our funny short story and then\n45:27 finally when it comes to the end where we''re doing you know prompts with system and human messages what we''re doing is\n45:33 creating three jokes about lawyers and that''s exactly what it did here are your three lawyer jokes so why don''t sh\n45:38 sharks attack lawyers because of professional courtesy so once again pretty corny but um yeah so I wanted you guys to see prompts with prompt\n45:45 templates because right now we''re calling invoke to get the prompt we''re calling invoke again to get the result\n45:51 which leads us perfectly to chains because this is exactly what we''re about to do next where we''re actually like\n45:57 technically we''re chaining prompts with chat models so this will make more sense so we set ourselves up perfectly to get\n46:02 to dive into section three which is all about change so let''s go ahead and start diving into that model now and this will\n46:07 all make sense and it''ll actually be a lot simpler so you''ll see what I mean in just a\n46:13 second all right guys so welcome to the third module in this Lane chain crash course this chain section is by far my\n46:19 favorite part of the whole tutorial and I hope you guys love it just as much as I do so let''s walk through what the heck is a chain why is it important and then\n46:27 just go through a few different examples real quick and then we''ll hop over to the code okay so at a high level what the heck is a chain well inside a lane\n46:33 chain it''s nothing more than tying together a series of task so if you\n46:38 remember so far we''ve been using invoke the invoke function a lot so we''ve been using it to create prompts we''ve been\n46:45 using it to basically have chat models respond to us so what what we could do instead is have the prompt be chained\n46:52 like the output of a prompt whenever we invoke it that output be fed to the model and then that model whenever we\n46:58 call it invoke again it will spit its outputs and it could just go to the next item so as you can see we''re kind of just chaining together different task\n47:05 and we''re passing the input from the first one over to the next one and we just keep going down and down and down\n47:10 so that''s exactly what''s happening right here have a prompt invoke it spit the outputs over to the next thing Tak in\n47:17 those inputs process it with invoke and then spit it out to the next thing so that''s exactly what''s happening with chains and I think you''ll think when you\n47:23 see the code it''s super simple and there''s another common thing when you''re using using chains inside of Lang chain\n47:28 and it''s called Lang chain expression language it''s a fancy way to say and describe like how you can create chains\n47:35 cuz you can go the hardcore code way which we''re not going to get into we''re going to stick to this which is just anytime you want to chain items together\n47:42 in L chain you''re going to use the pipe operator so this is like right above your return key on your keyboard it''s\n47:48 just the straight up and down basically pipe key so that''s how you''re going to create a chain you''re just going to put\n47:53 you know a prompt a chat model and there''s a few other things you could put in here too but this is how you''re going to do it item or a task a pipe operator\n48:00 another task a pipe operator another task super easy to set up and then whenever you want to run the chain all\n48:06 you do is you call the chain and you call invoke on it and then you pass in your input dictionary of all your\n48:11 different keys and values this actually should be a quick uh my bad this should just be a quick dictionary just like\n48:17 this and that''s how it works so this initial key dictionary gets passed in over here as the first into the first\n48:23 prompt then it gets passed along all the way through so that''s just at a high level just a simple chain now let''s just\n48:28 I just want to show you the realm of the possibilities when it comes to Lang chain first and then we''ll hop into the code okay so when it comes to chain\n48:34 possibilities the first thing to know is like you don''t have to do just prompt chat model you can actually keep going\n48:40 so you can just continually keep going as long as you want that''s option one the other thing that''s super cool is you\n48:45 can run task in parallel so you can have task you know just like oh kick off kick\n48:50 off and you can just start running your chains in parallel and then you can actually even at the end of them you can actually join them together and have\n48:57 your final results all come back and be fed into one so this is a cool way just to like if you need to do some parallel\n49:03 processing this is a cool way to do that the final thing is branching so branching is a way inside of your chains\n49:10 to have it to go like let''s you know at this node let''s kick off some actions cool well based on the results of those\n49:16 actions let''s just say like oh it''s um if we''re doing a review of a movie well if the movie was good cool we''re going\n49:23 to go down this branch and this chain path if the movie was awful we''re going to go down this path and if it was so so\n49:28 we''re going to go down path C so that''s just kind of like the realm of the possibility when it comes to chains but enough like theoretical highle stuff\n49:35 let''s actually dive into the code and start working on the simple model first and then we''ll work our way up through all the different possibilities so let''s\n49:41 go ahead and dive into the code all right everybody Welcome to the first code example when it comes to\n49:48 chains I think once you see how this all works you''re going to be like oh my gosh this is so much easier than what we were doing beforehand and what''s cool is\n49:55 we''re going to use basically the same examples that we did from beforehand but just reformat them so they work with chains so let''s go ahead and dive\n50:01 through this and walk through it step by step so the first thing that you notice we''re in the chain Basics file what we''re going to do is exactly what we''ve\n50:08 been doing so far with everything else we''re going to create our chat model once we create our chap model we''re\n50:13 going to create our prompt template our prompt template is going to contain our variables that we want to replace in\n50:19 this case it''s going to be the topic and the joke count from there what we''re going to do is create our chain now what\n50:25 you''ll notice this is very similar to to the drawings I just created a few seconds ago the first thing we''re going\n50:30 to do is have our prompt template which is going to contain all the messages that we want to basically replace with\n50:35 our inputs later on create our prompt from there we''re going to pass that prompt to our model once our model then\n50:42 has basically is able to process that prompt what we''re going to do eventually is spit everything out to a string\n50:48 parser at first I''m going to get rid of this string parser just so you can see why we need it but just know at a high\n50:53 level all it does is it takes you know how so far every time we get the result we do do content well that''s exactly\n50:59 what the string parser does for us it just goes ahead and grab the content but we''ll we''ll add it back in in a little bit so comment it out just so you guys\n51:06 can see it how it works okay cool so what I''m going to do is go ahead and run this chain just so you guys can see how\n51:11 easy it works and how simple it is how much less code we''re doing in this approach versus what we were doing last time so what I''m going to do ahead is go\n51:18 ahead and open up our terminal and then if you remember now we''re in the third module cuz we''re working with chains and\n51:23 this is the basic example so we''re just going to run python three so we can go open our chain file and this is the\n51:29 first example and what I would expect to see is once we print the result we''re going to get that long response that we\n51:35 normally get whenever we don''t grab the content so you can see yep here are whenever we look at the content you can\n51:41 see yep here''s the three lawyer jokes and then we''re seeing all that extra information that we get back from our LM\n51:46 like tokens and so forth but if we undo our changes earlier and just use the\n51:51 string parser that you can see right here where we''re just doing the string output parser this also comes from we''ll\n51:57 talk about this more but the output parser Library that''s where it''s coming from but if we rerun it now what you''ll\n52:02 notice is it actually just spits out the final text like this is what we would actually expect to return to our\n52:08 customers if this was a web app or you know this is what we would like most likely pass to the next model if we were\n52:14 doing this you know and continuing the chain so this is awesome I hope you guys see how much easier this is to use\n52:20 compared to what we were doing last time i'' actually let''s just do a side by side so you guys can kind of see exactly what\n52:25 this looks like so over here what we were doing yeah at the bottom we had our you know we\n52:31 created our prompt templates then we did our prompt then we invoked it then after we were done invoking it we saved the\n52:37 result then passed it to our model then did it again and again so all of this code right here basically uh just gets\n52:44 replaced with this single one line which is beautiful so I hope you guys like this so now that we understand the\n52:49 basics what I want to do is we''re going to dive deep for a second in the next example where we''re going to understand\n52:55 how chains work under the hood and after we do that we''re going to start building off those examples you saw earlier where we''re going to do you\n53:01 know pillow chains and then branching and much more so let''s go ahead and dive into the next\n53:07 example all right so welcome to the second example in the chains module and\n53:12 in this example what we''re going to do is a deep dive into how chains work under the hood now this isn''t super\n53:18 important for you to like memorize or understand exactly how everything works under the hood it''s just I want you to\n53:23 be aware of a few Concepts such as runnable sequences runnables and runnable lambdas those are the main\n53:29 three things that we''re going to talk about in this section okay so let''s dive into the code and we''ll look around so\n53:34 per usual what we''re doing is we''re creating our models and loading our environment variables creating our prompts and then most of the new action\n53:42 when it comes to creating basically chains the manual way happens right here\n53:47 so what we''re going to do is talk about this at a high level and then we''re going to dive into each part so at a\n53:52 high level what we''re doing is we are creating runnable lambdas so think of a\n53:58 runnable is a task whenever I was talking about task in the initial outline for chains under the hood the\n54:04 way it actually works in Lane chain is it''s just called a runnable so what we''re doing a runnable Lambda is nothing\n54:10 more than a task that''s a Lambda function and if you''re newer to python don''t worry about this Lambda functions\n54:17 kind of get like a little bit more into weeds but they''re just a quick way to make functions so like here''s the input\n54:22 to a function and then here''s the actions we want to take so that''s just kind of what Lambda functions are so\n54:28 that''s part one part two is this thing called a runnable sequence and a runnable sequence is nothing more than\n54:34 we''re going to do this task then this task and this task then this task and put all those tasks together you have a\n54:40 sequence AKA a runable sequence AKA a chain so that''s exactly what''s happening\n54:46 so let''s actually dive in deeper into these runnable lambdas and runnable sequence so you can actually see what the heck''s going on so the first thing\n54:52 is what we''re going to do is we''re going to create the format prompt runnable Lambda and what this is going to do is\n55:00 all it''s going to do is take in our prompt template that we''ve done in the past and then instead of calling invoke\n55:05 on it what we''re going to do is just call the format prompt and the format prompt is basically just going to like\n55:11 yep replace all these you know values right here with these values down here\n55:17 so that''s exactly what''s happening under the hood and you can see we are passing all of our arguments and spreading them\n55:23 right here that''s exactly what that''s doing basically we''re just passing this dictionary and spreading it so that we can replace all these key values with\n55:30 these values down here so that''s what''s happening don''t have to understand how it works exactly I just I just want you to be aware of like what''s happening\n55:36 from there once we have our prompt and we''ve created that task we''re going to create an invoke model task and the\n55:41 whole point of this runnable Lambda once again just a task that takes in a Lambda function all this is going to do is it''s\n55:47 going to say yep we have a model we''re going to invoke the model and what we''re going to do is pass in a you know pass\n55:54 in the input from the previous function into this one and we''re going to convert whatever the heck we just received to\n56:00 messages so that''s what''s exactly happening there finally we''re going to parse the output so if you remember the string output parser well under the hood\n56:07 all it''s doing was grabbing the content from the output and that''s what we''re going to do right here so here''s how now\n56:14 that we have all these individual runnables here''s how it works with a sequence and let me just show you under\n56:19 the hood what a sequence looks like so um just in case you haven''t seen this if you have a Mac you can hit command and\n56:25 click basically any any class and it''ll take you to the definition if you have a Windows just hit control and then click\n56:32 the the class and it''ll take you to the basically the the definition but basically this is recapping everything\n56:37 that we have just talked about you know a runnable is nothing more than just like it takes the output from one passes\n56:44 it to the next as an input so just we really just creating a sequence of task that need to occur they give us a cool\n56:49 little example just to show us like yes whenever we use Lang chain expression language to where we just like pipe task\n56:56 and runnables together it''s actually the same thing as you know calling runnable sequence first and last and then once\n57:02 you have a sequence you can invoke it and pass items into it so that''s what''s happening under the hood so let''s\n57:07 actually hop back to our code example and finish this one up real quick so we can actually keep keep going down the chain what you''ll notice is whenever I\n57:14 created my runnable sequence you''ll have three different properties that you can call and this is because if you scroll\n57:19 down to the actual code part you can see there''s three different things there''s first middle and last first is a single\n57:25 run last is a single runable and middle is a list so that''s just how you have to pass\n57:31 in your sequence so if you have two items you''ll call first and last if you have five items 10 items 100 you know\n57:38 you''ll have to call first for the first item last for the 100th and everything in between will just get passed in as a\n57:44 list so that''s exactly what''s happening so in our case the format prompt where we convert our text over to a prompt\n57:50 happens as the first task our middle task where we have our formatted prompt and pass it to to our model so we can\n57:57 actually process it that gets passed in the middle list and then finally for the last thing once we you know have our\n58:03 prompt pass it to our model process it in Ai and we just want to get that final text output well that''s why we''re going\n58:09 to call the last runnable task which is just going to be Parts output and when you put all that together you end up\n58:14 getting a chain and that chain is exactly we''ll call it the exact same way that we were doing earlier so what I''m\n58:19 going to do I''m going to run the code real fast just so you can see how it works and then I''m going to compare them side by side just so you can see like\n58:25 that''s crazy that they do the same thing but this just takes so much more work cuz we''re not using the L chain\n58:31 expression language so this is chains under the hood so we''re going to run it and as you can see whenever it finally\n58:36 gets done it''s just going to spit out a response which is you know just three jokes about lawyers so yeah that''s just\n58:43 that''s three different jokes and just to show you a quick side by side let''s go open up three one chain Basics and zoom\n58:51 out yeah so instead of doing all this nonsense where we''re creating runnable lambdas and passing stuff over and over\n58:56 and creating runable sequence just stick to L chain expression language and it just works so I hope you guys Now\n59:02 understand why L chain expression language is so nice and that''s probably what you''ll use 99% of the time whenever\n59:08 you''re working with building chains but enough of that going deep let''s actually hop back up and start working on some of\n59:14 the additional ways we can use chains so we''re going to broaden them out in the extended section and then we''ll keep moving on from there so let''s go ahead\n59:20 and hop over to part number three all right guys so welcome to the\n59:26 third example in the chain module and in this example we''re going to Showcase how you can continually extend your chains\n59:33 and add on additional runnables or probably the easier way to think of it is additional task to continually happen\n59:39 and you can just keep growing that chain forever so let''s just go ahead and look at the code in this example so you can see how it actually works so what we''re\n59:45 going to do is the per usual where we''re going to load our environment variables then we''re going to create our model and\n59:50 then we''re going to create our prompt that''s all the usual stuff all of the new chain extending has happens right\n59:57 here where we''re going to uppercase the output and then we''re going to count Words so let''s actually dive into what''s\n1:00:02 happening well what we''re going to notice is we''re creating our usual chain and then we''ve piped in an uppercase\n1:00:09 output well under the hood when we actually look at what''s going on in this function because it is important\n1:00:14 whenever you extend or add new items to your chain by doing the pipe operator you need to add runnables that was one\n1:00:21 of the key things we talked about in the under the hood section so in this case we''re going to add a run Lambda and\n1:00:26 runnable lambdas just make it super easy to like basically process any type of function so in our case we''re just going\n1:00:32 to do some like string manipulation where we''re going to Upper C like sorry I I''ll slow down for you guys but like\n1:00:37 we''re going to take in an input and what we''re going to do is call uper on it which is going to capitalize the entire output so that''s what''s happening under\n1:00:43 the hood but what''s cool with runnable Lambda functions is you could also use these to go off and make like an API call you could you know you could really\n1:00:50 do anything that you normally could do with python code you could just throw it in a runnable Lambda function so that''s the part that makes this really cool but\n1:00:56 so now that we have uppercased the output what we''re going to do is continue building on our chain and we''re going to pipe the output of this over to\n1:01:03 count words so in this case when we come look at our count Words runnable Lambda you can see we''re going to taking an\n1:01:08 input this input is going to return basically a string that we''re going to do some interpolation on which basically\n1:01:15 just means some like formatting and passing in values so the first thing we''re going to do is count the word count so what you can see is we''re going\n1:01:21 to split the input that''s given to us and then once we''re done with that we''re going to count count how long the actual\n1:01:28 input was and then once we''re done with that we''re just going to spit out like right here it just says you know curly\n1:01:33 brackets X that''s where we''re just going to print out the input that was given to us enough talk let''s actually dive in so\n1:01:38 you can look at what actually happens under the hood when we actually run this so this is the third module for chains this is the third example and when we\n1:01:45 spit it out we would expect to see the word count yep just like that and then jokes so as you can see this code ran\n1:01:51 perfectly we spit out the output and you can actually see that this code ran too because everything is capitalized where\n1:01:57 it wasn''t originally so this is how you can continually build chains and as you''re going off and becoming you know a\n1:02:03 real world professional L chain developer some of the other practical things you would do is just like I said you would use runnable lambdas to go off\n1:02:09 and like create API calls to go do something you would then there''s just a ton of stuff that you could do and\n1:02:14 chains make it super easy to you know pipe an input from the previous result over to your current function and just\n1:02:20 continue going down the chain so this is awesome so now that we''ve covered extending and continually growing your chain let''s hop over to talk about how\n1:02:27 you can actually run chains in parallel I think you''re really going to like this section too all right guys so welcome to the\n1:02:33 fourth example in the chain module in this code what we''re going to be doing is showing off how you can run chains in\n1:02:40 parallel so I think this one''s super cool cuz if you''ve ever thought about creating social media posts where you\n1:02:45 have like an idea and then you want to like Branch off to write something on LinkedIn and then Twitter and then elsewhere this is exactly how you''re\n1:02:52 going to go about doing it so let''s go ahead and dive into the code at a high level and then I have a visual for you guys just cuz it is it is a little bit\n1:02:58 weirder to see how it all works and then we''ll come back to the code to like really dive in okay so at a high level\n1:03:04 this file is a little bit longer and I''m just going to hop down to the chain part and then from there we''re going to go over to the visual I was just talking\n1:03:10 about so at a high level what you can do is you can see we are creating a chain this chain has the usual prompt has a\n1:03:16 model we do some string outputting and then we start doing this funny thing called runnable parallel under the hood\n1:03:22 basically what this is doing is it''s creating different branches to run you know if we''re looking at Pros cuz this\n1:03:28 is going to be a pro con list and we''re just going to run run our Pros in one chain and we''re going to run our cons in\n1:03:34 another chain and eventually merge those Pro cons into a final list so that''s why we have two branches one for pro a pro\n1:03:40 branch and then a con branch and that''s where they''re going to go off and generate pros and cons and we''re going to come back and put into a final list\n1:03:46 so that was high level just talking through the code so let''s hop back over to what this looks like under the hood\n1:03:52 when we''re actually like you know visually what does this look like so that chain that we were just looking at\n1:03:57 what we''re going to do is it''s going to take in a prompt this prompt is going to take in a product name from there it''s going to construct a prompt using in our\n1:04:04 case I think we''re going to do the MacBook Pro and then from there it''s up to the model to produce a list of features about that product from there\n1:04:11 we''re going to grab the content like we normally do from the result from the model and just pull out the content string and what''s cool is we''re going to\n1:04:18 pass that string to our two separate chains so I don''t know if you saw back over a second ago but we had a pro chain\n1:04:24 and a con chain that''s exactly what we''re doing here so the pros it''s going to take in you know the list of features\n1:04:30 and in our case we''re going to pull out all the pros from those features and then what we''re going to do on the other side in parallel is find all the cons\n1:04:37 for all those features and then finally once we''re done with both of those different chains that are running in\n1:04:42 parallel we''re going to grab both of the outputs and then put them together in a final Lambda runnable so we''re going to\n1:04:50 get both of those outputs and then put them in a basically a nice print statement for our users to read pretty easily so that''s exactly what we''re\n1:04:56 doing now let''s hop back to the code and do a deeper dive now you have a good understanding of like what''s happening\n1:05:01 at a super high level visually all right let''s hop back okay let''s dive into this part by part and I think the best way to\n1:05:07 go about it is just start you know at the first item in each chain and then walk our way through so when it comes to\n1:05:13 the prompt template this is where we''re going to be doing our normal prompt template stuff meaning you know we''re just going to say like hey you''re an\n1:05:18 expert on doing product reviews and I need you to basically list the main features for this product we''re going to\n1:05:24 create a prompt for it in our case like I said this is going to be a MacBook Pro and then we''re going to pass that prompt over to our model who''s going to go off\n1:05:30 and actually list those features out then we''re going to do our string output parser none of this should be new so far\n1:05:36 but then we''re going to hop into this runnable parallel basically you know runnables are how chains work under the\n1:05:42 hood so in this case we''re doing parallel meaning we have a bunch of branches that we can spin off our\n1:05:47 different chains so in in this case we''re going to have a pro branch and a cons branch and you can actually dive in\n1:05:54 and see what each one of these these are so this Pros Branch under the hood it''s a runnable Lambda and what we''re doing\n1:06:00 is we''re actually just creating in our case we''re creating a new chain right here that''s exactly what it is I can\n1:06:06 actually rename it for you guys so you can actually see exactly what it is we''ll call this one a chain that one a\n1:06:12 chain perfect just so it makes a lot more sense and it''s even more clear what''s actually happening so we''re creating a chain for our prob branch in\n1:06:18 this case we''re saying like hey take in the input which in this case is a feature list I want you to analyze those\n1:06:25 probes so analyze Pros all it does is it creates a new prompt template just like we normally do and once we have that\n1:06:31 prompt template what we do is we pass that prompt template over to our model for our model to go off and generate\n1:06:36 those pros and then finally per usual we use the string output parser to convert that output into a nice string and the\n1:06:43 con branch is the exact same thing we take in that list of features about our product we pass that list of features\n1:06:49 over to this prompt template so you know given these features list the cons of each one of those features from there\n1:06:56 pass it over to our model string output so all this is completely normal we''ve done this a ton of times now here''s\n1:07:01 what''s different after we run our parallel branches what we end up with is actually one big dictionary and that\n1:07:08 dictionary if we''ll print it out I''ll print it out for you guys so you can actually see what I''m talking about right here so we''ll say final output\n1:07:14 what you can see is it''ll actually include something called branches and this branches will under the hood\n1:07:21 actually list out our pros and cons cuz that''s how we''re going to save them and you''ll see it in a second whenever I print it out but that''s the final part\n1:07:27 what we''re going to do is pass in our Pro output and our con output into this combined Pros con function and what this\n1:07:34 is going to do is just print out a nice list string final output for us so we can compare like yep should we get the\n1:07:40 Mac should we not get the Mac so let''s run it and actually dive into the outputs so let''s come down here let''s\n1:07:45 clear all old outputs up and start running it now what''s going to do under the hood like I said it''s going to start running and the first print output we\n1:07:52 should see is this final output right here where we''re going to have our Pros chain and our cons chain we''re going to\n1:07:58 be able to view both of them that''s what''s going to happen in here and then finally afterwards we should expect another final output I just want to show\n1:08:04 you guys under the hood how the inputs are getting passed into this and how we can actually view like yeah it does make\n1:08:10 sense that we need to grab the Branch''s object and then within the Branch''s object we need to grab the pros and cons\n1:08:15 so let''s scroll up so you guys can see exactly what I''m talking about takes a few seconds Okay cool so here''s where\n1:08:21 everything''s in a dictionary that''s why it''s all cluttered to see but you can see final output what it does you can\n1:08:27 see it''s a dictionary inside the dictionary you can see the first object inside of it is called branches and then\n1:08:34 within branches we can grab pros and then if you scroll down a little bit later you can also see cons if I just\n1:08:40 search for it you''ll be able to see it cons let''s see yeah so you can see here''s the cons object so how does this\n1:08:46 actually work under the hood well it''s because everything in this runnable parallel is equal to we basically\n1:08:52 created a dictionary right here because you can see we have branches is equal to and then we''re creating our object so\n1:08:58 Pros are going to be saved as the result of this Pro chain and cons are going to come from this Con chain so that''s\n1:09:05 exactly how it works so I hope that makes sense once again this is getting a little bit deeper into the weeds when it\n1:09:10 comes to working with Lane chain and we''re starting to do a little bit more like fanciness with python so if you have any questions on any of this please\n1:09:16 definitely go over to the school community that I''ve created for you guys drop a comment you know take a screenshot of what''s con confusing you\n1:09:22 and I''d be happy to help or hop on our weekly coaching calls but I hope you guys see the value in this because in\n1:09:28 the past you would have had to run your Pros chain that would have taken 30 seconds then you had to run your cons\n1:09:33 chain that would take another 30 seconds but now when we can run stuff in parallel we can cut it down and make things a lot more efficient if you''re\n1:09:39 going to be building applications for your users where they''re expecting a speedy output this is exactly what you''re going to want to do to help\n1:09:45 automate and speed up some of your task all right so enough of talking about running chains in parallel let''s go over to learning how we can start branching\n1:09:52 our chains based on different conditions let''s go ahead and dive into task numberb five next see\n1:09:58 y all right guys so welcome to the fifth example in the chain module and this\n1:10:03 final example what we''re going to do is cover branching and I think the best way to cover this topic before diving into\n1:10:09 the code is to actually look at what this code looks like visually so let''s hop back over here to excal draw and let\n1:10:16 me run you through what''s actually happening under the hood so there are two parts to this chain the first part\n1:10:22 is what we''re going to call the classification chain and what we''re trying to do is we''re trying to take in feedback from our customers so they''re\n1:10:29 going to say like man that was awesome or like oh that was the worst product I ever bought and depending on what type of feedback they give us we''re going to\n1:10:36 have different prompts to generate different types of messages so we''re going to have a positive Branch we''re\n1:10:41 going to have a negative Branch a neutral branch and then if something goes wrong we''re going to have an escalation Branch so here''s a quick run\n1:10:47 through of what''s Happening visually and then we''ll hop back over to the code so visually what we''re going to do is do the normal chain for our classification\n1:10:54 where we''re going to have have a feedback prompt what we''re going to do is in this prompt we''re obviously going\n1:11:00 to populate the prompt with the user''s input in this case it''s going to be their feedback about what we''re doing\n1:11:06 then we''re going to categorize that feedback into one of you know one of these four keywords positive neutral\n1:11:11 negative or escalate so that''s how we''re going to basically classify everything then we''re going to pull out the\n1:11:17 category with a string parser like we''ve done a ton of times by now and here''s where the New Concept comes in we''re\n1:11:23 going to it''s going to be called a runnable brand and basically the way this works is it''s think of it as like an if statement so\n1:11:29 if the you know if a conditions met we''re going to run a different chain so this is a really good way to like really\n1:11:36 you know make your chains more intelligent and in different scenarios run different chains so in this case\n1:11:41 just you know going deeper we''re going to check if the category was positive if it was fantastic we''re going to run the\n1:11:47 positive chain was it negative cool we''ll run the negative chain so forth and so forth so I hope this all makes\n1:11:52 sense now let''s hop back over to the code so you can see what this actually look looks like inside of Lang chain all\n1:11:57 right so welcome back to the code and I think what you''ll notice is this is going to be super straightforward so\n1:12:03 what we have to start is we have a bunch of different prompt templates each one of these prompt templates for positive\n1:12:09 feedback negative feedback neutral feedback and escalate feedback all these different prop templates come from right\n1:12:15 here so as you can see across the board we have all these different prompts that''s what we''re doing we''re building up each one of these chains for the\n1:12:21 different types of feedback all right let''s keep going then eventually what we''re going to do is we also need to create the prompts for our\n1:12:27 classification part so this is you know how you know your helpful assistant help us classify the sentiment of this\n1:12:34 feedback is it positive negative neutral or do we need to escalate it and this is how we''re you know using our prompts to\n1:12:39 pass in our users feedback cool so here''s where we''re going to get into the new part which is the runnable branch so\n1:12:46 this is the part that is different than anything you''ve seen so far so what you''re going to do is you''re going to create a runnable branch and the\n1:12:52 synopsis of how this works is you''re going to have a input so in this case we''re just going to use a Lambda\n1:12:58 function and then you have a conditional right here so in this case we''re going to say like hey is the word positive in\n1:13:05 the input that you just gave me if so fantastic I''m going to run and this is basically oops sorry this is basically\n1:13:11 the positive chain I''ll just write right here back chain fantastic so this is\n1:13:17 actually what''s happening under the hood we didn''t full out you know call each one of these their own chains but each one of these is a neutral chain a\n1:13:24 positive chain and a negative chain so here''s you know this because this is once again we''re using Lane chain\n1:13:30 expression language to create a positive chain so as you can see we''re just going to take in the promp template that we saw above we''re going to pass that to a\n1:13:37 model and then we''re going to get back the output so this is a message that we could respond to our users saying like oh you loved our product fantastic I\n1:13:44 really appreciate it and we hope you you know continue to buy further products and services from us okay so now you''ve\n1:13:50 kind of seen how we''re going to use runnable branches to conditionally run different chain such as positive chains\n1:13:56 check to see if it''s negative to run the negative chain see if it''s neutral to run the negative chain the other part that I haven''t mentioned yet is there''s\n1:14:02 a default so when you''re using runnable branches if any of these messages didn''t\n1:14:08 trigger off like if we didn''t run you know the positive negative or neutral you basically can create a default\n1:14:13 branch and in this case we''re going to call it the escalate branch and what this one''s going to do is it''s actually\n1:14:19 just going to make a nice message for us that we can see where it''s going to like hey generate a message to escalate this to a human agent so that''s just kind of\n1:14:25 how it works and this could definitely be super helpful in a customer service type of situation so I just wanted to show you guys this cuz it''s super\n1:14:31 helpful and helps you build more complex tools all right so now that we got the runnable branches out of the way let''s go back to see how we can make this work\n1:14:37 so basically in the end we''re going to have our classification chain and then you know which is just going to be this\n1:14:42 whole part right here this is the entire classification chain right here and then the entire bottom part all of this huge\n1:14:49 square right here is going to be this second chain so we''re chaining chains together to make super long complicated\n1:14:55 ones I hope you guys like this all right so what we''re going to do next is I''m just going to go ahead and run it so you guys can see the results and if you want\n1:15:01 to play around with this code for yourself what you''ll notice is I went ahead and typed up a bunch of different types of reviews so you can experiment\n1:15:07 with each of the different branch pths so let''s go ahead and run it for a positive review fast just so you can see you know how it works so we''re just\n1:15:14 going to go open up our terminal and this time what we''re going to do we''re going to do Python and this is the third\n1:15:19 module so chains five to run this branching program so we''re going to run it and what we would expect to happen is\n1:15:25 it to ex to go down the positive branching path and generate a nice positive message so you can see you know\n1:15:31 I hope this finds you well I just want to personally thank you for your positive feedback so you can see it''s actually it''s actually working and what\n1:15:37 we could do too if you want to try out something different let''s try out a bad review just so you can kind of see how\n1:15:42 it would go down the in this case go down the negative branching path so I replaced it with the negative comment\n1:15:48 it''s terrible I broke it after just one use and then you can run it again so let''s see you know thank you for your\n1:15:54 feedback I''m sorry that your experience was not up to your expectations we take all this feedback so as you can see this\n1:15:59 is a way we can handle a bunch of different scenarios using GL chain expression so yeah that concludes the\n1:16:05 third module where we''re covering chains and what we''re going to do next is move over to rag which is going to be what\n1:16:11 we''re learning about retrieval augmented generation this is going to be a huge module I hope you guys are pumped for it\n1:16:16 let''s go ahead and dive into it now all right guys welcome to module number four where we''re going to start\n1:16:22 learning about retrieval augmented Generation all also known as rag so in this quick overview section what I want\n1:16:28 to do is I want to talk about what is rag why do we need it and then I want to do a high Lev overview of like just\n1:16:34 running through a simple example just so everything makes sense whenever we start to dive into these code examples that\n1:16:40 I''ve set up over here for you guys okay so let''s just go ahead and just talk about what is rag at a super high level\n1:16:45 and kind of why do we need it well these large language models that we have set up like chat gbt Gemini and llama 3 all\n1:16:52 of these models have a con straint on how much knowledge they already have and that can be a problem whenever we want\n1:16:59 to start answering questions or whenever we have questions about additional things such as like hey what''s happening\n1:17:05 today in the news well these llms have a cut off date of you know a few months or even a year ago so it has no idea what''s\n1:17:12 happening right now also when it comes to maybe just like inside of your company you have some specialized\n1:17:19 documents for y''s processes or you have some other information about a product You''re Building well the LMS obviously\n1:17:25 don''t know anything about it so it''s up to us to use rag to feed in additional\n1:17:31 new information to these lolms so under the hood all we''re trying to do is just give these lolms additional information\n1:17:37 so they can answer our question that''s all that''s going on at a high level so as you can see we can feed in things\n1:17:42 like PDFs so whenever you hear people talking to PDFs or chatting with them that''s exactly what''s happening they''re using rag so we can feed in websites we\n1:17:49 can feed in source code and video transcripts and the list goes on and on but okay that''s enough at a high level\n1:17:55 so what I want to quickly do next is let''s dive over to a big example that I set up for you guys where we''re going to\n1:18:01 walk through exactly how rag works at a super high level so when we hop over to the code and we go through all these\n1:18:06 different examples you''ll have a high level understanding of like oh that''s why we''re doing what we''re doing so let''s go ahead and dive into that high\n1:18:12 level overview right now all right guys so welcome to this quick overview of how rag works now I''ve\n1:18:19 put together this flowchart for you guys so that we can understand two critical components of working with Rag and I''m\n1:18:26 going to walk through both of them at a high level and kind of go super quick because we''re going to go through this a bunch more times as we actually dive\n1:18:32 into the code but the main thing I want us to take away from this demo is for you understand the Core highle Concepts\n1:18:39 such as terms like vector store embeddings llms chunks tokens we''re\n1:18:44 going to learn we''re going to just quickly say all these words and it''ll mean a lot more when we dive into the code and that''s where we''re going to be covering in part one and then part two\n1:18:50 of this quick demo I want to walk through like great well once we have actually saved all of the information\n1:18:56 from our PDF over to this special database called a vector store well how can we actually start asking questions\n1:19:03 and pulling out information so that we can get a AI generated response that''s\n1:19:09 informed meaning if we''re you know chatting with a document about how to like cook a specific food well we want\n1:19:16 to pull out the specific instructions for that food along with our question to generate an AI response so that''s what\n1:19:22 we''re going to be talking about at a high level and we''re going to go pip part and this will hopefully all make sense and I do think one of the core\n1:19:29 parts to help this make sense of like why we''re doing what we''re doing is to actually start at the vector store and\n1:19:36 work our way back up so you understand why we''re doing it because and this will make more sense so hang on for two\n1:19:42 minutes and it''ll make all sense okay so at the end of the day everything that we''re trying to do comes to this part\n1:19:47 right here it''s the bridge and connection between our database which stores all the information about our\n1:19:53 Tech Source plus this Retriever and this retriever is where we''re going to like\n1:19:59 hey man please give me all the information about you know let''s just say we''re working with Harry Potter and\n1:20:05 we ask like hey which Professor also is a werewolf well what we want to do in this example is we want to be able to\n1:20:12 look up things such as like professor and werewolf how well if we were manually doing this what we would do\n1:20:18 just like command F in a PDF and just search for those words a thousand times but there''s going to be a ton of entries\n1:20:24 and what we''re trying to do is really find a werewolf and a professor that''s the information we''re looking for okay\n1:20:29 so what we''re trying to do and this is a keyword here we''re trying to look for similarity inside of the original teex\n1:20:37 source that we''re looking for that''s all we''re trying to do we want to look for similar pieces of information inside\n1:20:43 this original PDF that talk about professors and werewolves okay so that''s all we''re trying to do here now well how\n1:20:49 does this actually come into play when it comes to rag well what''s going to happen under the hood is this PDF of\n1:20:55 Harry Potter is going to be broken up into a thousand different chunks because you can see this PDF is has about 10\n1:21:01 million tokens and that''s way too big for us to answer any questions or like analyze what''s going on because chat gbt\n1:21:09 has a window of about 8,000 tokens and 10 million is obviously way too big to pass into it so what we need to do\n1:21:15 because we''re just trying to pull out the similar pieces of information and pass it over to jbt so it can generate\n1:21:20 an AI response about it so what we need to do is we need to split this up into a bunch of smaller chunks that are about a\n1:21:27 th000 each what''s great about this is because like I said chbt can take in 8,000 prompts 8,000 tokens so if we pass\n1:21:35 in one chunk up to three or four chunks there''s still plenty of room for us to also pass in a question about those\n1:21:42 chunks okay hang on we''re getting there all right well each one of these chunks is just plain text so you can see like\n1:21:48 chunk one is just like you know Harry you''re a wizard and you know and basically how this is working under the hood is we''re just starting at the\n1:21:55 beginning of the book taking out a thousand words putting it a chunk putting it getting the next few thousand words putting in another chunk well the\n1:22:01 problem is we said our original goal is we went to look up similar phrases well how the heck does AI compare and search\n1:22:09 for similar phrases well this is where the term embedding comes into play an embedding is nothing more than the\n1:22:17 numerical representation of text so that was a mouthful what does that actually mean well if we were to like in this\n1:22:23 super simple example if we were to embed the word dog cat in house you can see\n1:22:28 that a dog has this embedding a cat has this embedding and a house has this\n1:22:34 embedding and you''ll notice they''re nothing more than just a list of numbers but they actually under the hood have\n1:22:39 some meaning and relationship to to one another so for example a dog and a cat\n1:22:44 are super similar the only thing that''s different are these last two numbers but even then they''re not that different cuz\n1:22:50 they got four legs they''re furry they live in a house but a house on the other hand it''s got a roof it''s got walls\n1:22:56 brick they''re completely different subjects so if we were to ever like search about dogs or animals with four\n1:23:03 legs well these two things would come together these are super similar so we would get responses about dogs and cats\n1:23:10 so with the point I''m just trying to get across is embeddings allow us to easily in search for similar items and the way\n1:23:18 we''re able to do this cuz like machines love numbers and numbers are easy to compare and see how different and\n1:23:24 similar things are okay so what we''re going to do is we''re going to convert these chunks of text over to embeddings\n1:23:31 so that we can easily compare them and contrast them and once we have all these different chunks of text embedded what\n1:23:38 we''re going to do is start saving the embedding plus the original text to a vector store and the reason why this is\n1:23:44 important is because going back to our original problem what we want to do is we want to be able to ask questions and\n1:23:50 retrieve related documents from this Vector store all right so now let''s walk through this flow down here now that\n1:23:56 we''re in part two and you''ll see exactly why everything that we just did above is super important so going back to us with\n1:24:02 our Harry Potter fans if we ask a question which Professor is also a werewolf well that question will be\n1:24:09 embedded itself because what we''re trying to do is see we''re trying to look up similar documents to our question and\n1:24:17 the way we''re able to do that is because our embedding we now have a numerical representation of Professor and werewolf\n1:24:24 and and then eventually cuz now it''s embedded now whenever we go to our use our it''s going to be called a retriever\n1:24:30 whenever we use our retriever to look up similar embeddings in the vector store it''ll go like oh well chapter 4\n1:24:36 paragraph 1 2 and 3 all talk about this professor that''s a werewolf let me give you back those chunks of data or those\n1:24:44 Blobs of text so that you can then ask questions about them so that''s all that''s happening under the hood we go\n1:24:50 step one here''s my question here''s my embedding that has my question great let''s return this chunk and this chunk\n1:24:57 because those were the most similar and once we have those chunks returned what we can do is pass them over to chat gbt\n1:25:04 and what''s going to happen under the hood that you''ll see is all that''s doing is is just adding those pieces of text\n1:25:09 at the very beginning of our prompt and then at the very bottom we''ll have our question and then we can now ask\n1:25:15 questions about those pieces of information so what it would really be is like which Professor is a werewolf\n1:25:21 and then these three chapters or these three PA paragraphs will have all the information we need to generate an AI\n1:25:27 informed response now that was a ton of information don''t worry we''re going to go part by part when we look at the code\n1:25:33 it''s just at a high level the main thing I want you to understand is we have huge Tech sources that have way too much\n1:25:38 information for any AI to consume we need to break it up into smaller chunks those chunks need to be converted over\n1:25:45 to eddings the reason we need to do that is because we need to see how similar each different chunk is to one another\n1:25:51 so that later whenever we go to ask questions about those different pieces\n1:25:56 of text and embeddings we can go oh yep these two are super similar here''s the information you requested back so that\n1:26:02 you can ask a question so that was a mouthful let''s actually go ahead and start diving over to the code now so we\n1:26:08 can see all of this in action and I promise you after you see this a few times it''ll all make sense and you''ll be\n1:26:13 able to start building your own rag use cases so let''s go ahead and hop back over to the code all right guys so welcome to the\n1:26:20 first code example inside of the rag module and I''m super excited you guys to see how everything ties together that we\n1:26:27 just walked through in the highle drawing so this is exactly where you''re going to connect the dots you''re just to paint a road map of what we''re about to\n1:26:33 do is I have broken up the first code example into two parts as you can see there''s a 1 a and a 1B 1 a directly ties\n1:26:41 to the initial breakdown of what we were setting up top in our initial drawing\n1:26:46 drawing where we were first converting some Tech Source over to our Vector store so that''s going to be in 1 a and\n1:26:52 then 1B is going to be all about actually asking questions and retrieving documents from that Vector store so\n1:26:59 everything we talked about when it come to like embedding chunking saving stuff through the vector store that''s all\n1:27:04 going to happen in 1 a and then building our retrievers once again embedding you''re going to see that all in 1 B okay\n1:27:10 so we''re going to go ahead now and start walking through this code part by part and some of this code is a little bit\n1:27:15 more advanced just because there''s a lot more moving Parts but don''t worry I''m going to cover everything and once again\n1:27:21 if you have questions you can always head over to the fre School Community I created for you guys just take a screenshot or a Tim stamp of the video\n1:27:27 and say Here''s Where I''m stuck I don''t understand this and myself or someone else in the community will be sure to help get you unstuck so you can keep\n1:27:33 going and building out your awesome projects all right so let''s go through this part by part and also walk through some of the folder structure just so it\n1:27:40 all makes sense okay so the first thing that we''re going to be doing is inside of 1A is we need to set up some file\n1:27:47 paths so that we can connect to different parts of our project so the main things that we need to connect to\n1:27:53 is our initial text source so this is the document we''re going to be asking questions about in this case we''re going to be asking questions about the Odyssey\n1:27:59 and you can see how we access that book is by looking inside of our current directory go to the books folder and\n1:28:06 then look for the Odyssey text so if right now U my current directory is inside of the rag folder if I open up\n1:28:13 books you can see that I actually have the Odyssey text file right in here so that''s how we''re going to access that and then next thing is you can see I\n1:28:19 have this what I''m calling a persistent directory so that''s actually just where I''m going to be storing our chroma\n1:28:25 database and same thing this one we just look at the current database open up the DB folder whenever you start to save\n1:28:32 files this will actually Auto get created so you might not see DB out the gate and then next we''re going to save\n1:28:38 everything to the chroma DB folder so that''s what you can see now this is since I''ve already ran this code before\n1:28:44 I already have this but I''m going to delete it so I''m on the same page as you guys Okay cool so let''s keep chugg along\n1:28:50 in our code and so you can understand what''s going on so if you remember the first part of what we''re trying to do in\n1:28:55 this highle code is go from a Tech Source over to our Vector storm so if we''ve already went ahead and chunked our\n1:29:03 document converted everything to embeddings and saved it to our Vector store there''s no reason to rerun 1A so\n1:29:10 this is why I have this code right here it''s just going to say hey does a persistent directory already exist in\n1:29:16 this case the chroma DB already exists if it does just completely skip all of this because there''s no reason for us to\n1:29:23 go off and you know there''s no reason for us to go through and pay because embeddings do cost money to convert from\n1:29:28 text to embeddings so we''re just going to completely skip it but if we do not have that persistent directory what we''re going to do is go off and actually\n1:29:35 set everything up so the first thing that we''re going to do is we''re going to start at the top and actually load in\n1:29:42 the file path and what I''m going to do to help this make sense is we''re just going to quickly hop back and forth from part to part so it all makes sense so\n1:29:48 first thing we are going to load that file path which is connected to our Odus text and we''re going to use a text\n1:29:54 loader a text loader is just a great way to actually like Point like yep here''s the file path to my document that I want\n1:30:01 to load later on we''ll talk about things like web loaders to where you can actually pull information from the websites but just I''m just want to like\n1:30:07 expose you to the realm of the possible but right now we''re just going to load a text document once we have that file loaded what we''re going to do is we want\n1:30:14 to actually start splitting up this document into small chunks exactly like\n1:30:20 we talked about here so so far we''ve loaded it and now what we''re going to do is start chunking it and there''s a bunch\n1:30:25 of different chunking strategies that we will talk about later on in the text splitting Deep dive but for right now\n1:30:31 just know we''re going to try and chunk our basically each section of the book\n1:30:37 every 1,000 characters and just to go like a little you know help explain a little bit more just so we''re on the\n1:30:42 same page you can also set this thing called chunk overlap and all that does\n1:30:47 is let''s just say this is a chunk of text and let''s just say in the middle of\n1:30:52 a paragraph Was A characters what it''ll do is it''ll cut off so you kind of get cut off in the middle of a sentence and\n1:30:59 then when you load the next chunk it''s kind of like well I have a query about the end of the first chunk but it''s\n1:31:05 about the second chunk too so it just I don''t know it''s it''s just difficult for us to understand semantically what''s happening in a chunk so what you''ll\n1:31:12 notice a lot of times people do is they''ll set some overlap so what that''s what that''ll mean is just like you''ll\n1:31:18 always grab a little bit of the next section too that way there''s there''s just always overlap and a lot of times\n1:31:23 you''ll get better performance results so a lot of times just like give you numbers a lot of people will do things such as like 200 or 400 or common values\n1:31:31 so yeah but right now for example we''re just going to keep everything lean and mean and set the chunk size to 1,000 tokens fantastic so once we have set up\n1:31:38 our text splitter what we''re going to do is actually split up our original documents into a ton of those chunks\n1:31:43 that you see right here each one''s going to be a th000 tokens now once we have all of our chunks also docs whatever you\n1:31:50 want to call them what we can do next is we need to start moving over to in embedding them now in this casee we''re\n1:31:55 just going to use the open Ai embeddings and we''ll actually go through later on in an embedding Deep dive to show you\n1:32:01 other options that you have but for right now just know that we''re going to use open AI to convert our text chunks\n1:32:07 over to embeddings and we''re going to be using the text embedding three small model and there''s actually a few\n1:32:14 different models that you can use there''s one that''s three the three small which is the cheapest one and I that''s\n1:32:20 one I primarily use and there''s also a three large just just know as we talked about earlier there''s this things called\n1:32:27 numerical representations where just which were just long arrays of like a dog is equal to like an array of like 1\n1:32:33 two three well the large model just has a lot more values so we can get a lot more precise with our embeddings so just\n1:32:39 just know three small is probably completely fine for the projects that you''re working on okay let''s keep\n1:32:44 chugging along so once we have set up our embeddings what we''re going to do is we''re going to use our chroma Vector\n1:32:50 store to actually save all of our documents with our beddings and we''re going to say you are going to save the\n1:32:57 results over here into this persistent directory so what I''m going to do is I''m going to run this just so you guys can\n1:33:02 see what it looks like under the hood so we''re going to run python this is the fourth module and this is 1A and we''re\n1:33:08 going to run R rag Basics I''m going to zoom out so we can actually see it so you can see we are creating our embeddings we''re finished creating our\n1:33:14 embeddings and then now we''re going to create our Vector store if you scroll up actually a little bit you can see we\n1:33:20 saved a little bit of our first chunk cuz in our code we said you you know how many chunks do we create in this case\n1:33:26 826 where each chunk has a th000 tokens and you can see the first sample chunk and this first sample chunk is actually\n1:33:32 the very first part of the book so if you go open the book you can see at like line one this is the exact same so yeah\n1:33:39 we''ve actually just you know this is working hope you guys are hype to see it''s all all coming together all right so let''s go back down to the bottom so\n1:33:46 you can see our Vector stor is created and we can confirm that by going over to our database folder and checking for our\n1:33:53 chroma DB and you can see at the beginning DB chroma DB yep it made it\n1:33:59 and just to show you guys if I actually try running it again the way we''ve set up the code is it''ll give us an alert saying Yep this Vector store is already\n1:34:05 created no need to redo it cool so now that we have that set up let''s go ahead and move on to Part B where we can\n1:34:11 actually start asking questions and pulling information from the vector store so let''s walk over here to 1B and\n1:34:17 start going through this once again part by part so per usual what we need to do is we need to have we need to point\n1:34:23 point to our Vector store so that we can actually start you know referencing it so in this case copying the same thing\n1:34:29 look inside our current directory then actually access the database folder and look for chrom ADB once we have that set\n1:34:36 up we need to specify which embedding we''re going to be using and we''re just it''s important to use the same one just\n1:34:42 so that you know you wouldn''t want to change languages when you''re asking questions so like we just need to be very specific and use the same thing\n1:34:48 consistent or else you''re just going to get some weird results fantastic next what we''re going to do is we''re going to use chroma DB once again we''re just\n1:34:55 going to spin it up saying yep here is the persistent directory where you''ve already saved all of your edings in\n1:35:01 those text chunks and here''s the embedding function so that whenever you get asked queries you know how to go off\n1:35:07 and do a similarity check and see which relevant documents are the most similar all right so in this case all we''re\n1:35:12 going to do is for this question we''re just going to ask the Odyssey so the main character is Odus we just want to know who is his wife so this is the\n1:35:19 query that we''re going to be asking to our Vector store and I do think it is important for you guys if you haven''t\n1:35:24 seen what Vector stores look like I actually think they did a great job this is the chroma DB website and I think\n1:35:32 they did a great job of helping you guys visualize what''s going on so under the hood you can see we''re going to ask a\n1:35:37 query so who''s ODS is''s wife we''re going to convert that query over to an\n1:35:42 embedding once we have that embedding what we''re going to do is access chroma DB and it will pull out the documents\n1:35:49 and they''re connected embeddings cuz like that''s how we can do the similarity score door and we can see like oh this\n1:35:55 one''s the most similar and here''s the related document let me pass that back and once you get passed back you know it\n1:36:00 eventually gets passed back to your whatever llm you''re working with and generate that AI response so I just thought this was a neat little graphic I\n1:36:06 wanted to show you guys let''s go back to ours though so we can keep cranking this out so what we need to do next is work\n1:36:12 on that Retriever and the retriever is what''s going to take in our query and\n1:36:18 eventually return back those related documents but when we''re setting up our retriever we have a few different\n1:36:23 different options for what we can do and per usual what we''re going to do is I have a deep dive later on so you can see\n1:36:29 all the possibilities when it comes to working with Achievers but for right now just know we have a few different search\n1:36:35 types that give us different results and for this example we''re going to be doing a similarity score threshold all that\n1:36:41 does is pretty much exactly what it says it''s going to do a similarity check to find the most related documents and it''s\n1:36:47 only going to return documents that you know up to us how similar so this number you can see we have something called a\n1:36:53 score threshold well the higher this number is the higher the text document has to be to our query and we''ll play\n1:37:00 around with this just so you can see it in action and the other argument that you''ll see is this random letter K with\n1:37:05 a number so what the heck is that well what it''s going to do is it''s going to return the K closest documents so if\n1:37:12 there was 100 documents that were you know that were Above This threshold what this will do is it will return the top\n1:37:18 three closest results so that''s how you have to kind of set up your retriever if you want to uh to do this and feel free\n1:37:24 to bump this number up or down just remember depending on which llm you''re working with you''re kind of capped at you know right around usually 8,000\n1:37:31 tokens so in this example we''re going to be pulling out 3,000 tokens whenever we''re all said and done and passing it\n1:37:36 to our llm okay so now that we have set up our retriever set up the parameters what we can do is we''re going to call\n1:37:43 that invoke function remember we talked about that a lot in the chat model section invoke is the magic word inside of link chain for actually taking action\n1:37:50 on the models that you set up so in this case on our we''re going to pass in the query invoke it and it''s going to give\n1:37:56 us back the relevant documents so I''m going to go ahead and run this and then we''re going to play around with some of these parameters just so you can see\n1:38:02 what''s happening under the hood and I do want to point out all we''re doing in this example is just showing the\n1:38:08 relevant documents later on we''ll actually pass these documents over to a chat model so we can start interacting\n1:38:14 with them but for right now I just want to show you guys that yes our Vector store is returning related documents and\n1:38:20 we''ll just look through them okay so let''s come down here clear this up we''re going to run inside of our fourth module\n1:38:27 we''re going to run 1B to actually start grabbing those documents and let''s run it okay so what I would expect is to\n1:38:33 eventually get back three different documents that relate to OD deus''s wife so you can see we have document one\n1:38:40 document two document three that''s cool it also includes a source we''ll talk more about that later but the main thing\n1:38:46 right now that you can see is his wife is Penelope so you can actually like kind of see like we''re talking about in\n1:38:52 this document M we''ll actually just do a quick search so you can look up like the term wife it gets mentioned in a few of\n1:38:58 these documents so yeah you are indeed blessed in the possession of a wife endowed with such rare excellent of\n1:39:04 understanding and so faithful as Penelope so that''s like you can see this document perfectly describes you know\n1:39:10 like this is his wife so in the other documents too kind of talk about you know some of the other they they refer\n1:39:16 to wife so they''re not as similar but you can see this was the closest one that''s why it''s document number one all\n1:39:22 right now that you''ve seen that work what I want to do is show you how we can play around with some of these parameters to get different results so I\n1:39:28 can actually bump this up to k equal 10 and I''ll bump this down so we basically\n1:39:33 we''re we''re lowering the threshold and saying we want more results so what we''re going to do is I would expect to get back a bunch more documents this\n1:39:39 time that are related to a wife so you can see yeah we got back 10 more documents this time let''s see how many\n1:39:45 times his wife''s mentioned penelopy yeah so these These are getting less relevant\n1:39:51 seven out of the 10 documents referred to her okay so what I want to do next is I want to show you guys what happens if\n1:39:57 you get a little too stringent so let''s just say we only want to have documents that are like super super close to our\n1:40:04 initial question so if we were to run this updated code where we''ve updated the threshold to 0.09 meaning like very\n1:40:10 very similar what we''ll do is we''ll probably run into the problem of like hey you were too strict with your search\n1:40:16 results I couldn''t find anything so we''re going to give it a second and whenever it gets back to us it''ll\n1:40:21 probably tell us hey man cannot find that oops it looks like there''s a quick bug let me just close this out clear it\n1:40:27 out let me try this one more time and now it should hopefully give us back an\n1:40:32 answer yeah so you can see this time for whatever reason it just hit a bug yeah there are no relevant documents that retrieve using the relevant score 09 so\n1:40:39 we know we got too strict so that''s uh just you know as you''re building your own rag applications if you''re not\n1:40:44 getting back results you might just be a little too strict with your similarity score okay cool well that''s it for\n1:40:50 module number one now what we''re going to do from here is just to paint a road map for you guys we''re going to head into part two where we''re going to start\n1:40:56 adding metadata to our document so we can actually kind of see like oh this is\n1:41:02 where this Source came from so we''re going to go ahead and learn how to add metadata and view it in our request\n1:41:07 let''s go ahead and start working on that now hey guys so welcome to the second example inside of the rag tutorial and\n1:41:14 what we''re going to work on in this one is start setting up metadata now why is metadata important in context of rag\n1:41:21 well it all has to come back to the fact that lolms hallucinate and what I mean by that is Hallucination is a really\n1:41:27 nice way of saying they just get stuff wrong but what''s cool is whenever we start adding in metadata to our rag\n1:41:32 queries what we''re basically doing is allowing our llms to respond with like here is the source of information for\n1:41:39 where I''m generating this information I''m passing back to you so if we were asking a question about a meeting that\n1:41:46 happened in the past at our company it would tell us like oh John said this and Bob responded with this and it would say\n1:41:53 in here''s the source where I got this information from so you could always just click the source go check it out on\n1:41:58 your own and be like oh yeah that is exactly what happened and dive into more of the details that went on in that meeting so that''s metadata at a high\n1:42:04 level now let''s actually dive into the code example so you can actually see how we set this up in practice and you''re going to notice this is super similar to\n1:42:11 the rag Basics tutorial we''re just adding in a little bit more complexity so you can understand all the capabilities of working with rag okay so\n1:42:19 per usual what we''re going to do is set up our current directories but this time what you''re going to notice is instead\n1:42:24 of loading a specific book we''re loading a bunch of books and that''s because what we''re trying to do in the end of this\n1:42:29 tutorial is ask about a certain character and a certain book and we would expect the documents to we would\n1:42:35 expect our retriever eventually to only respond with documents related to that character so you''ll see exactly what I''m\n1:42:42 talking about we''re going to ask about and Romeo and Juliet so we would expect to only get back information about Romeo and Juliet and and the metadata related\n1:42:48 to it but you''ll see that set up in just a second okay so what we''re going to do this time is we''re going to start using a different database so this time we''re\n1:42:55 going to be using chroma DB with metadata I''ve already set mine up and you''ll set Yours up in just a second but\n1:43:00 the main thing this code is identical to the other one we''re just going to check to see hey does this persistent directory not exist if not let''s just go\n1:43:08 through that main process we talked about last time which is going to be you know setting up our Vector store if we\n1:43:13 already have the database completely skip it so let''s walk through what''s happening under the hood when we are setting up this Vector store because\n1:43:18 there are a few differences in this one the main one is to start off is we are going going and searching through each\n1:43:24 one of the files in our books directory and we''re going to grab all the files that end in a.txt so we''re going to be\n1:43:30 grabbing all of these different books right here and what we''re going to do with these book files is we''re going to\n1:43:35 iterate through each of them and start copying the same process we did last time except for one tweak where we''re\n1:43:41 going to add metadata here''s what I mean first we''re going to actually set up a new file path by joining in you know the\n1:43:46 name of the book file with our local books directory then we''re going to load that book just like we did in the last\n1:43:53 example but what we''re going to do now is once we have that document loaded we''re going to add some metadata to it\n1:43:58 in this case we''re going to set up this source for this document so whenever we''re loading for example the\n1:44:04 Frankenstein book we''re going to say the source of this book is Frankenstein or whenever we''re looking at the ilad what\n1:44:10 we''re going to do is we and you know retrieve a document it''s going to say the source of the retrieved information\n1:44:15 comes from the ilad and you could definitely get a lot fancier with this you could break up your documents and set it up to be per chapter or per you\n1:44:23 know if you had a long meeting you could set it up to intro you know speaker one speaker two you could do it however you\n1:44:29 wanted to just so whenever you''re setting up your metadata you could have like chapter and so forth and so forth\n1:44:35 but in this example we''re just going to stick to a book Source cool once we have set up our metadata we''re just going to add it to our documents array or list\n1:44:41 and then from there we''re going to copy the same process first we''re going to iterate through each one of those documents that we have and we''re going\n1:44:47 to use our character splitter to chunk everything up into 1,000 tokens once we have our Docs we''re going to do the\n1:44:53 exact same thing where we''re going to pass in our docs plus our embeddings over to our Vector store down here so\n1:45:01 that we can go ahead and save everything okay so I hope this is making sense because now we''re going to hop to 2B so we can actually start asking questions\n1:45:07 to our Vector store and pull out documents so what we''re going to do in this example is once again set up our\n1:45:13 file directories and make sure we point to that new Vector store that we just set up the one that''s going to be having\n1:45:19 metadata so this one right here and what we''re going to do is we''re going to use our our new embeddings plus our Vector\n1:45:26 store so that we can instantiate our new chroma instance so we can start passing questions to it now this Vector store\n1:45:33 retriever is the exact same as last time all we''re going to do is just use similarity score I want the top three\n1:45:38 result and here''s my threshold I set it super low for this one just cuz for whatever reason it was being a little\n1:45:43 weird however let''s go ahead and run this code so you can see what it''s doing under the hood so we will do python we\n1:45:50 are in the rag module this is the second exle to be so we''re going to go ahead and run it and what I would expect to\n1:45:56 get back is information on how did Juliet die plus the metadata okay so you\n1:46:01 can actually kind of see it right here like in document number two Juliet''s\n1:46:06 talking she obviously thinks Romeo killed himself and out of true love down\n1:46:12 here she stabs herself so like you can like it actually went ahead and just gave us the exact piece of information\n1:46:18 we were looking for which is crazy that it did that so efficiently and the part that''s EXT cool is you can see this is\n1:46:24 the source where this information came from so you can go back and double check the AI to make sure it didn''t just come up and hallucinate with some some weird\n1:46:31 facts yeah that''s how adding metadata to your rag queries works at a basic level\n1:46:36 I hope you guys appreciated that and what we''re going to do next is actually start doing some deep dives into each of\n1:46:41 the elements that we have been using to basically do our rag queries so we''re going to head over next and do a deep\n1:46:48 dive into text splitting then we''ll you know keep going down the chain from there so you can see all As specs and\n1:46:53 how we can add in some variety to our rag queries so let''s go ahead and move over to part number\n1:46:59 three so welcome to example number three inside the rag module this example is\n1:47:04 all about going deep into different options when it comes to using the text splitter so what I what I mean by that\n1:47:11 is there are a bunch of different ways you can actually go about splitting up your large documents and you''re going to\n1:47:16 in this example explore all of them so how to split it up based on characters sentences you know paragraphs and more\n1:47:22 we''re going to be doing all that in this example and we''re actually going to be spinning up a bunch of different Vector stores so that you can see how all of\n1:47:29 the different text splitting methods compare contrast when we actually make a query so you''ll see exactly what I''m\n1:47:35 talking about here in just a second and per usual we''re just going to start at the top of the code work our way down so that you understand everything that''s\n1:47:41 going on all right so let''s go ahead and dive into the code so the first thing that we''re going to be doing is setting up our basically our normal file\n1:47:48 directories so we''re going to just say like Yep this is the book we want to analyze and here is the path to our\n1:47:54 databases and you''ll notice that we don''t have a specific database called out yet and that''s because we''re going\n1:48:00 to be dynamically spinning up about four or five Vector stores down below okay so\n1:48:05 what we''re going to do just make sure everything exists and we''re going to do the normal you know kind of set up our\n1:48:10 normal loaders this time we''re just going to load our book Romeo and Juliet grab the document and stand up and\n1:48:16 specify our Vector store or sorry our embeddings now here''s where things get interesting what we''re going to do as\n1:48:23 you can see I have I think five different examples to show you guys and each one of these examples kind of\n1:48:29 specifies which text splitter we''re going to be doing and when it''s useful to use this text splitter just so you\n1:48:35 guys always have a reference to come back to of like oh yeah I think it was text splitter number three that was good for this scenario and you can always\n1:48:41 just come back and actually explore but here''s what''s happening at a high level and then we''ll dive into this Vector store function that''s a little bit kind\n1:48:47 of it''s it''s different so what you''ll notice is we''re going to specify a specific type of text splitter along\n1:48:54 with the parameters we want to use and after that we''re then going to actually go ahead and split up the document and\n1:49:00 once we have all those split up chunks what we''re going to do is then call this create Vector store with all of our\n1:49:06 chunks and then we''re going to give this database a name and once we do that and pass over that information we''re going\n1:49:12 to call this create Vector store and what you can see in here is we''re going to finally specify our persistent\n1:49:19 directory here and then actually go ahead and Save save everything to our new chroma database so that''s exactly\n1:49:26 what''s happening under the hood okay so let''s walk through each one of these and I''ll tell you when you should use each\n1:49:32 method and then at the very end we''re actually going to go off and query the vector store okay so to start off the\n1:49:38 first type of splitting we''re going to do is just the normal character-based splitting in the past what we''ve been\n1:49:44 doing I''m just going to show you what we''ve been doing for 2A in the past so you can see what we''ve nor been doing is\n1:49:50 character based splitting so this is nothing new this time we''re just adding some overlap and you can see this is a\n1:49:56 great method to use when the type of your content doesn''t really like there''s no syntactical kind of meaning for\n1:50:02 example like if you''re reading a book we talked about maybe keeping paragraphs together because we kind of want to like keep chunks of information together so\n1:50:09 this is just great when we just need to Chunk Up and split a bunch of random information the next example I want to show you guys is sentence-based text\n1:50:16 splitting so this one is when we actually want to split Things based on you know a sentence so this way you know\n1:50:21 maybe pair graphs are too long so we''re just going to split everything based up on sentences so we''re going to look for\n1:50:27 you know periods explanation marks question marks so and that''s how we''re going to split things up and once again\n1:50:32 we''re going to be splitting things into chunk sizes of a thousand we''re going to call this our chroma sentence then keep\n1:50:39 chugging along the next one is going to be our token text splitter and so this actually splits based on a token so in\n1:50:46 this one what''s going to happen is we''ll actually maybe split in the middle of a word so if let''s say a word word was\n1:50:53 super long and you know maybe the first part of that sentence was right over the\n1:50:58 the chunk limit well half of the words going to get cut out so I would not recommend this for a lot of you know a\n1:51:05 lot of like documents based but I just wanted to show you guys that this does exist all right the next one that I just\n1:51:10 want to show you guys is going to be the recursive character splitter this is the one that I would actually most people use I''ve seen people use this the most\n1:51:17 and I would definitely recommend using number four the most cuz like it tries to naturally split around sentences and\n1:51:22 paragraphs within the character limit so it does a really good job of like making sure the information that gets pulled\n1:51:27 out just kind of it makes sense it would make sense for this cluster of information to be stored together okay\n1:51:33 cool and the final one I just want to show you guys is going to be custom teex splitting and all we''re doing with just to show you guys we can make our own\n1:51:39 Splitters and these Splitters you can specify like oh I want to split chunks based on at the end of a paragraph so\n1:51:45 this is what this you know double backspace it''s just a it''s a double Escape or two new lines so in our case\n1:51:50 we''re assuming that''s a new paragraph okay cool and we''re going to save that custom Tech splitter to our chroma DB\n1:51:56 custom fantastic so what we''re going to do next I''m just going to show you is for each one of our databases that we just sped up Vector stores we''re going\n1:52:03 to go off and query those Vector stores with this question how did Juliet die\n1:52:08 and the way we''re going to do that is we''re going to pass in the name of the vector store along with our query to this function right here and all it''s\n1:52:15 going to do is look up our persistent directory that''s going to be our Vector store and we''re just going to do the\n1:52:20 normal thing that we''ve done so far to where we have have our Vector store we set up our retriever set up you know our\n1:52:26 search parameters for similarity score and then we''re just going to grab the relevant documents and spit them out so\n1:52:31 that''s exactly what we''re going to do so let''s go ahead and start running this query to embed everything first off using the different text spits and start\n1:52:37 asking questions so let''s go ahead and run this and this is our third example so let''s go ahead and do this is in the\n1:52:43 rag module and this is example number three so let''s go ahead and run this super fast and this one will take longer\n1:52:49 because we''re doing five six different types of settings but you can see that it''s working we''re creating everything\n1:52:55 so far we''re based doing it for characters next we''re going to do it based on sentences tokens and then from\n1:53:00 there our recursive and then finally we''re going to do our custom text splitter and once this is all done we''re going to then finally ask you know hey\n1:53:06 so how did how did she die and you can see let''s just go result by result real fast just so you can kind of compare\n1:53:13 each one so the first one for character this is what we''ve been using so far and this does look super similar to what\n1:53:19 we''ve been doing it''s just a lot of text from there what you can see when it comes over to the sentence one this one\n1:53:25 actually looks a lot more structured and then finally let''s just keep going along when it comes to our token what I would\n1:53:31 expect to see at the very end of this one is like it get cut off at a random word so in this case it got cut off\n1:53:37 right at my but if this had been a longer word it would have got cut off and we wouldn''t have been able to like you know we could have potentially lost\n1:53:42 some information when it comes to our recursive text butter you can see that this has done a really good job you know\n1:53:49 cutting at the very end of a of a sentence so that did a really good job or verse I can''t remember what happens\n1:53:55 inside of a how Shakespeare wrote and then finally when it comes to doing our custom DB it didn''t it you know it kind\n1:54:02 of just the way it chunked it it just didn''t do a good job and that''s because the way we sped up to split based on two\n1:54:08 new returns it just didn''t do a good job of grabbing information but the main thing I just want you to learn is there''s a ton of different ways that you\n1:54:14 can actually go about using different text Splitters to grab information my recommendation for you is to always use\n1:54:20 the recursive text splitter whenever for you''re starting out and then you can get a little bit fancy using the token and\n1:54:26 text splitter and some of the sentence ones but just main thing always use the recursive but enough of that let''s go\n1:54:32 ahead and dive into the next module where we''re going to start talking about the different types of embeddings that you guys can use inside of your your rag\n1:54:38 setup so let''s go ahead and dive into example number four right now so welcome to example number four\n1:54:46 inside the rag module and in this example we''re going to do a deep dive into embedding just so you can see\n1:54:52 what''s possible okay and just to give a little bit of background so far we have strictly used open AIS embeddings and in\n1:54:59 this example I''m going to show you how you can start to use custom embeddings why you''d want to do that and show you some other models inside of the open AI\n1:55:06 embeddings that you might want to use okay so let''s quickly run through the code so you can see what''s going on and\n1:55:11 then at the end we''ll run it so you can actually compare and contrast two different results using two different embeddings so you can see which one\n1:55:18 works better and which one doesn''t okay so what we''re going to do is our per usual we''re going to load in a book that\n1:55:25 we want to ask questions about and what we''re going to do in this one uh per usual you''ll see we have a database\n1:55:30 directory because we''re going to end up creating two different Vector stores in this example and after that what we''re\n1:55:35 going to do is per usual go ahead and load our document and we''re going to split it up in this case we''re just\n1:55:41 going to use the character splitter and once we have that set up what we''re going to do is we''re going to end up\n1:55:46 using this function called create Vector store more than that in a second the important part is I have two different\n1:55:53 embeddings down here so let''s walk through each one super fast so the first one is once again like I said we''re just\n1:55:59 going to be using the open AI embedding and so far we''ve been using the let me\n1:56:04 go ahead and just show you the exact one we''ve been using we''ve been using it''s called text let''s see text embedding\n1:56:10 three small this is the embedding that we''ve been using the entire tutorial and now we''re branching out and we''re going\n1:56:16 to be using the Ada embedding okay so let''s actually go ahead and look how these compare price wise and actually\n1:56:23 look at what the different options you have so over here on the open AI website you can actually look up embedding and\n1:56:30 you can see there''s a few different examples so this one is going to be the Ada example version two this one is one\n1:56:36 of the more pricey ones and you can actually go under the hood and and look at the different options they have so\n1:56:43 Ada so you can see if you scroll deeper down they have different qualities so the adaa example has it performs a\n1:56:51 little it it doesn''t perform as well as some of the other options but you''re able to add more data compared to the\n1:56:59 large model so it''s all a procon but personally I just go with a small model for most of the stuff I work on just\n1:57:05 because it''s the cheapest and you get really good results but the are other options and I def want you to be aware of that okay all right let''s hop back to\n1:57:11 it so what we''re going to do in this case is we''re going to create a new open AI embedding using this new model and\n1:57:18 that''s option one and like I said these open a Bings they''re best just for general purpose with really good\n1:57:24 accuracy and then but remember you have to pay because this is all happening on\n1:57:29 open AI servers now option two what this embedding is going to be is we''re actually going to download an embedding\n1:57:36 model from hugging face hugging face is just a like I''ll just go ahead and show you guys what it is so you can see it in\n1:57:43 action so let me pull up these models for yall real fast so as you can see whenever we head over to hugging face\n1:57:48 which is just a great place to go ahead and download models that other people have created ated locally on your machine so you can run them completely\n1:57:54 for free so you can see there''s a bunch of different models here and we can actually search for embeddings we\n1:58:00 actually already searching for beddings and you can see there''s just a ton of them yeah dozens but in our example what\n1:58:05 we''re going to do is use one of the more common ones which is just going to be a sentence Transformer and all it''s going to do is its main thing is it''s going to\n1:58:13 like I said it''s basically just going to run locally and perform in beddings personally I don''t really use these that much I just want to show you what''s\n1:58:18 possible the main Pro and the reason why you would want to use one of these models from hugging face is because you\n1:58:24 can run it locally for free which is one of the biggest benefits it''s usually not as performative so you''re kind of\n1:58:30 trading performance for cost so it really just depends where you fall on that Spectrum okay so what we''re going\n1:58:35 to do next is now that we have gone off and specify the two different embedding models that we want to use and the\n1:58:41 different names we want to call these different models for our Vector stores we''re going to go ahead and create those databases now you''ve seen this before\n1:58:48 what we''re going to do just pass in the the embedding and this is going to just passing right here so we''re going to set up the vector store name and then we''re\n1:58:54 going to say which embedding to use and that''s how we''re going to go off and create our two Vector stores one for open Ai and one for hugging face now it\n1:59:01 is important to mention whenever you do use this embedding because it is running locally on your computer you''re going to download a pretty big file I think it''s\n1:59:08 half a gigabyte but that''s how big the file is to perform your embeddings locally so it just beware if you''re\n1:59:14 going to download this code and run it it does uh take up a good bit of space and might be even more okay so what\n1:59:20 we''re going to do next is we''re going to do our normal comparison where we''re just going to ask a question to our\n1:59:25 Vector stores and we''re just going to compare the results of open AI to our hugging face example so let''s go ahead\n1:59:31 and go ahead and open this up and we''re going to run our fourth example so four\n1:59:37 because we''re in the rag module fourth example going to go off and run it now what you''ll notice is I''ve already gone\n1:59:43 ahead and actually created these models in the past so it''s not going to generate a new hugging face Transformer\n1:59:50 cuz I''ve already done it in the past but you will see that we are going to be able to query these documents so in our\n1:59:57 open AI example output let''s see what words we get back so we didn''t actually\n2:00:03 get a good example in either one of these so what we can do is update our Vector store and we can actually update\n2:00:10 our K to get back more results clear it run it again and this time we can actually hopefully search to see if we\n2:00:17 got back any information about OD deus''s wife who in this case is penelopy and it''s so funny because sometimes you''ll\n2:00:22 get back an example that does include the right information so you can see now it just changes per run so you can see\n2:00:28 now in our hugging face example we got some information about Penelope we got one of the documents contained\n2:00:34 information about her and then if we go back up you can see that our open AI\n2:00:40 example came back with a few more examples talking about Penelope so overall they both got towards the right\n2:00:45 answer it just open a returned back a lot more relevant information so kind of what we expected it''s more performative\n2:00:51 at the cost of spinning a little bit more M on it okay cool enough with doing our embedding Deep dive what we''re going\n2:00:57 to do next is we''re going to move over to working with retrievers and learning about different search types and\n2:01:02 different arguments we can pass in to our retrievers and this is really going to help up our game when we''re working with different rag applications all\n2:01:08 right let''s go ahead and head over to example number five so welcome to this fifth example in\n2:01:15 the rag module in this example we''re going to be doing a deep dive into retrievers and understanding all the\n2:01:22 different ways we can actually update how we search for different documents inside of our Vector stores so that''s\n2:01:27 the main goal of this quick Deep dive okay so what we''re going to do as a high level just to also understand we''re going to by the end of this example\n2:01:34 showcase how by fine-tuning different search parameters and our retrievers how we''re going to get different results so\n2:01:40 you''re going to learn more about different types of search queries and different ways to basically you know pass in custom parameters for those\n2:01:46 different types of searches and we''re going to be able to at the end of this compare and contrast the different results okay cool so let''s quickly run\n2:01:52 through this one so the main thing we''re going to do our normal part of setting up a accessing our Vector store in this\n2:01:59 case we''re going to reuse our old Vector store which included all of our metadata so this is one that had all of our books\n2:02:05 with the different sources outside that we''re going to use our usual embedding model we''re going to spin up our chroma\n2:02:11 DB instance and then from here what we''re going to do I''m going to close that out and we''re going to come back to\n2:02:16 this query Vector store but what you can see is we''re trying out three or four\n2:02:22 different examples of searching inside of our Vector store so what you''ll notice is let me just show you like a a\n2:02:28 compare and contrast so the first thing is we pass in the name of the vector store that we want to search the query\n2:02:35 so this is like hey what question do you have what type of embeddings are we going to use and then here comes the\n2:02:41 important part we''re going to pass in what type of search basically what type\n2:02:46 of search model we want to use and then any parameters so let''s actually hop into this query Vector stor you can see\n2:02:52 all this in action under the hood so like I said you can see the search type and the search type parameters right\n2:02:57 here so we''re dynamically spinning up and creating our retriever because that''s the main area that we''re focusing\n2:03:03 on in this tutorial we want to compare and contrast all the different options and not only do we want to like access\n2:03:09 these retrievers we want to go off eventually and fetch information with these different types of retrievers so\n2:03:15 that we can compare and contrast the relevant documents just so we can see like oh yeah this search type performs better than this search type okay so\n2:03:22 let''s hop back down to our three examples below so that you can actually see how they work so so far what we''ve\n2:03:28 been using is and the rest of our code has been let''s scroll down so I can show you we''ve been using similarity score\n2:03:34 with threshold basically so this is how we say yep I want to get the top three results and I only want to get back\n2:03:40 information that''s over this score threshold that way we only get back relevant data however sometimes you\n2:03:46 don''t want to actually do that threshold scoring if you know for a fact like yes every question I''m going to be asking is\n2:03:53 relevant to that database you really don''t need the threshold so this is a very quick way to say yep here is my\n2:04:01 data store I want to just grab all the similar results don''t care how relevant they are I but I do want to grab the top\n2:04:08 three results that''s exactly what''s going to happen with this similarity score and there''s no way of really filtering out the like oh yeah this\n2:04:14 one''s not that relevant okay the next one that we want to do is working with\n2:04:19 the max marginal relev an search query so what does this one do why is it\n2:04:25 important well what it does is it tries to not only get the most relevant information but it tries to kind of\n2:04:31 spread out meaning if for example how we''re going to ask about questions about\n2:04:37 Juliet''s death well what this type of search score would do instead of sorry I''m going compare and contrast with this\n2:04:43 similarity score let''s just say all the top three documents about her death were all right next to each other so it''s\n2:04:50 just going to grab all of that information and maybe leave out some additional context and what''s cool about\n2:04:56 this Max margin result is not only is it looking for super relevant information but it''s kind of also looking for\n2:05:02 adjacent information around her death so it''s not just going to be like yeah she died because of blank it''s going to\n2:05:08 maybe skip around to like a few paragraphs later that''s also talking about her death but it might not be the\n2:05:14 most relevant information to help generate a more well-rounded response so very cool search method and you can\n2:05:20 actually see Let''s uh scroll down here so you can kind of see so yeah so K is how many different responses we want to\n2:05:27 get fetch K specify the number of documents to initially fetch so this is you know how we can actually kind of\n2:05:33 specify like we''re going to grab a ton of documents and then inside of that we''re going to return the top three and\n2:05:38 we''re going to like space out those results so you''re going to get a well Diversified set of results so this is\n2:05:44 what you can see kind of right here so feel free to play with this Lambda multiplier like I said the main thing it\n2:05:50 does is control the diversity so if you want a ton of spread out information to get like as much wide range of context\n2:05:56 around the topic as possible you''re going to want to bump that number to zero to get the maximum diversity and if\n2:06:01 you want super similar information keep it to one okay cool so enough of that I''m excited to show you guys this one\n2:06:07 actually running so you can see how it performs and the final one this is the one that we''ve been using the entire time which is just our similarity score\n2:06:15 with a threshold okay so let''s go ahead and actually run this so you can see how it performs and we''re going to actually\n2:06:20 this time because there''s going to be so much information um over here in my terminal and we''ll look at the results over here so what we''re going to do is\n2:06:27 run python this is in the rag section and we are on the fifth example so we''re\n2:06:33 going to go ahead and run this code and it''s going to go off and actually oh my bad I need to actually load in my\n2:06:38 environment variables I didn''t do that real fast we''ll just quickly fix that on the fly so we''ll do load oops\n2:06:46 load. EnV and then once we have that we''ll come over here and import it fantastic that was my bad save it now\n2:06:53 when we run it again it''ll work and it''ll actually go ahead and show all the different results for the different Vector stores okay so let''s run through\n2:07:00 this part by part just so you can kind of see how it works so the first example that we''re going to look at we have to\n2:07:06 come all the way up just because there are a ton of different examples so the first one is just the regular similarity score so you can see it is going to\n2:07:13 return three different documents that are the most similar to our initial quest which was how did Juliet die so\n2:07:20 we''re going to get back a ton of information and then hopefully one of these will actually include the way that she died\n2:07:25 yeah so she stabs her health so yeah you can see document two include the exact pieces of information that we needed so\n2:07:31 the next one that we were wanting to do was the max marginal relevant score and this is the one that allows us to get\n2:07:36 not only the piece of information we want but some of more of the contextual information around her death so let''s\n2:07:42 actually see if this result includes anything around her dagger so this one does talk about you know laying her\n2:07:48 dagger down and it also yeah this one didn''t perform as well as the other one\n2:07:54 so this would be a good way for us to go off and actually potentially like oh maybe let''s just get some less diverse\n2:08:00 information so we really hone in so like I said everything''s a game and you have to optimize and tune those parameters to\n2:08:06 get the results that you''re looking for and the third one which is our usual one which is hey let''s go off and actually\n2:08:12 get everything that''s within a similarity score this one is going to return per usual the one where she she\n2:08:19 snatches a dagger and stabs herself so like I said there''s a few different ways you can work with these different\n2:08:24 retrieval messages retrieval methods to experiment with grabbing information from your vector store so I just want\n2:08:30 you guys to be aware of a few of the most common approaches but enough of that what we''re going to do next is Hop on to the next example where we''re\n2:08:37 actually going to not only be retrieving information now but we''re going to actually ask a one-off question of like\n2:08:43 hey what happened and we''re going to get an AI generated response let''s go ahead and dive into this example\n2:08:50 next so welcome to example number six inside of the rag module I''m super\n2:08:55 excited for this example because you''re finally going to learn how to tie all the information that we''ve been storing\n2:09:00 in our Vector store we''re going to be grabbing that information and passing it over to an llm so we can actually\n2:09:06 finally generate an AI generated response so I''m pumped for you guys to see it so now that you know what we''re\n2:09:11 going to be doing let''s actually dive into the code so you can see how you can start chatting with your data okay so\n2:09:17 per usual what we''re going to do is we are going to go off and grab our Vector store location per usual we''re going to\n2:09:24 be chatting with our Vector store that has all the different options from all the different books what we''re going to\n2:09:29 be doing is use our usual text embedding from there we''re going to spin up an instant of our Vector store from there\n2:09:35 we''re going to be passing in a new question and this one is just going to be hey how can I learn more about Lang\n2:09:40 chain and now that we have our query we''re going to set up a new retriever in this one we''re just going to use the\n2:09:47 similarity search method which is going to like I said just how we just learned about it''s just going to grab the most\n2:09:53 relevant document and it''s going to return only one result and from there what we''re going to do is use the\n2:09:58 Retriever and go ahead and search our Vector store for that query and return relevant documents so it''s important to\n2:10:05 realize that this is a list of documents now in our example we''re actually going to print it so you can see the\n2:10:10 underlying document but here''s where the magic happens what we''re doing under the\n2:10:15 hood is we are generating a r query and we''re not only going to just use our\n2:10:21 query we''re going to pass in the content from our Vector store that we just pulled out combine them and we''re going\n2:10:27 to use our chat models to generate a response so this is going to be awesome and let me just walk you through the\n2:10:33 prompt so you can understand what''s happening at a high level so the first thing is we are creating our prompt and we''re going to say you know here''s some\n2:10:39 documents that might help you answer this question here''s the question now here''s the relevant documents we could\n2:10:44 have probably used a prompt template to do this and you''ll see we''re going to do some of that later on but we could have even done it up here and then what\n2:10:51 you''re going to see is we''re going to join basically we''re just going to do some string manipulation here to where we''re going to grab all the content from\n2:10:57 our relevant document and really just spit it out in a nice text format and if you remember each one of these documents\n2:11:04 because we set this up earlier when we were doing our embeddings each one of these is going to be a th tokens long so\n2:11:09 we''re going to have plenty of space to pass in this information into our query because hey we''re only getting one result so 1K tokens and we have up to\n2:11:17 eight so 8,000 tokens so what we''re going to do is once we have this combined input we are going to spin up\n2:11:23 our new chat model and we''re going to use the latest version of open AI so ct40 and we are going to go ahead and\n2:11:30 generate our messages and what what we''re going to do is pass in our messages with our combined input\n2:11:36 containing our query and all the information from our Vector store into our model and we''re going to invoke it\n2:11:42 and what this is going to do is generate a result for us which is going to contain the AI response and for this\n2:11:48 example let''s just only generate the content only so you can just see the actual like response that it''s going to\n2:11:53 say like well if you want to learn more about langing chain here is my recommendation so what we''re going to do\n2:11:59 is go ahead and run this is module number four for rag this is example number six for a one-off question so\n2:12:06 let''s go ahead and run it so what you can see is it''ll hopefully spit back some relevant documents and then at the end it''ll do the AI generated response\n2:12:13 with information from our documents so this is so cool so let me just show you the AI generated response first and\n2:12:20 we''ll hop back up to the relevant doc so I was a little bit uh a little cheeky and what I did is I put in a document\n2:12:26 about myself inside of the book section and you can see I have something oh\n2:12:31 where did it go for you guys yeah Lang chain demo and what you can see in here is you know hey if you\n2:12:37 want to learn more about Lang chain you can go over here to the official documentation or if you want more\n2:12:42 in-depth tutorials and insights on Lang chain check out my YouTube channel and uh here''s a link to it don''t forget to\n2:12:49 like And subscribe especially if youve made it this far this video don''t forget to uh to like And subscribe if you want to learn more about AI yeah as you can\n2:12:55 see it''s generating an AI response that officially responds to our query in a conversational way and you can see this\n2:13:02 is the exact document text that we were able to like manipulate and turn and use for our AI response and if we actually\n2:13:10 look inside this text demo you can see I put a bunch of information and we got back the part in our response for\n2:13:17 further exploration yeah so in our the way just the way the chunking worked is we got this piece of text and yeah so I\n2:13:23 hope you guys think that is super cool for our oneoff example what we''re going to do next is I''m going to show you how\n2:13:29 you can have a full-on conversation with your rag data so let''s go off and start working on example number\n2:13:36 seven so welcome to example number seven inside the rag module this is definitely\n2:13:42 the most complicated example in this section however it is the most useful\n2:13:47 one and probably the example you''ll use the most often when ever actually building out a rag solution inside of\n2:13:53 your applications for your users okay so what we''re going to do in this one is we''re going to quickly speed through all\n2:13:59 the parts that are similar and then we''re going to focus on the parts of this code example that are different that allow us to actually have a\n2:14:05 conversation with our rag application so that we can you know ask a question follow up with it get additional\n2:14:11 information from our Vector store and just keep going so that''s what we''re striving to do here in this example so\n2:14:16 let''s go ahead and dive in so in part one what we''re doing is per usual we''re just spinning up a Vector store that\n2:14:21 points to all the books we''ve defined what we''re going to do is we''re just going to use our usual retriever which\n2:14:27 is just going to be a similarity one to get the top results for this example we''re just going to use CH gbt 40 as our\n2:14:32 llm okay so now let''s dive into this part of the code that''s all new so the main thing that we''re trying to do here\n2:14:38 I think it''s best if we actually start at the bottom to understand what''s happening so what we''re trying to do is\n2:14:43 we are trying to set up this retrieval chain all this is going to do is we want to be able to retrieve information from\n2:14:50 our Vector store we want to have awareness of all the conversations we''ve had up to this point and based on the\n2:14:56 information from our Vector store and our conversation we want to generate a result that is all we''re trying to do\n2:15:02 and that''s what''s happening under the hood okay so how is this happening well there''s a few things that need to happen\n2:15:09 part one is we need to be able to grab information from our Vector store well\n2:15:14 how are we going to do that well we''re going to be using this library and function called create stuff documents\n2:15:21 chain what does that mean I know it''s a weird term but basically what it''s doing under the hood is it makes a chain for\n2:15:28 us that will take in a list of documents and pass it to a model so that''s what it''s doing under the hood okay so we\n2:15:35 have documents and we need to feed those over to an llm that''s exactly what this Chain''s going to do for us so cuz we\n2:15:42 have open Ai and now you''ll be like okay well where do these documents you know where does this conversation and\n2:15:47 documents come from well let''s keep working back up so next is to create this document chain we also need to\n2:15:54 provide some information about like what the heck''s going on so that our llm is aware of what it needs to do well this\n2:15:59 is where our we are going to make a prompt for us we''re just going to call this the QA prompt so you know question\n2:16:06 answering prompt and we''re just going to say hey you''re an assistant who does question answering use the following pieces of retrieve context to answer the\n2:16:12 question if you don''t know the answer just say you don''t know use three sentences maximum and keep the answer concise so we''re really just going to\n2:16:18 say hey here''s the Act exact piece of information you need to know from this rag query Okay cool so this prompt is\n2:16:25 then you know we''re going to use our prompt templates back from module number two to pass in our basically just pass\n2:16:32 in our QA system prompt here to say this is your system context this is what you should be doing and then we''re going to\n2:16:38 pass in our chat history our chat history will happen later on and that''s as we chat with the our llm we''re just\n2:16:44 going to slowly build out a list of messages and then finally we''re just going to continually add in the human inputs every time the human has a\n2:16:50 question it''s just going to get passed in as an input right here okay cool so now you understand how we have this\n2:16:56 question answering chain so this is going to kind of set up the whole part where we''re like responding to questions\n2:17:02 now we need to have what we''re going to do and call the history aware retriever so what is this well the history aware\n2:17:10 retriever this is where we''re actually going to start passing in and working with our Vector store retriever remember\n2:17:17 retriever is how we interface and pull information from our Vector store and we''re also you''ll see here in just a\n2:17:22 second but we''re going to do something very similar to where we''re going to have our our AI our llm in this case who''s going to be doing the thinking and\n2:17:28 generating some responses we''re going to have a retriever which is pulling down the information from our Vector store and then we''re going to have a prompt\n2:17:34 which is kind of sets the scene for what''s going to happen next so let''s go part by part so this all makes sense\n2:17:40 okay so for this history aware retriever we''ll go start with a prompt so this prompt is saying like hey you''re going\n2:17:45 to have some chat history and it''s up to you to basically interpret what''s being\n2:17:51 said and not answer the question but just kind of provide context for what''s going on so this is all you''re doing\n2:17:58 it''s just you it''s up to you to reformulate the questions so that we can properly search for information inside\n2:18:05 of the vector store so that''s all we''re doing you''re going to get a question just rephrase it for the vector store so\n2:18:10 that we can retrieve the proper information so I hope that makes sense and then let''s just go back and look at the other parts uh the retriever we''ve\n2:18:17 already set up we''re just going to get the similarity score and we''re just going to grab the top three result results and then when it comes to our llm we''re just using chat GPT 40 okay so\n2:18:25 now that we have all of these different things set up we can actually go ahead now that we have our full retrieval\n2:18:31 chain that has two parts our history aware retriever which is pulling information from the vector store and we\n2:18:37 have our qu question answer chain which is actually like taking in user input and actually responding and generating\n2:18:44 answers to our users now that we have this entire retrieval chain set up we''re going to call it our rag chain what we\n2:18:50 can do is start having a conversation with our llm and our Vector store so\n2:18:55 this is the exciting part so I hope you guys like it so what we''re going to do is start asking uh we''re going to go\n2:19:01 ahead and run the program so you can see it in action but I just want to show you at a quick level what''s happening first\n2:19:06 we have a chat history and that chat history was important because we were using it up here inside of our\n2:19:12 contextualized prompt just so we can have a historical reference to what''s being said okay and we''re also using it\n2:19:18 in our question a prompt Okay so so that we''re constantly adding messages to this\n2:19:23 so you can see here whenever we invoke our our rag chain we''re passing in the users query and we''re also passing in\n2:19:30 that chat history very similar to what we did when we were setting up our first conversations with our chat models in\n2:19:35 section one second what we''re doing next is once we get the result what we''re going to do is just print it out for the user to see and then we are going to\n2:19:42 append and update our chat history so that we have our initial query and we get back the result from the AI and we\n2:19:49 continually just add it in a pin it to chat history so that it''s aware and we can have a full-on conversation all right enough talking let''s go ahead and\n2:19:56 dive into the example so you can see that it''s working so what I''m going to do is just go ahead and open up the terminal clear everything out and start\n2:20:01 our conversation so this is in module 4 and this is example number seven so what\n2:20:07 we can do is go ahead and run it and then you know it''s going to start our\n2:20:13 chat conversation for us so we''ll ask the same question we did last time how can I learn more about Lang\n2:20:21 chain what this is going to do is go go off and search through our Vector store and actually spit out a you know a\n2:20:27 response to us so yeah hey to learn more you''re responding to us concisely like we asked using the information from the\n2:20:32 vector store it''s basically consolidating to 3 CES as maximum answering the exact question that''s\n2:20:38 awesome I also just finished Romeo and\n2:20:45 Juliet how did uh she die so let''s just this is kind of going out out on a limb\n2:20:51 not sure if this one will work I just want to show you guys that we can actually have a conversation with it and we''ll come back to my initial question\n2:20:56 just a second so that''s awesome hope fingers crossed it''ll come back and actually respond like yeah she kills\n2:21:02 herself with Romeo''s dagger so the cool part is we have specified I just want to show you something we specified do\n2:21:09 not basically yeah if you don''t know the answer just say you don''t know and what''s cool about this is the fact that\n2:21:15 it''s not making up information it''s pulling information strictly from our rag documents and going from there so we\n2:21:21 can try and ask something else so who is Brandon again and this is where we''re\n2:21:26 actually going to start using some more of the conversational awareness because it''s going to have to go Brandon oh yeah\n2:21:32 this is his YouTube channel so we would expect uh we would expect it to refer up to two messages ago yeah Brandon is the\n2:21:38 creator you can find his videos here so yeah so it''s kind of doing the the full-on conversation part as well as the\n2:21:44 additional retrieval part where it''s actually accessing our Vector store so I hope this makes sense to you guys this\n2:21:49 is like I said by far one of the most complicated examples in this whole module and really this whole course so\n2:21:55 if you have any questions please head over to school it''s a preschool community and pop a question take a\n2:22:01 screenshot and say I''m stuck here don''t understand it or I''ve been building this on my own and I keep getting stuck here\n2:22:06 please help and myself or one of the other developers in the community will try and help you get you unstuck but\n2:22:12 enough of this one let''s go ahead and hop over to the last module we have which is all about web scraping and\n2:22:18 actually using information we get from the the web in our Vector stores so let''s go ahead and dive over to example\n2:22:24 number eight right now hey guys so welcome to the final\n2:22:29 example in the rag module what we''re going to be doing in this video is going over two methods for you to go off and\n2:22:36 scrape information from the web and add it to your vector store so that you can start asking questions and interacting\n2:22:41 with the data and what we''re going to do is I''m going to show you a basic example first so you can see it''s working and show you some of the cons and then we''re\n2:22:47 going to go over to using a tool called fire crawl which is is awesome and want to show you how it''s going to perform a\n2:22:53 lot better and provide much more better results really okay let''s go ahead and dive in and go part byart so the main\n2:22:59 thing that we''re going to see for the beginning is same thing per usual we''re just going to set up our directories and\n2:23:05 set up our file paths and this one what we''re going to do next is we need to start specifying a place where we want\n2:23:12 to start scraping the web in this case we''re just going to go on apple.com and what we''re going to do is instead of using the text loader like we''ve been\n2:23:18 using for everything else we''re just going to use the web based loader which is going to go off and scrape the\n2:23:25 basically scrape a website for us so it''s you know instead of looking through a book and loading it it''s going to just\n2:23:30 go over to a website and load the information so that''s what''s happening under the hood and then everything else\n2:23:35 is going to be identical we''re just going to split the text once we have split up the text into all the documents\n2:23:41 we''re going to start passing over basically those split up chunks plus our embeddings over to a vector store that\n2:23:47 you can see right here so yeah we''re just going to create a new Vector store from scratch and that way we can start accessing it once we have set up our\n2:23:53 Vector store what we can do is start asking questions to it and we''re just going to use a similarity score and this\n2:23:58 one we''re just going to grab the top three results and you know we''re not going to chat with this data at all we''re just in this example we''re just\n2:24:04 going to get back results of like oh yeah these these documents have the relevant information okay so I''m going\n2:24:09 to run this one we''re going to look through the results together and then we''re going to hop over to fir crawl after we kind of go over the pros and cons so let''s go ahead and clear this up\n2:24:17 and then we''re running the you know we''re in the module this is the eth example and this is the basic one so\n2:24:24 what we''re going to do I''m going to zoom out so we can actually go through the results together so it''s going to spit\n2:24:29 out a ton of information so this is just like the first one so you can see that it''s going like okay well U we got five\n2:24:37 chunks from the Apple website here''s the first one where you can kind of see like oh yeah we''re talking about the Apple\n2:24:42 iPhone 15 Pro we''re looking at you know just some of the main key phrases and\n2:24:48 we''re adding all this information to our database so that''s just an example one and then when it comes to actually\n2:24:54 asking questions to our Vector store we said what products are announced it''s just spitting back this information so\n2:24:59 it''s just like it''s saying Apple intelligence it''s saying iPhone 15 iPad Pro so it''s it''s really not like saying\n2:25:05 what''s new just kind of spitting out what''s on the website and uh yeah just going to keep doing that and just return\n2:25:11 a bunch of a bunch of information not super helpful so but it did work we can actually interact and it did a good job\n2:25:17 of web scraping so plus plus on both of those but the important part is I want to show you example number two which is\n2:25:23 using fir crawl just so you guys can see and compare and contrast the different results now fir crawl it is a free tool\n2:25:30 that you can use however they do have like you know they have some free usage for you however you do have like a paid\n2:25:37 version so let me just show you yeah so they do have like a free plan and a hobby plan but once you see how cool\n2:25:42 they are like like some of the results I think you like dang if you''re doing a lot of web based scraping this is the tool for you make sure like super easy\n2:25:49 their key you know key slogan is like hey we turn websites into llm ready data so like you know here''s a website that''s\n2:25:55 gross oh we''ll actually pull like a markdown version of this data for you so that it''s easy to use and interact with\n2:26:02 inside of your llms so enough talking about let''s actually go ahead and and use it and the one thing I do want to\n2:26:07 point out is once you''ve signed up a major free account you will have to copy and paste this API key over to your\n2:26:13 environment variables folder that we have or file that we''ve set up and you''ll just go ahead and paste it in there so you''ll just update the fir\n2:26:19 crawl and Pi just put your key in your you''ll just put yours right here okay cool enough of that let''s go ahead and\n2:26:25 actually start running this so you can see how this fire crawl example compares to the other one and the only thing that\n2:26:31 I''m going to just say different uh cuz some of this code is like well this is weird what''s what''s happening I will run\n2:26:36 through it real fast you know set up your create your new persistent Vector database for your vector store we''re\n2:26:42 going to call this one fir crawl and this time because we can''t just do web loader which goes off and does everything for us we actually have to uh\n2:26:48 be a little bit more specific using the fir craw loader tool so this is a package that you can install and if you\n2:26:55 actually go back to our py project. tommo you will see that we have where is it down here yeah you will see that we\n2:27:01 have a fir crawl package so this is how we''re accessing this tool but hey we''re going to use the fir craw loader here''s\n2:27:06 our API key so you can validate that we are allowed to do this here''s the website I want you to go scrape and they\n2:27:11 have a few different modes ones I think called crawl and one''s called scrape crawl just go to a whole website and\n2:27:17 mode goes through like a single page so I would defin I''d use scrape to start off with CU you can easily blow all your\n2:27:23 tokens if you crawl so once you have scraped that website we''re going to load all that information like we normally do\n2:27:30 and what we''re going to do in this one is we''re actually going to chunk through that document that we have pulled out\n2:27:36 and we''re going to add a lot of metadata that you''ll see later on so it''s actually pretty cool this the setup they\n2:27:41 have and then what you''re going to do next split everything up into different chunks pass over those chunks plus the\n2:27:48 embeddings over to to a new Vector store and once we have that Vector store set up we can start asking and making\n2:27:55 queries to it okay so enough of that let''s go ahead and start you know let''s go ahead and start testing it out so\n2:28:01 what we''re going to do in this one we''re going to do Python and the rag module this is the eighth example and this is\n2:28:06 fir crawl so we''re going to go ahead and run it and it''ll take a few seconds to go off and scrape the website beginning\n2:28:13 to crawl the website and I might need to update my question just because uh wwc 24 was a little bit ago and uh I''m sure\n2:28:21 a lot has happened since okay so the main thing is we grabbed 14 chunks of information compared to you know\n2:28:28 whenever we were running the other example we only grabbed you know three or five so we''re grabbing a lot more information because the fir crawl does a\n2:28:35 much better job of actually you know pulling out information and it actually I don''t know how it works under the hood\n2:28:41 but they do a much better job of like getting around like a lot of websites only load HTML whenever you try to use a\n2:28:47 tool like web loader they only give you h HML so you''re missing out a lot of the JavaScript code that gets rendered so\n2:28:53 that''s how we''re grabbing so much more information when it comes to fir craw so under the hood you can see now whenever\n2:28:58 we ask you so you can see in a sample chunk that like hey we grabbed well for whatever reason it chunked it too much\n2:29:04 but you can see whenever we look at like relevant documents we can start to see some of the updated questions that are\n2:29:11 related now it didn''t do a fantastic job but let''s update it to talk about Apple\n2:29:16 intelligence tell yeah whenever I wrote this first query I made it for uh basically I did it back\n2:29:23 in the day whenever wwc 24 was about to happen so now we''re going to do a second question and this time what it''s going\n2:29:29 to do is provide information strictly about Apple intelligence so yeah document two all about Apple\n2:29:35 intelligence so like I said much cooler you can actually see a lot more of like\n2:29:41 the links now if you want to go off and actually like interact and actually pull this information and using a sales page\n2:29:47 isn''t the best example using things like like Reddit or places like that where you want to actually like pull actually\n2:29:53 a lot of content from like a lot of users conversation that''s a great place to try out fir crawl so but enough of\n2:29:58 that I hope you guys enjoyed seeing how you can actually start pulling information from the web you can go\n2:30:04 really deep into a rabbit hole when it comes to this but just for now when it comes to like scraping Basics this is\n2:30:09 plenty to get you guys started but yeah enough for module number four where we learned about Rag and the next step\n2:30:16 we''re going to head over to start working with agents and tools and this is going to blow your mind so let''s go ahead and\n2:30:22 head over to module number five hey guys so welcome to this fifth\n2:30:27 module in this Lang chain master class in this module you''re going to learn everything you need to know about using\n2:30:33 agents and tools and before we dive into actually going through all the awesome code examples that I''ve set up for you\n2:30:38 guys what we''re going to do at a super high level is go through what the heck are agents what are tools how do they\n2:30:44 relate and I will say before I like actually dove in to work with agents for the first time I thought they were going\n2:30:49 to be this super magical thing that I would never understand it was going to be super overwhelming but it really\n2:30:54 wasn''t once you U now that you''ve gotten this far inside the master class and you have a really good understanding of prompts and you have a good\n2:31:01 understanding of chat models and so forth this isn''t going to be too complex so stick around I think you guys are\n2:31:06 going to be like oh that''s actually pretty easy to understand so let''s dive into it what is an agent well under the\n2:31:11 hood an agent is nothing more than an llm that has been provided a specific prompt to guide Its Behavior so the best\n2:31:19 way I like to think of it is a state machine and if you''ve never heard of a state machine it''s basically just a machine that has certain States and it\n2:31:25 can perform different actions at each state so let''s just say it''s at State one well at State one it can do\n2:31:31 something else and once it''s done with that it goes to the next state and basically it will just perform a certain Loop of task and that''s basically what a\n2:31:39 state machine is in the same way that''s exactly how agents work we are creating a prompt that defines certain States and\n2:31:45 behaviors for our agent to do and what it''s going to do is just flow through all of those different states and at\n2:31:51 each state perform a different action so let''s walk through it in this actual diagram and we''re going through it part by part so the first thing that most\n2:31:57 prompts have inside of an when they Define our agents is they usually have an action State and this action state\n2:32:02 says well this is where you can actually perform an action so think of it if we''re creating a writer agent well the\n2:32:09 whole point of that agent is to write so usually we''ll say our end goal is to write a bullet list about what''s going\n2:32:15 on in the news today well this agent will know it needs to write a bullet list so let''s just say it starts off\n2:32:21 with our input and just talks about the recent Apple event and it writes a bullet list well then what it''ll go is\n2:32:26 go to the next state where it can make observations about its actions and from here what it''ll do is go that was a\n2:32:33 little weird you only produced four bullet points then we''ll come back down to the thought and when we are in the\n2:32:39 thought stage this is usually we''ll plan out plan is the key word here we usually plan out what upcoming actions or\n2:32:46 behaviors we need to do next so in this case we''ll go oh for whatever reason we only had four bullet points I need to\n2:32:52 rewrite my article or my bullet list and make sure I have five bullet points this time so let''s add one more and then\n2:32:59 we''ll just go back up action we''ll update our bullet list and have one more so we''ll hit a total of five observation\n2:33:04 phase yep we have all five bullets things look good and then now that we get back to our thought stage you know\n2:33:10 we''re going to say everything looks great we look like we''re done and our thought phase will go okay cool I no longer need to work as an agent here''s\n2:33:16 my final answer and that''s a super simplified version but really under the hood that''s exactly what''s happening an\n2:33:22 agent gets a specific prompt with an llm that tells it how to behave we pass in a\n2:33:27 goal and it''ll just work towards that goal and just cycle through these stages and there''s a bunch of different types\n2:33:32 of Agents but this is the core Loop that you''ll hear about the most right here okay now what the heck is a tool why do\n2:33:38 we care about uh care about it when it comes to agents and I when doing this course at first I thought about breaking\n2:33:43 them up but I didn''t just because they''re pretty much useless without each other and you''ll understand why in just\n2:33:48 a second so a tool is nothing more than a basically some usually some code and\n2:33:54 that provides our agents with additional functionality so basically these tools provide our agents with superpowers is\n2:34:01 the best way I like to think about them we give them upgrades is the best way I like to like kind of think of it my in my head so a few common tools that\n2:34:08 you''ll hear of are tools that will allow you to search the internet so think tavali serper duck ducko there''s a ton\n2:34:15 of these tools and they allow our agent to interact with the outside world cuz if you remember these llms they have\n2:34:21 constraints they''re just you know U they have cut off dates and they basically you know can only think thoughts and\n2:34:28 write text and this is how we allow them to interact with the outside world so that''s what the search internet tool can\n2:34:33 do from there we could also set up our agents to actually execute code so we\n2:34:38 can hook them up to an interpreter to like oh here''s the python code let''s go over here oh yeah you can actually now\n2:34:45 write and draw some you can you know run some code you can generate some graphs with plot Le like you can do a lot of\n2:34:51 stuff when it comes to having your agents interact with code the final one you could also have tools that allow\n2:34:56 your agents to go interact with databases so obviously R so far we''ve kind of like hardcoded and wired up R in\n2:35:03 the past with in our rag section to work with a vector store but you can actually set up tools to allow your agent to\n2:35:09 interact with a vector store a SQL database like there''s a ton that you can do so it''s just up to you to create\n2:35:15 those tools and then give those uh tools to your agents and so just give you an example if we were trying to you know\n2:35:22 find today''s news find today''s like top five most talked about articles and then\n2:35:28 plot them inside of a a python plotly chart for like how correlated they were\n2:35:33 something silly but basically you know we could do that with tools because going through the same Loop we would\n2:35:38 understand first I need to go off and find all the news articles and find the most talked about ones well whenever I\n2:35:45 take that action I''ll search the internet come back I now have information about the outside world\n2:35:50 thought hm okay well now that I have information about the outside world I need to start mapping the information I\n2:35:56 found to a to some sort of chart where we can visualize how correlated everything is just you know Silly\n2:36:01 example but when now we come back to the action step and goes okay well I now know the five most talked about articles\n2:36:08 now I can start actually generating code to make a visualization out of this\n2:36:13 information so then I''ll start executing code oh NOP it didn''t work it didn''t properly run go through it a few times\n2:36:20 okay great now everything''s working here''s all the python code so you can generate some graphics so that''s exactly how it works under the hood and you''re\n2:36:27 just going to keep going through this core Loop and we''ve now supercharged our agent with tools to go off and interact\n2:36:32 with the outside world and actually start taking some action so that''s enough of everything at a high level what we''re going to do in the rest of\n2:36:38 this example is I''ve provided about five to six different code examples that you''ll see where we''re actually like\n2:36:43 code everything up that we just talked about so enough talking let''s go ahead and dive into our first example so you can see all this in\n2:36:51 action all right guys so welcome to the first example inside of the agent and Tool module now what we''re going to do\n2:36:58 inside of this example is just walk through everything at a super high level because this is going to be the first\n2:37:04 time we''re going to be introducing agents and tools inside of actual code so we''re going to go through it super slowly and then after that we go through\n2:37:11 this example we''re going to later do a deep dive into agents so you can understand all the different ways we can\n2:37:16 use them and then we''re going to do a deeper dive into tools so you can learn learn how to use existing tools and learn how to create your own so that''s\n2:37:22 what we''re about to do let''s go ahead and dive into this part by part so you guys can Master agents and tools all\n2:37:28 right so some of this is going to be super similar to start off we''re going to be loading our environment variables because we need to use open AI now the\n2:37:36 first thing that we''re going to do in here is go off and create a tool and for this one we''re just going to create a\n2:37:41 super simple tool that allows us to access the current date and time for our local computer CU if you think about it\n2:37:48 our llms you they were generated in the past they have constraints of and cut off dates and they actually have no way\n2:37:55 of knowing what time it is currently so whenever we''re actually using these llms in Agent form we can add a tool time so\n2:38:03 that our agents can access our current time this is going to be super helpful as you go off to build larger and larger\n2:38:09 agents where they need to interact so this is just a super basic one so let''s walk through what''s happening so the\n2:38:14 first thing is we''re going to create just a regular function that all it does is it reports date time because date\n2:38:20 time is just a standard library and we''re just going to grab this the current time and then we''re going to\n2:38:26 return the current time in this format to where it''s hour minute minute so that''s all we''re trying to do and that''s\n2:38:31 the whole purpose on this function we''ll dive later into why had to set up our parameters like this for the function\n2:38:37 but we''ll come back to that later all right cool well now once we have defined our specific function that we want to do\n2:38:43 we have to wrap it inside of a tool and later we''ll get into actually like using existing tools but for right now this is\n2:38:49 just like how you can make your own simple tool with your own code but basically all you do is you call the tool class which comes from up here\n2:38:56 blank chain core tools that''s how we can create a tool and it''s just up to you to give your tool a name a descriptive name\n2:39:02 about like you know what does this tool do that way whenever the agent''s executing and it''s like oh I need to\n2:39:08 solve a Time problem oh it makes sense for me to use the tool that talks about time so that''s why it''s super important\n2:39:15 for us to give the name and description to be very representative of what what going to happen under the hood from\n2:39:21 there we pass in the function that we want the tool to perform whenever it gets called on so in this case whenever\n2:39:27 we say like hey give me the time under the hood it''s going to trigger this function which is going to return our\n2:39:33 string representation of the current time okay so that''s how we make a tool that''s how we add it to our tools list\n2:39:39 and once we eventually have our tools list we can actually pass this over to our agents and you''ll see that here in just a little bit okay so now we''ve got\n2:39:46 tools out of the way at a super basic level now we''re going to work on creating our agents so if you look at\n2:39:52 this you might be like what the heck are we doing what are we what are we pulling what is this word react well uh react\n2:39:59 stands for reason and action and all we''re doing here is we are pulling out a\n2:40:04 prompt because you remember from earlier earlier from prompt templates all we''re doing is pulling out a prompt template\n2:40:10 that tells our llm how to act so if you actually go and read what this uh what''s\n2:40:17 going to happen at this prompt you''ll head over to to a website like this I''ll actually copy the link for you guys so\n2:40:22 you can read it yourselves but you can see kind of how we talked about earlier an agent''s nothing more than an lolm\n2:40:29 with a super specific prompt telling it how to behave so this is exactly what''s happening under the hood this prompt\n2:40:35 template that we''re going to create an agent around you know it''s just like hey answer the following the best you can\n2:40:40 here''s the tools you have access to and then that Loop that we kind of talked about earlier where you''re taking action\n2:40:46 you''re making observations planning out thoughts and providing a fin answer this is exactly what you know what this\n2:40:52 prompt template is telling you to do so you know hey use the following format to basically perform actions so you know\n2:40:59 taking the input think about it take action here''s the you know like as we''re\n2:41:04 working with tools sometimes when whenever we''re taking an action like get time well sometimes we might have to\n2:41:11 pass it in parameters to our functions so like I want to get the time in Tokyo\n2:41:16 I want to get the weather in San Francisco so that''s what actions like you know use the tool and then action\n2:41:21 input is like whatever parameters we want to pass over to it so just hope hopefully that makes sense observation\n2:41:27 hey look at the results of the action thought you know go go off and here''s what I need to do next basically plan\n2:41:33 yeah so and you can kind of see we''re telling our llm to repeat this over and over and over until we get a final\n2:41:38 answer okay so that''s what''s happening under the hood and we''re actually able to like the part that''s nice is it''s\n2:41:45 kind of like GitHub where we''re able to just pull down and reuse other people''s code that''s working but in this case\n2:41:51 we''re just pulling down other people''s prompts which is pretty cool if you think about it okay so now that we have our prompt which is going to tell our\n2:41:57 llm how to act we need to keep chugging along and we actually need to specify which llm we want to create our agent\n2:42:04 around and this time we''re just going to use cat gbt 40 that''s the latest open AI\n2:42:09 model that''s out at the time of this recording and then from there what we''re going to do is go off and actually use\n2:42:15 the agent we''ve defined we''re going to pass in the prompt that we fine and we''re also going to pass in all the\n2:42:21 tools we''ve set up to actually go off and create our agent so if you look under the hood what''s actually happening\n2:42:28 is this create react agent it''s coming from the the Lang chain agents repo so\n2:42:33 I''ll just scroll up to the top so you guys can see it yeah this is coming all from Lang chain agents and we''re really\n2:42:39 just importing these two classes to create our agents and run them but the main thing for our agents is we''re\n2:42:45 sticking to that react part to where we are going to you know reason about what we want to do take action and just\n2:42:51 continue that cycle over and over in order to actually achieve our goals so this is kind of what what it looks like\n2:42:57 under the hood if you want to actually dive in here and read more about what''s going on but really we''re just combining\n2:43:02 all the ingredients of everything that''s needed to make an agent and just putting it into one one class so that we can\n2:43:09 actually start passing information to it and running our agent and that''s exactly what we''re going to do next once we have\n2:43:14 our agent it''s up to us to then pass in everything over to an agent executor so\n2:43:21 an agent executor is just basically going to help manage the Run of an agent as it goes off to solve problems so if\n2:43:28 you come over here you can see an agent executor we''re just going to say like hey this is allowing us to use tools and\n2:43:34 and if you actually dive into the code here too you can see this is where we''re going to be like actually performing The\n2:43:40 Run and accessing the tools putting information back and forth and and continuing to go from there but really\n2:43:45 don''t need to like dive into it the main thing is just understand that like yes whenever I want to run my agent I need\n2:43:51 to use an agent executor Okay cool so once we have our agent executor set up what we can do is once again use our\n2:43:57 magical word when it comes to Lang chain and we need to invoke our agent executor to start actually spinning everything up\n2:44:04 and what you''ll notice is instead of just passing in a string we''re passing in a dictionary and the keyword in our\n2:44:11 dictionary is input this is the main thing that our agents expect to read is\n2:44:16 an input so in our case what time is it is the question that our agents are going to be answering so enough of that\n2:44:22 what we''re going to do is let''s go ahead and run it so you can see all of this happen in real time so let''s open up our terminal and we''re going to run python\n2:44:30 this is the fifth module because we''re learning about agents and tools and this is the first example so let''s go ahead and run it and see what happens so what\n2:44:36 it''s going to do as you go off and actually use agents you''ll see a lot of colorcoded text usually coming out of\n2:44:42 the output as it''s going through that reasoning taking action and making observations so you''ll usually see things like purple green and white and\n2:44:48 this is is uh anytime the agents''s performing an action you can see like all right what''s the action I want to\n2:44:54 perform Let Me zoom in for you guys so what''s the action I want to perform I went to get the time so that''s pretty\n2:44:59 cool that it was able to look through all the available tools that it had and pick the correct one and then what it\n2:45:04 did next is it passed in an action input and because we didn''t specify any\n2:45:10 parameters for AR tools it just gave in none but you''ll notice sometimes when you''re creating tools and we''ll dive\n2:45:15 into this here in a little bit if you do not have any parameters for your function sometimes they''ll mess up so we just\n2:45:21 accept all parameters for arguments and keyword arguments but we just don''t do anything with them so that''s just a quick work around but what''s cool is\n2:45:28 once we you know perform the action pass in the action input you can see here in blue we get the local time back for when\n2:45:35 I''m recording right now and then now that we have that answer from our tool the agent goes I now know the final\n2:45:42 answer and once it knows it''s the final answer because if you actually come back over here the thought is I know I now know the final answer it then gives us\n2:45:49 back the final answer so then that''s super cool cuz you can see right here it said the current time is 8:31 p.m. so\n2:45:55 that''s the agent thinking and then finally under the hood I''ve said that 100 times right now but what''s happening\n2:46:01 is it finally now that it has the answer it generates a object that it returns\n2:46:06 back to us and you can see this object contains nothing more than the original input plus the output of the final\n2:46:12 answer from this agent so that''s what''s happening and what we''re going to do next is um and I hope you guys first off\n2:46:17 I hope you think that is aome awesome because it''s very cool that we can have agents reason and take action in the\n2:46:24 world and uh you know everything we''re going to do from here we''re only going to add in more complexity and show off\n2:46:29 cooler features just so that you guys can match uh Master agents and tools and just to dive in what we''re going to do\n2:46:34 next like I said we have two different folders for you guys to go over all the different ways that we can use agents\n2:46:40 and tools and the first one we''re going to do is we''re going to do a deeper dive into agents so that you can understand\n2:46:47 different ways that we can like figure these agents to work and eventually we''re going to hop over to showing you\n2:46:52 how to use more custom tools so let''s go ahead and head over to our first agent Deep dive example and start looking at\n2:46:58 react chat hey guys so welcome to the second example in this agent and tools module\n2:47:06 and in this example we''re going to go a little bit deeper into working with different types of agents and really\n2:47:12 expanding what they''re capable of doing so in this example we''re actually going to focus on swapping up the underlying\n2:47:19 prompt that we''re using in the agent sprinkle a few more tools and then kind of add some chat capabilities to it so\n2:47:26 that our agents can now go off and do some internet searching for us and give us answers so let''s go ahead and dive\n2:47:31 into this example so you can U understand what''s going on okay so to start we''re going to do the exact same\n2:47:38 thing we did last time but we''re going to add in some more tools so this time we''re going to do another get current time tool exact same thing and then next\n2:47:45 we''re going to add in the search Wikipedia tool and this this is really just a function to go off and use the\n2:47:50 Wikipedia library and what it''s going to do is basically get a summary about whatever topic we''re interested in so we\n2:47:57 get ask a questions about famous people events times and Wikipedia is going to give us back a quick summary you know\n2:48:03 two sentences of information it has on that topic okay so now that we''ve defined those functions we now need to\n2:48:09 package them into our tools list so and we need to format everything to be in the proper tool class so that''s where\n2:48:16 we''re going to set up the name a description so that our agent can go oh yeah that''s the tool I want to use and\n2:48:22 then the actual underlying functionality we want to trigger cool so now that we have our tools defined for our agents we\n2:48:27 can now move over to actually creating our agent and what you''ll notice in this example is we''re using a different type\n2:48:35 of agent this time for our prompt excuse me so last time we were just using the react agent but this time we''re using\n2:48:41 the structur chat agent so what this prompt is focused on it''s actually having a chat so if we head over to uh\n2:48:49 over here where we actually can see the underly prompt template it looks a lot different than the last one so you can\n2:48:55 see like hey you''re responding to a human with helpful information you have access to the tools in our tools list\n2:49:01 and I need you to basically create a a Json blob for these tools to go off and perform actions that''s enough of that\n2:49:08 and then what''s going to happen next is like all right cool here''s you''re going to search for a question go off and\n2:49:15 basically have thoughts about it take action so this is where we''re going to use that Json blob and this is basically\n2:49:20 going to be like you know information from the past that we''ve taken like so whenever we get a response back we''re\n2:49:26 going to save everything as a Jason blob and I''ll keep it at that but the main thing is it''s going to allow us to have\n2:49:32 a conversation with our agents and it''s going to go off and you know use tools to perform actions on our behalf okay so\n2:49:38 that''s what''s happening under the hood now what are we going to do next per usual we''re going to uh spin up a chat\n2:49:44 model I''ll actually go ahead and make sure we''re using the right model so we''re going to GPT 40 make sure using\n2:49:51 the latest one and then from there what we''re going to do is we''re going to use this new we haven''t used it before but\n2:49:57 it''s called a conversation buffer memory all it does is it allows us to store our\n2:50:03 chat history in in memory that''s all that''s happening so in the past we''ve done things such as you know we''ll just\n2:50:10 do like chat history and we''ll save it to a list well this is just a more and\n2:50:15 you know we''ll always add in our system messages and human messages well this conversation buffer memory it\n2:50:21 just it does a better job of doing it so uh we''re just going to use it for this one it makes things simpler to set up\n2:50:27 for us okay well now that we have everything initialized let''s start combining things to go off and create\n2:50:33 our agents so that we can start running them so per usual we''re going to create our agent and we''re going to pass in the\n2:50:39 main ingredients which are going to be our prompt our tools that we defined in the specific llm we want to use in the\n2:50:45 underlying agent and then from there what we can do is create our agent executor and this is going to be you can\n2:50:52 see we''re adding in more tools to our agent executor this time these are the same as last time our agents and tools\n2:50:59 but now we''re going to add in memory to our agent executor and this is how we''re going to keep track of our previous\n2:51:05 responses and messages with our agent because we''re going to be talking to this one and what you can see from here\n2:51:11 is we''re now going to now that we have our agent executor been up and it''s ready to run what we''re going to do is\n2:51:17 start go ahead and start our chat conversation with the user so in our case we''re going to start chatting with\n2:51:22 our agent so this is going to look exactly like our initial chat example we did except now we''ve supercharged it\n2:51:28 with an agent that''s not just talking to a vector store it''s going off and searching the internet for us to get\n2:51:33 responses so this is super cool and I hope you guys can see the value of it so let''s go ahead and hop down here and\n2:51:39 start running our agent and actually seeing how it works so I''m going to make this a lot bigger because you''ll see how\n2:51:47 cool it is cuz it''s going to actually show us behind the scene what the agent is thinking and before I trigger it I do want to show you guys one thing let''s go\n2:51:54 ahead and get the code ready so agent Deep dive and this is example number one the thing that per usual we''re going to use the magic word when it comes to\n2:52:01 running Lang chain tools so the agent executor and per usual we do have to pass in the input and you''ll notice as\n2:52:08 you use these agents tools a lot more everything''s kind of structured as an input and an output so that''s how we can\n2:52:13 show the response from the AI we''re just going to say yep grab the output and per usual when the user submits a message\n2:52:20 we''re going to add a human message and whenever the AI generates a message we''re going to add an AI message okay let''s go ahead and run it so you can see\n2:52:26 what''s happening under the hood so we''re going to start off asking a question we''re just going to say who is George\n2:52:32 Washington and this will go off and use Wikipedia so that''s cool it found the proper tool to use and as you can see it\n2:52:38 passed in the action input of like well who are we trying to query about and it''s so cool that the AI went from a\n2:52:46 question like who is George Washington to picking out the core topic of that question and then passed it as the\n2:52:53 action input so it''s amazing that this AI can figure out based on what parameters we need to pass in yeah just\n2:52:58 grab the core part of it so that''s super cool now in the yellow we''re actually getting back a response from our tool so\n2:53:05 our tool from Wikipedia says you know like hey here''s everything you need to know about him and so forth and so forth\n2:53:12 and then once we have the final answer it generates a response to us so you can see the bot came back and said yep give\n2:53:19 us everything that Wikipedia said so uh let''s see we can actually ask because this is a conversation we can add to our\n2:53:26 original question so how old was he when he died so because this is a\n2:53:32 conversation we can actually refer to our previous messages so in this case it didn''t even have to go off to Wikipedia\n2:53:38 this time because it was like oh wait already know the information so I can just use use that to respond and we can\n2:53:44 ask other things such as who is Elon mus and how old is he right now so this\n2:53:51 question is a little bit more interesting because it''s a two-part question first you have to go off and figure out who is the main person that\n2:53:58 we''re talking about so in this case it''s Elon Musk for whatever reason it''s kind of struggling right now to to find out\n2:54:04 who he is but usually this is just the part of Agents where it''s just going to go in a loop until it finally gets an answer for whatever reason it couldn''t\n2:54:10 figure out that one so we''ll just ask a different question because I do want to show you guys the key key underlying part who is Steve Jobs\n2:54:19 and how old was he when he died and the main reason I''m doing these two-part questions is because I want to show you\n2:54:25 guys how these agents can understand what we''re trying to do yeah and for some reason I think Wikipedia is just\n2:54:31 crapping out on me but the main thing to know is what would happen under the hood normally is these agents would go off\n2:54:38 and find the result for the first part which is who is Steve Jobs and once it knows the answer it would then go off\n2:54:43 and actually find a you know using the response it would then trigger the next part and the question so you can see\n2:54:49 like yep he''s an American entrepreneur obviously known for founding apple and he died at the age of 56 so that''s\n2:54:56 that''s very cool that I can do a two-part question and you can even go deeper to where it triggers off a second\n2:55:01 Wikipedia call but I''m not going to go into that for this example so I hope you guys are like wow these agents are\n2:55:07 powerful they can act on my behalf and go you know find information inform for me and I can just talk to them so I hope\n2:55:13 you find that super cool and what we''re going to do next is dive into the next agent example that I''ve set up for you\n2:55:19 guys where you can actually talk to a document store so we''re going to work on this one now hey guys so welcome to the third\n2:55:26 example in this agents and tools module in this example we''re going to be diving into how we can connect our agents up to\n2:55:34 a vector store so that they can kind of work together to answer questions about our data so that''s exactly what we''re\n2:55:39 going to be setting up in this example so let''s go ahead and dive in and what you''ll notice is a lot of the information that I''m going to be showing\n2:55:45 in this module was built off of everything that we''ve kind of done in module 4 with Rag and all the previous\n2:55:51 modules before that so the log this is going to look super familiar so I''m not going to dive super deep into it but to\n2:55:56 start off we are going to be setting up all of our file path directories so we can point to our Vector store so this is\n2:56:03 the same Vector store that we did earlier that read all those different books for us so we''re going to go off\n2:56:08 from there set up our embedding so that we can actually you know uh whenever we ask a question it we can embed it and\n2:56:16 compare it to all of our other documents to you know grab the most similar answers so we''re going to then spin up our Vector store with our Vector\n2:56:22 database and the specific embedding function we want to use and then we''re going to use the exact same retriever\n2:56:28 that we used the entire time in the past we''re just going to use the similarity one this time which is just going to\n2:56:33 grab the most similar results not worry about a threshold and in this case we''re going to grab the three top results and\n2:56:39 if you remember we used a 1,000 tokens per result uh per document so this is\n2:56:44 going to give us 3,000 tokens worth of information for our agent to use all right let''s keep chugging along from\n2:56:50 here we''re going to start working on creating our agents or sorry llm so in this case we''re just going to use Chad\n2:56:56 gbt 4.0 and then now we''re going to be kind of combining two different examples so in the past we had an example where\n2:57:04 you could ask specific questions about the doc store to the vector store that we''ve set up and but now we''re going to\n2:57:09 be combining it with our agents so this is the exact same demo we did before where we kind of first set up like you\n2:57:16 know just like contextual what we''re doing here in the first place so like hey you are working with chat history to\n2:57:23 solve an answer that''s all you''re doing and the second part was the history aware retriever so this is like hey go\n2:57:29 look at the previous questions that we''ve worked with plus you can look at the uh you can use the retriever to go\n2:57:34 off and answer or grab information from our Vector store so this is definitely going to go a little in the weeds but we\n2:57:39 already covered this in detail in our previous rag example right here where we did rag conversation so you''ve already\n2:57:46 done this in the past we''re just now building on top of it okay cool per usual we we''re just going off and\n2:57:52 creating stuff from our document chain this is how we can actually if you go under the hood this is how you can go\n2:57:58 about passing a list of documents over to a model so that they can be processed once we'' set up all that we can set up\n2:58:04 our retrieval chain which will basically be able to interact with our Vector store okay enough of talking about\n2:58:10 setting up our Vector store now it''s time for us to go off and set up our agent so we can have our agent\n2:58:16 communicate with our Vector store on on our behalf and perform actions and and lookups information so in this one what\n2:58:22 we''re going to do is we are going to use the same react agent that we did in the first time which is just going to it''s\n2:58:27 going to think about stuff take action make observations and just keep performing that in a loop and per usual when we''re working with agents we have\n2:58:34 to set up a tool well this time we''re going to set up a Custom Tool and what''s interesting is under the hood this tool\n2:58:40 is going to do is it''s actually going to invoke our rag chain so what''s very cool is now anytime our agent has a problem\n2:58:48 where it needs to answer a question what it''s going to do is it''s going to go well I don''t know the answer to that question but you know what I bet this\n2:58:55 tool does because it''s useful for whenever you need to answer questions about the context of whatever the\n2:59:01 question is so what''s cool is all we''re going to do here is this we''re going to invoke this rag chain and what we''re\n2:59:07 going to do to it is we''re going to pass in the input so it''s going to be the person''s question but we''re also going\n2:59:12 to pass in the chat history so that we can have some uh contextual awareness of\n2:59:18 previous messages so that''s super cool so now that we have made a new tool for\n2:59:23 our agent we''re going to go through the normal process of setting up and creating our agents so that we can\n2:59:28 actually start running them so now that we have all of our agents set up with a proper tooling we can now go into another while loop and in this while\n2:59:35 loop we''re going to once again start chatting to our agent so I''m going to come down here we''re going to clear\n2:59:40 things up and now we''re going to actually start running this example so you can see it in action so we''re going to do python this is the agents and\n2:59:48 tools module we''re working inside of the agent deep dive and we''re going to start using the react doc store so let''s go\n2:59:55 ahead and run this one and you can see so we can start asking questions and what''s nice already at the gate we are\n3:00:01 you know accessing the vector store so because there was that one file I set up where I talked about Lang chain that''s\n3:00:07 the first question I''m going to ask so how can I learn more about Lang\n3:00:14 chain so what it''s going to do is it''s going to well this is a question I need to get an answer about and then what\n3:00:20 it''s going to do is actually trigger the AI to go off and get that information\n3:00:25 and what''s cool is like it responded with the exact part of like yeah this is you know to learn more about Lang chain\n3:00:31 that what the document said was like yeah go watch Brandon''s YouTube channel here''s a link to the YouTube channel so\n3:00:37 as you can see that''s pretty freaking cool and then you can follow up because it is a conversation so you can say who\n3:00:42 is Brandon and then it''ll it''ll go off and actually you know say oh he has a\n3:00:48 YouTube channel where he talks about like AI does not mention the name Brandon oh I guess I should have\n3:00:54 capitalized it so so yeah that''s how it works at a super high level when it comes to working with the dock store\n3:01:01 this example was a little weird but I just wanted to show it to you guys just so you can understand like oh yeah you can use you can really start connect\n3:01:07 these agents with different basically different tools and use these agents in different ways and I would do want to\n3:01:12 show you guys real fast when it does come to the agent Deep dive I do want to show you whenever we set everything to\n3:01:18 verbose verbose normally yeah so here in the agents and tools I just want to show\n3:01:23 you guys when this is actually going off and grabbing the information so we''ll do this here for both and I''m going to\n3:01:29 rerun this example just so you guys can see that it is it''s actually grabbing from the doc store because I I like to\n3:01:34 see the agent think that''s one of my favorite things so whenever we run this again we''ll now do python fifth module\n3:01:40 fifth module agent Deep dive example number two we''re going to run it I''m going to ask who is branded again so um\n3:01:46 or how do I learn more about L chain how do I learn more about Lang\n3:01:52 chain and then now you can see it''s actually whenever it''s running you can see that it''s saying like Okay I need to\n3:01:58 answer a question about I need to to answer a question because I have no idea what is Lang chain in the context of\n3:02:04 what I''m learning about okay cool well I''m going to call the answer question tool the input I''m going to pass in is\n3:02:10 how do I learn more about Lang chain and then so you can see cuz we passed in an\n3:02:15 input coming back down here to our code yeah so here''s our input the query is now the same as this query the chat\n3:02:21 history this was our first question so it''s not updating it yet and then the context well this is all the information\n3:02:27 that we get back from our Vector store so this is some information from our Vector store some of it looks like it is\n3:02:35 yeah this is all documents from our Vector store and once it has those 3,000 tokens worth of information and then\n3:02:41 converts that into a final answer that it then spits back to us that we can talk to here so yeah I hope you guys I\n3:02:47 think that''s super cool to see how the agent''s thinking and operating you know whenever it''s in verbos mode but enough\n3:02:53 of doing the agent Deep dive what we''re going to do from here is we''re going to head over and start going deeper into\n3:02:58 different ways that you can create your tools so that you guys can Master this and create your own tools and really\n3:03:04 supercharge your agents so let''s go ahead and head into that next hey guys so welcome to the third\n3:03:11 example inside of the agents and tools module and in this example I''m going to show you guys the most basic way we can\n3:03:18 go about creating tools and that''s going to be using the tool Constructor and you''ve already done this a few times but\n3:03:23 now we''re just going to go into a deeper dive of understanding like oh that''s actually what''s going on and here''s how\n3:03:29 I can start making tools so what I want to do is to start off I just want to show you the three functions that we''re\n3:03:35 going to try and add to our tool set so the first one is just going to be greet User it''s going to take in a name and\n3:03:41 then it''s just going to come back and say like hello we''re going to do reverse string all this is going to do is it''s\n3:03:47 going to take in some text and reverse it and split it back and then finally we''re going to set up a concatenate\n3:03:53 strings tool that just takes in two strings and concatenates them together and spits out a string so well obviously\n3:04:00 you know that''s just regular python let''s dive into the tool section where we''re going to start using the tool\n3:04:05 Constructor so this is exactly what we''ve done so far whenever we''ve created tools in the past where we''ve kind of\n3:04:12 set up a name set up a description and then Define the specific function that we want to call but the main thing I\n3:04:17 want what to bring your focus to whenever we''re using the tool Constructor is this is a great way to go\n3:04:23 about creating simple tools because all we''re doing is just saying like here''s the name here''s the description and then\n3:04:28 we''re just passing it a function we''re not specifying anywhere the like oh yeah this takes in two parameters one''s an\n3:04:35 one''s a string one''s one''s a number like we''re just relying 100% on the llm underneath to understand what the tool\n3:04:43 needs and provide it to it so and most of the time that actually works really well and that''s whenever you just have a\n3:04:49 simple tool this is the best way to go about it it''s super simple to do and the part that I did want to show you guys as well is if you''re not using this\n3:04:57 complete basic way to create a tool you can go off and use something called the structure tool and the structure tool is\n3:05:04 great whenever you want to kind of set up more complex functions so like this one takes in two parameters so you know\n3:05:11 if you''re doing anything that takes in more than one parameter I would recommend going with a structured tool and the part that makes this different\n3:05:17 different is it takes in all the same information as before except then now it takes in this schema and this schema\n3:05:24 just we''re just going to use pedantic which is just a great way to like define basically like typed models um so we''re\n3:05:30 just going to say like hey here are our arguments for concatenating string I expect to have two two properties A and\n3:05:38 B where a is the first string and B is the second string so we''re clearly defining oh wow whenever I pass in I\n3:05:44 need to basically give two strings and that''s how it''s going to how it''s going to work so now that you''ve kind of seen\n3:05:50 how this works I''m going to go ahead and actually trigger this to run and so you can actually see all these tools working\n3:05:56 in action and the only thing I do want to mention that we haven''t shown so far is uh I am using a new prompt for this\n3:06:03 one it''s called open aai tools agent and what this one is doing is it''s going to\n3:06:09 it''s basically just focused more on using tools so you can kind of see you''re a helpful assistant here''s your\n3:06:14 chat history here''s your human inputs so this one all it does really is it''s just\n3:06:19 yeah it kind of just it works really well whenever you''re trying to use tools and so what we''re going to do is go\n3:06:25 ahead and Trigger this and what we''re expecting to see back is three different responses one we''re going to tell the\n3:06:31 agent executor to greet Alice so greet is going to hopefully trigger our first\n3:06:36 tool then eventually we''re going to say reverse the string hello and then we''re going to say concatenate and we''re\n3:06:42 expecting our agent to go up and use the appropriate tools to make this happen and we''re going to log the entire process okay so what we''re going to do\n3:06:48 come down here clear things up and we''re going to run our example so we''re going\n3:06:54 to do Python and this is the fifth module this is a tool Deep dive and we''re going to look at our first example\n3:07:00 together so like I said under the hood this is going to go off and Trigger each one of these so let''s look and see how\n3:07:06 it did so for the first one we said hey we''re trying to greet Alice so you can\n3:07:11 see that it said I need to you know invoke greeting user with the keyword Alicia or Alice and what it''s going to\n3:07:19 do is come back with an answer because if you come back to our original function all it was supposed to do is\n3:07:24 pass in hello and the person''s name and that''s exactly what it did so thumbs up for the first one the next one is\n3:07:30 reversing the string so you can see that like yep we''re invoking reverse string with hello it did it it performed the\n3:07:36 code and now we print it back and then finally what we''re going to do for the last one is concatenate strings so you\n3:07:43 can see I need to invoke concatenate strings and it did a really good job of creating a dictionary where it specified\n3:07:50 what is a and what is B and this all comes back to the fact that we set up our structur tool and we passed in an\n3:07:57 argument schema that was the magic that allowed this to happen and now you can see that it did a great job of splitting\n3:08:03 it uh concatenating it together okay cool so that was hopefully a quick Deep\n3:08:08 dive into creating tools and now we''re going to go into the next module where\n3:08:13 we''re going to start learning how you can use the tool decorator to kind of simplify some of this so let''s go ahead and start working on example number two\n3:08:21 in the tool deep dive right now hey guys so welcome to the third and\n3:08:26 final example in the tool Deep dive and in this example you''re going to learn how to create tools but you''re going to learn how to do it where you have the\n3:08:33 maximum control over how these tools behave and what we''re going to do in this module is we''re going to set up two\n3:08:38 tools the first one is going to be a simple Search tool where we''re going to go off and use Tavi to search the\n3:08:44 internet and the next one is just going to be a multiply numbers tool just a super super basic one so that''s what we''re going to be doing in this example\n3:08:50 and I just want to walk you through the major parts together real fast cuz some of this is pretty similar so the first thing that we''re going to be doing is\n3:08:56 setting up our pedantic models and this is what we''re going to be doing to define the specific inputs for both of\n3:09:02 our tools so this is nothing new you''ve seen this so far all right now let''s actually dive into the part where we''re\n3:09:08 going to create our tools and how we have maximum control over them so the main way we''re going to be creating our\n3:09:13 tools is by using the base tool class now hop over to the L chain doc so you\n3:09:18 can kind of see it so the main way it works is we''re going to be using the subass base tool to generate new tools\n3:09:25 so we''re going to inherit from based tools to create new classes that are tools basically so what you can see as\n3:09:31 Lang chain says like hey this is the way you can have Maxum control over your tools but you know it takes a little bit\n3:09:37 more work and the part that''s interesting is you can kind of see like we are basically using this example I\n3:09:43 just want to like walk you through the setup of it but the part that''s nice is you can actually set up your tools to have different functionality so you can\n3:09:49 set up a tool to have a run so you can see this is a private run method and\n3:09:55 what you can see is we can actually say hey here''s what you need to do here''s your inputs don''t worry about run\n3:10:01 manager I''ve never really found it helpful and then you can Define the output and what''s super cool is you can actually if you wanted to go even harder\n3:10:08 you could actually Define the output as a pedantic model and if it fails during the run to generate the proper response\n3:10:14 it''ll redo it so this is a really cool way if you plan on doing doing you know spitting out some Json or you know you\n3:10:20 want to make sure that you print out an object that for sure has a like a person''s contact info so it for sure had\n3:10:26 to have the person''s first name last name and email you could set up a pedantic model as the output and you can\n3:10:32 do that all here inside of your custom tools when you''re inheriting from base tool and then finally you can set up\n3:10:37 things to run synchronously or you can set them up to run asynchronously we''re just going to stick to synchronously for\n3:10:42 this example okay so let''s hop back over to our example so the first thing that we''re going to be doing is setting up a\n3:10:48 Search tool because we want our agent to go off and access the internet well what we''re going to do is we are going to do\n3:10:54 exactly kind of what we did in the tool Constructor where we have to give our tool a name because we need to let our\n3:10:59 AI know like Yep this is the name of the tool and here''s when I would need to use it whenever I need to answer questions\n3:11:05 about current events so you know anything that''s not inside of the ai''s knowledge base and then finally what is\n3:11:11 the argument schema well this is exactly what I want to be passed in the simple search input which just contains a query\n3:11:18 okay now whenever this tool gets triggered and calls underlying run method what''s going to happen is we\n3:11:25 expect to get that query of a string of what we need to go search now here''s where things get interesting this is\n3:11:30 where we''ve kind of defined what this function should do so in our case we''re going to use Tav and if you head over to\n3:11:36 their documents this is how you can connect your llms to the web and it just makes it super simple to go off and\n3:11:42 search the internet uh and then just I''m sure some of you guys are curious how much it cost it''s free every month for\n3:11:48 1,000 calls and then it goes from up from there but no I''ve I''ve used them and I''ve really liked them so far so definitely recommend checking them out\n3:11:54 and to get started it''s super simple you''re going to basically just make sure you have access to this class you''re\n3:12:00 going to set up an account so you get API key and then once you have an API key you can just start using their client and making request and that''s\n3:12:07 exactly what we''re doing in rcode we''re importing that client that we would have installed I''ve already added to the uh\n3:12:12 Tomo file so it''s already going to work for you guys now we''re going to and then once you have have your environment\n3:12:17 variable set up and grab it you''re just going to make sure you paste it in here and once you''ve done that you can now actually start using the client to make\n3:12:24 requests to the Internet so let''s go ahead and we''re going to run through the second tool real fast and then we''re going to run it so you can see that like\n3:12:30 wow we are actually communicating with the internet and it''s super cool all right and the final one this is just a super simple example I just wanted to\n3:12:36 show you guys like yeah you can create a tool to multiply numbers this one''s super simple I just wanted to to add it in here for you guys but once you\n3:12:42 created your two tools what you need to do is you need to go off and basically in your tools list we''re going to create\n3:12:48 two new tools so as you can see each one of these is a class so to create a new instance of the class we''re just kind of\n3:12:54 come down here and just you know create them down here so once we have two instances of these classes we''re going to do the normal thing with our agents\n3:13:01 set up the llm we want to use give it the prompt to guide its instructions set up our tool calling agent um this one is\n3:13:09 a little bit different from what we''ve done in the past in the past we''ve been doing the the react agent so the tool\n3:13:14 calling agent it just specializes in using tools so that''s what it''s kind of doing and then finally what we''re going\n3:13:20 to do is once we have our agent set up we''re going to use the agent executor to actually start uh handling our runs all\n3:13:26 right so let''s go ahead and run our agent with these two different examples the first example we''re going to be\n3:13:32 using is search for Apple intelligence and I''m going to take off these quotes just so we have to make our uh AI learn\n3:13:40 a little bit more about Apple intelligence and then finally we''re just going to pass in a sentence and we\n3:13:46 expect this sentence to get converted over to numbers that you can see specifically floats that we can then\n3:13:52 start to use to to to multiply so that''s what we I expect to see happen whenever we run this so let''s go ahead and run it\n3:13:58 real fast so this is in the agent module and we are in the tools section and this is the final one let''s go ahead and run\n3:14:05 it so we can see the search results and look at just the math so you can see out the gate what it did is it said all\n3:14:11 right I need to do a simple surge because that was the name of our tool let''s go back up here a simple surge and\n3:14:17 we just want to what is the query of what we want to look up oh we want to look up Apple intelligence so it pulled out the key topic and then Tav our\n3:14:24 Search tool came back to us and said well here''s your query here''s the what you want to look up and then it gave us\n3:14:29 back the results so it went off and searched the internet in a way to where we get quick titles URLs so if we wanted\n3:14:37 to we can go off and actually explore some of these URLs and dive deeper but you can see it came back to us with some\n3:14:43 more information about each each of these things and today is is the middle of June so you can see this is grabbing\n3:14:49 all recent information about what''s happening with the new Apple intelligence release okay cool and then\n3:14:54 once it gave us back the actual like this is the raw score and actually you can see it here here''s some of the\n3:15:00 relevant information so just putting it in a nice format for us um that''s what''s\n3:15:05 happening with the tool and once it has that information um it''s just going to spit out the final response for us so\n3:15:12 you can kind of see it all right here this is the final response and the outputs right here it just going to look\n3:15:17 exactly this is basically the output if you printed it out that''s what you would see uh um that you you were to pass it\n3:15:22 out to your user okay and enough of that the tool went super simple the reason why I kind of like this example is\n3:15:28 because usually tools struggle with like if you just to run this as a regular tool decorator for whatever reason it\n3:15:34 kind of struggles to recognize like oh yeah that''s a float so I just thought this was a cool example of like oh yeah we have to convert a string to a\n3:15:40 numerical representation so yeah and we get back to the correct answer and it''s a float which is what we want okay all\n3:15:46 right so that''s it for the tool example and this is actually it for the entire uh agent and Tool Deep dive just to like\n3:15:53 plant some seeds in your brains of like what else is possible crew AI is by far one of my favorite pack uh products out\n3:15:59 there when it comes to working with agents because they make it super cool for us to actually set up agents to work\n3:16:05 with other agents so right now we have one agent you know performing a task but what happens if you had like a writer\n3:16:11 agent collaborating with a researcher agent who each specialized like the uh if the if the researcher agent\n3:16:17 specialized in exploring the web and the writer uh specialized in writing articles that were had a good Rhythm\n3:16:23 used proper like languaging to write interesting articles well these guys could work together to generate an\n3:16:29 awesome report so that''s just scratching the service of what cre I can do but I just wanted to like plant some seeds for\n3:16:35 like what''s next after you guys learn like how to make an individual agent well multi-agents tools are the next and\n3:16:42 I definitely recommend creai and I have a ton of videos on crei inside of my YouTube channel so I definitely recommend and checking that out next but\n3:16:48 yeah that''s it for the fifth module so let''s go ahead and wrap things up hey guys so I hope you guys have enjoyed\n3:16:54 this Lang chain master class we covered a ton of information in all five of the different modules hopefully you guys are\n3:17:00 now Pros when it comes to working with chat models prompt templates chains Rag\n3:17:06 and your agents and tools and just as a recap all the source code in this video is completely for free there''s a link\n3:17:11 down the description below while you''re down there it would mean a lot to me if y''all can hit that like And subscribe\n3:17:17 especially if you''ve made it this far in in this video also there''s that free school Community where you can meet like-minded AI developers and we have\n3:17:24 those weekly free coaching calls so you''ll definitely want to take advantage of that and then outside of that I have\n3:17:29 a ton of other AI related content on my YouTube channel everything from Full stack AI tutorials all the way to like\n3:17:36 crew AI Deep dive so you''re definitely going want check out one of those after this video but enough of that I hope you\n3:17:42 guys have a great day and I can''t wait to see youall around in the next one see you\n0:00 hey guys welcome to this Lang chain master class for beginners in this video you''re going to learn everything you\n0:06 need to know about Lang chain so that you can start your own AI development journey and by the end of this master\n0:12 class you''re going to learn everything that you need to know about Lang chain so that you can go off and create your own rag chat Bots create your own agents\n0:19 and tools and use Lang chain to automate task and even though there''s a ton of information in this tutorial I''ve done\n0:26 my best to make it as beginner friendly as possible you''ll see as we go through this tutorial that I''ve structured\n0:31 everything to start out with the absolute Basics so that we can build up a strong foundation and from there we''re\n0:37 going to add in more advanced features and complexities so that you can see what Lane chain is fully capable of and\n0:43 because I want you to get started building your own lane chain projects as fast as possible I''ve actually included over 20 different examples in this video\n0:50 so that you can just copy my code and start using it in your own projects and to make things even easier I have a link\n0:57 down the description below where you can download all the source code in this video completely for free and it would\n1:02 mean a lot to me if you could hit that like And subscribe button while you''re down there too also if you get stuck at all during this tutorial you''re in luck\n1:09 because I created a free school Community for AI developers just like you in the community you can go over there and ask questions get support join\n1:15 our weekly free coaching calls and we have over, 1500 different active members in the community so it''s a great place\n1:22 for you to meet like-minded AI developers on your development Journey so I definitely recommend checking out it''s completely for free and I have a\n1:29 link down the description below below so come over and join the party but enough of that let''s go ahead and dive into the rest of the\n1:35 video all right so let''s go ahead and cover the outline for this tutorial so that you can understand what you''re\n1:40 getting into and so that you can get the most out of this tutorial so to start off the first thing we''re going to do is\n1:45 set up the environment on your local computer so that you can run the over 20 different Lang chain examples that I''ve\n1:51 built for you guys from there we''re going to start diving deep into each of the core components of Lang chain so to\n1:56 start off we''re going to start working with chat models and this is basically how we''re going to start interfacing with you know open AI chat gbt we''re\n2:03 going to start working with CLA that''s all going to happen in the chat model section and what we''re going to do from there is we''re going to start working\n2:08 with prompt templates next and this is how we''re going to be able to format the inputs that we pass over to our chat\n2:14 models and this is really just helping us build up a good strong foundation so that eventually when we start trying to\n2:19 automate task using our chains which is my favorite part of this whole tutorial we''re going to be able to really\n2:25 understand how we can start you know automating task by putting together chat models or prompts and other tasks and\n2:31 really run them all together to automate your workflows from there we''re going to start working with rag now this is a\n2:36 huge section of this entire tutorial as you can see we have a ton of different examples in here and if you''ve heard of\n2:42 people chatting with their PDFs or documents this is what they were doing rag retrieval augmented generation so\n2:47 we''re going to do a huge Deep dive into this one and then finally what we''re going to do to wrap up this tutorial is we''re going to do a deep dive into\n2:54 agents and tools and as you can see we''re going to start off with the basics and then we''re going to do a deep dive into agents which are are basically just\n3:00 you know chat models however they can make decisions on their own and act it''s super cool how it works and then we''re\n3:05 going to do a deep dive into tools and tools are how we''re going to supercharge our agents to provide them more\n3:11 capabilities so as you can see we have a ton of information in this tutorial so let''s go ahead and start diving into the\n3:17 first section which is going to be setting up your local environment so that you can run all the different examples inside of this code base oh\n3:23 real quick I want to mention if you want to get the most out of this video to learn linkchain as quickly as possible\n3:28 here''s my recommendation first I recommend watching the video the whole way through on two times speed just so\n3:33 that you can understand all the highlevel Core Concepts of blank chain and then I recommend coming back through\n3:40 this video a second time and just skip to the part that you want to learn about for whatever project you''re building for\n3:45 example if you''re learning about rag I would definitely recommend just skipping down through the time sense below to\n3:50 watch rewatch the rag section and do a deep dive into the exact part that you want to learn about this is how I go\n3:56 about learning new Concepts so I just want to throw it out to you guys too so you can speed up your development journey and start gilding projects so\n4:02 all right enough of that let''s actually go in and start setting up your environment all right guys so it''s time\n4:08 for us to start working on setting up our local environment so that we can run all the different examples inside of\n4:13 this project now inside of the read me I have outlined all the different steps that you need to take in order to start\n4:19 running these and it''s actually super straightforward so let me just quickly walk you through it to start off we need to install Python and we need to install\n4:25 poetry python I''m sure you know what that is poetry basically if you haven''t heard of it before or it''s a dependency\n4:30 management tool for python so as you can see we have something called a p project. tommo and this includes all of\n4:37 the different dependencies that we need to install in order to run all the different examples that I''ve set up for\n4:43 you and poetry makes this super simple to do so what you''ll do go over to click this link right here and it will walk\n4:50 you through all the installation steps to install poetry on your local computer and once You'' have installed poetry\n4:55 you''ll be able to run commands like this so poetry to confirm that it''s working and you can see yep poetry is working\n5:01 it''s giving me back information and then from there what we can do is once you''re inside of your code base and remember\n5:07 all the source code for this project is completely for free just click the link down the description below and you can download it but once you''ve done that\n5:13 what you can do is type in poetry install D- noout and what it''ll do is\n5:18 it''ll go through and install all the different dependencies that we saw back over in our py project. Tomo file over\n5:26 here it''ll install it and what''s awesome is then we can eventually start to run commands like this poetry shell and what\n5:33 this will do is it''ll actually spin an interactive shell up that we can see right here it actually has the name of\n5:39 basically our project that we''re working on you can see right here it has our name of our project that we''re working on and we''ve created and we can actually\n5:45 start you know running Python and actually start calling all the different example code bases that we have all the\n5:51 different projects so you can see we can do something like python one. chat models and then we can start running you\n5:56 know all of our different code examples so if you got this far you are good to go go for the rest of the course when it comes to python now we just have a few\n6:03 more cleanup things that we need to do first off uh coming back to our read me is I mentioned that you need to update\n6:10 your environment variables so uh when you download the source code you will only see a EnV example file and this is\n6:18 where you''re going to instore your environment variables eventually what we''re going to do is you are going to\n6:24 rename this file to justv and what this will do is is it''ll\n6:29 become your environment variable folder so that whenever you go to run your you know start using open Ai and some of\n6:36 your other different projects they require an open AI key or a Google key or you know all these keys and that''s\n6:42 going to get stored to your environment variables I''ve already have mine set up and we''ll be walking through it later but that''s the second thing you need to\n6:48 do and actually in addition to that go over to open AI Google fir craw and\n6:53 actually start adding in those open ad keys but we''ll talk more about that later on okay all right enough of talking about environment if you''ve\n7:00 gotten this far everything should be working and we can now move on to actually start playing with the code and Diving deep into L Jing so we''re going\n7:06 to go over next and start working with chat models hey guys I meant to show you this quick tip that''s going to make coding inside a visual studio code and\n7:13 cursor with python so much easier and solve a big headache that you''ll probably have so if you head over to one\n7:18 of your python files you''ll notice that you have a bunch of squigglies inside of your project and the reason why is cuz\n7:25 Visual Studio code is not properly hooked up to the new python environment that you just created with all the\n7:31 proper dependencies so here''s how we fix that first you''re just going to open up a terminal down here and what you''ll do\n7:37 is you''ll type in poetry shell like we did earlier and this will actually access and you can see yep we''re in the\n7:42 right shell but the important part is is it gives us the location of where this basically all of our dependencies were\n7:49 installed so here''s where the magic happens you''ll come down here and click that you know interpreter path and\n7:54 you''ll say enter interpreter path and then you''ll just paste in the path you just copied\n7:59 and whenever you do that it''ll actually get rid of all the squigglies because now Visual Studio code or cursor is\n8:05 hooked up to your development environment with python so now every time you add a new dependency with poetry ad or whatever you do it will\n8:12 actually you know sync up and you won''t have all those random squiggly marks even though you have the Right Packages installed and just as a final note if\n8:18 you ever like well I want to not use this poetry shell environment anymore all you have to do is just type in the word exit and it''ll put you back to your\n8:25 base environment so yeah that''s a quick crash course on poetry and setting up visual studio code and after that let''s dive back into the\n8:32 video all right so it''s time for us to dive into our first core component of Lang chain which is going to be chat\n8:39 models now what the heck are chat models and what do they do well a chat model is\n8:44 basically Lang Chain''s way of making it super easy for us developers to talk to all the different large language models\n8:51 out there like chat BT claw Gemini and a bunch more they abstract away all the complexity and allow us to basically\n8:58 have conversation ations with these models hints chat and chat models it''s all about conversations so what is super\n9:05 nice about Lang chain if we go over to their documentation you can see they have a list of all the models that they\n9:12 currently support now it is very important to mention that for this whole tutorial we''re going to be using version\n9:18 0.2 of Lang chain this is the most upto-date one and a lot of the features and version 0.1 will soon be deprecated\n9:26 uh whenever they upgrade you know probably the next you know five six months from now but enough of that let''s dive back into chat models and talk\n9:32 about how we can use them so as you can see looking at chat models here''s a huge list of all the different models that we\n9:39 have access to some of these models are better at other things and what''s very interesting is chat models inside of\n9:45 Lang chain they provide different functionality such as tool calling outputs you know do they support\n9:51 outputting content in Json are they multimodal such as accepting images and audio so that''s what you can see at a\n9:58 high level going over here and what''s nice is if you want to work with any of these different models you can just\n10:03 click uh you know use poetry ad and you can type in the name of the package and you''ll add it to the environment that\n10:09 you just set up a few seconds ago so let''s go ahead and do a deeper dive and look at chat open AI because that''s the\n10:15 one we''re going to be using mostly inside of this tutorial so if we go over here to chat open aai you can do a\n10:21 deeper dive and look at some of the examples that they already have set up for you so you can see once again they\n10:26 recap what it''s capable of doing and then they walk walk you through Yep this is how you can start setting up your\n10:33 files to start using this new package to start chatting with open Ai and eventually down here they dive into\n10:39 showing you y this is how you can start actually using it updating the models and so forth but of there documentation\n10:45 let''s go ahead and actually head over to the example that I created for you guys where we''re going to start at the absolute Basics and work our way up so\n10:51 you can see what these chat models are capable of so let''s come over here start looking at the code and uh take it from\n10:57 here and before we dive into the code on this specific file I just want to give you a\n11:03 quick overview of what you can expect from each of these files so what I''ll do in each one of the examples that we''re\n11:08 going to run through I will try to provide documentation for you guys up top so you on your own can do a deeper\n11:14 dive into whatever concept we''re just learning for example we just talked about chat models and I showed you a\n11:19 link and I also did open AI chat models and we walked through another link so in all the files wherever I point out\n11:24 something you''ll be able to go ahead and click those links and a deeper dive on your own if you ever want to learn more about those topics but enough of that\n11:30 let''s actually go ahead and start talking about chat models on a basic\n11:35 level all right guys so let''s walk through the Three core steps that we need to take to start interacting with\n11:41 our chat models in this case our open AI chat model at a super basic level so the first thing that we''re going to do is\n11:47 load our environment variables and if you remember from the beginning we set up aemv file which stored all of our\n11:53 keys to all of the different platforms we were going to access in this case we''re trying to work with open AI so it''s important that we have an open AI\n12:00 key feel free if you haven''t set that up just head over to the open a website go create an account and you''ll actually be\n12:06 able to access your open AI key and bring it back and copy and paste it over here now once you have done that what''s\n12:12 nice is this load. EnV is going to add all those environment variables so that we can start accessing them in this file\n12:19 and you might be wondering like Brandon what the heck I don''t see us accessing the open AI key anywhere well if we\n12:25 actually Peak under the hood inside of the chat model which actually gets imported from linkchain open Ai and\n12:31 that''s actually if you head back over to those packages that we had set up earlier that''s where it was stored but if you actually hit command on your\n12:37 keyboard if you''re on Mac or control if you''re on Windows and actually click on it you can actually start looking at the\n12:43 source code under the hood and you can see hey in order to start using this chat model you need to have this API key\n12:50 and we''re actually going to start using it if you scroll down a little bit you can actually see under the hood it''s\n12:56 actually automatically grabbing this environment key here and unless you pass it in manually and just to like give you\n13:01 guys a full tool you could actually manually pass in your API key here it''s just not the safest manner because if\n13:07 you accidentally save your open a key to the public other people could actually grab your API key from GitHub and it''s\n13:14 not the most secure so that''s why you''re going to store everything in your environment variables file okay enough of that let''s keep chugging along to\n13:20 actually you know walk all the way through this example the next thing that we''re going to do is once we have created our chat model which in this\n13:26 case we''re going to say we''re using jet gbt 4.0 but we easily could have done something like chat gbt 4 we could have\n13:32 done something like chat gbt for uh you know 3.5 turbo we could have easily changed things up but once we''ve created\n13:38 that model we can actually St uh start now interacting with it and using it and the key lesson here is we''re going to\n13:45 interact with our models in pretty much everything in Lane chain using the do invoke property this is a function that\n13:52 really just whatever we''re using it triggers its core functionality so in this case we''re working with chat models so it''s going to go trigger off like hey\n13:59 open AI or Claud start processing the request I give you but when we''re using chains or rag or agents later on\n14:07 everything uses invoke so that''s a very key thing to keep in mind as you''re working with Lang chain all right so\n14:12 let''s just keep walking through what''s going on well in our case we''re telling our model hey go perform this query for\n14:18 me and we''re going to get back a result so let''s actually dive into these results so we can see what''s happening under the hood and the way we''re going\n14:23 to do that is we''re going to open up our terminal and let''s close it out and give you guys some more space but once again\n14:29 we''re going to use poetry shell and this is going to open up that interactive shell that we created earlier which uses\n14:35 you know our new Lang chain crash course environment what we can do is we can actually start calling this function so\n14:41 we''re just going to call it Python and this is our chat model so one Tab and it''ll go ahead and autocomplete for me\n14:47 and this is the first example so I''ll hit one and tab again and it''ll autocomplete now I can start running it\n14:52 so let''s actually go ahead and press enter and start looking what''s happening under the hood so here''s what''s actually\n14:59 super interesting so you can see we had two print statements one was for the full result and one was content only so\n15:04 for full results you can see that open AI under the hood gives us back a ton of information they give us back the\n15:11 content which is the exact answer we wanted 81 divid by 9 is 9 but then they give us this metadata which is like how\n15:17 many tokens did we use which you know basically why did we finish there''s a ton of information which run number was\n15:23 this basically there''s a ton of information that they give back to us 99% time you don''t care about it however\n15:29 I just want to show you that it is accessible if you ever need to use it so most of the time when you''re using you know these chat models the main thing\n15:36 that you want to grab is the content because the content is the example so whenever we grab the result and we\n15:42 access the content property we''ll get back the exact string that we usually want to you know show to our users or\n15:48 pull out and pass over to the next prompt in our you know our chat model so that''s under the hood how our first\n15:55 basic chat models are going to work so what I would like to do next is we''re going to go ahead and move over to the\n16:00 next example where we''re going to start actually showing you guys how to do a basic conversation using our chat models\n16:06 to where we can actually like you know pass in more information and actually having a full-on conversation let''s go and start working on this example\n16:13 now all right guys so welcome to the second example where we''re going to start focusing more on creating a\n16:18 conversation with our chat models now the three new important Concepts to know when going into this example are there\n16:25 are three different types of messages in our case that''s going to be a system message and a system message just sets\n16:31 like the broad context for the conversation so these types of messages are usually something like hey you are a\n16:37 professional accountant or hey you are a professional python software engineer help me write this code so that''s like\n16:43 just the broad what''s going on inside the conversation just the context and then from there there''s two different\n16:48 types of messages there are human messages which is us talking to the AI and then there''s AI messages which are\n16:54 the AI responding back to us so those are the three types of messages that you can have in a conversation with the AI\n17:01 okay cool now let''s dive into the rest of this code so you can kind of see what''s happening and this is just once again we''re building on our foundation\n17:07 so we''re just copying a lot of the code from the previous example and now we''re going to start building on top in this case we''re going to start creating a\n17:14 conversation so in our case you can see a conversation is nothing more than we have our messages list and our list is\n17:21 just going to store a combination and series of messages in our case we''re just GNA um start off with a system\n17:27 message and it''s important system messages best practice and I think it''s actually enforced a system message must\n17:32 come first and then from there you can alternate human AI human AI but system always comes first because remember this\n17:39 is the context for the conversation as a whole so in our case we''re going to say hey solve the math problem and then from\n17:45 there we can pass in a human message and what we would expect to get back is obviously an AI message so let''s\n17:51 actually kind of see how we can actually trigger this conversation so we can get a result so in this case you''ll see that\n17:57 we have a once again again we have our model our chat gbt model and we''re going to call that important function that we\n18:02 talked about in the last one which is invoke but this time we''re not going to pass in a you know a hardcoded string\n18:09 like we did over here where you can see we passed in a hardcoded string this time we''re actually going to pass in our entire message history and it''ll\n18:16 actually read through the entire conversation and then spit out a result if you worked with cat GPT it''s exactly like that if you''ve ever typed into like\n18:22 you know the chbt website it''s just like that okay cool so what we''re going to do is we''re going to go ahead I''m going to comment this out we''re going to run the\n18:29 code so we can actually see what answer we get so this time we''re going to do python this is you know still working\n18:34 with chat models and this is example two so you can actually see whenever we get an example back with working with chbt\n18:40 it says you know 81 divided by 9 is 9 so you know that''s exactly what we would expect to see okay cool but what''s nice\n18:46 is we can continue this example and actually have a full-blown conversation so I''ll show you what that looks like\n18:52 now so you can see in this second part we actually have continued on the conversation by adding in AI responses\n18:58 and then messages so this case we''ve added now like the response from the previous one and now we''re going to you\n19:04 know just continue adding human messages so I''m just going to run this so you can actually see what''s going to happen and this time it''s going to come back and\n19:10 give us you know you know 10times 5 so this is completely basic however here''s why chat conversations are super\n19:16 important in the real world when you''re building your conversations you know it''s very common for you to provide an\n19:22 example like hey chat gbt or AI model build me an email it returns a response\n19:28 and then you provide provide feedback cuz remember this is all about conversations and what''s going to happen is as you provide those you know\n19:34 feedback of like hey no make it less formal make it a bulleted list you know as you provide that feedback what''s nice\n19:39 is going forward in your conversation you can say okay great now do exactly what you did for that last email but now\n19:45 do it for this email and you know storing this message history like you have right here is how you''re going to\n19:50 be able to basically make your chat models have awareness in context of what''s good and what''s bad and what''s wrong so this is a very powerful tool\n19:57 and you actually use this a lot more whenever you know you''re working on bigger and larger projects but okay cool\n20:03 well now that we have that under the hood and we actually understand like just like basic conversations let''s keep it going and actually start working on\n20:10 actually exploring other alternatives for different chat models that we can use because right now we''ve been focusing on only open AI but let''s look\n20:16 at a few different examples of how we can use Lang chain chat models but with different you know llms so let''s go\n20:22 ahead and start working on that now all right guys so welcome to this third example where we''re going to start\n20:28 exploring different Alternatives of working with other models outside of open AI so in this case we''re going to\n20:35 explore Google''s Gemini models we''re going to look at anthropic or CLA and look at Open Eye because I just want to\n20:42 show you guys how easy Lang chain makes it to work with these different models so let''s scroll down so we can look and\n20:48 compare and contrast all the new code so what you can see up top this is exactly\n20:53 what we''ve been doing so far in all our examples we create our chat model once we have the model we we go off and\n20:59 invoke it and then we get back some sort of result well Lang chain abstracts all the complexity away and makes it super\n21:05 easy to do the same thing with our anthropic models and our Google Gemini models all we do is we instead of using\n21:12 the chat open AI model like we''ve been doing in the past we now just use chat anthropic and then we can pass in\n21:18 whatever specific model within Cloud that we want to use just like we did with chat gbt up here and then we''ll do\n21:24 the normal part where we just you know go off and invoke it with our messages and get back a result and is the exact\n21:29 same thing for Google''s giz models down here at the bottom so link chain makes it super easy to work with these\n21:35 different models and this is very important as you build larger projects because certain models are much better\n21:40 at you know at performing certain tasks some are cheaper some are faster so for different situations you need to use\n21:45 different models Lang chain makes it super easy to do and if you want to explore all the different models that\n21:50 each you know anthropic provides and Google provides I have links for you guys and just as a important reminder if\n21:56 you want to go off and explore all all the different chat models back over here in our first example back when we were\n22:02 working with a chat model documentation you could start searching through here so you can explore all the different models and all the different\n22:09 functionalities that these different models provide but okay cool enough of that let''s go ahead and start exploring our next example where we''re going to be\n22:16 diving in and actually having a conversation with our user through the terminal so let''s go and start working on this example\n22:22 now all right guys welcome to the fourth chat model example where we''re going to start actually having a realtime\n22:27 conversation with with our AI models this is going to feel just like the chat gbt website except it''s running locally\n22:33 on our computer so let''s walk through how we''re going to set this up so the important part per usual we''re going to create our chat model by loading all our\n22:39 environment variables and creating an instance of it now here''s where all the interesting part happens because we''re having a conversation we''re going to\n22:46 create a chat history list and this is going to store all our messages so as we ask questions we''re going to add\n22:52 messages to the chat history as the AI responds we''re going to add those messages to the chat history and we''re just going to continually keep add add\n22:58 in all of our messages to this list so here''s how that works in this code example so first off the first message\n23:04 we''re going to add is our system message because if you remember system messages are just general context for the\n23:09 conversation that''s about to happen so we''re going to create that system message and then we''re going to add it\n23:15 to our chat history by calling append on the chat history list cool so now here''s\n23:20 where all the core logic happens so this is where our chat Loop happens and basically here''s the core Loop we first\n23:27 ask our users hey what is the question or prompt that you have for the chat model we''re going to get back a query so\n23:33 this is whatever the user passed to us if the user gave us the keyword exit we''re just going to stop the\n23:38 conversation right then and there and we''re just going to print out the whole chat history however if they didn''t give us the keyword exit we''re just going to\n23:44 have a full-on conversation so what we''re going to do is we''re going to add the person''s query as a human message\n23:50 and we''re going to add it to our chat history then what we''re going to do is pass over that entire chat history list\n23:56 over to our model like you can see right here and then we''re going to invoke it so that model is going to then read all\n24:02 the messages in our chat history get a response and then we''re going to print back that response to our user and we''re\n24:09 also going to most importantly track that response by once again updating our chat history so we''re going to update\n24:15 our chat history with an AI message cuz you know this is the AI response so enough talk let''s go ahead and look at a\n24:21 code example so you guys can see this in action so I''m going to once again open up our terminal and I''m just going to\n24:26 call Python and then this is our chat model project and this is the fourth example so now we can actually start\n24:32 running it and per usual it''s now going to ask for query so I''m just going to ask it who are you so then what we''ll do\n24:39 is we''ll pass that question over to open AI open AI will generate a response and it actually prints it back to us so this\n24:45 is exactly you know if we''re talking to chat gbt on their website this is exactly what it feels like and we can\n24:50 actually ask questions about it because as a conversation we can refer to previous messages can you expand on that\n24:57 and what it''ll do is it''ll you know cuz it can refer back to the previous messages and actually provide additional\n25:02 context so this is super powerful and you''ll definitely be using this a lot more and finally we can pass in the word\n25:08 exit it''ll quit the conversation and what''s cool is you can see our entire message history here so you could save\n25:14 this off somewhere you could you know save it locally save it to the cloud do whatever you want so that whenever the user comes back in the future you can\n25:20 continue the conversation and we''ll actually do a much more deeper example later on where we actually save this to\n25:25 the cloud so you''ll learn about that in just a little bit all right cool enough of that let''s go ahead and start working on our fifth example which is exactly\n25:32 what I was talking about where we''re going to start saving our messages to the cloud so let''s go ahead and start working on this where we''re going to save all of our messages over to\n25:38 Firebase I think you''re really going to love this one all right let''s go ahead all right guys welcome to the\n25:44 fifth chat model example so this one is by far my favorite because you''re going to learn how to save all the messages\n25:49 you type locally over to the cloud and this case we''re going to be saving everything to Firebase so what I''ve done\n25:55 is I''ve actually typed out all the installation in steps for you guys because there''s there''s you know a little bit more background setup because\n26:02 we''re working with the cloud but once we knock out all the development setup in the cloud what we''ll do is just like run through the code so you can see how\n26:08 everything works but I think you''ll see that like all in all this is actually pretty straightforward the hard part is\n26:13 just setting everything up in the cloud so let''s go ahead and do that now and the first thing I do want to mention about the cloud is this is actually\n26:18 based on this Google fire store codebase example so you can see like this is exactly what we''re doing however uh I\n26:25 feel like they didn''t do the best job walking through how to get it set up and I struggled a ton the first time I did this so I just want to share all the\n26:31 lessons learned that I have done and and I''ve copied all my lessons learned here okay so the first thing that you need to\n26:37 do is create a Firebase account so just head over to you know console firebase.com and you''re going to create\n26:44 an account and once you''ve created an account you''re going to then go over here and create a project to so I''m not going to like walk through creating a\n26:50 project cuz it''s it''s super simple but just come over here click the drop- down menu and click create a project so\n26:55 that''s how you create a project once you''ve done that you''ll then need to create when you go over to the build tab\n27:01 there''s something called the firestore database and this is where we''re going to be storing all of our messages inside of our chat history except our chat\n27:08 history is now in the cloud so whenever you click fir store database if it''s the first time you''re using the project you''re going to want to turn this on so\n27:15 there''ll just be a turn on button here for you and you''ll want to click that and once you''ve done that and you run the code eventually all of your messages\n27:21 and user sessions will get saved up to the cloud and more on this later but I just want to go ahead and like paint a picture of where we''re going fantastic\n27:27 so once you''ve done that let''s heading back to our examples you''ll need to start copying some information about your Firebase project so what I mean by\n27:34 that is you need to start copying information such as the project ID so in our case what you''ll do come back over\n27:40 here you''re going to click the gear icon you''ll hit project settings and this is where you''ll find your project ID\n27:46 project number and everything that''s you know related to what you''re what you''re doing with the project you just created\n27:51 over here in Firebase Okay cool so that''s the easy part now this is where things get a little bit more complicated\n27:57 because we''re trying to work with the cloud so Google cloud and we''re trying to have our local computer communicate\n28:04 with the cloud so this is where we''re going to have to go a little bit deeper so the first thing we''re going to have to do is install the Google Cloud\n28:10 command line interface on our computer because what we''re trying to do is authenticate our local computer to make\n28:17 requests to the back end that''s all we''re trying to do so what you''ll do is first thing you''ll click this link over\n28:22 here and it will take you to this and this will basically just walk you through exactly what you need need to\n28:28 type to install the Google Cloud CLI it''s super straightforward you''ll just click download once you''ve downloaded it\n28:34 literally walks you through step by-step how to install it so that''s super super easy then once you''ve gone through and\n28:40 installed everything and initialized it the final part that we''re going to do is you need to authenticate your local\n28:46 Google Cloud CLI to your account so once again click this link it''ll take you over here and it will just walk you\n28:52 through how you can actually authenticate your local computer to Google Cloud cuz we''re just trying to create some default credal\n28:58 that way whenever our codee''s running it just can you know seamlessly make a call to the back end and the main things that\n29:04 you need to do first off just run Google Cloud off application default login a\n29:09 signin like normal Google Cloud signin screen will pop up you''ll just log in it''ll make a service account for you\n29:14 life fantastic and then you''ll just run you''ll run this basically this script as well to finally get everything working\n29:20 so I hope that''s enough I don''t want to go too deep just because I want to focus more on Lang chain but run through those\n29:25 steps you guys will be good to go and then you''ll find finally have everything working inside your code so all right\n29:31 enough of that here''s how we''re going to get things working over here so you''re now going to come in here uh CU you have your project ID that you just copied\n29:37 you''re then going to have a session ID so we''re just going to make a new one so we''re going to call this user session\n29:43 new on this is where all of our messages are going to be saved and then finally there''s a thing called a collection name\n29:48 this is heading back over to Firebase just the way our fir store saves data\n29:53 it''s in a collection document basically a database type so you can see collection document collection so it\n30:00 just keeps alternating so we''re going to store all of our messages in the chat history collection and each session is\n30:07 going to be a document and that document is just going to contain a list of messages so that''s what''s about to happen so what we need to do first off\n30:14 is we need to initialize our firestore client and what that''s going to do is allow us with our client to go off and\n30:21 make requests so that''s why we''re doing it and then what we''re going to do next is have something called firestore chat\n30:27 message history this is basically we''ll just click in here so you can see what''s going on but basically what we''re trying to do is in the past you and I were\n30:34 storing all of our messages in a list well Lang chain has provided a bunch of different options for us to store all of\n30:41 the different ways that we can have and store our messages so in this case we''re using fir store to save our messages\n30:48 there''s a few different examples inside of Lang chain where we can actually save our messages using I think the like file\n30:53 message history class where we can actually save all the messages locally on our computer so there''s a bunch of different options that I want you guys\n31:00 to be aware of and feel free to explore through the L chain documentation to see other options but just know at its core\n31:06 all it''s doing is just saving a list of messages you know human messages AI messages back and forth except now they''re just being saved off to\n31:12 somewhere else so that''s what''s happening under the hood but let''s keep going so in our case now that we have\n31:17 basically we''ve created our new chat history except our new chat history is up in the cloud so what we can do now is\n31:22 actually start running through the exact same Loop that we did last time which is where the human''s going to ask a question question and now what we''re\n31:29 going to do is just keep adding with our chat history we''re just going to add user messages or we''re going to add AI messages and we''re just going to go back\n31:34 and forth so enough talking let''s actually dive into the demo because this is super cool in my opinion so we''re going to go step by step because there''s\n31:40 a lot of cool ways to show this off so the first thing that we''re going to do let''s clear this out and we''re just going to run python so we''re in the chat\n31:47 models example where this is the fifth example now what this will do is it''ll start you know initializing firestore\n31:52 client it''ll start the chat message history and usually this takes a few seconds to get started and then now we\n31:58 can actually start chatting so it''s cool though uh and you''ll see this get populated in second our current chat history well because this is the first\n32:04 time I created that new session ID there''s no history of this chat in the cloud you know there''s no user session d\n32:11 new so there''s no messages to load but now I can actually start talking about it so I can say who is Sam Alman so\n32:20 it''ll go off and answer questions and you know so here''s here''s who Sam is but what''s super cool is if now if I come\n32:26 back over to the class we can actually see our messages being stored in real time you''ll notice they''re stored as\n32:31 bite strings and they''re not actual just like plain text messages that''s just totally fine that''s how Firebase and fire store are storing messages but\n32:38 let''s keep going does he have a brother so we can keep asking and then\n32:44 it''ll say yes he has a brother here''s his name and then what we can do is exit it so we''ll exit Okay fantastic all of\n32:51 our messages are cleared you can see over over here we actually still have additional messages but what''s nice is\n32:56 whenever we restart the same file with the same you know chat session it''ll\n33:02 actually say the current chat history and it''ll pull all of the different you know messages that we have previously\n33:08 sent to the AI so this is a super cool way and I hope you guys are like this is awesome this is a super cool way for us\n33:14 to continue conversations at a later date without having to completely refresh from start so I hope you guys\n33:20 thought this was awesome because we can still you know do additional stuff like who was I just talking about let me fix\n33:26 this oh sorry wrong button I can say who we just talking about and then because\n33:32 this is a chat history it can refer to the previous messages so yeah this is awesome so I hope you guys are pumped\n33:37 and that concludes the first module where we''re just run through all the different capabilities with working with\n33:43 chat models and from here we''re going to move on to prompt templates next so I do just want to point out before we keep going if you have any questions\n33:49 definitely hit that link down the description below for school go over there and ask any questions you have or hop on our weekly coaching calls would\n33:55 love to help you guys out but let''s keep chugging along and start moving over to prompt\n34:00 templates all right guys welcome to the second module in this Lang chain crash course this whole module is completely\n34:06 dedicated to prompt templates and I think you''ll find this is a super simple concept but it''s going to make our lives\n34:12 much easier as developers as we work with link chain so what the heck is a prompt template why does it matter how\n34:17 does it work with chat models those are the main things we''re going to be tackling in this module so the first thing is prompt templates the best way\n34:23 to think of them is exactly what the name says we are building up a prompt that''s our whole goal we''re trying to\n34:29 create a prompt but we''ve created some sort of template like this and it''s up to us to pass in values into that\n34:36 template so you know kind of think of like Fillin the blank is the best way to describe it so you know you could do\n34:41 generate three jokes about dogs or something like that is kind of what we''re what we''re shooting for here so\n34:47 enough like high level let''s actually dive into like what this actually looks like in action as like a quick overview\n34:54 and then after this we''ll dive into the code okay so like I said our whole goal is we''re striving to generate a prompt\n35:00 but we''re going to be passing in some variables to make this work so you know we''re going to be taking like a user\n35:05 response to help us really fill out this prompt and this is important because eventually these prompts are going to\n35:11 get passed over to our chat models to you know to to perform some action so in our case like this is a joke template So\n35:17 eventually this template once it gets populated it''s going to get passed over to you know chbt or Claude one of those\n35:23 models and it''s actually going to spit out some jokes for us so that''s just this prompt is going to help us structure The Prompt that we pass over\n35:29 so it''s super helpful we''ll dive into some more examples here in just a second but this is exactly how it works it''s up to you as a you know whenever you''re\n35:35 creating these prompts to basically create a string and inside that string it''s up to you to use these curly\n35:41 brackets to create variables these variables are later going to be populated with these values and what''s\n35:48 important to notice here is once we''ve created our prompt template we''re going to Define an input dictionary and this\n35:54 input dictionary is going to Define for each one of these variables the values that are going to replace them because\n35:59 eventually whenever we have our template pass in these inputs and call invoke because remember invoke is the super\n36:06 magic word for all of Lang chain what it''s going to do is it''s going to basically do some string interpolation and replace all these values so that we\n36:13 end up with an output that looks just like this obviously this was a super simple example but let''s just think like\n36:18 higher level like for a real world setting you could eventually be creating some sort of tool that helps developer\n36:24 automatically debug their code well in your case you would create a prompt template that''s like here is the user''s\n36:30 code and then you know you would have user''s code here''s the error they''re getting that would be the next variable\n36:37 please help them debug this and then basically you would pass in that entire prompt filled with the user''s code and\n36:42 error over to open Ai and then open AI would tell you oh it looks like here''s a bug here''s how it fix it so this is just\n36:48 a really nice way for us to create structured prompts that make it easy you know where we can do some prompt engineering and easily add in our users\n36:55 input to The Prompt templates that we''re so uh enough jargon let''s actually dive into an example and start walking\n37:01 through our first example working with prompt templates and I think this will make a ton of sense and it''ll be super easy to\n37:08 understand all right guys so let''s go ahead and dive into the code and this one''s going to be a super simple Code\n37:13 walkthrough there''s three parts to this and then I have one little extra part so you guys can see you know when not to\n37:19 use prompt templates but let''s walk through this part by part just so you understand everything at a basic level and then we''ll work our way up from\n37:24 there okay so if you remember our whole goal is we''re trying to create prompts that we could eventually pass over to\n37:30 our chat models so we''re focusing in on creating prompt templates that''s all we''re trying to do so let''s go part by\n37:36 part in this code and what we''re going to be trying to do is trying to relate it back to this outline that we have over here so the first thing we need to\n37:41 do is create a prompt template that stores our variables of things we want to replace so that''s exactly what we''re\n37:47 doing right here in this code we''re creating a template which is just a string and what we''re going to do is\n37:52 we''re going to call chat prompt template and like well what the heck is this well if you remember at high level we''re\n37:58 eventually trying to our end goal is we just talked about chat models that''s all we were just talking about in module one\n38:04 well what we''re trying to do is create prompts that we could eventually pass over to those chat models so that you\n38:09 know AI can basically answer whatever questions or ask that we''re putting together in our prompts so that''s our\n38:14 end goal so what this chat prompt template does what it''s trying to do is it converts our string into a template\n38:20 that makes it easy to work with to actually replace you know basically replace all these variables with these\n38:26 values so basically what''s happening under the hood so once we create this prompt template so once we have our\n38:31 template what we''re trying to do is we want to pass in an input dictionary that has all of our keys and values so that''s\n38:39 basically exactly what was happening right here and what we''re trying to do is once again we''re calling that magical\n38:44 word invoke so in this case what''s going to happen under the hood is we have this prompt template we''re going to call\n38:49 invoke with these specific values and all it''s going to do is it''s going to go through each variable and replace it\n38:55 with the appropriate value so let''s go ahead and run this code so you can see an action so in our case we''re going to\n39:00 do python now we''re in prompt templates and we''re going to run the first example so two one and what this will do is now\n39:07 when I run it what you''ll notice is it actually doesn''t spit out a string it actually spits out a human message so\n39:13 tell me a joke about cats so that''s pretty cool it actually filled in the values that we told it to and if you\n39:19 remember what it did is it spit out basically a messages array that we could eventually pass over to our chat models\n39:25 cuz that''s where we''re trying to go so that was super simple example one of the other things I want to show you in this is we can actually instead of just\n39:31 replacing one value let''s go ahead and look at part two what you''ll notice in this one is we can actually replace\n39:37 multiple values in a prompt template so this is the exact same exact same thing\n39:42 except this time what we''re doing is replacing multiple values so let''s go ahead and yeah we''re still calling the same chat prompt template from template\n39:49 so let''s go ahead and run the same example again just so you can see it in action so yeah running with multiple placeholders this time tell me a funny\n39:56 story about a panda that''s exactly what happens okay enough of the simple examples let''s actually get on to some\n40:01 more of the cooler parts which is part three so if you can uh if you''ve seen so far every time it''s created something it\n40:08 has basically been from a human message point of view but sometimes you actually want to do a little bit more you want to\n40:14 have a little bit more control so what you''ll notice in part three is we can actually start defining basically we can\n40:22 specify the types of messages we want to create and not only what type of message we want to create we can actually still\n40:28 you know replace all the variables in here and what you might notice and which I thought was weird the first time I used it I would have expected that you\n40:34 know what the heck is this like Tuple where the first part''s a simple uh you know the system and then the message\n40:41 then we have another human then message like why aren''t we just using like human messages and system messages well it all\n40:47 comes back to the way the chat prop template works is it just expects this tupal format where you define the\n40:53 message type first and then the content so just know if you want to basically use prompt templates and also Define the\n40:59 message type you have to go with this Tuple approach but yeah let''s go ahead and see an action so you can see exactly what it looks like so we''re running part\n41:06 three so we''re just going to run the code again so you can see now we have a list of messages and the first message\n41:12 in this array is going to be a system message and you know you''re a comedian who tells jokes about lawyers so that''s\n41:17 setting the context and then you can say tell me three jokes so we actually just replace the variables there too so yeah\n41:23 so once again we''ve created these messages and we could actually pass this message list over to our chat models and\n41:28 it would spit out those jokes about lawyers okay now I just want to show you this is just a lesson learn that I thought was pretty interesting so if we\n41:35 come down here to the final part what you will notice I''m just going to comment this out so you can see it so\n41:41 this code does work so you know how we just talked about tupal well anywhere\n41:46 you want to do string interpolation you know where we''re replacing values you have to use the tupal so you''ll notice\n41:52 we want to replace Topic in this system''s tupal so we have to put it in a tupal and then well for the human\n41:58 message we''re not doing any interpolation there so we''re just going to leave it how it is so this example would 100% work let''s clear it out and\n42:05 I''m just going to show you it works so this one works yep it looks just like we talked about in the first example well\n42:11 if you come down here to the final example that I have in this file you''ll notice I say this does not work and\n42:17 basically what we''re doing here is we''re trying to do interpolation on a human message so let me just show you what\n42:22 happens when you try and like if you don''t use a tupal you''ll basically get uh you''ll basically notice here tell me\n42:28 a joke and it never did the replacing part I ran into a lot of issues when I first started using Lane chain and doing\n42:34 prompt template Replacements and I was like why isn''t it replacing it and it''s like oh it has to be in this tupal\n42:39 format okay I hopefully um that''s enough prompt templates like Basics let''s actually start moving on to the next\n42:45 part where we''re going to start using actually start using these prompt templates with a chat model in our second example so let''s go ahead and\n42:50 start working on that now all right so welcome to the second prompt template example this one we''re\n42:57 actually going to tie together everything for module one which is creating our chat models and everything we just learned about prompt templates\n43:03 so let''s go ahead and merge both these together in the example so you''ll actually really understand the whole value of creating these different prompt\n43:09 templates okay so per usual like we did in all of module one we''re going to load our environment variables and we''re\n43:15 going to go ahead and create a chat model that''s using open AI in this case now let''s go part by part so you can\n43:20 actually see the exact same basically all the prompt templates that we created in demo part one we''re actually now just\n43:26 going to like not only create those prompt templates but we''re actually going to pass over those prompts to our\n43:32 model so let''s look at example one in depth and then we''ll just run it you know for everything else so per usual uh\n43:38 we''re going to create our template with the variable we want to replace once we have that string template we''re going to call chat prompt template and we''re\n43:44 going to make an actual template from that string just so it''s easy for Lang chain to actually start manipulating it\n43:51 and from there we''re going to do exactly what we did last time which is call invoke cuz that''s the magic word for our\n43:56 prompt templates to take in that string and actually start replacing all the values so once we call invoke we''re\n44:02 going to get back that prompt and what''s cool is we can now pass that prompt which is if you remember from the last\n44:08 one it''s now just going to be a message array that has all these values in here so you know it''ll say tummy joke about\n44:14 cat and that''ll be one of the messages one of a messages in our message history that we''re going to pass to our model I\n44:20 hope that makes sense um you whenever I run it it''ll actually make more sense so um but what you can see is we''re now going to pass that prompt over to our\n44:27 model and we''re going to tell our model invoke because we want our model to actually like perform actions on these\n44:33 messages and we''re going to print the result so you''ll start to see you know invoke is going to get called a lot as\n44:39 you start to work with Lane chain more and more and I''m not going to go too deep in part two and part three because we just copied and pasted over all the\n44:45 examples from the last time and now we''re just actually going to see let AI actually run basically run the request\n44:50 which is creating jokes about all these different prompts we''ve created so what we''re going to do call python example two and this is the second example for\n44:57 prompt templates with a chat model let''s go ahead and run that and this will take a few different examples seconds but you\n45:03 can see so prompt template prompt from a single template which was right here tell me a joke about cats so you know\n45:09 sure here''s a joke about cats for you why was the cat sitting on the computer because I wanted to keep an eye on the\n45:15 mouse so obviously they''re pretty corny and same with multiple placeholders so it actually told a short story about you\n45:21 know a short funny story about pandas so here''s our funny short story and then\n45:27 finally when it comes to the end where we''re doing you know prompts with system and human messages what we''re doing is\n45:33 creating three jokes about lawyers and that''s exactly what it did here are your three lawyer jokes so why don''t sh\n45:38 sharks attack lawyers because of professional courtesy so once again pretty corny but um yeah so I wanted you guys to see prompts with prompt\n45:45 templates because right now we''re calling invoke to get the prompt we''re calling invoke again to get the result\n45:51 which leads us perfectly to chains because this is exactly what we''re about to do next where we''re actually like\n45:57 technically we''re chaining prompts with chat models so this will make more sense so we set ourselves up perfectly to get\n46:02 to dive into section three which is all about change so let''s go ahead and start diving into that model now and this will\n46:07 all make sense and it''ll actually be a lot simpler so you''ll see what I mean in just a\n46:13 second all right guys so welcome to the third module in this Lane chain crash course this chain section is by far my\n46:19 favorite part of the whole tutorial and I hope you guys love it just as much as I do so let''s walk through what the heck is a chain why is it important and then\n46:27 just go through a few different examples real quick and then we''ll hop over to the code okay so at a high level what the heck is a chain well inside a lane\n46:33 chain it''s nothing more than tying together a series of task so if you\n46:38 remember so far we''ve been using invoke the invoke function a lot so we''ve been using it to create prompts we''ve been\n46:45 using it to basically have chat models respond to us so what what we could do instead is have the prompt be chained\n46:52 like the output of a prompt whenever we invoke it that output be fed to the model and then that model whenever we\n46:58 call it invoke again it will spit its outputs and it could just go to the next item so as you can see we''re kind of just chaining together different task\n47:05 and we''re passing the input from the first one over to the next one and we just keep going down and down and down\n47:10 so that''s exactly what''s happening right here have a prompt invoke it spit the outputs over to the next thing Tak in\n47:17 those inputs process it with invoke and then spit it out to the next thing so that''s exactly what''s happening with chains and I think you''ll think when you\n47:23 see the code it''s super simple and there''s another common thing when you''re using using chains inside of Lang chain\n47:28 and it''s called Lang chain expression language it''s a fancy way to say and describe like how you can create chains\n47:35 cuz you can go the hardcore code way which we''re not going to get into we''re going to stick to this which is just anytime you want to chain items together\n47:42 in L chain you''re going to use the pipe operator so this is like right above your return key on your keyboard it''s\n47:48 just the straight up and down basically pipe key so that''s how you''re going to create a chain you''re just going to put\n47:53 you know a prompt a chat model and there''s a few other things you could put in here too but this is how you''re going to do it item or a task a pipe operator\n48:00 another task a pipe operator another task super easy to set up and then whenever you want to run the chain all\n48:06 you do is you call the chain and you call invoke on it and then you pass in your input dictionary of all your\n48:11 different keys and values this actually should be a quick uh my bad this should just be a quick dictionary just like\n48:17 this and that''s how it works so this initial key dictionary gets passed in over here as the first into the first\n48:23 prompt then it gets passed along all the way through so that''s just at a high level just a simple chain now let''s just\n48:28 I just want to show you the realm of the possibilities when it comes to Lang chain first and then we''ll hop into the code okay so when it comes to chain\n48:34 possibilities the first thing to know is like you don''t have to do just prompt chat model you can actually keep going\n48:40 so you can just continually keep going as long as you want that''s option one the other thing that''s super cool is you\n48:45 can run task in parallel so you can have task you know just like oh kick off kick\n48:50 off and you can just start running your chains in parallel and then you can actually even at the end of them you can actually join them together and have\n48:57 your final results all come back and be fed into one so this is a cool way just to like if you need to do some parallel\n49:03 processing this is a cool way to do that the final thing is branching so branching is a way inside of your chains\n49:10 to have it to go like let''s you know at this node let''s kick off some actions cool well based on the results of those\n49:16 actions let''s just say like oh it''s um if we''re doing a review of a movie well if the movie was good cool we''re going\n49:23 to go down this branch and this chain path if the movie was awful we''re going to go down this path and if it was so so\n49:28 we''re going to go down path C so that''s just kind of like the realm of the possibility when it comes to chains but enough like theoretical highle stuff\n49:35 let''s actually dive into the code and start working on the simple model first and then we''ll work our way up through all the different possibilities so let''s\n49:41 go ahead and dive into the code all right everybody Welcome to the first code example when it comes to\n49:48 chains I think once you see how this all works you''re going to be like oh my gosh this is so much easier than what we were doing beforehand and what''s cool is\n49:55 we''re going to use basically the same examples that we did from beforehand but just reformat them so they work with chains so let''s go ahead and dive\n50:01 through this and walk through it step by step so the first thing that you notice we''re in the chain Basics file what we''re going to do is exactly what we''ve\n50:08 been doing so far with everything else we''re going to create our chat model once we create our chap model we''re\n50:13 going to create our prompt template our prompt template is going to contain our variables that we want to replace in\n50:19 this case it''s going to be the topic and the joke count from there what we''re going to do is create our chain now what\n50:25 you''ll notice this is very similar to to the drawings I just created a few seconds ago the first thing we''re going\n50:30 to do is have our prompt template which is going to contain all the messages that we want to basically replace with\n50:35 our inputs later on create our prompt from there we''re going to pass that prompt to our model once our model then\n50:42 has basically is able to process that prompt what we''re going to do eventually is spit everything out to a string\n50:48 parser at first I''m going to get rid of this string parser just so you can see why we need it but just know at a high\n50:53 level all it does is it takes you know how so far every time we get the result we do do content well that''s exactly\n50:59 what the string parser does for us it just goes ahead and grab the content but we''ll we''ll add it back in in a little bit so comment it out just so you guys\n51:06 can see it how it works okay cool so what I''m going to do is go ahead and run this chain just so you guys can see how\n51:11 easy it works and how simple it is how much less code we''re doing in this approach versus what we were doing last time so what I''m going to do ahead is go\n51:18 ahead and open up our terminal and then if you remember now we''re in the third module cuz we''re working with chains and\n51:23 this is the basic example so we''re just going to run python three so we can go open our chain file and this is the\n51:29 first example and what I would expect to see is once we print the result we''re going to get that long response that we\n51:35 normally get whenever we don''t grab the content so you can see yep here are whenever we look at the content you can\n51:41 see yep here''s the three lawyer jokes and then we''re seeing all that extra information that we get back from our LM\n51:46 like tokens and so forth but if we undo our changes earlier and just use the\n51:51 string parser that you can see right here where we''re just doing the string output parser this also comes from we''ll\n51:57 talk about this more but the output parser Library that''s where it''s coming from but if we rerun it now what you''ll\n52:02 notice is it actually just spits out the final text like this is what we would actually expect to return to our\n52:08 customers if this was a web app or you know this is what we would like most likely pass to the next model if we were\n52:14 doing this you know and continuing the chain so this is awesome I hope you guys see how much easier this is to use\n52:20 compared to what we were doing last time i'' actually let''s just do a side by side so you guys can kind of see exactly what\n52:25 this looks like so over here what we were doing yeah at the bottom we had our you know we\n52:31 created our prompt templates then we did our prompt then we invoked it then after we were done invoking it we saved the\n52:37 result then passed it to our model then did it again and again so all of this code right here basically uh just gets\n52:44 replaced with this single one line which is beautiful so I hope you guys like this so now that we understand the\n52:49 basics what I want to do is we''re going to dive deep for a second in the next example where we''re going to understand\n52:55 how chains work under the hood and after we do that we''re going to start building off those examples you saw earlier where we''re going to do you\n53:01 know pillow chains and then branching and much more so let''s go ahead and dive into the next\n53:07 example all right so welcome to the second example in the chains module and\n53:12 in this example what we''re going to do is a deep dive into how chains work under the hood now this isn''t super\n53:18 important for you to like memorize or understand exactly how everything works under the hood it''s just I want you to\n53:23 be aware of a few Concepts such as runnable sequences runnables and runnable lambdas those are the main\n53:29 three things that we''re going to talk about in this section okay so let''s dive into the code and we''ll look around so\n53:34 per usual what we''re doing is we''re creating our models and loading our environment variables creating our prompts and then most of the new action\n53:42 when it comes to creating basically chains the manual way happens right here\n53:47 so what we''re going to do is talk about this at a high level and then we''re going to dive into each part so at a\n53:52 high level what we''re doing is we are creating runnable lambdas so think of a\n53:58 runnable is a task whenever I was talking about task in the initial outline for chains under the hood the\n54:04 way it actually works in Lane chain is it''s just called a runnable so what we''re doing a runnable Lambda is nothing\n54:10 more than a task that''s a Lambda function and if you''re newer to python don''t worry about this Lambda functions\n54:17 kind of get like a little bit more into weeds but they''re just a quick way to make functions so like here''s the input\n54:22 to a function and then here''s the actions we want to take so that''s just kind of what Lambda functions are so\n54:28 that''s part one part two is this thing called a runnable sequence and a runnable sequence is nothing more than\n54:34 we''re going to do this task then this task and this task then this task and put all those tasks together you have a\n54:40 sequence AKA a runable sequence AKA a chain so that''s exactly what''s happening\n54:46 so let''s actually dive in deeper into these runnable lambdas and runnable sequence so you can actually see what the heck''s going on so the first thing\n54:52 is what we''re going to do is we''re going to create the format prompt runnable Lambda and what this is going to do is\n55:00 all it''s going to do is take in our prompt template that we''ve done in the past and then instead of calling invoke\n55:05 on it what we''re going to do is just call the format prompt and the format prompt is basically just going to like\n55:11 yep replace all these you know values right here with these values down here\n55:17 so that''s exactly what''s happening under the hood and you can see we are passing all of our arguments and spreading them\n55:23 right here that''s exactly what that''s doing basically we''re just passing this dictionary and spreading it so that we can replace all these key values with\n55:30 these values down here so that''s what''s happening don''t have to understand how it works exactly I just I just want you to be aware of like what''s happening\n55:36 from there once we have our prompt and we''ve created that task we''re going to create an invoke model task and the\n55:41 whole point of this runnable Lambda once again just a task that takes in a Lambda function all this is going to do is it''s\n55:47 going to say yep we have a model we''re going to invoke the model and what we''re going to do is pass in a you know pass\n55:54 in the input from the previous function into this one and we''re going to convert whatever the heck we just received to\n56:00 messages so that''s what''s exactly happening there finally we''re going to parse the output so if you remember the string output parser well under the hood\n56:07 all it''s doing was grabbing the content from the output and that''s what we''re going to do right here so here''s how now\n56:14 that we have all these individual runnables here''s how it works with a sequence and let me just show you under\n56:19 the hood what a sequence looks like so um just in case you haven''t seen this if you have a Mac you can hit command and\n56:25 click basically any any class and it''ll take you to the definition if you have a Windows just hit control and then click\n56:32 the the class and it''ll take you to the basically the the definition but basically this is recapping everything\n56:37 that we have just talked about you know a runnable is nothing more than just like it takes the output from one passes\n56:44 it to the next as an input so just we really just creating a sequence of task that need to occur they give us a cool\n56:49 little example just to show us like yes whenever we use Lang chain expression language to where we just like pipe task\n56:56 and runnables together it''s actually the same thing as you know calling runnable sequence first and last and then once\n57:02 you have a sequence you can invoke it and pass items into it so that''s what''s happening under the hood so let''s\n57:07 actually hop back to our code example and finish this one up real quick so we can actually keep keep going down the chain what you''ll notice is whenever I\n57:14 created my runnable sequence you''ll have three different properties that you can call and this is because if you scroll\n57:19 down to the actual code part you can see there''s three different things there''s first middle and last first is a single\n57:25 run last is a single runable and middle is a list so that''s just how you have to pass\n57:31 in your sequence so if you have two items you''ll call first and last if you have five items 10 items 100 you know\n57:38 you''ll have to call first for the first item last for the 100th and everything in between will just get passed in as a\n57:44 list so that''s exactly what''s happening so in our case the format prompt where we convert our text over to a prompt\n57:50 happens as the first task our middle task where we have our formatted prompt and pass it to to our model so we can\n57:57 actually process it that gets passed in the middle list and then finally for the last thing once we you know have our\n58:03 prompt pass it to our model process it in Ai and we just want to get that final text output well that''s why we''re going\n58:09 to call the last runnable task which is just going to be Parts output and when you put all that together you end up\n58:14 getting a chain and that chain is exactly we''ll call it the exact same way that we were doing earlier so what I''m\n58:19 going to do I''m going to run the code real fast just so you can see how it works and then I''m going to compare them side by side just so you can see like\n58:25 that''s crazy that they do the same thing but this just takes so much more work cuz we''re not using the L chain\n58:31 expression language so this is chains under the hood so we''re going to run it and as you can see whenever it finally\n58:36 gets done it''s just going to spit out a response which is you know just three jokes about lawyers so yeah that''s just\n58:43 that''s three different jokes and just to show you a quick side by side let''s go open up three one chain Basics and zoom\n58:51 out yeah so instead of doing all this nonsense where we''re creating runnable lambdas and passing stuff over and over\n58:56 and creating runable sequence just stick to L chain expression language and it just works so I hope you guys Now\n59:02 understand why L chain expression language is so nice and that''s probably what you''ll use 99% of the time whenever\n59:08 you''re working with building chains but enough of that going deep let''s actually hop back up and start working on some of\n59:14 the additional ways we can use chains so we''re going to broaden them out in the extended section and then we''ll keep moving on from there so let''s go ahead\n59:20 and hop over to part number three all right guys so welcome to the\n59:26 third example in the chain module and in this example we''re going to Showcase how you can continually extend your chains\n59:33 and add on additional runnables or probably the easier way to think of it is additional task to continually happen\n59:39 and you can just keep growing that chain forever so let''s just go ahead and look at the code in this example so you can see how it actually works so what we''re\n59:45 going to do is the per usual where we''re going to load our environment variables then we''re going to create our model and\n59:50 then we''re going to create our prompt that''s all the usual stuff all of the new chain extending has happens right\n59:57 here where we''re going to uppercase the output and then we''re going to count Words so let''s actually dive into what''s\n1:00:02 happening well what we''re going to notice is we''re creating our usual chain and then we''ve piped in an uppercase\n1:00:09 output well under the hood when we actually look at what''s going on in this function because it is important\n1:00:14 whenever you extend or add new items to your chain by doing the pipe operator you need to add runnables that was one\n1:00:21 of the key things we talked about in the under the hood section so in this case we''re going to add a run Lambda and\n1:00:26 runnable lambdas just make it super easy to like basically process any type of function so in our case we''re just going\n1:00:32 to do some like string manipulation where we''re going to Upper C like sorry I I''ll slow down for you guys but like\n1:00:37 we''re going to take in an input and what we''re going to do is call uper on it which is going to capitalize the entire output so that''s what''s happening under\n1:00:43 the hood but what''s cool with runnable Lambda functions is you could also use these to go off and make like an API call you could you know you could really\n1:00:50 do anything that you normally could do with python code you could just throw it in a runnable Lambda function so that''s the part that makes this really cool but\n1:00:56 so now that we have uppercased the output what we''re going to do is continue building on our chain and we''re going to pipe the output of this over to\n1:01:03 count words so in this case when we come look at our count Words runnable Lambda you can see we''re going to taking an\n1:01:08 input this input is going to return basically a string that we''re going to do some interpolation on which basically\n1:01:15 just means some like formatting and passing in values so the first thing we''re going to do is count the word count so what you can see is we''re going\n1:01:21 to split the input that''s given to us and then once we''re done with that we''re going to count count how long the actual\n1:01:28 input was and then once we''re done with that we''re just going to spit out like right here it just says you know curly\n1:01:33 brackets X that''s where we''re just going to print out the input that was given to us enough talk let''s actually dive in so\n1:01:38 you can look at what actually happens under the hood when we actually run this so this is the third module for chains this is the third example and when we\n1:01:45 spit it out we would expect to see the word count yep just like that and then jokes so as you can see this code ran\n1:01:51 perfectly we spit out the output and you can actually see that this code ran too because everything is capitalized where\n1:01:57 it wasn''t originally so this is how you can continually build chains and as you''re going off and becoming you know a\n1:02:03 real world professional L chain developer some of the other practical things you would do is just like I said you would use runnable lambdas to go off\n1:02:09 and like create API calls to go do something you would then there''s just a ton of stuff that you could do and\n1:02:14 chains make it super easy to you know pipe an input from the previous result over to your current function and just\n1:02:20 continue going down the chain so this is awesome so now that we''ve covered extending and continually growing your chain let''s hop over to talk about how\n1:02:27 you can actually run chains in parallel I think you''re really going to like this section too all right guys so welcome to the\n1:02:33 fourth example in the chain module in this code what we''re going to be doing is showing off how you can run chains in\n1:02:40 parallel so I think this one''s super cool cuz if you''ve ever thought about creating social media posts where you\n1:02:45 have like an idea and then you want to like Branch off to write something on LinkedIn and then Twitter and then elsewhere this is exactly how you''re\n1:02:52 going to go about doing it so let''s go ahead and dive into the code at a high level and then I have a visual for you guys just cuz it is it is a little bit\n1:02:58 weirder to see how it all works and then we''ll come back to the code to like really dive in okay so at a high level\n1:03:04 this file is a little bit longer and I''m just going to hop down to the chain part and then from there we''re going to go over to the visual I was just talking\n1:03:10 about so at a high level what you can do is you can see we are creating a chain this chain has the usual prompt has a\n1:03:16 model we do some string outputting and then we start doing this funny thing called runnable parallel under the hood\n1:03:22 basically what this is doing is it''s creating different branches to run you know if we''re looking at Pros cuz this\n1:03:28 is going to be a pro con list and we''re just going to run run our Pros in one chain and we''re going to run our cons in\n1:03:34 another chain and eventually merge those Pro cons into a final list so that''s why we have two branches one for pro a pro\n1:03:40 branch and then a con branch and that''s where they''re going to go off and generate pros and cons and we''re going to come back and put into a final list\n1:03:46 so that was high level just talking through the code so let''s hop back over to what this looks like under the hood\n1:03:52 when we''re actually like you know visually what does this look like so that chain that we were just looking at\n1:03:57 what we''re going to do is it''s going to take in a prompt this prompt is going to take in a product name from there it''s going to construct a prompt using in our\n1:04:04 case I think we''re going to do the MacBook Pro and then from there it''s up to the model to produce a list of features about that product from there\n1:04:11 we''re going to grab the content like we normally do from the result from the model and just pull out the content string and what''s cool is we''re going to\n1:04:18 pass that string to our two separate chains so I don''t know if you saw back over a second ago but we had a pro chain\n1:04:24 and a con chain that''s exactly what we''re doing here so the pros it''s going to take in you know the list of features\n1:04:30 and in our case we''re going to pull out all the pros from those features and then what we''re going to do on the other side in parallel is find all the cons\n1:04:37 for all those features and then finally once we''re done with both of those different chains that are running in\n1:04:42 parallel we''re going to grab both of the outputs and then put them together in a final Lambda runnable so we''re going to\n1:04:50 get both of those outputs and then put them in a basically a nice print statement for our users to read pretty easily so that''s exactly what we''re\n1:04:56 doing now let''s hop back to the code and do a deeper dive now you have a good understanding of like what''s happening\n1:05:01 at a super high level visually all right let''s hop back okay let''s dive into this part by part and I think the best way to\n1:05:07 go about it is just start you know at the first item in each chain and then walk our way through so when it comes to\n1:05:13 the prompt template this is where we''re going to be doing our normal prompt template stuff meaning you know we''re just going to say like hey you''re an\n1:05:18 expert on doing product reviews and I need you to basically list the main features for this product we''re going to\n1:05:24 create a prompt for it in our case like I said this is going to be a MacBook Pro and then we''re going to pass that prompt over to our model who''s going to go off\n1:05:30 and actually list those features out then we''re going to do our string output parser none of this should be new so far\n1:05:36 but then we''re going to hop into this runnable parallel basically you know runnables are how chains work under the\n1:05:42 hood so in this case we''re doing parallel meaning we have a bunch of branches that we can spin off our\n1:05:47 different chains so in in this case we''re going to have a pro branch and a cons branch and you can actually dive in\n1:05:54 and see what each one of these these are so this Pros Branch under the hood it''s a runnable Lambda and what we''re doing\n1:06:00 is we''re actually just creating in our case we''re creating a new chain right here that''s exactly what it is I can\n1:06:06 actually rename it for you guys so you can actually see exactly what it is we''ll call this one a chain that one a\n1:06:12 chain perfect just so it makes a lot more sense and it''s even more clear what''s actually happening so we''re creating a chain for our prob branch in\n1:06:18 this case we''re saying like hey take in the input which in this case is a feature list I want you to analyze those\n1:06:25 probes so analyze Pros all it does is it creates a new prompt template just like we normally do and once we have that\n1:06:31 prompt template what we do is we pass that prompt template over to our model for our model to go off and generate\n1:06:36 those pros and then finally per usual we use the string output parser to convert that output into a nice string and the\n1:06:43 con branch is the exact same thing we take in that list of features about our product we pass that list of features\n1:06:49 over to this prompt template so you know given these features list the cons of each one of those features from there\n1:06:56 pass it over to our model string output so all this is completely normal we''ve done this a ton of times now here''s\n1:07:01 what''s different after we run our parallel branches what we end up with is actually one big dictionary and that\n1:07:08 dictionary if we''ll print it out I''ll print it out for you guys so you can actually see what I''m talking about right here so we''ll say final output\n1:07:14 what you can see is it''ll actually include something called branches and this branches will under the hood\n1:07:21 actually list out our pros and cons cuz that''s how we''re going to save them and you''ll see it in a second whenever I print it out but that''s the final part\n1:07:27 what we''re going to do is pass in our Pro output and our con output into this combined Pros con function and what this\n1:07:34 is going to do is just print out a nice list string final output for us so we can compare like yep should we get the\n1:07:40 Mac should we not get the Mac so let''s run it and actually dive into the outputs so let''s come down here let''s\n1:07:45 clear all old outputs up and start running it now what''s going to do under the hood like I said it''s going to start running and the first print output we\n1:07:52 should see is this final output right here where we''re going to have our Pros chain and our cons chain we''re going to\n1:07:58 be able to view both of them that''s what''s going to happen in here and then finally afterwards we should expect another final output I just want to show\n1:08:04 you guys under the hood how the inputs are getting passed into this and how we can actually view like yeah it does make\n1:08:10 sense that we need to grab the Branch''s object and then within the Branch''s object we need to grab the pros and cons\n1:08:15 so let''s scroll up so you guys can see exactly what I''m talking about takes a few seconds Okay cool so here''s where\n1:08:21 everything''s in a dictionary that''s why it''s all cluttered to see but you can see final output what it does you can\n1:08:27 see it''s a dictionary inside the dictionary you can see the first object inside of it is called branches and then\n1:08:34 within branches we can grab pros and then if you scroll down a little bit later you can also see cons if I just\n1:08:40 search for it you''ll be able to see it cons let''s see yeah so you can see here''s the cons object so how does this\n1:08:46 actually work under the hood well it''s because everything in this runnable parallel is equal to we basically\n1:08:52 created a dictionary right here because you can see we have branches is equal to and then we''re creating our object so\n1:08:58 Pros are going to be saved as the result of this Pro chain and cons are going to come from this Con chain so that''s\n1:09:05 exactly how it works so I hope that makes sense once again this is getting a little bit deeper into the weeds when it\n1:09:10 comes to working with Lane chain and we''re starting to do a little bit more like fanciness with python so if you have any questions on any of this please\n1:09:16 definitely go over to the school community that I''ve created for you guys drop a comment you know take a screenshot of what''s con confusing you\n1:09:22 and I''d be happy to help or hop on our weekly coaching calls but I hope you guys see the value in this because in\n1:09:28 the past you would have had to run your Pros chain that would have taken 30 seconds then you had to run your cons\n1:09:33 chain that would take another 30 seconds but now when we can run stuff in parallel we can cut it down and make things a lot more efficient if you''re\n1:09:39 going to be building applications for your users where they''re expecting a speedy output this is exactly what you''re going to want to do to help\n1:09:45 automate and speed up some of your task all right so enough of talking about running chains in parallel let''s go over to learning how we can start branching\n1:09:52 our chains based on different conditions let''s go ahead and dive into task numberb five next see\n1:09:58 y all right guys so welcome to the fifth example in the chain module and this\n1:10:03 final example what we''re going to do is cover branching and I think the best way to cover this topic before diving into\n1:10:09 the code is to actually look at what this code looks like visually so let''s hop back over here to excal draw and let\n1:10:16 me run you through what''s actually happening under the hood so there are two parts to this chain the first part\n1:10:22 is what we''re going to call the classification chain and what we''re trying to do is we''re trying to take in feedback from our customers so they''re\n1:10:29 going to say like man that was awesome or like oh that was the worst product I ever bought and depending on what type of feedback they give us we''re going to\n1:10:36 have different prompts to generate different types of messages so we''re going to have a positive Branch we''re\n1:10:41 going to have a negative Branch a neutral branch and then if something goes wrong we''re going to have an escalation Branch so here''s a quick run\n1:10:47 through of what''s Happening visually and then we''ll hop back over to the code so visually what we''re going to do is do the normal chain for our classification\n1:10:54 where we''re going to have have a feedback prompt what we''re going to do is in this prompt we''re obviously going\n1:11:00 to populate the prompt with the user''s input in this case it''s going to be their feedback about what we''re doing\n1:11:06 then we''re going to categorize that feedback into one of you know one of these four keywords positive neutral\n1:11:11 negative or escalate so that''s how we''re going to basically classify everything then we''re going to pull out the\n1:11:17 category with a string parser like we''ve done a ton of times by now and here''s where the New Concept comes in we''re\n1:11:23 going to it''s going to be called a runnable brand and basically the way this works is it''s think of it as like an if statement so\n1:11:29 if the you know if a conditions met we''re going to run a different chain so this is a really good way to like really\n1:11:36 you know make your chains more intelligent and in different scenarios run different chains so in this case\n1:11:41 just you know going deeper we''re going to check if the category was positive if it was fantastic we''re going to run the\n1:11:47 positive chain was it negative cool we''ll run the negative chain so forth and so forth so I hope this all makes\n1:11:52 sense now let''s hop back over to the code so you can see what this actually look looks like inside of Lang chain all\n1:11:57 right so welcome back to the code and I think what you''ll notice is this is going to be super straightforward so\n1:12:03 what we have to start is we have a bunch of different prompt templates each one of these prompt templates for positive\n1:12:09 feedback negative feedback neutral feedback and escalate feedback all these different prop templates come from right\n1:12:15 here so as you can see across the board we have all these different prompts that''s what we''re doing we''re building up each one of these chains for the\n1:12:21 different types of feedback all right let''s keep going then eventually what we''re going to do is we also need to create the prompts for our\n1:12:27 classification part so this is you know how you know your helpful assistant help us classify the sentiment of this\n1:12:34 feedback is it positive negative neutral or do we need to escalate it and this is how we''re you know using our prompts to\n1:12:39 pass in our users feedback cool so here''s where we''re going to get into the new part which is the runnable branch so\n1:12:46 this is the part that is different than anything you''ve seen so far so what you''re going to do is you''re going to create a runnable branch and the\n1:12:52 synopsis of how this works is you''re going to have a input so in this case we''re just going to use a Lambda\n1:12:58 function and then you have a conditional right here so in this case we''re going to say like hey is the word positive in\n1:13:05 the input that you just gave me if so fantastic I''m going to run and this is basically oops sorry this is basically\n1:13:11 the positive chain I''ll just write right here back chain fantastic so this is\n1:13:17 actually what''s happening under the hood we didn''t full out you know call each one of these their own chains but each one of these is a neutral chain a\n1:13:24 positive chain and a negative chain so here''s you know this because this is once again we''re using Lane chain\n1:13:30 expression language to create a positive chain so as you can see we''re just going to take in the promp template that we saw above we''re going to pass that to a\n1:13:37 model and then we''re going to get back the output so this is a message that we could respond to our users saying like oh you loved our product fantastic I\n1:13:44 really appreciate it and we hope you you know continue to buy further products and services from us okay so now you''ve\n1:13:50 kind of seen how we''re going to use runnable branches to conditionally run different chain such as positive chains\n1:13:56 check to see if it''s negative to run the negative chain see if it''s neutral to run the negative chain the other part that I haven''t mentioned yet is there''s\n1:14:02 a default so when you''re using runnable branches if any of these messages didn''t\n1:14:08 trigger off like if we didn''t run you know the positive negative or neutral you basically can create a default\n1:14:13 branch and in this case we''re going to call it the escalate branch and what this one''s going to do is it''s actually\n1:14:19 just going to make a nice message for us that we can see where it''s going to like hey generate a message to escalate this to a human agent so that''s just kind of\n1:14:25 how it works and this could definitely be super helpful in a customer service type of situation so I just wanted to show you guys this cuz it''s super\n1:14:31 helpful and helps you build more complex tools all right so now that we got the runnable branches out of the way let''s go back to see how we can make this work\n1:14:37 so basically in the end we''re going to have our classification chain and then you know which is just going to be this\n1:14:42 whole part right here this is the entire classification chain right here and then the entire bottom part all of this huge\n1:14:49 square right here is going to be this second chain so we''re chaining chains together to make super long complicated\n1:14:55 ones I hope you guys like this all right so what we''re going to do next is I''m just going to go ahead and run it so you guys can see the results and if you want\n1:15:01 to play around with this code for yourself what you''ll notice is I went ahead and typed up a bunch of different types of reviews so you can experiment\n1:15:07 with each of the different branch pths so let''s go ahead and run it for a positive review fast just so you can see you know how it works so we''re just\n1:15:14 going to go open up our terminal and this time what we''re going to do we''re going to do Python and this is the third\n1:15:19 module so chains five to run this branching program so we''re going to run it and what we would expect to happen is\n1:15:25 it to ex to go down the positive branching path and generate a nice positive message so you can see you know\n1:15:31 I hope this finds you well I just want to personally thank you for your positive feedback so you can see it''s actually it''s actually working and what\n1:15:37 we could do too if you want to try out something different let''s try out a bad review just so you can kind of see how\n1:15:42 it would go down the in this case go down the negative branching path so I replaced it with the negative comment\n1:15:48 it''s terrible I broke it after just one use and then you can run it again so let''s see you know thank you for your\n1:15:54 feedback I''m sorry that your experience was not up to your expectations we take all this feedback so as you can see this\n1:15:59 is a way we can handle a bunch of different scenarios using GL chain expression so yeah that concludes the\n1:16:05 third module where we''re covering chains and what we''re going to do next is move over to rag which is going to be what\n1:16:11 we''re learning about retrieval augmented generation this is going to be a huge module I hope you guys are pumped for it\n1:16:16 let''s go ahead and dive into it now all right guys welcome to module number four where we''re going to start\n1:16:22 learning about retrieval augmented Generation all also known as rag so in this quick overview section what I want\n1:16:28 to do is I want to talk about what is rag why do we need it and then I want to do a high Lev overview of like just\n1:16:34 running through a simple example just so everything makes sense whenever we start to dive into these code examples that\n1:16:40 I''ve set up over here for you guys okay so let''s just go ahead and just talk about what is rag at a super high level\n1:16:45 and kind of why do we need it well these large language models that we have set up like chat gbt Gemini and llama 3 all\n1:16:52 of these models have a con straint on how much knowledge they already have and that can be a problem whenever we want\n1:16:59 to start answering questions or whenever we have questions about additional things such as like hey what''s happening\n1:17:05 today in the news well these llms have a cut off date of you know a few months or even a year ago so it has no idea what''s\n1:17:12 happening right now also when it comes to maybe just like inside of your company you have some specialized\n1:17:19 documents for y''s processes or you have some other information about a product You''re Building well the LMS obviously\n1:17:25 don''t know anything about it so it''s up to us to use rag to feed in additional\n1:17:31 new information to these lolms so under the hood all we''re trying to do is just give these lolms additional information\n1:17:37 so they can answer our question that''s all that''s going on at a high level so as you can see we can feed in things\n1:17:42 like PDFs so whenever you hear people talking to PDFs or chatting with them that''s exactly what''s happening they''re using rag so we can feed in websites we\n1:17:49 can feed in source code and video transcripts and the list goes on and on but okay that''s enough at a high level\n1:17:55 so what I want to quickly do next is let''s dive over to a big example that I set up for you guys where we''re going to\n1:18:01 walk through exactly how rag works at a super high level so when we hop over to the code and we go through all these\n1:18:06 different examples you''ll have a high level understanding of like oh that''s why we''re doing what we''re doing so let''s go ahead and dive into that high\n1:18:12 level overview right now all right guys so welcome to this quick overview of how rag works now I''ve\n1:18:19 put together this flowchart for you guys so that we can understand two critical components of working with Rag and I''m\n1:18:26 going to walk through both of them at a high level and kind of go super quick because we''re going to go through this a bunch more times as we actually dive\n1:18:32 into the code but the main thing I want us to take away from this demo is for you understand the Core highle Concepts\n1:18:39 such as terms like vector store embeddings llms chunks tokens we''re\n1:18:44 going to learn we''re going to just quickly say all these words and it''ll mean a lot more when we dive into the code and that''s where we''re going to be covering in part one and then part two\n1:18:50 of this quick demo I want to walk through like great well once we have actually saved all of the information\n1:18:56 from our PDF over to this special database called a vector store well how can we actually start asking questions\n1:19:03 and pulling out information so that we can get a AI generated response that''s\n1:19:09 informed meaning if we''re you know chatting with a document about how to like cook a specific food well we want\n1:19:16 to pull out the specific instructions for that food along with our question to generate an AI response so that''s what\n1:19:22 we''re going to be talking about at a high level and we''re going to go pip part and this will hopefully all make sense and I do think one of the core\n1:19:29 parts to help this make sense of like why we''re doing what we''re doing is to actually start at the vector store and\n1:19:36 work our way back up so you understand why we''re doing it because and this will make more sense so hang on for two\n1:19:42 minutes and it''ll make all sense okay so at the end of the day everything that we''re trying to do comes to this part\n1:19:47 right here it''s the bridge and connection between our database which stores all the information about our\n1:19:53 Tech Source plus this Retriever and this retriever is where we''re going to like\n1:19:59 hey man please give me all the information about you know let''s just say we''re working with Harry Potter and\n1:20:05 we ask like hey which Professor also is a werewolf well what we want to do in this example is we want to be able to\n1:20:12 look up things such as like professor and werewolf how well if we were manually doing this what we would do\n1:20:18 just like command F in a PDF and just search for those words a thousand times but there''s going to be a ton of entries\n1:20:24 and what we''re trying to do is really find a werewolf and a professor that''s the information we''re looking for okay\n1:20:29 so what we''re trying to do and this is a keyword here we''re trying to look for similarity inside of the original teex\n1:20:37 source that we''re looking for that''s all we''re trying to do we want to look for similar pieces of information inside\n1:20:43 this original PDF that talk about professors and werewolves okay so that''s all we''re trying to do here now well how\n1:20:49 does this actually come into play when it comes to rag well what''s going to happen under the hood is this PDF of\n1:20:55 Harry Potter is going to be broken up into a thousand different chunks because you can see this PDF is has about 10\n1:21:01 million tokens and that''s way too big for us to answer any questions or like analyze what''s going on because chat gbt\n1:21:09 has a window of about 8,000 tokens and 10 million is obviously way too big to pass into it so what we need to do\n1:21:15 because we''re just trying to pull out the similar pieces of information and pass it over to jbt so it can generate\n1:21:20 an AI response about it so what we need to do is we need to split this up into a bunch of smaller chunks that are about a\n1:21:27 th000 each what''s great about this is because like I said chbt can take in 8,000 prompts 8,000 tokens so if we pass\n1:21:35 in one chunk up to three or four chunks there''s still plenty of room for us to also pass in a question about those\n1:21:42 chunks okay hang on we''re getting there all right well each one of these chunks is just plain text so you can see like\n1:21:48 chunk one is just like you know Harry you''re a wizard and you know and basically how this is working under the hood is we''re just starting at the\n1:21:55 beginning of the book taking out a thousand words putting it a chunk putting it getting the next few thousand words putting in another chunk well the\n1:22:01 problem is we said our original goal is we went to look up similar phrases well how the heck does AI compare and search\n1:22:09 for similar phrases well this is where the term embedding comes into play an embedding is nothing more than the\n1:22:17 numerical representation of text so that was a mouthful what does that actually mean well if we were to like in this\n1:22:23 super simple example if we were to embed the word dog cat in house you can see\n1:22:28 that a dog has this embedding a cat has this embedding and a house has this\n1:22:34 embedding and you''ll notice they''re nothing more than just a list of numbers but they actually under the hood have\n1:22:39 some meaning and relationship to to one another so for example a dog and a cat\n1:22:44 are super similar the only thing that''s different are these last two numbers but even then they''re not that different cuz\n1:22:50 they got four legs they''re furry they live in a house but a house on the other hand it''s got a roof it''s got walls\n1:22:56 brick they''re completely different subjects so if we were to ever like search about dogs or animals with four\n1:23:03 legs well these two things would come together these are super similar so we would get responses about dogs and cats\n1:23:10 so with the point I''m just trying to get across is embeddings allow us to easily in search for similar items and the way\n1:23:18 we''re able to do this cuz like machines love numbers and numbers are easy to compare and see how different and\n1:23:24 similar things are okay so what we''re going to do is we''re going to convert these chunks of text over to embeddings\n1:23:31 so that we can easily compare them and contrast them and once we have all these different chunks of text embedded what\n1:23:38 we''re going to do is start saving the embedding plus the original text to a vector store and the reason why this is\n1:23:44 important is because going back to our original problem what we want to do is we want to be able to ask questions and\n1:23:50 retrieve related documents from this Vector store all right so now let''s walk through this flow down here now that\n1:23:56 we''re in part two and you''ll see exactly why everything that we just did above is super important so going back to us with\n1:24:02 our Harry Potter fans if we ask a question which Professor is also a werewolf well that question will be\n1:24:09 embedded itself because what we''re trying to do is see we''re trying to look up similar documents to our question and\n1:24:17 the way we''re able to do that is because our embedding we now have a numerical representation of Professor and werewolf\n1:24:24 and and then eventually cuz now it''s embedded now whenever we go to our use our it''s going to be called a retriever\n1:24:30 whenever we use our retriever to look up similar embeddings in the vector store it''ll go like oh well chapter 4\n1:24:36 paragraph 1 2 and 3 all talk about this professor that''s a werewolf let me give you back those chunks of data or those\n1:24:44 Blobs of text so that you can then ask questions about them so that''s all that''s happening under the hood we go\n1:24:50 step one here''s my question here''s my embedding that has my question great let''s return this chunk and this chunk\n1:24:57 because those were the most similar and once we have those chunks returned what we can do is pass them over to chat gbt\n1:25:04 and what''s going to happen under the hood that you''ll see is all that''s doing is is just adding those pieces of text\n1:25:09 at the very beginning of our prompt and then at the very bottom we''ll have our question and then we can now ask\n1:25:15 questions about those pieces of information so what it would really be is like which Professor is a werewolf\n1:25:21 and then these three chapters or these three PA paragraphs will have all the information we need to generate an AI\n1:25:27 informed response now that was a ton of information don''t worry we''re going to go part by part when we look at the code\n1:25:33 it''s just at a high level the main thing I want you to understand is we have huge Tech sources that have way too much\n1:25:38 information for any AI to consume we need to break it up into smaller chunks those chunks need to be converted over\n1:25:45 to eddings the reason we need to do that is because we need to see how similar each different chunk is to one another\n1:25:51 so that later whenever we go to ask questions about those different pieces\n1:25:56 of text and embeddings we can go oh yep these two are super similar here''s the information you requested back so that\n1:26:02 you can ask a question so that was a mouthful let''s actually go ahead and start diving over to the code now so we\n1:26:08 can see all of this in action and I promise you after you see this a few times it''ll all make sense and you''ll be\n1:26:13 able to start building your own rag use cases so let''s go ahead and hop back over to the code all right guys so welcome to the\n1:26:20 first code example inside of the rag module and I''m super excited you guys to see how everything ties together that we\n1:26:27 just walked through in the highle drawing so this is exactly where you''re going to connect the dots you''re just to paint a road map of what we''re about to\n1:26:33 do is I have broken up the first code example into two parts as you can see there''s a 1 a and a 1B 1 a directly ties\n1:26:41 to the initial breakdown of what we were setting up top in our initial drawing\n1:26:46 drawing where we were first converting some Tech Source over to our Vector store so that''s going to be in 1 a and\n1:26:52 then 1B is going to be all about actually asking questions and retrieving documents from that Vector store so\n1:26:59 everything we talked about when it come to like embedding chunking saving stuff through the vector store that''s all\n1:27:04 going to happen in 1 a and then building our retrievers once again embedding you''re going to see that all in 1 B okay\n1:27:10 so we''re going to go ahead now and start walking through this code part by part and some of this code is a little bit\n1:27:15 more advanced just because there''s a lot more moving Parts but don''t worry I''m going to cover everything and once again\n1:27:21 if you have questions you can always head over to the fre School Community I created for you guys just take a screenshot or a Tim stamp of the video\n1:27:27 and say Here''s Where I''m stuck I don''t understand this and myself or someone else in the community will be sure to help get you unstuck so you can keep\n1:27:33 going and building out your awesome projects all right so let''s go through this part by part and also walk through some of the folder structure just so it\n1:27:40 all makes sense okay so the first thing that we''re going to be doing is inside of 1A is we need to set up some file\n1:27:47 paths so that we can connect to different parts of our project so the main things that we need to connect to\n1:27:53 is our initial text source so this is the document we''re going to be asking questions about in this case we''re going to be asking questions about the Odyssey\n1:27:59 and you can see how we access that book is by looking inside of our current directory go to the books folder and\n1:28:06 then look for the Odyssey text so if right now U my current directory is inside of the rag folder if I open up\n1:28:13 books you can see that I actually have the Odyssey text file right in here so that''s how we''re going to access that and then next thing is you can see I\n1:28:19 have this what I''m calling a persistent directory so that''s actually just where I''m going to be storing our chroma\n1:28:25 database and same thing this one we just look at the current database open up the DB folder whenever you start to save\n1:28:32 files this will actually Auto get created so you might not see DB out the gate and then next we''re going to save\n1:28:38 everything to the chroma DB folder so that''s what you can see now this is since I''ve already ran this code before\n1:28:44 I already have this but I''m going to delete it so I''m on the same page as you guys Okay cool so let''s keep chugg along\n1:28:50 in our code and so you can understand what''s going on so if you remember the first part of what we''re trying to do in\n1:28:55 this highle code is go from a Tech Source over to our Vector storm so if we''ve already went ahead and chunked our\n1:29:03 document converted everything to embeddings and saved it to our Vector store there''s no reason to rerun 1A so\n1:29:10 this is why I have this code right here it''s just going to say hey does a persistent directory already exist in\n1:29:16 this case the chroma DB already exists if it does just completely skip all of this because there''s no reason for us to\n1:29:23 go off and you know there''s no reason for us to go through and pay because embeddings do cost money to convert from\n1:29:28 text to embeddings so we''re just going to completely skip it but if we do not have that persistent directory what we''re going to do is go off and actually\n1:29:35 set everything up so the first thing that we''re going to do is we''re going to start at the top and actually load in\n1:29:42 the file path and what I''m going to do to help this make sense is we''re just going to quickly hop back and forth from part to part so it all makes sense so\n1:29:48 first thing we are going to load that file path which is connected to our Odus text and we''re going to use a text\n1:29:54 loader a text loader is just a great way to actually like Point like yep here''s the file path to my document that I want\n1:30:01 to load later on we''ll talk about things like web loaders to where you can actually pull information from the websites but just I''m just want to like\n1:30:07 expose you to the realm of the possible but right now we''re just going to load a text document once we have that file loaded what we''re going to do is we want\n1:30:14 to actually start splitting up this document into small chunks exactly like\n1:30:20 we talked about here so so far we''ve loaded it and now what we''re going to do is start chunking it and there''s a bunch\n1:30:25 of different chunking strategies that we will talk about later on in the text splitting Deep dive but for right now\n1:30:31 just know we''re going to try and chunk our basically each section of the book\n1:30:37 every 1,000 characters and just to go like a little you know help explain a little bit more just so we''re on the\n1:30:42 same page you can also set this thing called chunk overlap and all that does\n1:30:47 is let''s just say this is a chunk of text and let''s just say in the middle of\n1:30:52 a paragraph Was A characters what it''ll do is it''ll cut off so you kind of get cut off in the middle of a sentence and\n1:30:59 then when you load the next chunk it''s kind of like well I have a query about the end of the first chunk but it''s\n1:31:05 about the second chunk too so it just I don''t know it''s it''s just difficult for us to understand semantically what''s happening in a chunk so what you''ll\n1:31:12 notice a lot of times people do is they''ll set some overlap so what that''s what that''ll mean is just like you''ll\n1:31:18 always grab a little bit of the next section too that way there''s there''s just always overlap and a lot of times\n1:31:23 you''ll get better performance results so a lot of times just like give you numbers a lot of people will do things such as like 200 or 400 or common values\n1:31:31 so yeah but right now for example we''re just going to keep everything lean and mean and set the chunk size to 1,000 tokens fantastic so once we have set up\n1:31:38 our text splitter what we''re going to do is actually split up our original documents into a ton of those chunks\n1:31:43 that you see right here each one''s going to be a th000 tokens now once we have all of our chunks also docs whatever you\n1:31:50 want to call them what we can do next is we need to start moving over to in embedding them now in this casee we''re\n1:31:55 just going to use the open Ai embeddings and we''ll actually go through later on in an embedding Deep dive to show you\n1:32:01 other options that you have but for right now just know that we''re going to use open AI to convert our text chunks\n1:32:07 over to embeddings and we''re going to be using the text embedding three small model and there''s actually a few\n1:32:14 different models that you can use there''s one that''s three the three small which is the cheapest one and I that''s\n1:32:20 one I primarily use and there''s also a three large just just know as we talked about earlier there''s this things called\n1:32:27 numerical representations where just which were just long arrays of like a dog is equal to like an array of like 1\n1:32:33 two three well the large model just has a lot more values so we can get a lot more precise with our embeddings so just\n1:32:39 just know three small is probably completely fine for the projects that you''re working on okay let''s keep\n1:32:44 chugging along so once we have set up our embeddings what we''re going to do is we''re going to use our chroma Vector\n1:32:50 store to actually save all of our documents with our beddings and we''re going to say you are going to save the\n1:32:57 results over here into this persistent directory so what I''m going to do is I''m going to run this just so you guys can\n1:33:02 see what it looks like under the hood so we''re going to run python this is the fourth module and this is 1A and we''re\n1:33:08 going to run R rag Basics I''m going to zoom out so we can actually see it so you can see we are creating our embeddings we''re finished creating our\n1:33:14 embeddings and then now we''re going to create our Vector store if you scroll up actually a little bit you can see we\n1:33:20 saved a little bit of our first chunk cuz in our code we said you you know how many chunks do we create in this case\n1:33:26 826 where each chunk has a th000 tokens and you can see the first sample chunk and this first sample chunk is actually\n1:33:32 the very first part of the book so if you go open the book you can see at like line one this is the exact same so yeah\n1:33:39 we''ve actually just you know this is working hope you guys are hype to see it''s all all coming together all right so let''s go back down to the bottom so\n1:33:46 you can see our Vector stor is created and we can confirm that by going over to our database folder and checking for our\n1:33:53 chroma DB and you can see at the beginning DB chroma DB yep it made it\n1:33:59 and just to show you guys if I actually try running it again the way we''ve set up the code is it''ll give us an alert saying Yep this Vector store is already\n1:34:05 created no need to redo it cool so now that we have that set up let''s go ahead and move on to Part B where we can\n1:34:11 actually start asking questions and pulling information from the vector store so let''s walk over here to 1B and\n1:34:17 start going through this once again part by part so per usual what we need to do is we need to have we need to point\n1:34:23 point to our Vector store so that we can actually start you know referencing it so in this case copying the same thing\n1:34:29 look inside our current directory then actually access the database folder and look for chrom ADB once we have that set\n1:34:36 up we need to specify which embedding we''re going to be using and we''re just it''s important to use the same one just\n1:34:42 so that you know you wouldn''t want to change languages when you''re asking questions so like we just need to be very specific and use the same thing\n1:34:48 consistent or else you''re just going to get some weird results fantastic next what we''re going to do is we''re going to use chroma DB once again we''re just\n1:34:55 going to spin it up saying yep here is the persistent directory where you''ve already saved all of your edings in\n1:35:01 those text chunks and here''s the embedding function so that whenever you get asked queries you know how to go off\n1:35:07 and do a similarity check and see which relevant documents are the most similar all right so in this case all we''re\n1:35:12 going to do is for this question we''re just going to ask the Odyssey so the main character is Odus we just want to know who is his wife so this is the\n1:35:19 query that we''re going to be asking to our Vector store and I do think it is important for you guys if you haven''t\n1:35:24 seen what Vector stores look like I actually think they did a great job this is the chroma DB website and I think\n1:35:32 they did a great job of helping you guys visualize what''s going on so under the hood you can see we''re going to ask a\n1:35:37 query so who''s ODS is''s wife we''re going to convert that query over to an\n1:35:42 embedding once we have that embedding what we''re going to do is access chroma DB and it will pull out the documents\n1:35:49 and they''re connected embeddings cuz like that''s how we can do the similarity score door and we can see like oh this\n1:35:55 one''s the most similar and here''s the related document let me pass that back and once you get passed back you know it\n1:36:00 eventually gets passed back to your whatever llm you''re working with and generate that AI response so I just thought this was a neat little graphic I\n1:36:06 wanted to show you guys let''s go back to ours though so we can keep cranking this out so what we need to do next is work\n1:36:12 on that Retriever and the retriever is what''s going to take in our query and\n1:36:18 eventually return back those related documents but when we''re setting up our retriever we have a few different\n1:36:23 different options for what we can do and per usual what we''re going to do is I have a deep dive later on so you can see\n1:36:29 all the possibilities when it comes to working with Achievers but for right now just know we have a few different search\n1:36:35 types that give us different results and for this example we''re going to be doing a similarity score threshold all that\n1:36:41 does is pretty much exactly what it says it''s going to do a similarity check to find the most related documents and it''s\n1:36:47 only going to return documents that you know up to us how similar so this number you can see we have something called a\n1:36:53 score threshold well the higher this number is the higher the text document has to be to our query and we''ll play\n1:37:00 around with this just so you can see it in action and the other argument that you''ll see is this random letter K with\n1:37:05 a number so what the heck is that well what it''s going to do is it''s going to return the K closest documents so if\n1:37:12 there was 100 documents that were you know that were Above This threshold what this will do is it will return the top\n1:37:18 three closest results so that''s how you have to kind of set up your retriever if you want to uh to do this and feel free\n1:37:24 to bump this number up or down just remember depending on which llm you''re working with you''re kind of capped at you know right around usually 8,000\n1:37:31 tokens so in this example we''re going to be pulling out 3,000 tokens whenever we''re all said and done and passing it\n1:37:36 to our llm okay so now that we have set up our retriever set up the parameters what we can do is we''re going to call\n1:37:43 that invoke function remember we talked about that a lot in the chat model section invoke is the magic word inside of link chain for actually taking action\n1:37:50 on the models that you set up so in this case on our we''re going to pass in the query invoke it and it''s going to give\n1:37:56 us back the relevant documents so I''m going to go ahead and run this and then we''re going to play around with some of these parameters just so you can see\n1:38:02 what''s happening under the hood and I do want to point out all we''re doing in this example is just showing the\n1:38:08 relevant documents later on we''ll actually pass these documents over to a chat model so we can start interacting\n1:38:14 with them but for right now I just want to show you guys that yes our Vector store is returning related documents and\n1:38:20 we''ll just look through them okay so let''s come down here clear this up we''re going to run inside of our fourth module\n1:38:27 we''re going to run 1B to actually start grabbing those documents and let''s run it okay so what I would expect is to\n1:38:33 eventually get back three different documents that relate to OD deus''s wife so you can see we have document one\n1:38:40 document two document three that''s cool it also includes a source we''ll talk more about that later but the main thing\n1:38:46 right now that you can see is his wife is Penelope so you can actually like kind of see like we''re talking about in\n1:38:52 this document M we''ll actually just do a quick search so you can look up like the term wife it gets mentioned in a few of\n1:38:58 these documents so yeah you are indeed blessed in the possession of a wife endowed with such rare excellent of\n1:39:04 understanding and so faithful as Penelope so that''s like you can see this document perfectly describes you know\n1:39:10 like this is his wife so in the other documents too kind of talk about you know some of the other they they refer\n1:39:16 to wife so they''re not as similar but you can see this was the closest one that''s why it''s document number one all\n1:39:22 right now that you''ve seen that work what I want to do is show you how we can play around with some of these parameters to get different results so I\n1:39:28 can actually bump this up to k equal 10 and I''ll bump this down so we basically\n1:39:33 we''re we''re lowering the threshold and saying we want more results so what we''re going to do is I would expect to get back a bunch more documents this\n1:39:39 time that are related to a wife so you can see yeah we got back 10 more documents this time let''s see how many\n1:39:45 times his wife''s mentioned penelopy yeah so these These are getting less relevant\n1:39:51 seven out of the 10 documents referred to her okay so what I want to do next is I want to show you guys what happens if\n1:39:57 you get a little too stringent so let''s just say we only want to have documents that are like super super close to our\n1:40:04 initial question so if we were to run this updated code where we''ve updated the threshold to 0.09 meaning like very\n1:40:10 very similar what we''ll do is we''ll probably run into the problem of like hey you were too strict with your search\n1:40:16 results I couldn''t find anything so we''re going to give it a second and whenever it gets back to us it''ll\n1:40:21 probably tell us hey man cannot find that oops it looks like there''s a quick bug let me just close this out clear it\n1:40:27 out let me try this one more time and now it should hopefully give us back an\n1:40:32 answer yeah so you can see this time for whatever reason it just hit a bug yeah there are no relevant documents that retrieve using the relevant score 09 so\n1:40:39 we know we got too strict so that''s uh just you know as you''re building your own rag applications if you''re not\n1:40:44 getting back results you might just be a little too strict with your similarity score okay cool well that''s it for\n1:40:50 module number one now what we''re going to do from here is just to paint a road map for you guys we''re going to head into part two where we''re going to start\n1:40:56 adding metadata to our document so we can actually kind of see like oh this is\n1:41:02 where this Source came from so we''re going to go ahead and learn how to add metadata and view it in our request\n1:41:07 let''s go ahead and start working on that now hey guys so welcome to the second example inside of the rag tutorial and\n1:41:14 what we''re going to work on in this one is start setting up metadata now why is metadata important in context of rag\n1:41:21 well it all has to come back to the fact that lolms hallucinate and what I mean by that is Hallucination is a really\n1:41:27 nice way of saying they just get stuff wrong but what''s cool is whenever we start adding in metadata to our rag\n1:41:32 queries what we''re basically doing is allowing our llms to respond with like here is the source of information for\n1:41:39 where I''m generating this information I''m passing back to you so if we were asking a question about a meeting that\n1:41:46 happened in the past at our company it would tell us like oh John said this and Bob responded with this and it would say\n1:41:53 in here''s the source where I got this information from so you could always just click the source go check it out on\n1:41:58 your own and be like oh yeah that is exactly what happened and dive into more of the details that went on in that meeting so that''s metadata at a high\n1:42:04 level now let''s actually dive into the code example so you can actually see how we set this up in practice and you''re going to notice this is super similar to\n1:42:11 the rag Basics tutorial we''re just adding in a little bit more complexity so you can understand all the capabilities of working with rag okay so\n1:42:19 per usual what we''re going to do is set up our current directories but this time what you''re going to notice is instead\n1:42:24 of loading a specific book we''re loading a bunch of books and that''s because what we''re trying to do in the end of this\n1:42:29 tutorial is ask about a certain character and a certain book and we would expect the documents to we would\n1:42:35 expect our retriever eventually to only respond with documents related to that character so you''ll see exactly what I''m\n1:42:42 talking about we''re going to ask about and Romeo and Juliet so we would expect to only get back information about Romeo and Juliet and and the metadata related\n1:42:48 to it but you''ll see that set up in just a second okay so what we''re going to do this time is we''re going to start using a different database so this time we''re\n1:42:55 going to be using chroma DB with metadata I''ve already set mine up and you''ll set Yours up in just a second but\n1:43:00 the main thing this code is identical to the other one we''re just going to check to see hey does this persistent directory not exist if not let''s just go\n1:43:08 through that main process we talked about last time which is going to be you know setting up our Vector store if we\n1:43:13 already have the database completely skip it so let''s walk through what''s happening under the hood when we are setting up this Vector store because\n1:43:18 there are a few differences in this one the main one is to start off is we are going going and searching through each\n1:43:24 one of the files in our books directory and we''re going to grab all the files that end in a.txt so we''re going to be\n1:43:30 grabbing all of these different books right here and what we''re going to do with these book files is we''re going to\n1:43:35 iterate through each of them and start copying the same process we did last time except for one tweak where we''re\n1:43:41 going to add metadata here''s what I mean first we''re going to actually set up a new file path by joining in you know the\n1:43:46 name of the book file with our local books directory then we''re going to load that book just like we did in the last\n1:43:53 example but what we''re going to do now is once we have that document loaded we''re going to add some metadata to it\n1:43:58 in this case we''re going to set up this source for this document so whenever we''re loading for example the\n1:44:04 Frankenstein book we''re going to say the source of this book is Frankenstein or whenever we''re looking at the ilad what\n1:44:10 we''re going to do is we and you know retrieve a document it''s going to say the source of the retrieved information\n1:44:15 comes from the ilad and you could definitely get a lot fancier with this you could break up your documents and set it up to be per chapter or per you\n1:44:23 know if you had a long meeting you could set it up to intro you know speaker one speaker two you could do it however you\n1:44:29 wanted to just so whenever you''re setting up your metadata you could have like chapter and so forth and so forth\n1:44:35 but in this example we''re just going to stick to a book Source cool once we have set up our metadata we''re just going to add it to our documents array or list\n1:44:41 and then from there we''re going to copy the same process first we''re going to iterate through each one of those documents that we have and we''re going\n1:44:47 to use our character splitter to chunk everything up into 1,000 tokens once we have our Docs we''re going to do the\n1:44:53 exact same thing where we''re going to pass in our docs plus our embeddings over to our Vector store down here so\n1:45:01 that we can go ahead and save everything okay so I hope this is making sense because now we''re going to hop to 2B so we can actually start asking questions\n1:45:07 to our Vector store and pull out documents so what we''re going to do in this example is once again set up our\n1:45:13 file directories and make sure we point to that new Vector store that we just set up the one that''s going to be having\n1:45:19 metadata so this one right here and what we''re going to do is we''re going to use our our new embeddings plus our Vector\n1:45:26 store so that we can instantiate our new chroma instance so we can start passing questions to it now this Vector store\n1:45:33 retriever is the exact same as last time all we''re going to do is just use similarity score I want the top three\n1:45:38 result and here''s my threshold I set it super low for this one just cuz for whatever reason it was being a little\n1:45:43 weird however let''s go ahead and run this code so you can see what it''s doing under the hood so we will do python we\n1:45:50 are in the rag module this is the second exle to be so we''re going to go ahead and run it and what I would expect to\n1:45:56 get back is information on how did Juliet die plus the metadata okay so you\n1:46:01 can actually kind of see it right here like in document number two Juliet''s\n1:46:06 talking she obviously thinks Romeo killed himself and out of true love down\n1:46:12 here she stabs herself so like you can like it actually went ahead and just gave us the exact piece of information\n1:46:18 we were looking for which is crazy that it did that so efficiently and the part that''s EXT cool is you can see this is\n1:46:24 the source where this information came from so you can go back and double check the AI to make sure it didn''t just come up and hallucinate with some some weird\n1:46:31 facts yeah that''s how adding metadata to your rag queries works at a basic level\n1:46:36 I hope you guys appreciated that and what we''re going to do next is actually start doing some deep dives into each of\n1:46:41 the elements that we have been using to basically do our rag queries so we''re going to head over next and do a deep\n1:46:48 dive into text splitting then we''ll you know keep going down the chain from there so you can see all As specs and\n1:46:53 how we can add in some variety to our rag queries so let''s go ahead and move over to part number\n1:46:59 three so welcome to example number three inside the rag module this example is\n1:47:04 all about going deep into different options when it comes to using the text splitter so what I what I mean by that\n1:47:11 is there are a bunch of different ways you can actually go about splitting up your large documents and you''re going to\n1:47:16 in this example explore all of them so how to split it up based on characters sentences you know paragraphs and more\n1:47:22 we''re going to be doing all that in this example and we''re actually going to be spinning up a bunch of different Vector stores so that you can see how all of\n1:47:29 the different text splitting methods compare contrast when we actually make a query so you''ll see exactly what I''m\n1:47:35 talking about here in just a second and per usual we''re just going to start at the top of the code work our way down so that you understand everything that''s\n1:47:41 going on all right so let''s go ahead and dive into the code so the first thing that we''re going to be doing is setting up our basically our normal file\n1:47:48 directories so we''re going to just say like Yep this is the book we want to analyze and here is the path to our\n1:47:54 databases and you''ll notice that we don''t have a specific database called out yet and that''s because we''re going\n1:48:00 to be dynamically spinning up about four or five Vector stores down below okay so\n1:48:05 what we''re going to do just make sure everything exists and we''re going to do the normal you know kind of set up our\n1:48:10 normal loaders this time we''re just going to load our book Romeo and Juliet grab the document and stand up and\n1:48:16 specify our Vector store or sorry our embeddings now here''s where things get interesting what we''re going to do as\n1:48:23 you can see I have I think five different examples to show you guys and each one of these examples kind of\n1:48:29 specifies which text splitter we''re going to be doing and when it''s useful to use this text splitter just so you\n1:48:35 guys always have a reference to come back to of like oh yeah I think it was text splitter number three that was good for this scenario and you can always\n1:48:41 just come back and actually explore but here''s what''s happening at a high level and then we''ll dive into this Vector store function that''s a little bit kind\n1:48:47 of it''s it''s different so what you''ll notice is we''re going to specify a specific type of text splitter along\n1:48:54 with the parameters we want to use and after that we''re then going to actually go ahead and split up the document and\n1:49:00 once we have all those split up chunks what we''re going to do is then call this create Vector store with all of our\n1:49:06 chunks and then we''re going to give this database a name and once we do that and pass over that information we''re going\n1:49:12 to call this create Vector store and what you can see in here is we''re going to finally specify our persistent\n1:49:19 directory here and then actually go ahead and Save save everything to our new chroma database so that''s exactly\n1:49:26 what''s happening under the hood okay so let''s walk through each one of these and I''ll tell you when you should use each\n1:49:32 method and then at the very end we''re actually going to go off and query the vector store okay so to start off the\n1:49:38 first type of splitting we''re going to do is just the normal character-based splitting in the past what we''ve been\n1:49:44 doing I''m just going to show you what we''ve been doing for 2A in the past so you can see what we''ve nor been doing is\n1:49:50 character based splitting so this is nothing new this time we''re just adding some overlap and you can see this is a\n1:49:56 great method to use when the type of your content doesn''t really like there''s no syntactical kind of meaning for\n1:50:02 example like if you''re reading a book we talked about maybe keeping paragraphs together because we kind of want to like keep chunks of information together so\n1:50:09 this is just great when we just need to Chunk Up and split a bunch of random information the next example I want to show you guys is sentence-based text\n1:50:16 splitting so this one is when we actually want to split Things based on you know a sentence so this way you know\n1:50:21 maybe pair graphs are too long so we''re just going to split everything based up on sentences so we''re going to look for\n1:50:27 you know periods explanation marks question marks so and that''s how we''re going to split things up and once again\n1:50:32 we''re going to be splitting things into chunk sizes of a thousand we''re going to call this our chroma sentence then keep\n1:50:39 chugging along the next one is going to be our token text splitter and so this actually splits based on a token so in\n1:50:46 this one what''s going to happen is we''ll actually maybe split in the middle of a word so if let''s say a word word was\n1:50:53 super long and you know maybe the first part of that sentence was right over the\n1:50:58 the chunk limit well half of the words going to get cut out so I would not recommend this for a lot of you know a\n1:51:05 lot of like documents based but I just wanted to show you guys that this does exist all right the next one that I just\n1:51:10 want to show you guys is going to be the recursive character splitter this is the one that I would actually most people use I''ve seen people use this the most\n1:51:17 and I would definitely recommend using number four the most cuz like it tries to naturally split around sentences and\n1:51:22 paragraphs within the character limit so it does a really good job of like making sure the information that gets pulled\n1:51:27 out just kind of it makes sense it would make sense for this cluster of information to be stored together okay\n1:51:33 cool and the final one I just want to show you guys is going to be custom teex splitting and all we''re doing with just to show you guys we can make our own\n1:51:39 Splitters and these Splitters you can specify like oh I want to split chunks based on at the end of a paragraph so\n1:51:45 this is what this you know double backspace it''s just a it''s a double Escape or two new lines so in our case\n1:51:50 we''re assuming that''s a new paragraph okay cool and we''re going to save that custom Tech splitter to our chroma DB\n1:51:56 custom fantastic so what we''re going to do next I''m just going to show you is for each one of our databases that we just sped up Vector stores we''re going\n1:52:03 to go off and query those Vector stores with this question how did Juliet die\n1:52:08 and the way we''re going to do that is we''re going to pass in the name of the vector store along with our query to this function right here and all it''s\n1:52:15 going to do is look up our persistent directory that''s going to be our Vector store and we''re just going to do the\n1:52:20 normal thing that we''ve done so far to where we have have our Vector store we set up our retriever set up you know our\n1:52:26 search parameters for similarity score and then we''re just going to grab the relevant documents and spit them out so\n1:52:31 that''s exactly what we''re going to do so let''s go ahead and start running this query to embed everything first off using the different text spits and start\n1:52:37 asking questions so let''s go ahead and run this and this is our third example so let''s go ahead and do this is in the\n1:52:43 rag module and this is example number three so let''s go ahead and run this super fast and this one will take longer\n1:52:49 because we''re doing five six different types of settings but you can see that it''s working we''re creating everything\n1:52:55 so far we''re based doing it for characters next we''re going to do it based on sentences tokens and then from\n1:53:00 there our recursive and then finally we''re going to do our custom text splitter and once this is all done we''re going to then finally ask you know hey\n1:53:06 so how did how did she die and you can see let''s just go result by result real fast just so you can kind of compare\n1:53:13 each one so the first one for character this is what we''ve been using so far and this does look super similar to what\n1:53:19 we''ve been doing it''s just a lot of text from there what you can see when it comes over to the sentence one this one\n1:53:25 actually looks a lot more structured and then finally let''s just keep going along when it comes to our token what I would\n1:53:31 expect to see at the very end of this one is like it get cut off at a random word so in this case it got cut off\n1:53:37 right at my but if this had been a longer word it would have got cut off and we wouldn''t have been able to like you know we could have potentially lost\n1:53:42 some information when it comes to our recursive text butter you can see that this has done a really good job you know\n1:53:49 cutting at the very end of a of a sentence so that did a really good job or verse I can''t remember what happens\n1:53:55 inside of a how Shakespeare wrote and then finally when it comes to doing our custom DB it didn''t it you know it kind\n1:54:02 of just the way it chunked it it just didn''t do a good job and that''s because the way we sped up to split based on two\n1:54:08 new returns it just didn''t do a good job of grabbing information but the main thing I just want you to learn is there''s a ton of different ways that you\n1:54:14 can actually go about using different text Splitters to grab information my recommendation for you is to always use\n1:54:20 the recursive text splitter whenever for you''re starting out and then you can get a little bit fancy using the token and\n1:54:26 text splitter and some of the sentence ones but just main thing always use the recursive but enough of that let''s go\n1:54:32 ahead and dive into the next module where we''re going to start talking about the different types of embeddings that you guys can use inside of your your rag\n1:54:38 setup so let''s go ahead and dive into example number four right now so welcome to example number four\n1:54:46 inside the rag module and in this example we''re going to do a deep dive into embedding just so you can see\n1:54:52 what''s possible okay and just to give a little bit of background so far we have strictly used open AIS embeddings and in\n1:54:59 this example I''m going to show you how you can start to use custom embeddings why you''d want to do that and show you some other models inside of the open AI\n1:55:06 embeddings that you might want to use okay so let''s quickly run through the code so you can see what''s going on and\n1:55:11 then at the end we''ll run it so you can actually compare and contrast two different results using two different embeddings so you can see which one\n1:55:18 works better and which one doesn''t okay so what we''re going to do is our per usual we''re going to load in a book that\n1:55:25 we want to ask questions about and what we''re going to do in this one uh per usual you''ll see we have a database\n1:55:30 directory because we''re going to end up creating two different Vector stores in this example and after that what we''re\n1:55:35 going to do is per usual go ahead and load our document and we''re going to split it up in this case we''re just\n1:55:41 going to use the character splitter and once we have that set up what we''re going to do is we''re going to end up\n1:55:46 using this function called create Vector store more than that in a second the important part is I have two different\n1:55:53 embeddings down here so let''s walk through each one super fast so the first one is once again like I said we''re just\n1:55:59 going to be using the open AI embedding and so far we''ve been using the let me\n1:56:04 go ahead and just show you the exact one we''ve been using we''ve been using it''s called text let''s see text embedding\n1:56:10 three small this is the embedding that we''ve been using the entire tutorial and now we''re branching out and we''re going\n1:56:16 to be using the Ada embedding okay so let''s actually go ahead and look how these compare price wise and actually\n1:56:23 look at what the different options you have so over here on the open AI website you can actually look up embedding and\n1:56:30 you can see there''s a few different examples so this one is going to be the Ada example version two this one is one\n1:56:36 of the more pricey ones and you can actually go under the hood and and look at the different options they have so\n1:56:43 Ada so you can see if you scroll deeper down they have different qualities so the adaa example has it performs a\n1:56:51 little it it doesn''t perform as well as some of the other options but you''re able to add more data compared to the\n1:56:59 large model so it''s all a procon but personally I just go with a small model for most of the stuff I work on just\n1:57:05 because it''s the cheapest and you get really good results but the are other options and I def want you to be aware of that okay all right let''s hop back to\n1:57:11 it so what we''re going to do in this case is we''re going to create a new open AI embedding using this new model and\n1:57:18 that''s option one and like I said these open a Bings they''re best just for general purpose with really good\n1:57:24 accuracy and then but remember you have to pay because this is all happening on\n1:57:29 open AI servers now option two what this embedding is going to be is we''re actually going to download an embedding\n1:57:36 model from hugging face hugging face is just a like I''ll just go ahead and show you guys what it is so you can see it in\n1:57:43 action so let me pull up these models for yall real fast so as you can see whenever we head over to hugging face\n1:57:48 which is just a great place to go ahead and download models that other people have created ated locally on your machine so you can run them completely\n1:57:54 for free so you can see there''s a bunch of different models here and we can actually search for embeddings we\n1:58:00 actually already searching for beddings and you can see there''s just a ton of them yeah dozens but in our example what\n1:58:05 we''re going to do is use one of the more common ones which is just going to be a sentence Transformer and all it''s going to do is its main thing is it''s going to\n1:58:13 like I said it''s basically just going to run locally and perform in beddings personally I don''t really use these that much I just want to show you what''s\n1:58:18 possible the main Pro and the reason why you would want to use one of these models from hugging face is because you\n1:58:24 can run it locally for free which is one of the biggest benefits it''s usually not as performative so you''re kind of\n1:58:30 trading performance for cost so it really just depends where you fall on that Spectrum okay so what we''re going\n1:58:35 to do next is now that we have gone off and specify the two different embedding models that we want to use and the\n1:58:41 different names we want to call these different models for our Vector stores we''re going to go ahead and create those databases now you''ve seen this before\n1:58:48 what we''re going to do just pass in the the embedding and this is going to just passing right here so we''re going to set up the vector store name and then we''re\n1:58:54 going to say which embedding to use and that''s how we''re going to go off and create our two Vector stores one for open Ai and one for hugging face now it\n1:59:01 is important to mention whenever you do use this embedding because it is running locally on your computer you''re going to download a pretty big file I think it''s\n1:59:08 half a gigabyte but that''s how big the file is to perform your embeddings locally so it just beware if you''re\n1:59:14 going to download this code and run it it does uh take up a good bit of space and might be even more okay so what\n1:59:20 we''re going to do next is we''re going to do our normal comparison where we''re just going to ask a question to our\n1:59:25 Vector stores and we''re just going to compare the results of open AI to our hugging face example so let''s go ahead\n1:59:31 and go ahead and open this up and we''re going to run our fourth example so four\n1:59:37 because we''re in the rag module fourth example going to go off and run it now what you''ll notice is I''ve already gone\n1:59:43 ahead and actually created these models in the past so it''s not going to generate a new hugging face Transformer\n1:59:50 cuz I''ve already done it in the past but you will see that we are going to be able to query these documents so in our\n1:59:57 open AI example output let''s see what words we get back so we didn''t actually\n2:00:03 get a good example in either one of these so what we can do is update our Vector store and we can actually update\n2:00:10 our K to get back more results clear it run it again and this time we can actually hopefully search to see if we\n2:00:17 got back any information about OD deus''s wife who in this case is penelopy and it''s so funny because sometimes you''ll\n2:00:22 get back an example that does include the right information so you can see now it just changes per run so you can see\n2:00:28 now in our hugging face example we got some information about Penelope we got one of the documents contained\n2:00:34 information about her and then if we go back up you can see that our open AI\n2:00:40 example came back with a few more examples talking about Penelope so overall they both got towards the right\n2:00:45 answer it just open a returned back a lot more relevant information so kind of what we expected it''s more performative\n2:00:51 at the cost of spinning a little bit more M on it okay cool enough with doing our embedding Deep dive what we''re going\n2:00:57 to do next is we''re going to move over to working with retrievers and learning about different search types and\n2:01:02 different arguments we can pass in to our retrievers and this is really going to help up our game when we''re working with different rag applications all\n2:01:08 right let''s go ahead and head over to example number five so welcome to this fifth example in\n2:01:15 the rag module in this example we''re going to be doing a deep dive into retrievers and understanding all the\n2:01:22 different ways we can actually update how we search for different documents inside of our Vector stores so that''s\n2:01:27 the main goal of this quick Deep dive okay so what we''re going to do as a high level just to also understand we''re going to by the end of this example\n2:01:34 showcase how by fine-tuning different search parameters and our retrievers how we''re going to get different results so\n2:01:40 you''re going to learn more about different types of search queries and different ways to basically you know pass in custom parameters for those\n2:01:46 different types of searches and we''re going to be able to at the end of this compare and contrast the different results okay cool so let''s quickly run\n2:01:52 through this one so the main thing we''re going to do our normal part of setting up a accessing our Vector store in this\n2:01:59 case we''re going to reuse our old Vector store which included all of our metadata so this is one that had all of our books\n2:02:05 with the different sources outside that we''re going to use our usual embedding model we''re going to spin up our chroma\n2:02:11 DB instance and then from here what we''re going to do I''m going to close that out and we''re going to come back to\n2:02:16 this query Vector store but what you can see is we''re trying out three or four\n2:02:22 different examples of searching inside of our Vector store so what you''ll notice is let me just show you like a a\n2:02:28 compare and contrast so the first thing is we pass in the name of the vector store that we want to search the query\n2:02:35 so this is like hey what question do you have what type of embeddings are we going to use and then here comes the\n2:02:41 important part we''re going to pass in what type of search basically what type\n2:02:46 of search model we want to use and then any parameters so let''s actually hop into this query Vector stor you can see\n2:02:52 all this in action under the hood so like I said you can see the search type and the search type parameters right\n2:02:57 here so we''re dynamically spinning up and creating our retriever because that''s the main area that we''re focusing\n2:03:03 on in this tutorial we want to compare and contrast all the different options and not only do we want to like access\n2:03:09 these retrievers we want to go off eventually and fetch information with these different types of retrievers so\n2:03:15 that we can compare and contrast the relevant documents just so we can see like oh yeah this search type performs better than this search type okay so\n2:03:22 let''s hop back down to our three examples below so that you can actually see how they work so so far what we''ve\n2:03:28 been using is and the rest of our code has been let''s scroll down so I can show you we''ve been using similarity score\n2:03:34 with threshold basically so this is how we say yep I want to get the top three results and I only want to get back\n2:03:40 information that''s over this score threshold that way we only get back relevant data however sometimes you\n2:03:46 don''t want to actually do that threshold scoring if you know for a fact like yes every question I''m going to be asking is\n2:03:53 relevant to that database you really don''t need the threshold so this is a very quick way to say yep here is my\n2:04:01 data store I want to just grab all the similar results don''t care how relevant they are I but I do want to grab the top\n2:04:08 three results that''s exactly what''s going to happen with this similarity score and there''s no way of really filtering out the like oh yeah this\n2:04:14 one''s not that relevant okay the next one that we want to do is working with\n2:04:19 the max marginal relev an search query so what does this one do why is it\n2:04:25 important well what it does is it tries to not only get the most relevant information but it tries to kind of\n2:04:31 spread out meaning if for example how we''re going to ask about questions about\n2:04:37 Juliet''s death well what this type of search score would do instead of sorry I''m going compare and contrast with this\n2:04:43 similarity score let''s just say all the top three documents about her death were all right next to each other so it''s\n2:04:50 just going to grab all of that information and maybe leave out some additional context and what''s cool about\n2:04:56 this Max margin result is not only is it looking for super relevant information but it''s kind of also looking for\n2:05:02 adjacent information around her death so it''s not just going to be like yeah she died because of blank it''s going to\n2:05:08 maybe skip around to like a few paragraphs later that''s also talking about her death but it might not be the\n2:05:14 most relevant information to help generate a more well-rounded response so very cool search method and you can\n2:05:20 actually see Let''s uh scroll down here so you can kind of see so yeah so K is how many different responses we want to\n2:05:27 get fetch K specify the number of documents to initially fetch so this is you know how we can actually kind of\n2:05:33 specify like we''re going to grab a ton of documents and then inside of that we''re going to return the top three and\n2:05:38 we''re going to like space out those results so you''re going to get a well Diversified set of results so this is\n2:05:44 what you can see kind of right here so feel free to play with this Lambda multiplier like I said the main thing it\n2:05:50 does is control the diversity so if you want a ton of spread out information to get like as much wide range of context\n2:05:56 around the topic as possible you''re going to want to bump that number to zero to get the maximum diversity and if\n2:06:01 you want super similar information keep it to one okay cool so enough of that I''m excited to show you guys this one\n2:06:07 actually running so you can see how it performs and the final one this is the one that we''ve been using the entire time which is just our similarity score\n2:06:15 with a threshold okay so let''s go ahead and actually run this so you can see how it performs and we''re going to actually\n2:06:20 this time because there''s going to be so much information um over here in my terminal and we''ll look at the results over here so what we''re going to do is\n2:06:27 run python this is in the rag section and we are on the fifth example so we''re\n2:06:33 going to go ahead and run this code and it''s going to go off and actually oh my bad I need to actually load in my\n2:06:38 environment variables I didn''t do that real fast we''ll just quickly fix that on the fly so we''ll do load oops\n2:06:46 load. EnV and then once we have that we''ll come over here and import it fantastic that was my bad save it now\n2:06:53 when we run it again it''ll work and it''ll actually go ahead and show all the different results for the different Vector stores okay so let''s run through\n2:07:00 this part by part just so you can kind of see how it works so the first example that we''re going to look at we have to\n2:07:06 come all the way up just because there are a ton of different examples so the first one is just the regular similarity score so you can see it is going to\n2:07:13 return three different documents that are the most similar to our initial quest which was how did Juliet die so\n2:07:20 we''re going to get back a ton of information and then hopefully one of these will actually include the way that she died\n2:07:25 yeah so she stabs her health so yeah you can see document two include the exact pieces of information that we needed so\n2:07:31 the next one that we were wanting to do was the max marginal relevant score and this is the one that allows us to get\n2:07:36 not only the piece of information we want but some of more of the contextual information around her death so let''s\n2:07:42 actually see if this result includes anything around her dagger so this one does talk about you know laying her\n2:07:48 dagger down and it also yeah this one didn''t perform as well as the other one\n2:07:54 so this would be a good way for us to go off and actually potentially like oh maybe let''s just get some less diverse\n2:08:00 information so we really hone in so like I said everything''s a game and you have to optimize and tune those parameters to\n2:08:06 get the results that you''re looking for and the third one which is our usual one which is hey let''s go off and actually\n2:08:12 get everything that''s within a similarity score this one is going to return per usual the one where she she\n2:08:19 snatches a dagger and stabs herself so like I said there''s a few different ways you can work with these different\n2:08:24 retrieval messages retrieval methods to experiment with grabbing information from your vector store so I just want\n2:08:30 you guys to be aware of a few of the most common approaches but enough of that what we''re going to do next is Hop on to the next example where we''re\n2:08:37 actually going to not only be retrieving information now but we''re going to actually ask a one-off question of like\n2:08:43 hey what happened and we''re going to get an AI generated response let''s go ahead and dive into this example\n2:08:50 next so welcome to example number six inside of the rag module I''m super\n2:08:55 excited for this example because you''re finally going to learn how to tie all the information that we''ve been storing\n2:09:00 in our Vector store we''re going to be grabbing that information and passing it over to an llm so we can actually\n2:09:06 finally generate an AI generated response so I''m pumped for you guys to see it so now that you know what we''re\n2:09:11 going to be doing let''s actually dive into the code so you can see how you can start chatting with your data okay so\n2:09:17 per usual what we''re going to do is we are going to go off and grab our Vector store location per usual we''re going to\n2:09:24 be chatting with our Vector store that has all the different options from all the different books what we''re going to\n2:09:29 be doing is use our usual text embedding from there we''re going to spin up an instant of our Vector store from there\n2:09:35 we''re going to be passing in a new question and this one is just going to be hey how can I learn more about Lang\n2:09:40 chain and now that we have our query we''re going to set up a new retriever in this one we''re just going to use the\n2:09:47 similarity search method which is going to like I said just how we just learned about it''s just going to grab the most\n2:09:53 relevant document and it''s going to return only one result and from there what we''re going to do is use the\n2:09:58 Retriever and go ahead and search our Vector store for that query and return relevant documents so it''s important to\n2:10:05 realize that this is a list of documents now in our example we''re actually going to print it so you can see the\n2:10:10 underlying document but here''s where the magic happens what we''re doing under the\n2:10:15 hood is we are generating a r query and we''re not only going to just use our\n2:10:21 query we''re going to pass in the content from our Vector store that we just pulled out combine them and we''re going\n2:10:27 to use our chat models to generate a response so this is going to be awesome and let me just walk you through the\n2:10:33 prompt so you can understand what''s happening at a high level so the first thing is we are creating our prompt and we''re going to say you know here''s some\n2:10:39 documents that might help you answer this question here''s the question now here''s the relevant documents we could\n2:10:44 have probably used a prompt template to do this and you''ll see we''re going to do some of that later on but we could have even done it up here and then what\n2:10:51 you''re going to see is we''re going to join basically we''re just going to do some string manipulation here to where we''re going to grab all the content from\n2:10:57 our relevant document and really just spit it out in a nice text format and if you remember each one of these documents\n2:11:04 because we set this up earlier when we were doing our embeddings each one of these is going to be a th tokens long so\n2:11:09 we''re going to have plenty of space to pass in this information into our query because hey we''re only getting one result so 1K tokens and we have up to\n2:11:17 eight so 8,000 tokens so what we''re going to do is once we have this combined input we are going to spin up\n2:11:23 our new chat model and we''re going to use the latest version of open AI so ct40 and we are going to go ahead and\n2:11:30 generate our messages and what what we''re going to do is pass in our messages with our combined input\n2:11:36 containing our query and all the information from our Vector store into our model and we''re going to invoke it\n2:11:42 and what this is going to do is generate a result for us which is going to contain the AI response and for this\n2:11:48 example let''s just only generate the content only so you can just see the actual like response that it''s going to\n2:11:53 say like well if you want to learn more about langing chain here is my recommendation so what we''re going to do\n2:11:59 is go ahead and run this is module number four for rag this is example number six for a one-off question so\n2:12:06 let''s go ahead and run it so what you can see is it''ll hopefully spit back some relevant documents and then at the end it''ll do the AI generated response\n2:12:13 with information from our documents so this is so cool so let me just show you the AI generated response first and\n2:12:20 we''ll hop back up to the relevant doc so I was a little bit uh a little cheeky and what I did is I put in a document\n2:12:26 about myself inside of the book section and you can see I have something oh\n2:12:31 where did it go for you guys yeah Lang chain demo and what you can see in here is you know hey if you\n2:12:37 want to learn more about Lang chain you can go over here to the official documentation or if you want more\n2:12:42 in-depth tutorials and insights on Lang chain check out my YouTube channel and uh here''s a link to it don''t forget to\n2:12:49 like And subscribe especially if youve made it this far this video don''t forget to uh to like And subscribe if you want to learn more about AI yeah as you can\n2:12:55 see it''s generating an AI response that officially responds to our query in a conversational way and you can see this\n2:13:02 is the exact document text that we were able to like manipulate and turn and use for our AI response and if we actually\n2:13:10 look inside this text demo you can see I put a bunch of information and we got back the part in our response for\n2:13:17 further exploration yeah so in our the way just the way the chunking worked is we got this piece of text and yeah so I\n2:13:23 hope you guys think that is super cool for our oneoff example what we''re going to do next is I''m going to show you how\n2:13:29 you can have a full-on conversation with your rag data so let''s go off and start working on example number\n2:13:36 seven so welcome to example number seven inside the rag module this is definitely\n2:13:42 the most complicated example in this section however it is the most useful\n2:13:47 one and probably the example you''ll use the most often when ever actually building out a rag solution inside of\n2:13:53 your applications for your users okay so what we''re going to do in this one is we''re going to quickly speed through all\n2:13:59 the parts that are similar and then we''re going to focus on the parts of this code example that are different that allow us to actually have a\n2:14:05 conversation with our rag application so that we can you know ask a question follow up with it get additional\n2:14:11 information from our Vector store and just keep going so that''s what we''re striving to do here in this example so\n2:14:16 let''s go ahead and dive in so in part one what we''re doing is per usual we''re just spinning up a Vector store that\n2:14:21 points to all the books we''ve defined what we''re going to do is we''re just going to use our usual retriever which\n2:14:27 is just going to be a similarity one to get the top results for this example we''re just going to use CH gbt 40 as our\n2:14:32 llm okay so now let''s dive into this part of the code that''s all new so the main thing that we''re trying to do here\n2:14:38 I think it''s best if we actually start at the bottom to understand what''s happening so what we''re trying to do is\n2:14:43 we are trying to set up this retrieval chain all this is going to do is we want to be able to retrieve information from\n2:14:50 our Vector store we want to have awareness of all the conversations we''ve had up to this point and based on the\n2:14:56 information from our Vector store and our conversation we want to generate a result that is all we''re trying to do\n2:15:02 and that''s what''s happening under the hood okay so how is this happening well there''s a few things that need to happen\n2:15:09 part one is we need to be able to grab information from our Vector store well\n2:15:14 how are we going to do that well we''re going to be using this library and function called create stuff documents\n2:15:21 chain what does that mean I know it''s a weird term but basically what it''s doing under the hood is it makes a chain for\n2:15:28 us that will take in a list of documents and pass it to a model so that''s what it''s doing under the hood okay so we\n2:15:35 have documents and we need to feed those over to an llm that''s exactly what this Chain''s going to do for us so cuz we\n2:15:42 have open Ai and now you''ll be like okay well where do these documents you know where does this conversation and\n2:15:47 documents come from well let''s keep working back up so next is to create this document chain we also need to\n2:15:54 provide some information about like what the heck''s going on so that our llm is aware of what it needs to do well this\n2:15:59 is where our we are going to make a prompt for us we''re just going to call this the QA prompt so you know question\n2:16:06 answering prompt and we''re just going to say hey you''re an assistant who does question answering use the following pieces of retrieve context to answer the\n2:16:12 question if you don''t know the answer just say you don''t know use three sentences maximum and keep the answer concise so we''re really just going to\n2:16:18 say hey here''s the Act exact piece of information you need to know from this rag query Okay cool so this prompt is\n2:16:25 then you know we''re going to use our prompt templates back from module number two to pass in our basically just pass\n2:16:32 in our QA system prompt here to say this is your system context this is what you should be doing and then we''re going to\n2:16:38 pass in our chat history our chat history will happen later on and that''s as we chat with the our llm we''re just\n2:16:44 going to slowly build out a list of messages and then finally we''re just going to continually add in the human inputs every time the human has a\n2:16:50 question it''s just going to get passed in as an input right here okay cool so now you understand how we have this\n2:16:56 question answering chain so this is going to kind of set up the whole part where we''re like responding to questions\n2:17:02 now we need to have what we''re going to do and call the history aware retriever so what is this well the history aware\n2:17:10 retriever this is where we''re actually going to start passing in and working with our Vector store retriever remember\n2:17:17 retriever is how we interface and pull information from our Vector store and we''re also you''ll see here in just a\n2:17:22 second but we''re going to do something very similar to where we''re going to have our our AI our llm in this case who''s going to be doing the thinking and\n2:17:28 generating some responses we''re going to have a retriever which is pulling down the information from our Vector store and then we''re going to have a prompt\n2:17:34 which is kind of sets the scene for what''s going to happen next so let''s go part by part so this all makes sense\n2:17:40 okay so for this history aware retriever we''ll go start with a prompt so this prompt is saying like hey you''re going\n2:17:45 to have some chat history and it''s up to you to basically interpret what''s being\n2:17:51 said and not answer the question but just kind of provide context for what''s going on so this is all you''re doing\n2:17:58 it''s just you it''s up to you to reformulate the questions so that we can properly search for information inside\n2:18:05 of the vector store so that''s all we''re doing you''re going to get a question just rephrase it for the vector store so\n2:18:10 that we can retrieve the proper information so I hope that makes sense and then let''s just go back and look at the other parts uh the retriever we''ve\n2:18:17 already set up we''re just going to get the similarity score and we''re just going to grab the top three result results and then when it comes to our llm we''re just using chat GPT 40 okay so\n2:18:25 now that we have all of these different things set up we can actually go ahead now that we have our full retrieval\n2:18:31 chain that has two parts our history aware retriever which is pulling information from the vector store and we\n2:18:37 have our qu question answer chain which is actually like taking in user input and actually responding and generating\n2:18:44 answers to our users now that we have this entire retrieval chain set up we''re going to call it our rag chain what we\n2:18:50 can do is start having a conversation with our llm and our Vector store so\n2:18:55 this is the exciting part so I hope you guys like it so what we''re going to do is start asking uh we''re going to go\n2:19:01 ahead and run the program so you can see it in action but I just want to show you at a quick level what''s happening first\n2:19:06 we have a chat history and that chat history was important because we were using it up here inside of our\n2:19:12 contextualized prompt just so we can have a historical reference to what''s being said okay and we''re also using it\n2:19:18 in our question a prompt Okay so so that we''re constantly adding messages to this\n2:19:23 so you can see here whenever we invoke our our rag chain we''re passing in the users query and we''re also passing in\n2:19:30 that chat history very similar to what we did when we were setting up our first conversations with our chat models in\n2:19:35 section one second what we''re doing next is once we get the result what we''re going to do is just print it out for the user to see and then we are going to\n2:19:42 append and update our chat history so that we have our initial query and we get back the result from the AI and we\n2:19:49 continually just add it in a pin it to chat history so that it''s aware and we can have a full-on conversation all right enough talking let''s go ahead and\n2:19:56 dive into the example so you can see that it''s working so what I''m going to do is just go ahead and open up the terminal clear everything out and start\n2:20:01 our conversation so this is in module 4 and this is example number seven so what\n2:20:07 we can do is go ahead and run it and then you know it''s going to start our\n2:20:13 chat conversation for us so we''ll ask the same question we did last time how can I learn more about Lang\n2:20:21 chain what this is going to do is go go off and search through our Vector store and actually spit out a you know a\n2:20:27 response to us so yeah hey to learn more you''re responding to us concisely like we asked using the information from the\n2:20:32 vector store it''s basically consolidating to 3 CES as maximum answering the exact question that''s\n2:20:38 awesome I also just finished Romeo and\n2:20:45 Juliet how did uh she die so let''s just this is kind of going out out on a limb\n2:20:51 not sure if this one will work I just want to show you guys that we can actually have a conversation with it and we''ll come back to my initial question\n2:20:56 just a second so that''s awesome hope fingers crossed it''ll come back and actually respond like yeah she kills\n2:21:02 herself with Romeo''s dagger so the cool part is we have specified I just want to show you something we specified do\n2:21:09 not basically yeah if you don''t know the answer just say you don''t know and what''s cool about this is the fact that\n2:21:15 it''s not making up information it''s pulling information strictly from our rag documents and going from there so we\n2:21:21 can try and ask something else so who is Brandon again and this is where we''re\n2:21:26 actually going to start using some more of the conversational awareness because it''s going to have to go Brandon oh yeah\n2:21:32 this is his YouTube channel so we would expect uh we would expect it to refer up to two messages ago yeah Brandon is the\n2:21:38 creator you can find his videos here so yeah so it''s kind of doing the the full-on conversation part as well as the\n2:21:44 additional retrieval part where it''s actually accessing our Vector store so I hope this makes sense to you guys this\n2:21:49 is like I said by far one of the most complicated examples in this whole module and really this whole course so\n2:21:55 if you have any questions please head over to school it''s a preschool community and pop a question take a\n2:22:01 screenshot and say I''m stuck here don''t understand it or I''ve been building this on my own and I keep getting stuck here\n2:22:06 please help and myself or one of the other developers in the community will try and help you get you unstuck but\n2:22:12 enough of this one let''s go ahead and hop over to the last module we have which is all about web scraping and\n2:22:18 actually using information we get from the the web in our Vector stores so let''s go ahead and dive over to example\n2:22:24 number eight right now hey guys so welcome to the final\n2:22:29 example in the rag module what we''re going to be doing in this video is going over two methods for you to go off and\n2:22:36 scrape information from the web and add it to your vector store so that you can start asking questions and interacting\n2:22:41 with the data and what we''re going to do is I''m going to show you a basic example first so you can see it''s working and show you some of the cons and then we''re\n2:22:47 going to go over to using a tool called fire crawl which is is awesome and want to show you how it''s going to perform a\n2:22:53 lot better and provide much more better results really okay let''s go ahead and dive in and go part byart so the main\n2:22:59 thing that we''re going to see for the beginning is same thing per usual we''re just going to set up our directories and\n2:23:05 set up our file paths and this one what we''re going to do next is we need to start specifying a place where we want\n2:23:12 to start scraping the web in this case we''re just going to go on apple.com and what we''re going to do is instead of using the text loader like we''ve been\n2:23:18 using for everything else we''re just going to use the web based loader which is going to go off and scrape the\n2:23:25 basically scrape a website for us so it''s you know instead of looking through a book and loading it it''s going to just\n2:23:30 go over to a website and load the information so that''s what''s happening under the hood and then everything else\n2:23:35 is going to be identical we''re just going to split the text once we have split up the text into all the documents\n2:23:41 we''re going to start passing over basically those split up chunks plus our embeddings over to a vector store that\n2:23:47 you can see right here so yeah we''re just going to create a new Vector store from scratch and that way we can start accessing it once we have set up our\n2:23:53 Vector store what we can do is start asking questions to it and we''re just going to use a similarity score and this\n2:23:58 one we''re just going to grab the top three results and you know we''re not going to chat with this data at all we''re just in this example we''re just\n2:24:04 going to get back results of like oh yeah these these documents have the relevant information okay so I''m going\n2:24:09 to run this one we''re going to look through the results together and then we''re going to hop over to fir crawl after we kind of go over the pros and cons so let''s go ahead and clear this up\n2:24:17 and then we''re running the you know we''re in the module this is the eth example and this is the basic one so\n2:24:24 what we''re going to do I''m going to zoom out so we can actually go through the results together so it''s going to spit\n2:24:29 out a ton of information so this is just like the first one so you can see that it''s going like okay well U we got five\n2:24:37 chunks from the Apple website here''s the first one where you can kind of see like oh yeah we''re talking about the Apple\n2:24:42 iPhone 15 Pro we''re looking at you know just some of the main key phrases and\n2:24:48 we''re adding all this information to our database so that''s just an example one and then when it comes to actually\n2:24:54 asking questions to our Vector store we said what products are announced it''s just spitting back this information so\n2:24:59 it''s just like it''s saying Apple intelligence it''s saying iPhone 15 iPad Pro so it''s it''s really not like saying\n2:25:05 what''s new just kind of spitting out what''s on the website and uh yeah just going to keep doing that and just return\n2:25:11 a bunch of a bunch of information not super helpful so but it did work we can actually interact and it did a good job\n2:25:17 of web scraping so plus plus on both of those but the important part is I want to show you example number two which is\n2:25:23 using fir crawl just so you guys can see and compare and contrast the different results now fir crawl it is a free tool\n2:25:30 that you can use however they do have like you know they have some free usage for you however you do have like a paid\n2:25:37 version so let me just show you yeah so they do have like a free plan and a hobby plan but once you see how cool\n2:25:42 they are like like some of the results I think you like dang if you''re doing a lot of web based scraping this is the tool for you make sure like super easy\n2:25:49 their key you know key slogan is like hey we turn websites into llm ready data so like you know here''s a website that''s\n2:25:55 gross oh we''ll actually pull like a markdown version of this data for you so that it''s easy to use and interact with\n2:26:02 inside of your llms so enough talking about let''s actually go ahead and and use it and the one thing I do want to\n2:26:07 point out is once you''ve signed up a major free account you will have to copy and paste this API key over to your\n2:26:13 environment variables folder that we have or file that we''ve set up and you''ll just go ahead and paste it in there so you''ll just update the fir\n2:26:19 crawl and Pi just put your key in your you''ll just put yours right here okay cool enough of that let''s go ahead and\n2:26:25 actually start running this so you can see how this fire crawl example compares to the other one and the only thing that\n2:26:31 I''m going to just say different uh cuz some of this code is like well this is weird what''s what''s happening I will run\n2:26:36 through it real fast you know set up your create your new persistent Vector database for your vector store we''re\n2:26:42 going to call this one fir crawl and this time because we can''t just do web loader which goes off and does everything for us we actually have to uh\n2:26:48 be a little bit more specific using the fir craw loader tool so this is a package that you can install and if you\n2:26:55 actually go back to our py project. tommo you will see that we have where is it down here yeah you will see that we\n2:27:01 have a fir crawl package so this is how we''re accessing this tool but hey we''re going to use the fir craw loader here''s\n2:27:06 our API key so you can validate that we are allowed to do this here''s the website I want you to go scrape and they\n2:27:11 have a few different modes ones I think called crawl and one''s called scrape crawl just go to a whole website and\n2:27:17 mode goes through like a single page so I would defin I''d use scrape to start off with CU you can easily blow all your\n2:27:23 tokens if you crawl so once you have scraped that website we''re going to load all that information like we normally do\n2:27:30 and what we''re going to do in this one is we''re actually going to chunk through that document that we have pulled out\n2:27:36 and we''re going to add a lot of metadata that you''ll see later on so it''s actually pretty cool this the setup they\n2:27:41 have and then what you''re going to do next split everything up into different chunks pass over those chunks plus the\n2:27:48 embeddings over to to a new Vector store and once we have that Vector store set up we can start asking and making\n2:27:55 queries to it okay so enough of that let''s go ahead and start you know let''s go ahead and start testing it out so\n2:28:01 what we''re going to do in this one we''re going to do Python and the rag module this is the eighth example and this is\n2:28:06 fir crawl so we''re going to go ahead and run it and it''ll take a few seconds to go off and scrape the website beginning\n2:28:13 to crawl the website and I might need to update my question just because uh wwc 24 was a little bit ago and uh I''m sure\n2:28:21 a lot has happened since okay so the main thing is we grabbed 14 chunks of information compared to you know\n2:28:28 whenever we were running the other example we only grabbed you know three or five so we''re grabbing a lot more information because the fir crawl does a\n2:28:35 much better job of actually you know pulling out information and it actually I don''t know how it works under the hood\n2:28:41 but they do a much better job of like getting around like a lot of websites only load HTML whenever you try to use a\n2:28:47 tool like web loader they only give you h HML so you''re missing out a lot of the JavaScript code that gets rendered so\n2:28:53 that''s how we''re grabbing so much more information when it comes to fir craw so under the hood you can see now whenever\n2:28:58 we ask you so you can see in a sample chunk that like hey we grabbed well for whatever reason it chunked it too much\n2:29:04 but you can see whenever we look at like relevant documents we can start to see some of the updated questions that are\n2:29:11 related now it didn''t do a fantastic job but let''s update it to talk about Apple\n2:29:16 intelligence tell yeah whenever I wrote this first query I made it for uh basically I did it back\n2:29:23 in the day whenever wwc 24 was about to happen so now we''re going to do a second question and this time what it''s going\n2:29:29 to do is provide information strictly about Apple intelligence so yeah document two all about Apple\n2:29:35 intelligence so like I said much cooler you can actually see a lot more of like\n2:29:41 the links now if you want to go off and actually like interact and actually pull this information and using a sales page\n2:29:47 isn''t the best example using things like like Reddit or places like that where you want to actually like pull actually\n2:29:53 a lot of content from like a lot of users conversation that''s a great place to try out fir crawl so but enough of\n2:29:58 that I hope you guys enjoyed seeing how you can actually start pulling information from the web you can go\n2:30:04 really deep into a rabbit hole when it comes to this but just for now when it comes to like scraping Basics this is\n2:30:09 plenty to get you guys started but yeah enough for module number four where we learned about Rag and the next step\n2:30:16 we''re going to head over to start working with agents and tools and this is going to blow your mind so let''s go ahead and\n2:30:22 head over to module number five hey guys so welcome to this fifth\n2:30:27 module in this Lang chain master class in this module you''re going to learn everything you need to know about using\n2:30:33 agents and tools and before we dive into actually going through all the awesome code examples that I''ve set up for you\n2:30:38 guys what we''re going to do at a super high level is go through what the heck are agents what are tools how do they\n2:30:44 relate and I will say before I like actually dove in to work with agents for the first time I thought they were going\n2:30:49 to be this super magical thing that I would never understand it was going to be super overwhelming but it really\n2:30:54 wasn''t once you U now that you''ve gotten this far inside the master class and you have a really good understanding of prompts and you have a good\n2:31:01 understanding of chat models and so forth this isn''t going to be too complex so stick around I think you guys are\n2:31:06 going to be like oh that''s actually pretty easy to understand so let''s dive into it what is an agent well under the\n2:31:11 hood an agent is nothing more than an llm that has been provided a specific prompt to guide Its Behavior so the best\n2:31:19 way I like to think of it is a state machine and if you''ve never heard of a state machine it''s basically just a machine that has certain States and it\n2:31:25 can perform different actions at each state so let''s just say it''s at State one well at State one it can do\n2:31:31 something else and once it''s done with that it goes to the next state and basically it will just perform a certain Loop of task and that''s basically what a\n2:31:39 state machine is in the same way that''s exactly how agents work we are creating a prompt that defines certain States and\n2:31:45 behaviors for our agent to do and what it''s going to do is just flow through all of those different states and at\n2:31:51 each state perform a different action so let''s walk through it in this actual diagram and we''re going through it part by part so the first thing that most\n2:31:57 prompts have inside of an when they Define our agents is they usually have an action State and this action state\n2:32:02 says well this is where you can actually perform an action so think of it if we''re creating a writer agent well the\n2:32:09 whole point of that agent is to write so usually we''ll say our end goal is to write a bullet list about what''s going\n2:32:15 on in the news today well this agent will know it needs to write a bullet list so let''s just say it starts off\n2:32:21 with our input and just talks about the recent Apple event and it writes a bullet list well then what it''ll go is\n2:32:26 go to the next state where it can make observations about its actions and from here what it''ll do is go that was a\n2:32:33 little weird you only produced four bullet points then we''ll come back down to the thought and when we are in the\n2:32:39 thought stage this is usually we''ll plan out plan is the key word here we usually plan out what upcoming actions or\n2:32:46 behaviors we need to do next so in this case we''ll go oh for whatever reason we only had four bullet points I need to\n2:32:52 rewrite my article or my bullet list and make sure I have five bullet points this time so let''s add one more and then\n2:32:59 we''ll just go back up action we''ll update our bullet list and have one more so we''ll hit a total of five observation\n2:33:04 phase yep we have all five bullets things look good and then now that we get back to our thought stage you know\n2:33:10 we''re going to say everything looks great we look like we''re done and our thought phase will go okay cool I no longer need to work as an agent here''s\n2:33:16 my final answer and that''s a super simplified version but really under the hood that''s exactly what''s happening an\n2:33:22 agent gets a specific prompt with an llm that tells it how to behave we pass in a\n2:33:27 goal and it''ll just work towards that goal and just cycle through these stages and there''s a bunch of different types\n2:33:32 of Agents but this is the core Loop that you''ll hear about the most right here okay now what the heck is a tool why do\n2:33:38 we care about uh care about it when it comes to agents and I when doing this course at first I thought about breaking\n2:33:43 them up but I didn''t just because they''re pretty much useless without each other and you''ll understand why in just\n2:33:48 a second so a tool is nothing more than a basically some usually some code and\n2:33:54 that provides our agents with additional functionality so basically these tools provide our agents with superpowers is\n2:34:01 the best way I like to think about them we give them upgrades is the best way I like to like kind of think of it my in my head so a few common tools that\n2:34:08 you''ll hear of are tools that will allow you to search the internet so think tavali serper duck ducko there''s a ton\n2:34:15 of these tools and they allow our agent to interact with the outside world cuz if you remember these llms they have\n2:34:21 constraints they''re just you know U they have cut off dates and they basically you know can only think thoughts and\n2:34:28 write text and this is how we allow them to interact with the outside world so that''s what the search internet tool can\n2:34:33 do from there we could also set up our agents to actually execute code so we\n2:34:38 can hook them up to an interpreter to like oh here''s the python code let''s go over here oh yeah you can actually now\n2:34:45 write and draw some you can you know run some code you can generate some graphs with plot Le like you can do a lot of\n2:34:51 stuff when it comes to having your agents interact with code the final one you could also have tools that allow\n2:34:56 your agents to go interact with databases so obviously R so far we''ve kind of like hardcoded and wired up R in\n2:35:03 the past with in our rag section to work with a vector store but you can actually set up tools to allow your agent to\n2:35:09 interact with a vector store a SQL database like there''s a ton that you can do so it''s just up to you to create\n2:35:15 those tools and then give those uh tools to your agents and so just give you an example if we were trying to you know\n2:35:22 find today''s news find today''s like top five most talked about articles and then\n2:35:28 plot them inside of a a python plotly chart for like how correlated they were\n2:35:33 something silly but basically you know we could do that with tools because going through the same Loop we would\n2:35:38 understand first I need to go off and find all the news articles and find the most talked about ones well whenever I\n2:35:45 take that action I''ll search the internet come back I now have information about the outside world\n2:35:50 thought hm okay well now that I have information about the outside world I need to start mapping the information I\n2:35:56 found to a to some sort of chart where we can visualize how correlated everything is just you know Silly\n2:36:01 example but when now we come back to the action step and goes okay well I now know the five most talked about articles\n2:36:08 now I can start actually generating code to make a visualization out of this\n2:36:13 information so then I''ll start executing code oh NOP it didn''t work it didn''t properly run go through it a few times\n2:36:20 okay great now everything''s working here''s all the python code so you can generate some graphics so that''s exactly how it works under the hood and you''re\n2:36:27 just going to keep going through this core Loop and we''ve now supercharged our agent with tools to go off and interact\n2:36:32 with the outside world and actually start taking some action so that''s enough of everything at a high level what we''re going to do in the rest of\n2:36:38 this example is I''ve provided about five to six different code examples that you''ll see where we''re actually like\n2:36:43 code everything up that we just talked about so enough talking let''s go ahead and dive into our first example so you can see all this in\n2:36:51 action all right guys so welcome to the first example inside of the agent and Tool module now what we''re going to do\n2:36:58 inside of this example is just walk through everything at a super high level because this is going to be the first\n2:37:04 time we''re going to be introducing agents and tools inside of actual code so we''re going to go through it super slowly and then after that we go through\n2:37:11 this example we''re going to later do a deep dive into agents so you can understand all the different ways we can\n2:37:16 use them and then we''re going to do a deeper dive into tools so you can learn learn how to use existing tools and learn how to create your own so that''s\n2:37:22 what we''re about to do let''s go ahead and dive into this part by part so you guys can Master agents and tools all\n2:37:28 right so some of this is going to be super similar to start off we''re going to be loading our environment variables because we need to use open AI now the\n2:37:36 first thing that we''re going to do in here is go off and create a tool and for this one we''re just going to create a\n2:37:41 super simple tool that allows us to access the current date and time for our local computer CU if you think about it\n2:37:48 our llms you they were generated in the past they have constraints of and cut off dates and they actually have no way\n2:37:55 of knowing what time it is currently so whenever we''re actually using these llms in Agent form we can add a tool time so\n2:38:03 that our agents can access our current time this is going to be super helpful as you go off to build larger and larger\n2:38:09 agents where they need to interact so this is just a super basic one so let''s walk through what''s happening so the\n2:38:14 first thing is we''re going to create just a regular function that all it does is it reports date time because date\n2:38:20 time is just a standard library and we''re just going to grab this the current time and then we''re going to\n2:38:26 return the current time in this format to where it''s hour minute minute so that''s all we''re trying to do and that''s\n2:38:31 the whole purpose on this function we''ll dive later into why had to set up our parameters like this for the function\n2:38:37 but we''ll come back to that later all right cool well now once we have defined our specific function that we want to do\n2:38:43 we have to wrap it inside of a tool and later we''ll get into actually like using existing tools but for right now this is\n2:38:49 just like how you can make your own simple tool with your own code but basically all you do is you call the tool class which comes from up here\n2:38:56 blank chain core tools that''s how we can create a tool and it''s just up to you to give your tool a name a descriptive name\n2:39:02 about like you know what does this tool do that way whenever the agent''s executing and it''s like oh I need to\n2:39:08 solve a Time problem oh it makes sense for me to use the tool that talks about time so that''s why it''s super important\n2:39:15 for us to give the name and description to be very representative of what what going to happen under the hood from\n2:39:21 there we pass in the function that we want the tool to perform whenever it gets called on so in this case whenever\n2:39:27 we say like hey give me the time under the hood it''s going to trigger this function which is going to return our\n2:39:33 string representation of the current time okay so that''s how we make a tool that''s how we add it to our tools list\n2:39:39 and once we eventually have our tools list we can actually pass this over to our agents and you''ll see that here in just a little bit okay so now we''ve got\n2:39:46 tools out of the way at a super basic level now we''re going to work on creating our agents so if you look at\n2:39:52 this you might be like what the heck are we doing what are we what are we pulling what is this word react well uh react\n2:39:59 stands for reason and action and all we''re doing here is we are pulling out a\n2:40:04 prompt because you remember from earlier earlier from prompt templates all we''re doing is pulling out a prompt template\n2:40:10 that tells our llm how to act so if you actually go and read what this uh what''s\n2:40:17 going to happen at this prompt you''ll head over to to a website like this I''ll actually copy the link for you guys so\n2:40:22 you can read it yourselves but you can see kind of how we talked about earlier an agent''s nothing more than an lolm\n2:40:29 with a super specific prompt telling it how to behave so this is exactly what''s happening under the hood this prompt\n2:40:35 template that we''re going to create an agent around you know it''s just like hey answer the following the best you can\n2:40:40 here''s the tools you have access to and then that Loop that we kind of talked about earlier where you''re taking action\n2:40:46 you''re making observations planning out thoughts and providing a fin answer this is exactly what you know what this\n2:40:52 prompt template is telling you to do so you know hey use the following format to basically perform actions so you know\n2:40:59 taking the input think about it take action here''s the you know like as we''re\n2:41:04 working with tools sometimes when whenever we''re taking an action like get time well sometimes we might have to\n2:41:11 pass it in parameters to our functions so like I want to get the time in Tokyo\n2:41:16 I want to get the weather in San Francisco so that''s what actions like you know use the tool and then action\n2:41:21 input is like whatever parameters we want to pass over to it so just hope hopefully that makes sense observation\n2:41:27 hey look at the results of the action thought you know go go off and here''s what I need to do next basically plan\n2:41:33 yeah so and you can kind of see we''re telling our llm to repeat this over and over and over until we get a final\n2:41:38 answer okay so that''s what''s happening under the hood and we''re actually able to like the part that''s nice is it''s\n2:41:45 kind of like GitHub where we''re able to just pull down and reuse other people''s code that''s working but in this case\n2:41:51 we''re just pulling down other people''s prompts which is pretty cool if you think about it okay so now that we have our prompt which is going to tell our\n2:41:57 llm how to act we need to keep chugging along and we actually need to specify which llm we want to create our agent\n2:42:04 around and this time we''re just going to use cat gbt 40 that''s the latest open AI\n2:42:09 model that''s out at the time of this recording and then from there what we''re going to do is go off and actually use\n2:42:15 the agent we''ve defined we''re going to pass in the prompt that we fine and we''re also going to pass in all the\n2:42:21 tools we''ve set up to actually go off and create our agent so if you look under the hood what''s actually happening\n2:42:28 is this create react agent it''s coming from the the Lang chain agents repo so\n2:42:33 I''ll just scroll up to the top so you guys can see it yeah this is coming all from Lang chain agents and we''re really\n2:42:39 just importing these two classes to create our agents and run them but the main thing for our agents is we''re\n2:42:45 sticking to that react part to where we are going to you know reason about what we want to do take action and just\n2:42:51 continue that cycle over and over in order to actually achieve our goals so this is kind of what what it looks like\n2:42:57 under the hood if you want to actually dive in here and read more about what''s going on but really we''re just combining\n2:43:02 all the ingredients of everything that''s needed to make an agent and just putting it into one one class so that we can\n2:43:09 actually start passing information to it and running our agent and that''s exactly what we''re going to do next once we have\n2:43:14 our agent it''s up to us to then pass in everything over to an agent executor so\n2:43:21 an agent executor is just basically going to help manage the Run of an agent as it goes off to solve problems so if\n2:43:28 you come over here you can see an agent executor we''re just going to say like hey this is allowing us to use tools and\n2:43:34 and if you actually dive into the code here too you can see this is where we''re going to be like actually performing The\n2:43:40 Run and accessing the tools putting information back and forth and and continuing to go from there but really\n2:43:45 don''t need to like dive into it the main thing is just understand that like yes whenever I want to run my agent I need\n2:43:51 to use an agent executor Okay cool so once we have our agent executor set up what we can do is once again use our\n2:43:57 magical word when it comes to Lang chain and we need to invoke our agent executor to start actually spinning everything up\n2:44:04 and what you''ll notice is instead of just passing in a string we''re passing in a dictionary and the keyword in our\n2:44:11 dictionary is input this is the main thing that our agents expect to read is\n2:44:16 an input so in our case what time is it is the question that our agents are going to be answering so enough of that\n2:44:22 what we''re going to do is let''s go ahead and run it so you can see all of this happen in real time so let''s open up our terminal and we''re going to run python\n2:44:30 this is the fifth module because we''re learning about agents and tools and this is the first example so let''s go ahead and run it and see what happens so what\n2:44:36 it''s going to do as you go off and actually use agents you''ll see a lot of colorcoded text usually coming out of\n2:44:42 the output as it''s going through that reasoning taking action and making observations so you''ll usually see things like purple green and white and\n2:44:48 this is is uh anytime the agents''s performing an action you can see like all right what''s the action I want to\n2:44:54 perform Let Me zoom in for you guys so what''s the action I want to perform I went to get the time so that''s pretty\n2:44:59 cool that it was able to look through all the available tools that it had and pick the correct one and then what it\n2:45:04 did next is it passed in an action input and because we didn''t specify any\n2:45:10 parameters for AR tools it just gave in none but you''ll notice sometimes when you''re creating tools and we''ll dive\n2:45:15 into this here in a little bit if you do not have any parameters for your function sometimes they''ll mess up so we just\n2:45:21 accept all parameters for arguments and keyword arguments but we just don''t do anything with them so that''s just a quick work around but what''s cool is\n2:45:28 once we you know perform the action pass in the action input you can see here in blue we get the local time back for when\n2:45:35 I''m recording right now and then now that we have that answer from our tool the agent goes I now know the final\n2:45:42 answer and once it knows it''s the final answer because if you actually come back over here the thought is I know I now know the final answer it then gives us\n2:45:49 back the final answer so then that''s super cool cuz you can see right here it said the current time is 8:31 p.m. so\n2:45:55 that''s the agent thinking and then finally under the hood I''ve said that 100 times right now but what''s happening\n2:46:01 is it finally now that it has the answer it generates a object that it returns\n2:46:06 back to us and you can see this object contains nothing more than the original input plus the output of the final\n2:46:12 answer from this agent so that''s what''s happening and what we''re going to do next is um and I hope you guys first off\n2:46:17 I hope you think that is aome awesome because it''s very cool that we can have agents reason and take action in the\n2:46:24 world and uh you know everything we''re going to do from here we''re only going to add in more complexity and show off\n2:46:29 cooler features just so that you guys can match uh Master agents and tools and just to dive in what we''re going to do\n2:46:34 next like I said we have two different folders for you guys to go over all the different ways that we can use agents\n2:46:40 and tools and the first one we''re going to do is we''re going to do a deeper dive into agents so that you can understand\n2:46:47 different ways that we can like figure these agents to work and eventually we''re going to hop over to showing you\n2:46:52 how to use more custom tools so let''s go ahead and head over to our first agent Deep dive example and start looking at\n2:46:58 react chat hey guys so welcome to the second example in this agent and tools module\n2:47:06 and in this example we''re going to go a little bit deeper into working with different types of agents and really\n2:47:12 expanding what they''re capable of doing so in this example we''re actually going to focus on swapping up the underlying\n2:47:19 prompt that we''re using in the agent sprinkle a few more tools and then kind of add some chat capabilities to it so\n2:47:26 that our agents can now go off and do some internet searching for us and give us answers so let''s go ahead and dive\n2:47:31 into this example so you can U understand what''s going on okay so to start we''re going to do the exact same\n2:47:38 thing we did last time but we''re going to add in some more tools so this time we''re going to do another get current time tool exact same thing and then next\n2:47:45 we''re going to add in the search Wikipedia tool and this this is really just a function to go off and use the\n2:47:50 Wikipedia library and what it''s going to do is basically get a summary about whatever topic we''re interested in so we\n2:47:57 get ask a questions about famous people events times and Wikipedia is going to give us back a quick summary you know\n2:48:03 two sentences of information it has on that topic okay so now that we''ve defined those functions we now need to\n2:48:09 package them into our tools list so and we need to format everything to be in the proper tool class so that''s where\n2:48:16 we''re going to set up the name a description so that our agent can go oh yeah that''s the tool I want to use and\n2:48:22 then the actual underlying functionality we want to trigger cool so now that we have our tools defined for our agents we\n2:48:27 can now move over to actually creating our agent and what you''ll notice in this example is we''re using a different type\n2:48:35 of agent this time for our prompt excuse me so last time we were just using the react agent but this time we''re using\n2:48:41 the structur chat agent so what this prompt is focused on it''s actually having a chat so if we head over to uh\n2:48:49 over here where we actually can see the underly prompt template it looks a lot different than the last one so you can\n2:48:55 see like hey you''re responding to a human with helpful information you have access to the tools in our tools list\n2:49:01 and I need you to basically create a a Json blob for these tools to go off and perform actions that''s enough of that\n2:49:08 and then what''s going to happen next is like all right cool here''s you''re going to search for a question go off and\n2:49:15 basically have thoughts about it take action so this is where we''re going to use that Json blob and this is basically\n2:49:20 going to be like you know information from the past that we''ve taken like so whenever we get a response back we''re\n2:49:26 going to save everything as a Jason blob and I''ll keep it at that but the main thing is it''s going to allow us to have\n2:49:32 a conversation with our agents and it''s going to go off and you know use tools to perform actions on our behalf okay so\n2:49:38 that''s what''s happening under the hood now what are we going to do next per usual we''re going to uh spin up a chat\n2:49:44 model I''ll actually go ahead and make sure we''re using the right model so we''re going to GPT 40 make sure using\n2:49:51 the latest one and then from there what we''re going to do is we''re going to use this new we haven''t used it before but\n2:49:57 it''s called a conversation buffer memory all it does is it allows us to store our\n2:50:03 chat history in in memory that''s all that''s happening so in the past we''ve done things such as you know we''ll just\n2:50:10 do like chat history and we''ll save it to a list well this is just a more and\n2:50:15 you know we''ll always add in our system messages and human messages well this conversation buffer memory it\n2:50:21 just it does a better job of doing it so uh we''re just going to use it for this one it makes things simpler to set up\n2:50:27 for us okay well now that we have everything initialized let''s start combining things to go off and create\n2:50:33 our agents so that we can start running them so per usual we''re going to create our agent and we''re going to pass in the\n2:50:39 main ingredients which are going to be our prompt our tools that we defined in the specific llm we want to use in the\n2:50:45 underlying agent and then from there what we can do is create our agent executor and this is going to be you can\n2:50:52 see we''re adding in more tools to our agent executor this time these are the same as last time our agents and tools\n2:50:59 but now we''re going to add in memory to our agent executor and this is how we''re going to keep track of our previous\n2:51:05 responses and messages with our agent because we''re going to be talking to this one and what you can see from here\n2:51:11 is we''re now going to now that we have our agent executor been up and it''s ready to run what we''re going to do is\n2:51:17 start go ahead and start our chat conversation with the user so in our case we''re going to start chatting with\n2:51:22 our agent so this is going to look exactly like our initial chat example we did except now we''ve supercharged it\n2:51:28 with an agent that''s not just talking to a vector store it''s going off and searching the internet for us to get\n2:51:33 responses so this is super cool and I hope you guys can see the value of it so let''s go ahead and hop down here and\n2:51:39 start running our agent and actually seeing how it works so I''m going to make this a lot bigger because you''ll see how\n2:51:47 cool it is cuz it''s going to actually show us behind the scene what the agent is thinking and before I trigger it I do want to show you guys one thing let''s go\n2:51:54 ahead and get the code ready so agent Deep dive and this is example number one the thing that per usual we''re going to use the magic word when it comes to\n2:52:01 running Lang chain tools so the agent executor and per usual we do have to pass in the input and you''ll notice as\n2:52:08 you use these agents tools a lot more everything''s kind of structured as an input and an output so that''s how we can\n2:52:13 show the response from the AI we''re just going to say yep grab the output and per usual when the user submits a message\n2:52:20 we''re going to add a human message and whenever the AI generates a message we''re going to add an AI message okay let''s go ahead and run it so you can see\n2:52:26 what''s happening under the hood so we''re going to start off asking a question we''re just going to say who is George\n2:52:32 Washington and this will go off and use Wikipedia so that''s cool it found the proper tool to use and as you can see it\n2:52:38 passed in the action input of like well who are we trying to query about and it''s so cool that the AI went from a\n2:52:46 question like who is George Washington to picking out the core topic of that question and then passed it as the\n2:52:53 action input so it''s amazing that this AI can figure out based on what parameters we need to pass in yeah just\n2:52:58 grab the core part of it so that''s super cool now in the yellow we''re actually getting back a response from our tool so\n2:53:05 our tool from Wikipedia says you know like hey here''s everything you need to know about him and so forth and so forth\n2:53:12 and then once we have the final answer it generates a response to us so you can see the bot came back and said yep give\n2:53:19 us everything that Wikipedia said so uh let''s see we can actually ask because this is a conversation we can add to our\n2:53:26 original question so how old was he when he died so because this is a\n2:53:32 conversation we can actually refer to our previous messages so in this case it didn''t even have to go off to Wikipedia\n2:53:38 this time because it was like oh wait already know the information so I can just use use that to respond and we can\n2:53:44 ask other things such as who is Elon mus and how old is he right now so this\n2:53:51 question is a little bit more interesting because it''s a two-part question first you have to go off and figure out who is the main person that\n2:53:58 we''re talking about so in this case it''s Elon Musk for whatever reason it''s kind of struggling right now to to find out\n2:54:04 who he is but usually this is just the part of Agents where it''s just going to go in a loop until it finally gets an answer for whatever reason it couldn''t\n2:54:10 figure out that one so we''ll just ask a different question because I do want to show you guys the key key underlying part who is Steve Jobs\n2:54:19 and how old was he when he died and the main reason I''m doing these two-part questions is because I want to show you\n2:54:25 guys how these agents can understand what we''re trying to do yeah and for some reason I think Wikipedia is just\n2:54:31 crapping out on me but the main thing to know is what would happen under the hood normally is these agents would go off\n2:54:38 and find the result for the first part which is who is Steve Jobs and once it knows the answer it would then go off\n2:54:43 and actually find a you know using the response it would then trigger the next part and the question so you can see\n2:54:49 like yep he''s an American entrepreneur obviously known for founding apple and he died at the age of 56 so that''s\n2:54:56 that''s very cool that I can do a two-part question and you can even go deeper to where it triggers off a second\n2:55:01 Wikipedia call but I''m not going to go into that for this example so I hope you guys are like wow these agents are\n2:55:07 powerful they can act on my behalf and go you know find information inform for me and I can just talk to them so I hope\n2:55:13 you find that super cool and what we''re going to do next is dive into the next agent example that I''ve set up for you\n2:55:19 guys where you can actually talk to a document store so we''re going to work on this one now hey guys so welcome to the third\n2:55:26 example in this agents and tools module in this example we''re going to be diving into how we can connect our agents up to\n2:55:34 a vector store so that they can kind of work together to answer questions about our data so that''s exactly what we''re\n2:55:39 going to be setting up in this example so let''s go ahead and dive in and what you''ll notice is a lot of the information that I''m going to be showing\n2:55:45 in this module was built off of everything that we''ve kind of done in module 4 with Rag and all the previous\n2:55:51 modules before that so the log this is going to look super familiar so I''m not going to dive super deep into it but to\n2:55:56 start off we are going to be setting up all of our file path directories so we can point to our Vector store so this is\n2:56:03 the same Vector store that we did earlier that read all those different books for us so we''re going to go off\n2:56:08 from there set up our embedding so that we can actually you know uh whenever we ask a question it we can embed it and\n2:56:16 compare it to all of our other documents to you know grab the most similar answers so we''re going to then spin up our Vector store with our Vector\n2:56:22 database and the specific embedding function we want to use and then we''re going to use the exact same retriever\n2:56:28 that we used the entire time in the past we''re just going to use the similarity one this time which is just going to\n2:56:33 grab the most similar results not worry about a threshold and in this case we''re going to grab the three top results and\n2:56:39 if you remember we used a 1,000 tokens per result uh per document so this is\n2:56:44 going to give us 3,000 tokens worth of information for our agent to use all right let''s keep chugging along from\n2:56:50 here we''re going to start working on creating our agents or sorry llm so in this case we''re just going to use Chad\n2:56:56 gbt 4.0 and then now we''re going to be kind of combining two different examples so in the past we had an example where\n2:57:04 you could ask specific questions about the doc store to the vector store that we''ve set up and but now we''re going to\n2:57:09 be combining it with our agents so this is the exact same demo we did before where we kind of first set up like you\n2:57:16 know just like contextual what we''re doing here in the first place so like hey you are working with chat history to\n2:57:23 solve an answer that''s all you''re doing and the second part was the history aware retriever so this is like hey go\n2:57:29 look at the previous questions that we''ve worked with plus you can look at the uh you can use the retriever to go\n2:57:34 off and answer or grab information from our Vector store so this is definitely going to go a little in the weeds but we\n2:57:39 already covered this in detail in our previous rag example right here where we did rag conversation so you''ve already\n2:57:46 done this in the past we''re just now building on top of it okay cool per usual we we''re just going off and\n2:57:52 creating stuff from our document chain this is how we can actually if you go under the hood this is how you can go\n2:57:58 about passing a list of documents over to a model so that they can be processed once we'' set up all that we can set up\n2:58:04 our retrieval chain which will basically be able to interact with our Vector store okay enough of talking about\n2:58:10 setting up our Vector store now it''s time for us to go off and set up our agent so we can have our agent\n2:58:16 communicate with our Vector store on on our behalf and perform actions and and lookups information so in this one what\n2:58:22 we''re going to do is we are going to use the same react agent that we did in the first time which is just going to it''s\n2:58:27 going to think about stuff take action make observations and just keep performing that in a loop and per usual when we''re working with agents we have\n2:58:34 to set up a tool well this time we''re going to set up a Custom Tool and what''s interesting is under the hood this tool\n2:58:40 is going to do is it''s actually going to invoke our rag chain so what''s very cool is now anytime our agent has a problem\n2:58:48 where it needs to answer a question what it''s going to do is it''s going to go well I don''t know the answer to that question but you know what I bet this\n2:58:55 tool does because it''s useful for whenever you need to answer questions about the context of whatever the\n2:59:01 question is so what''s cool is all we''re going to do here is this we''re going to invoke this rag chain and what we''re\n2:59:07 going to do to it is we''re going to pass in the input so it''s going to be the person''s question but we''re also going\n2:59:12 to pass in the chat history so that we can have some uh contextual awareness of\n2:59:18 previous messages so that''s super cool so now that we have made a new tool for\n2:59:23 our agent we''re going to go through the normal process of setting up and creating our agents so that we can\n2:59:28 actually start running them so now that we have all of our agents set up with a proper tooling we can now go into another while loop and in this while\n2:59:35 loop we''re going to once again start chatting to our agent so I''m going to come down here we''re going to clear\n2:59:40 things up and now we''re going to actually start running this example so you can see it in action so we''re going to do python this is the agents and\n2:59:48 tools module we''re working inside of the agent deep dive and we''re going to start using the react doc store so let''s go\n2:59:55 ahead and run this one and you can see so we can start asking questions and what''s nice already at the gate we are\n3:00:01 you know accessing the vector store so because there was that one file I set up where I talked about Lang chain that''s\n3:00:07 the first question I''m going to ask so how can I learn more about Lang\n3:00:14 chain so what it''s going to do is it''s going to well this is a question I need to get an answer about and then what\n3:00:20 it''s going to do is actually trigger the AI to go off and get that information\n3:00:25 and what''s cool is like it responded with the exact part of like yeah this is you know to learn more about Lang chain\n3:00:31 that what the document said was like yeah go watch Brandon''s YouTube channel here''s a link to the YouTube channel so\n3:00:37 as you can see that''s pretty freaking cool and then you can follow up because it is a conversation so you can say who\n3:00:42 is Brandon and then it''ll it''ll go off and actually you know say oh he has a\n3:00:48 YouTube channel where he talks about like AI does not mention the name Brandon oh I guess I should have\n3:00:54 capitalized it so so yeah that''s how it works at a super high level when it comes to working with the dock store\n3:01:01 this example was a little weird but I just wanted to show it to you guys just so you can understand like oh yeah you can use you can really start connect\n3:01:07 these agents with different basically different tools and use these agents in different ways and I would do want to\n3:01:12 show you guys real fast when it does come to the agent Deep dive I do want to show you whenever we set everything to\n3:01:18 verbose verbose normally yeah so here in the agents and tools I just want to show\n3:01:23 you guys when this is actually going off and grabbing the information so we''ll do this here for both and I''m going to\n3:01:29 rerun this example just so you guys can see that it is it''s actually grabbing from the doc store because I I like to\n3:01:34 see the agent think that''s one of my favorite things so whenever we run this again we''ll now do python fifth module\n3:01:40 fifth module agent Deep dive example number two we''re going to run it I''m going to ask who is branded again so um\n3:01:46 or how do I learn more about L chain how do I learn more about Lang\n3:01:52 chain and then now you can see it''s actually whenever it''s running you can see that it''s saying like Okay I need to\n3:01:58 answer a question about I need to to answer a question because I have no idea what is Lang chain in the context of\n3:02:04 what I''m learning about okay cool well I''m going to call the answer question tool the input I''m going to pass in is\n3:02:10 how do I learn more about Lang chain and then so you can see cuz we passed in an\n3:02:15 input coming back down here to our code yeah so here''s our input the query is now the same as this query the chat\n3:02:21 history this was our first question so it''s not updating it yet and then the context well this is all the information\n3:02:27 that we get back from our Vector store so this is some information from our Vector store some of it looks like it is\n3:02:35 yeah this is all documents from our Vector store and once it has those 3,000 tokens worth of information and then\n3:02:41 converts that into a final answer that it then spits back to us that we can talk to here so yeah I hope you guys I\n3:02:47 think that''s super cool to see how the agent''s thinking and operating you know whenever it''s in verbos mode but enough\n3:02:53 of doing the agent Deep dive what we''re going to do from here is we''re going to head over and start going deeper into\n3:02:58 different ways that you can create your tools so that you guys can Master this and create your own tools and really\n3:03:04 supercharge your agents so let''s go ahead and head into that next hey guys so welcome to the third\n3:03:11 example inside of the agents and tools module and in this example I''m going to show you guys the most basic way we can\n3:03:18 go about creating tools and that''s going to be using the tool Constructor and you''ve already done this a few times but\n3:03:23 now we''re just going to go into a deeper dive of understanding like oh that''s actually what''s going on and here''s how\n3:03:29 I can start making tools so what I want to do is to start off I just want to show you the three functions that we''re\n3:03:35 going to try and add to our tool set so the first one is just going to be greet User it''s going to take in a name and\n3:03:41 then it''s just going to come back and say like hello we''re going to do reverse string all this is going to do is it''s\n3:03:47 going to take in some text and reverse it and split it back and then finally we''re going to set up a concatenate\n3:03:53 strings tool that just takes in two strings and concatenates them together and spits out a string so well obviously\n3:04:00 you know that''s just regular python let''s dive into the tool section where we''re going to start using the tool\n3:04:05 Constructor so this is exactly what we''ve done so far whenever we''ve created tools in the past where we''ve kind of\n3:04:12 set up a name set up a description and then Define the specific function that we want to call but the main thing I\n3:04:17 want what to bring your focus to whenever we''re using the tool Constructor is this is a great way to go\n3:04:23 about creating simple tools because all we''re doing is just saying like here''s the name here''s the description and then\n3:04:28 we''re just passing it a function we''re not specifying anywhere the like oh yeah this takes in two parameters one''s an\n3:04:35 one''s a string one''s one''s a number like we''re just relying 100% on the llm underneath to understand what the tool\n3:04:43 needs and provide it to it so and most of the time that actually works really well and that''s whenever you just have a\n3:04:49 simple tool this is the best way to go about it it''s super simple to do and the part that I did want to show you guys as well is if you''re not using this\n3:04:57 complete basic way to create a tool you can go off and use something called the structure tool and the structure tool is\n3:05:04 great whenever you want to kind of set up more complex functions so like this one takes in two parameters so you know\n3:05:11 if you''re doing anything that takes in more than one parameter I would recommend going with a structured tool and the part that makes this different\n3:05:17 different is it takes in all the same information as before except then now it takes in this schema and this schema\n3:05:24 just we''re just going to use pedantic which is just a great way to like define basically like typed models um so we''re\n3:05:30 just going to say like hey here are our arguments for concatenating string I expect to have two two properties A and\n3:05:38 B where a is the first string and B is the second string so we''re clearly defining oh wow whenever I pass in I\n3:05:44 need to basically give two strings and that''s how it''s going to how it''s going to work so now that you''ve kind of seen\n3:05:50 how this works I''m going to go ahead and actually trigger this to run and so you can actually see all these tools working\n3:05:56 in action and the only thing I do want to mention that we haven''t shown so far is uh I am using a new prompt for this\n3:06:03 one it''s called open aai tools agent and what this one is doing is it''s going to\n3:06:09 it''s basically just focused more on using tools so you can kind of see you''re a helpful assistant here''s your\n3:06:14 chat history here''s your human inputs so this one all it does really is it''s just\n3:06:19 yeah it kind of just it works really well whenever you''re trying to use tools and so what we''re going to do is go\n3:06:25 ahead and Trigger this and what we''re expecting to see back is three different responses one we''re going to tell the\n3:06:31 agent executor to greet Alice so greet is going to hopefully trigger our first\n3:06:36 tool then eventually we''re going to say reverse the string hello and then we''re going to say concatenate and we''re\n3:06:42 expecting our agent to go up and use the appropriate tools to make this happen and we''re going to log the entire process okay so what we''re going to do\n3:06:48 come down here clear things up and we''re going to run our example so we''re going\n3:06:54 to do Python and this is the fifth module this is a tool Deep dive and we''re going to look at our first example\n3:07:00 together so like I said under the hood this is going to go off and Trigger each one of these so let''s look and see how\n3:07:06 it did so for the first one we said hey we''re trying to greet Alice so you can\n3:07:11 see that it said I need to you know invoke greeting user with the keyword Alicia or Alice and what it''s going to\n3:07:19 do is come back with an answer because if you come back to our original function all it was supposed to do is\n3:07:24 pass in hello and the person''s name and that''s exactly what it did so thumbs up for the first one the next one is\n3:07:30 reversing the string so you can see that like yep we''re invoking reverse string with hello it did it it performed the\n3:07:36 code and now we print it back and then finally what we''re going to do for the last one is concatenate strings so you\n3:07:43 can see I need to invoke concatenate strings and it did a really good job of creating a dictionary where it specified\n3:07:50 what is a and what is B and this all comes back to the fact that we set up our structur tool and we passed in an\n3:07:57 argument schema that was the magic that allowed this to happen and now you can see that it did a great job of splitting\n3:08:03 it uh concatenating it together okay cool so that was hopefully a quick Deep\n3:08:08 dive into creating tools and now we''re going to go into the next module where\n3:08:13 we''re going to start learning how you can use the tool decorator to kind of simplify some of this so let''s go ahead and start working on example number two\n3:08:21 in the tool deep dive right now hey guys so welcome to the third and\n3:08:26 final example in the tool Deep dive and in this example you''re going to learn how to create tools but you''re going to learn how to do it where you have the\n3:08:33 maximum control over how these tools behave and what we''re going to do in this module is we''re going to set up two\n3:08:38 tools the first one is going to be a simple Search tool where we''re going to go off and use Tavi to search the\n3:08:44 internet and the next one is just going to be a multiply numbers tool just a super super basic one so that''s what we''re going to be doing in this example\n3:08:50 and I just want to walk you through the major parts together real fast cuz some of this is pretty similar so the first thing that we''re going to be doing is\n3:08:56 setting up our pedantic models and this is what we''re going to be doing to define the specific inputs for both of\n3:09:02 our tools so this is nothing new you''ve seen this so far all right now let''s actually dive into the part where we''re\n3:09:08 going to create our tools and how we have maximum control over them so the main way we''re going to be creating our\n3:09:13 tools is by using the base tool class now hop over to the L chain doc so you\n3:09:18 can kind of see it so the main way it works is we''re going to be using the subass base tool to generate new tools\n3:09:25 so we''re going to inherit from based tools to create new classes that are tools basically so what you can see as\n3:09:31 Lang chain says like hey this is the way you can have Maxum control over your tools but you know it takes a little bit\n3:09:37 more work and the part that''s interesting is you can kind of see like we are basically using this example I\n3:09:43 just want to like walk you through the setup of it but the part that''s nice is you can actually set up your tools to have different functionality so you can\n3:09:49 set up a tool to have a run so you can see this is a private run method and\n3:09:55 what you can see is we can actually say hey here''s what you need to do here''s your inputs don''t worry about run\n3:10:01 manager I''ve never really found it helpful and then you can Define the output and what''s super cool is you can actually if you wanted to go even harder\n3:10:08 you could actually Define the output as a pedantic model and if it fails during the run to generate the proper response\n3:10:14 it''ll redo it so this is a really cool way if you plan on doing doing you know spitting out some Json or you know you\n3:10:20 want to make sure that you print out an object that for sure has a like a person''s contact info so it for sure had\n3:10:26 to have the person''s first name last name and email you could set up a pedantic model as the output and you can\n3:10:32 do that all here inside of your custom tools when you''re inheriting from base tool and then finally you can set up\n3:10:37 things to run synchronously or you can set them up to run asynchronously we''re just going to stick to synchronously for\n3:10:42 this example okay so let''s hop back over to our example so the first thing that we''re going to be doing is setting up a\n3:10:48 Search tool because we want our agent to go off and access the internet well what we''re going to do is we are going to do\n3:10:54 exactly kind of what we did in the tool Constructor where we have to give our tool a name because we need to let our\n3:10:59 AI know like Yep this is the name of the tool and here''s when I would need to use it whenever I need to answer questions\n3:11:05 about current events so you know anything that''s not inside of the ai''s knowledge base and then finally what is\n3:11:11 the argument schema well this is exactly what I want to be passed in the simple search input which just contains a query\n3:11:18 okay now whenever this tool gets triggered and calls underlying run method what''s going to happen is we\n3:11:25 expect to get that query of a string of what we need to go search now here''s where things get interesting this is\n3:11:30 where we''ve kind of defined what this function should do so in our case we''re going to use Tav and if you head over to\n3:11:36 their documents this is how you can connect your llms to the web and it just makes it super simple to go off and\n3:11:42 search the internet uh and then just I''m sure some of you guys are curious how much it cost it''s free every month for\n3:11:48 1,000 calls and then it goes from up from there but no I''ve I''ve used them and I''ve really liked them so far so definitely recommend checking them out\n3:11:54 and to get started it''s super simple you''re going to basically just make sure you have access to this class you''re\n3:12:00 going to set up an account so you get API key and then once you have an API key you can just start using their client and making request and that''s\n3:12:07 exactly what we''re doing in rcode we''re importing that client that we would have installed I''ve already added to the uh\n3:12:12 Tomo file so it''s already going to work for you guys now we''re going to and then once you have have your environment\n3:12:17 variable set up and grab it you''re just going to make sure you paste it in here and once you''ve done that you can now actually start using the client to make\n3:12:24 requests to the Internet so let''s go ahead and we''re going to run through the second tool real fast and then we''re going to run it so you can see that like\n3:12:30 wow we are actually communicating with the internet and it''s super cool all right and the final one this is just a super simple example I just wanted to\n3:12:36 show you guys like yeah you can create a tool to multiply numbers this one''s super simple I just wanted to to add it in here for you guys but once you\n3:12:42 created your two tools what you need to do is you need to go off and basically in your tools list we''re going to create\n3:12:48 two new tools so as you can see each one of these is a class so to create a new instance of the class we''re just kind of\n3:12:54 come down here and just you know create them down here so once we have two instances of these classes we''re going to do the normal thing with our agents\n3:13:01 set up the llm we want to use give it the prompt to guide its instructions set up our tool calling agent um this one is\n3:13:09 a little bit different from what we''ve done in the past in the past we''ve been doing the the react agent so the tool\n3:13:14 calling agent it just specializes in using tools so that''s what it''s kind of doing and then finally what we''re going\n3:13:20 to do is once we have our agent set up we''re going to use the agent executor to actually start uh handling our runs all\n3:13:26 right so let''s go ahead and run our agent with these two different examples the first example we''re going to be\n3:13:32 using is search for Apple intelligence and I''m going to take off these quotes just so we have to make our uh AI learn\n3:13:40 a little bit more about Apple intelligence and then finally we''re just going to pass in a sentence and we\n3:13:46 expect this sentence to get converted over to numbers that you can see specifically floats that we can then\n3:13:52 start to use to to to multiply so that''s what we I expect to see happen whenever we run this so let''s go ahead and run it\n3:13:58 real fast so this is in the agent module and we are in the tools section and this is the final one let''s go ahead and run\n3:14:05 it so we can see the search results and look at just the math so you can see out the gate what it did is it said all\n3:14:11 right I need to do a simple surge because that was the name of our tool let''s go back up here a simple surge and\n3:14:17 we just want to what is the query of what we want to look up oh we want to look up Apple intelligence so it pulled out the key topic and then Tav our\n3:14:24 Search tool came back to us and said well here''s your query here''s the what you want to look up and then it gave us\n3:14:29 back the results so it went off and searched the internet in a way to where we get quick titles URLs so if we wanted\n3:14:37 to we can go off and actually explore some of these URLs and dive deeper but you can see it came back to us with some\n3:14:43 more information about each each of these things and today is is the middle of June so you can see this is grabbing\n3:14:49 all recent information about what''s happening with the new Apple intelligence release okay cool and then\n3:14:54 once it gave us back the actual like this is the raw score and actually you can see it here here''s some of the\n3:15:00 relevant information so just putting it in a nice format for us um that''s what''s\n3:15:05 happening with the tool and once it has that information um it''s just going to spit out the final response for us so\n3:15:12 you can kind of see it all right here this is the final response and the outputs right here it just going to look\n3:15:17 exactly this is basically the output if you printed it out that''s what you would see uh um that you you were to pass it\n3:15:22 out to your user okay and enough of that the tool went super simple the reason why I kind of like this example is\n3:15:28 because usually tools struggle with like if you just to run this as a regular tool decorator for whatever reason it\n3:15:34 kind of struggles to recognize like oh yeah that''s a float so I just thought this was a cool example of like oh yeah we have to convert a string to a\n3:15:40 numerical representation so yeah and we get back to the correct answer and it''s a float which is what we want okay all\n3:15:46 right so that''s it for the tool example and this is actually it for the entire uh agent and Tool Deep dive just to like\n3:15:53 plant some seeds in your brains of like what else is possible crew AI is by far one of my favorite pack uh products out\n3:15:59 there when it comes to working with agents because they make it super cool for us to actually set up agents to work\n3:16:05 with other agents so right now we have one agent you know performing a task but what happens if you had like a writer\n3:16:11 agent collaborating with a researcher agent who each specialized like the uh if the if the researcher agent\n3:16:17 specialized in exploring the web and the writer uh specialized in writing articles that were had a good Rhythm\n3:16:23 used proper like languaging to write interesting articles well these guys could work together to generate an\n3:16:29 awesome report so that''s just scratching the service of what cre I can do but I just wanted to like plant some seeds for\n3:16:35 like what''s next after you guys learn like how to make an individual agent well multi-agents tools are the next and\n3:16:42 I definitely recommend creai and I have a ton of videos on crei inside of my YouTube channel so I definitely recommend and checking that out next but\n3:16:48 yeah that''s it for the fifth module so let''s go ahead and wrap things up hey guys so I hope you guys have enjoyed\n3:16:54 this Lang chain master class we covered a ton of information in all five of the different modules hopefully you guys are\n3:17:00 now Pros when it comes to working with chat models prompt templates chains Rag\n3:17:06 and your agents and tools and just as a recap all the source code in this video is completely for free there''s a link\n3:17:11 down the description below while you''re down there it would mean a lot to me if y''all can hit that like And subscribe\n3:17:17 especially if you''ve made it this far in in this video also there''s that free school Community where you can meet like-minded AI developers and we have\n3:17:24 those weekly free coaching calls so you''ll definitely want to take advantage of that and then outside of that I have\n3:17:29 a ton of other AI related content on my YouTube channel everything from Full stack AI tutorials all the way to like\n3:17:36 crew AI Deep dive so you''re definitely going want check out one of those after this video but enough of that I hope you\n3:17:42 guys have a great day and I can''t wait to see youall around in the next one see you',
  '{"channel": "aiwithbrandon", "video_id": "yF9kGESAi3M", "duration": "3:17:50", "level": "BEGINNER", "application": "LangChain", "topics": ["Chat Models", "Prompt Templates", "Chains", "RAG", "Vector Databases", "Embeddings", "Retrievers", "Agents", "Tools", "LangChain V0.2", "OpenAI", "Claude", "Gemini"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=EtldFS3JbGs',
  1,
  'Conversational Memory in LangChain for 2025',
  '0:00 In this chapter, we''re going to be taking a look at conversational memory in Langchain. We''re going to be taking a\n0:06 look at the core like chat memory components that have already been in\n0:...',
  '0:00 In this chapter, we''re going to be taking a look at conversational memory in Langchain. We''re going to be taking a\n0:06 look at the core like chat memory components that have already been in\n0:12 line chain since the start, but are essentially no longer in the library.\n0:18 And we''ll be seeing how we actually implement those historic conversational\n0:24 memory utilities in the new versions of line chain. So 0.3. Now, as a\n0:31 pre-warning, this chapter is fairly long, but that is because conversational memory is just such a critical part of\n0:40 chat bots and agents. Conversational memory is what allows them to remember previous interactions. And without it,\n0:47 our chat bots and agents would just be responding to the most recent message\n0:53 without any understanding of previous interactions within a conversations. So they would just not be conversational.\n1:00 And depending on the type of conversation, we might want to go with\n1:05 various approaches to how we remember those interactions within a\n1:11 conversation. Now, throughout this chapter, we''re going to be focusing on these four memory types. We''ll be\n1:18 referring to these, and I''ll be showing you actually how each one of these works. But what we''re really focusing on\n1:24 is rewriting these for the latest version of lang chain using the what''s\n1:29 called the runnable with message\n1:36 history. So we''re going to be essentially taking a look at the original implementations for each of\n1:42 these four original memory types and then we''ll be rewriting them with the the runnable memory history class. So\n1:49 just taking a look at each of these four very quickly. Consential buffer memory\n1:55 is I think the simplest and most intuitive of these memory types. It is\n2:01 literally just you have your messages. They come into this object. They are\n2:08 stored in this object as essentially a list and when you need them again it\n2:13 will return them to you. There''s nothing nothing else to it. It''s super simple. the conversation buffet window memory.\n2:19 Okay, so new word in the middle there, window. This works in pretty much the\n2:25 same way, but those messages that it has stored, it''s not going to return all of them for you. Instead, it''s just going\n2:31 to return the most recent, let''s say, the most recent three, for example.\n2:37 Okay? And that is defined by a parameter K. conversational summary memory. Rather\n2:42 than keeping track of the entire uh interaction memory directly, what it''s\n2:48 doing is as those interactions come in, it''s actually going to take them and it''s going to compress them into a\n2:54 smaller little summary of what has been within that conversation. And as every a\n3:00 new interaction is coming in, it''s going to do that and keep iterating on that summary. And then that is going to be\n3:06 returned to us when we need it. And finally we have the conversational summary buffer memory. So this is it''s\n3:14 taking so the buffer part of this is actually referring to very similar thing\n3:19 to the buffer window memory but rather than it being a you know most K messages\n3:24 it''s looking at the number of tokens within your memory and it''s returning the most recent K tokens. That''s what\n3:33 the buffer part is there. And then it''s also merging that with the summary\n3:39 memory here. So essentially what you''re getting is almost like a list of the most recent messages based on the token\n3:46 length rather than the number of interactions plus a summary which would\n3:51 you know come at the the top here. So you get kind of both. The idea is that obviously this summary here would\n3:58 maintain all of your interactions in a very compressed form. So you''re you''re\n4:04 losing less information and you''re still maintaining you know maybe the very first interaction the user might have\n4:10 introduced themselves given you their name hopefully that would be maintained within the summary and it would not be\n4:17 lost and then you have almost like a higher resolution on the most recent um K or K tokens from your memory. Okay, so\n4:26 let''s jump over to the code. We''re going into the 04 chat memory notebook. Open that in Collab. Okay, now here we are.\n4:33 Let''s go ahead and install the prerequisites. Run all.\n4:39 We again can or cannot use alignmith. It is up to you. Enter that. And let''s come\n4:46 down and start. So firstly just initialize our LM using 40 mini in this\n4:53 example again low temperature and we''re going to start with conversation buffer\n4:58 memory. Okay. So this is the original version of this uh memory type. So let\n5:07 me uh where are we? We''re here. So memory conversation buffer memory and we''re returning messages that needs to\n5:14 be set to true. So the reason that we set return messages to true it mentions\n5:19 up here is if you do not do this, it''s going to be returning your chat history\n5:25 as a string to an LLM. whereas well chat lens nowadays would expect\n5:33 message objects. So yeah, you just want to be returning these as messages rather\n5:38 than as strings. Okay, otherwise yeah, you''re going to get some kind of strange behavior out from your LLMs if you\n5:45 return them strings. So you do want to make sure that it''s true. I think by default it might not be true, but this\n5:51 is coming this is deprecated, right? It does tell you here as deprecation warning. This is coming from older blank\n5:58 chain but it''s a good place to start just to understand this and then we''re going to rewrite this with the runnables which is the recommended way of doing so\n6:05 nowadays. Okay. So adding messages to our memory. We''re going to write this.\n6:12 Okay. So it''s just a it''s just a conversation user AI user AI so on random chat. Main things to note here is\n6:19 I do provide my name. We have the the model''s name right towards the start of those interactions. Okay. Okay. So, I''m\n6:25 just going to add all of those. We''ll do it like this. Okay. Then we can just see\n6:33 we can load our history like so. So, let''s just see what we have there. Okay.\n6:39 So, we have a human message, AI message, human message. Right? This is it exactly what we I showed you just here. It''s\n6:46 just in that message format from Langchain. Okay. So, we can do that.\n6:51 Alternatively, we can actually do this. So we can get our memory. We initialize the constational buffer memory as we did\n6:58 before and we can actually add it directly the message into our memory\n7:03 like that. So we can use this add user message add AI message so on and so on. Load again and it''s going to give us the\n7:09 exact same thing again. There''s multiple ways to do uh the same thing. Cool. So we have that to pass all of this into\n7:16 our LLM. Again this is all deprecated stuff. We''re going to learn how to use properly in a moment. But this is how\n7:23 long chain was doing it in the past. So to pass all of this into our lm, we''d be\n7:28 using this conversation chain, right? Again, this is deprecated. Nowadays, we\n7:33 would be using ll for this. So I I just want to show you, okay, how this would\n7:39 all go together. And then we would invoke, okay, what is my name again? Let''s run that and we''ll see what we\n7:45 get. It''s remembering everything, remember? So this conversation buffer memory, it doesn''t drop messages. It\n7:52 just remembers everything, right? And honestly with the sort of high context\n7:57 windows of many LMs, that might be what you do. It depends on how long you expect a conversation to go on for, but\n8:03 you could you probably in most cases would get away with this. Okay, so what\n8:09 let''s see what we get. Um I say, \"What is my name again?\" Okay, let''s see what it gives me. Says, \"Your name is James.\"\n8:15 Great. Thank you. That works. Now, as I mentioned, all of this that I just showed you is actually\n8:21 deprecated. That''s the old way of doing things. Let''s see how we actually do this in modern or up to-ate blank chain.\n8:28 So, we''re going to be using this runnable with message history. To implement that, we will need to use LSL.\n8:35 And for that, we will need to just define prompt templates, our LM as we usually would. Okay. So, we''re going to\n8:41 set up our system prompt, which is just your helpful assistant called Zeta. Okay, we''re going to put in this\n8:48 messages placeholder. Okay, so that''s important. Essentially, that is where our messages that are\n8:56 coming from our conversational buffer memory is going to be inserted. Right,\n9:01 so it''s going to be that chat history is going to be inserted after our system prompt but before our most recent query\n9:08 which is going to be inserted last here. Okay, so messages placeholder item\n9:13 that''s important and we use that throughout the course as well. So we use it both for chat history and we''ll see\n9:19 later on. We also use it for the intermediate thoughts that a agent would go through as well. So important to\n9:26 remember that little thing. We''ll link our prompt template to our LM again if\n9:32 we would like. We could also add in the I think we only have the query here. Oh,\n9:39 we would probably also want our history as well. Uh, but I''m not going to do that right now. Okay, so we have our\n9:46 pipeline and we can go ahead and actually define our runnable with message history. Now, this class or\n9:53 object when we are initializing it does require a few items. We can see them here. Okay, so we see that we have our\n10:00 pipeline with history. So, it''s basically going to be uh you can you can see here, right? We have that history\n10:05 messages key, right? This here has to align with what we provided as the\n10:10 messages placeholder in our pipeline. Right? So we have our pipeline prompt\n10:17 template here and here. Right? So that''s where it''s coming from. It''s coming from messages placeholder. The variable name\n10:23 is history. Right? That''s important. That links to this. Then for the input\n10:29 messages key here we have query that again links to this. Okay. are both\n10:37 important to have there. The other thing that is important is obviously we''re passing in that pipeline\n10:43 from before. But then we also have this get session history. Basically what this is doing is it''s saying okay I need to\n10:49 get uh the list of messages that make up my chat history that are going to be inserted into this variable. So that is\n10:55 a function that we define. Okay. And within within this function, what we''re trying to do here is actually replicate\n11:03 what we have with the previous conversation buffer memory. Okay, so\n11:09 that''s what we''re doing here. So it''s very simple, right? So we have uh this\n11:15 inmemory chat message history. Okay, so that''s just the object that we''re going to be returning. What this will do is it\n11:22 will set up a session ID. The session I is essentially like a unique identifier so that each conversation or interaction\n11:30 within a single conversation is being mapped to a specific conversation. So you don''t have overlapping let''s say of\n11:35 multiple users using the same system you want to have a unique session ID for each one of those. Okay. And what it''s\n11:41 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here. We are going\n11:48 to initialize that session with an inmemory chat message history. Okay, that''s it.\n11:57 And we return. Okay, and all that''s going to do is it''s going to basically append our messages. They will be\n12:03 appended within this chat map session ID and they''re going to get returned.\n12:08 There''s nothing really there''s nothing else to it to be honest. So, we invoke\n12:14 our runnable. Let''s see what we get. Oh, I need to run this.\n12:20 Okay, note that we do have this config. So, we have the session ID. That''s to again, as I mentioned, keep different\n12:27 conversations separate. Okay, so we''ve run that. Now, let''s run a few more. So,\n12:32 what is my name again? Let''s see if it remembers. Your name is James. How can I help you today, James? Okay, so it''s\n12:41 what we''ve just done there is literally conversation buffer memory but for up\n12:47 to-ate lang chain with L cell with runnables. So you know the recommended way of doing\n12:54 it nowadays. So that''s a very simple example. Okay, there''s really not that\n13:00 much to it. It gets a little more complicated as we start thinking about the different types of memory. Although\n13:06 with that being said, it''s not massively complicated. We''re only rarely going to be changing the way that we''re getting\n13:12 our interactions. So let''s uh let''s dive into that and see how we would do\n13:18 something similar with the conversation buffer window memory. But first, let''s actually just understand okay what is\n13:24 the conversation buffer window memory. So as I mentioned near the start, it''s going to keep track of the last k\n13:30 messages. So there''s a few things to keep in mind here. More messages does\n13:35 mean more tokens sent with each request. And if we have more tokens in each request, it means that we''re increasing\n13:41 the latency of our responses and also the cost. So with the previous memory\n13:46 type, we''re just sending everything. And because we''re sending everything, that is going to be increasing our cost. It''s\n13:52 going to be increasing our latency for every message, especially as a conversation gets longer and longer. And we don''t we might not necessarily want\n13:58 to do that. So with this conversation buffer window memory, we''re going to\n14:03 just say, okay, just return me the most recent messages. Okay, so let''s or let''s\n14:10 see how that would work. Here we''re going to return the most recent four messages. Okay, we are again make sure\n14:17 we''ve turned messages is set to true. Again, this is deprecated. This is just the old way of doing it. In a moment,\n14:23 we''ll see the updated way of doing this. We''ll add all of our messages.\n14:30 Okay, so we have this and just see here, right? So we''ve added in all these\n14:36 messages. There''s more than four messages here and we can actually see that here. So we have human message AI\n14:42 human AI human AI human AI. Right? So we''ve got four pairs of human AI\n14:49 interactions there. But here we don''t have as more than four pairs. So four pairs would take us back all the way to\n14:57 here. I''m researching different types of conversational uh memory. Okay. And if\n15:03 we take a look here, the most the first message we have is I''m researching different types of conversational memory. So it''s cut off these two here\n15:11 which will be a bit problematic when we ask you what our name is. Okay. So let''s just see going to be using conversation\n15:17 chain object again. Again just remember that is deprecated. And I want to say what is my name again?\n15:23 Let''s see. Let''s see what it says. Uh I''m sorry, but I don''t have access to\n15:29 your name or any personal information. If you like, you can tell me your name, right? So it doesn''t actually remember. Uh so that''s kind of like a negative of\n15:38 the conversation buffer window memory. Of course, the uh to fix that in this\n15:43 scenario, we might just want to increase K. Maybe we say remember previous eight interaction pairs and it will actually\n15:50 remember. So where''s my name again? Your name is James. So now it remembers. We''ve just modified how much it is\n15:57 remembering. But of course, you know, there''s pros and cons to this. It really depends on what you''re trying to build.\n16:02 So let''s take a look at how we would actually implement this with the\n16:07 runnable with message history. Okay. So you getting a little more\n16:13 complicated here. Although it it''s it''s not it''s not complicated but well we''ll\n16:19 see. Okay, so we have a buffer window message history. We''re creating a class here. This class is going to inherit\n16:26 from the base chat message history object from lang chain. Okay. And and\n16:31 all of our other message history objects are going to do the same thing before with the in-memory message object that\n16:39 was basically replicating the buffer memory. So we didn''t actually need to do\n16:44 anything. We didn''t need to define our own class here. So in this case we do.\n16:50 So we follow the same pattern that lang chain follows with this base chat\n16:56 message history. And you can see a few of the functions here that are important. So add messages and clear are\n17:01 the ones that we''re going to be focusing on. We also need to have messages which this object attribute here. Okay. So\n17:07 we''re just implementing the synchronous methods here. If we want this to be\n17:14 async, if we want to support async, we would have to add a add messages, um, a\n17:19 get messages and a clear as well. So, let''s go ahead and do that. We have\n17:24 messages. We have K. Again, we''re looking at remembering the top K messages or most recent K messages only.\n17:30 So, it''s important that we have that variable. We are adding messages through this class. This is going to be used by\n17:37 line chain within our runnable. So, we need to make sure that we do have this method. And all we''re going to be doing\n17:42 is extending the self messages uh list here. And then we''re actually just going to be trimming that down so that we''re\n17:49 not remembering anything beyond those, you know, most recent K messages\n17:55 that we have set from here. And then we also have the clear method\n18:00 as well. So we need to include that. That''s just going to clear the history. Okay. So it''s not this isn''t\n18:05 complicated, right? it just gives us this nice default sandal interface for\n18:11 message history and we just need to make sure we''re following that pattern. Okay, I''ve included the uh this print here\n18:17 just so we can see what''s happening. Okay, so we have that and now for that\n18:23 get chat history function that we defined earlier rather than using the built-in method we''re going to be using\n18:30 our own object which is a buffer window message history which we defined just here. Okay, so if session ID is not in\n18:39 the chat map as we did before, we''re going to be initializing our buffer window message history. We''re setting K\n18:45 up here with a default value of four. And then we just return it. Okay. And and and that is it. So uh let''s run\n18:51 this. We have our runnable with message history. We have all of these variables\n18:56 which are exactly the same as before. But then we also have these variables here with this history factory config.\n19:03 And this is where if we have um new variables that we''ve added to our\n19:11 message history, in this case K that we have down here, we need to provide that\n19:17 to line chain and tell it this is a new configurable field. Okay. And we''ve also added it for the session ID here as\n19:23 well. So we''re just being explicit and have everything in there. So we have that and we run. Okay. Now let''s go\n19:32 ahead and invoke and see what we get. Okay, so important here this history\n19:38 factory config that is kind of being fed through into our invoke so that we can actually modify those variables from\n19:45 here. Okay, so we have config configurable session ID. Okay, we just put whatever we want in here and then we\n19:51 also have the number K. Okay, so remember the previous four interactions.\n19:58 I think in this one we''re doing something slightly different. I think we''re remembering the four interactions\n20:03 rather than the previous four interaction pairs. Okay, so my name is James. Uh we''re going to go through I''m\n20:09 just going to actually clear this and then I''m going to start again and we''re going to use the exact same add user\n20:15 message add AI message that we used before. We''re just manually inserting all that into our history so that we can\n20:21 then just see okay what is the result and you can see that k equals 4 is actually unlike before where we were\n20:28 having the uh saving the top four interaction pairs we''re now saving the\n20:35 most recent four interactions not pairs just interactions. And honestly I just\n20:41 think that''s clearer. I think it''s weird that the number four for K would actually save the most recent eight\n20:48 messages, right? I I think that''s odd. So, I''m just not replicating that weirdness. We could if we wanted to. I\n20:56 just don''t like it. So, I''m not doing that. And anyway, we can see from\n21:01 messages that we''re returning just the most four recent messages. Okay, which\n21:06 would be these four. Okay, cool. So we''ve just using the runnable we''ve\n21:11 replicated the old way of having a window memory and okay I''m going to say\n21:18 what is my name again as before it''s not going to remember so we can come to here I''m sorry but I don''t say personal\n21:24 information no so on and so on if you like tell me your name it doesn''t know now let''s try a new one where we\n21:31 initialize a new session okay so we''re going with ID K14 so that''s going to create a new\n21:37 conversation in there and we''re going to say we''re going to set K to 14.\n21:43 Okay, great. I''m going to manually insert the other uh messages as we did before. Okay, and we can see all of\n21:50 those. You can see at the top here, we are still maintaining that hi, my name is James message. Now, let''s see if it\n21:57 remembers my name. Your name is James. Okay, there we go. Cool. So, that is\n22:03 working. We can also see. So, we just added this. What is my name again? Let''s just see if did that get added to our\n22:10 list of messages, right? What is my name again? Nice. And then we also have the response your name is James. So just by\n22:17 invoking this because we''re using the the runnable with message history, it''s\n22:22 just automatically adding all of that into our message history which is nice.\n22:28 Cool. All right. So that is the buffer window memory. Now we are going to take a look\n22:34 at how we might do something a little more complicated which is uh the the summaries. Okay. So when you think about\n22:41 the summary you know what are we doing? We''re actually taking the messages we''re\n22:46 using that lm call to summarize them to compress them and then we''re storing\n22:52 them within messages. So let''s see how we would actually uh do that. So to\n22:57 start with let''s just see how uh it was done in old line chain. So we have conversation summary memory go through\n23:06 that and let''s just see what we get. So again same interactions\n23:13 right I''m just invoking invoking invoking I''m not adding these directly to the messages because it actually\n23:18 needs to go through a um like that summarization process and if we have a\n23:25 look we can see it happening. Okay, current conversation. So, sorry, current\n23:30 conversation. Hello there, my name is James. AI is generating current conversation. The human introduces\n23:36 himself as James. AI greets James warmly and expresses its readiness to chat and assists inquiring about how his day is\n23:43 going, right? So, it''s summarizing the the previous interactions. And then we\n23:49 have, you know, after that summary, we have the most recent human message and then the AI is going to generate its\n23:55 response. Okay? And that continues going continues going and you can see that the the final summary here is going to be a\n24:01 lot longer. Okay. And it''s different that first summary of course as day he mentions stage of researching different\n24:07 types of conversational memory. The AI responds enthusiastically explaining that conversational memory includes\n24:12 short-term memory, long-term memory, contextual memory, personalized memory and then inquires if James is focused on the specific type of memory. Okay, cool.\n24:21 So we get essentially the summary is just getting uh longer and longer as we go. But at some point the idea is that\n24:28 it''s not going to keep just growing and it should actually be shorter than if you were saving every single interaction\n24:33 whilst maintaining as much of the information as possible. But of course\n24:39 you are going to maintain all of the information that you would with for example the the buffer memory. Right?\n24:46 With the summary, you are going to lose information, but hopefully less information than if you''re just cutting\n24:54 interactions. So, you''re trying to reduce your token count whilst maintaining as much information as\n25:00 possible. Now, let''s go and ask uh what is my name again? It should be able to answer\n25:06 because we can see in the summary here that I introduce myself as James.\n25:12 Okay, response. Your name is James. How is your research going? Okay. So, has that cool? Let''s see how we''d implement\n25:19 that. So, again, as before, we''re going to go with that conversation summary\n25:25 message history. We''re going to be importing a system message. Uh we''re going to be using that not for the LM\n25:30 that we''re chatting with, but for the LM that will be generating our summary. So,\n25:37 actually that is not quite correct. There''s create a summary. Not that it matters. It''s just the dock string. So,\n25:43 we have our messages and we also have the llm. So different different attribute here to what we had before.\n25:48 When we initialize a conversation summary message history, we need to passing in our LM. We have the same\n25:55 methods as before. We have add messages and clear. And what we''re doing is as messages coming, we extend with our\n26:02 current messages, but then we''re modifying those. Okay. So we construct\n26:08 our like instructions to make a summary. Okay. So that is here we have the system\n26:14 prompt uh given the existing conversation summary and the new messages generate a new summary of the conversation ensuring to maintain as\n26:21 much relevant information as possible. Okay. Then we have a human message here through that we''re passing the existing\n26:28 summary. Okay. And then we''re passing in the new messages.\n26:33 Okay. Cool. So we format those and invoke the lm\n26:41 here. And then what we''re doing is in the messages we''re actually replacing the existing history that we had before\n26:48 with a new history which is just a single system summary message. Okay,\n26:55 let''s see what we get. As before, we have that get chat history exactly the same as before. The only real difference\n27:01 is that we''re passing in the LM parameter here. And of course, as we''re passing in the lm parameter in here, it\n27:07 does also mean that we''re going to have to include that in the configurable field spec and that we''re going to need\n27:14 to include that when we''re invoking our pipeline. Okay, so we run that pass in\n27:21 the lm. Now, of course, one side effect of uh\n27:26 generating summaries for everything is that we''re actually, you know, we''re generating more. So you are actually\n27:32 using quite a lot of tokens. Whether or not you are saving tokens or not actually depends on the length of a\n27:38 conversation. As a conversation gets longer. If you''re storing everything after a little while that the token\n27:45 usage is actually going to increase. So if in your use case you expect to have\n27:50 shorter conversations, you would be saving money and tokens by just using this standard buffer memory. Whereas if\n27:59 you''re expecting very long conversations, you would be saving tokens and money by using the summary\n28:05 history. Okay, so let''s see what we got from there. We have a summary of the conversation. James introduced himself\n28:11 by saying, \"Hi, my name responded warmly, asking me, \"Hi, James.\" Introduction included details about\n28:16 token usage. Okay, so we actually uh included everything here, which we\n28:22 probably should not have done. Why did we do that? Uh so in here we''re including all of the\n28:35 oh in here so using we''re including all of the content from the messages. So I think maybe if we just do\n28:42 hcontent for x in messages that should resolve\n28:49 that. Okay there we go. So we quickly fix\n28:55 that. So yeah, before we''re passing in the entire message object, which obviously includes all of this information, whereas actually we just\n29:01 want to be passing in the content. So we modified that and now we''re getting what\n29:08 we''d expect. Okay, cool. And then we can keep going, right? So as we as we keep going, the\n29:13 summary should get more like abstract like as we just saw here, it''s literally\n29:19 just giving us the messages directly almost. Okay, so we''re getting bit summary there and we can keep going.\n29:25 We''re going to add just more messages to that. We''ll see the, you know, as we''ll\n29:31 get send those, we''ll get a response, send it again, get response. We''re just adding all of that, invoking all of\n29:38 that, and that will be, of course, adding everything into our message history. Okay, cool. So, we''ve run that.\n29:44 Let''s see what the uh latest summary is.\n29:49 Okay, and then we have this. So this is a summary that we have inside of our our chat history.\n29:55 Okay, cool. Now finally, let''s see what is my name again. We can just double\n30:01 check, you know, has my name in there, so it should be able to tell us.\n30:09 Okay, cool. So your name is James. Pretty interesting. So let''s have a\n30:14 quick look over at Langsmith. So the reason I want to do this is just to point out okay the different essentially\n30:22 token usage that we''re getting with each one of these. Okay. So we can see that we have these runnable message history\n30:27 which probably improved in naming there. But we can see okay how long is each one\n30:33 of these taken? How many tokens are they also using? Come back to here we have\n30:39 this runnable message history. This is we''ll go through a few of these maybe to\n30:45 here I think. And we can see here this is that first interaction where we''re using the buffer memory and we can see\n30:52 how many tokens we used here. So 112 tokens when we''re asking what is my name again. Okay then we modified this to\n31:01 include I think it was like 14 interactions or something along those lines and obviously increases the number\n31:06 of tokens that we''re using. Right? So we can could see that actually happening all in Lang stuff which is quite nice\n31:11 and we can compare okay how many tokens is each one of these using. Now this is\n31:16 looking at the buffer window and then if we come down to here and look at this\n31:22 one. So this is using our summary. Okay so our summary with what is my name again actually use more tokens in this\n31:28 scenario right which is interesting because we''re trying to compress information. The reason there''s more is\n31:34 because there''s not there hasn''t been that many interactions. As the conversation length increases\n31:41 with the summary, this total number of tokens, especially if we prompt it correctly to keep that low, that should\n31:48 remain relatively small. Whereas with the buffer memory, that will just keep\n31:54 increasing and increasing as a as a conversation gets longer. So useful little way of using lang there\n32:02 to just kind of figure out okay in terms of tokens and costs what we looking at for each of these memory types. Okay so\n32:09 our final memory type acts as a mix of the summary memory and the buffer\n32:16 memory. So, what it''s going to do is keep the buffer up until an N number of\n32:23 tokens and then once a message exceeds the number of token limit for the\n32:28 buffer, it is actually going to be added into our summary. So this memory has the\n32:34 benefit of remembering in detail the most recent interactions whilst also not\n32:42 having the limitation of using too many tokens as a conversation gets longer and\n32:49 even potentially exceeding context windows if you try super hard. So this is a very interesting approach. Now as\n32:56 before let''s try the original way of implementing this then we will go ahead\n33:03 and use our update method for implementing this. So we come down to here and we''re going to line chain\n33:09 memory import conversation summary buffer memory. Okay, a few things here.\n33:15 lm for summary. We have the n number of tokens that we can keep before they get\n33:22 added to the summary. and then return messages of course. Okay, you can see again this is deprecated. We use the\n33:28 conversation chain and then we just pass in our memory there and then we can chat. Okay, so super straightforward.\n33:36 First message, we''ll add a few more here. Again, we have to invoke because our\n33:43 memory type here is using MLM to create those summaries as it goes. And let''s\n33:49 see what they look like. Okay. So we can see for the first message here we have a human message and then an AI message.\n33:57 Then we come a little bit lower down again. Same thing human message is the first thing in our history here. Then\n34:04 it''s a system message. So this is at the point where we''ve exceeded that 300 token limit and the memory type here is\n34:11 generating those summaries. So that summary comes in as a message and we can see okay the human named James\n34:18 introduces himself and mentions he''s researching different types of conversational memory and so on and so on right okay cool so we have that then\n34:27 let''s come down a little bit further we can see okay so the summary there okay\n34:34 so that''s what we that''s what we have that is the implementation for the old\n34:40 version of this memory again we can see is deprecated. So, how do we implement this for our more recent versions of\n34:49 lang chain and specifically 0.3? Well, again, we''re using that runnable with message history and it looks a little\n34:57 more complicated than we were getting before, but it''s actually just, you know, it''s nothing too complex. We''re\n35:05 just creating a summary as we did with the previous memory type. But the\n35:10 decision for adding to that summary is based on in this case actually the number of messages. So I didn''t go with\n35:16 the the lang chain version where it''s a number of tokens. I don''t like that. I\n35:22 prefer to go with messages. So what I''m doing is saying okay last k messages.\n35:27 Okay. Once we exceed k messages, the messages beyond that are going to be\n35:33 added to the memory. Okay, cool. So let''s see. We first initialize our\n35:41 conversation summary buffer message history class with lm and k. Okay, so\n35:48 these two here. So lm of course to create summaries and k is just the the limit of the number of messages that we\n35:53 want to keep before adding them to the summary or dropping them from our messages and adding them to the summary.\n36:00 Okay. So we will begin with okay do we have an existing summary. So the reason\n36:07 we set this to none is we can''t extract the summary the existing summary unless\n36:14 it already exists. And the only way we can do that is by checking okay do we\n36:19 have any messages. If yes we want to check if within those messages we have a system message because we''re we''re doing\n36:25 the same structure as what we have up here where the system message that first system message is actually our summary.\n36:32 So that''s what we''re doing here. We''re checking if there is a summary message already stored within our messages.\n36:38 Okay. So we''re checking for that. If we find it, we just do we have this little print\n36:45 statement so we can see that we found something and then we just make our existing summary. I should actually move\n36:53 this to the first instance here. Okay, so that existing summary will be set to\n37:02 the first message. Okay, and this would be a system message\n37:08 rather than a string. Cool. So we have that. Then we want to\n37:14 add any new messages to our history. Okay. So we''re extending the history\n37:20 there. And then we''re saying, okay, if the length of our history is exceeds the\n37:25 K value that we set, we''re going to say, okay, we found that many messages. We''re going to be dropping the latest. It''s\n37:30 going to be the latest two messages. this I will say here one thing or one\n37:36 problem with this is that we''re not going to be saving that many tokens if we''re summarizing every two messages.\n37:43 So, what I would probably do is in in an actual like production setting, I would\n37:50 probably say let''s go up to 20 messages. And once we hit 20\n37:56 messages, let''s take the previous 10, we''re going to summarize them and put them into our summary alongside any, you\n38:02 know, previous summary that already existed. But in in, you know, this is also fine as well. Okay. So, we say we\n38:11 found those measures. We''re going to drop the latest two messages. Okay. So, we pull the the oldest messages out. I\n38:20 should say not the latest. It''s the oldest. Not the latest. Want to keep the latest\n38:26 and drop the oldest. So, we pull out the oldest messages and keep only the most\n38:32 recent messages. Okay. Then I''m saying, okay, if we if we don''t\n38:38 have any old messages to summarize, we don''t do anything. and we just return. Okay, so let''s indicates that this has\n38:44 not been triggered, we would hit this. But in the case this has been triggered\n38:51 and we do have old messages, we''re going to come to here. Okay, so this is we can\n38:59 see we have a system message prompt template saying given the existing conversation summary and the new messages generate a new summary of the\n39:06 conversation ensuring to maintain as much relevant information as possible. So if you want to be more conservative\n39:12 with tokens, we could modify this prompt here to say keep the summary to within\n39:17 the length of a single paragraph for example. And then we have our human\n39:23 message prompt template which can say okay here''s the existing conversation summary and here are new messages. Now\n39:28 new messages here is actually the old messages but the way that we''re framing it to the LM here is that we want to\n39:35 summarize the whole conversation, right? It doesn''t need to have the most recent measures that we''re storing within our\n39:42 buffer. It doesn''t need to know about those. That''s irrelevant to the summary. So, we just tell it that we have these\n39:47 new measures. And as far as this LM is concerned, this is like the full set of interactions. Okay. So, then we would\n39:54 format those and invoke our LM. And then we''ll print out our new summary so we\n40:00 can see what''s going on there. And we would prepend that new summary to our\n40:06 conversation history. Okay. And and this will work. So we can just prepend it\n40:13 like this because we''ve already popped\n40:18 where was it up here. If we have an existing summary, we already popped that from the list. So\n40:24 it''s already been pulled out of that list. So it''s okay for us to just we don''t need to say like we don''t need to\n40:31 do this because we''ve already dropped that initial system message if it existed. Okay. And then we have the\n40:37 clear method as before. So that''s all of the logic for our conversational summary\n40:45 buffer memory. We redefine our get chat history function with the lm and k\n40:53 parameters there. And then we''ll also want to set the configurable fields again. So that is just going to be of\n40:59 course session id lm and k. Okay. So now we can invoke the k value\n41:07 to begin with is going to be four. Okay. So we can see no old messages to\n41:13 update summary with. That''s good. Let''s invoke this a few times and let''s see\n41:18 what we get. Okay. So no old messages to update summary with.\n41:26 found six matches dropping the oldest two. And then we have new summary in the conversation. James and Bruce himself is\n41:33 interested researching different types of conversational memory. Right? So you can see there''s quite a lot in here at\n41:38 the moment. So we would definitely want to prompt the LLM, the summary LLM to keep\n41:45 that short. Otherwise, we''re just getting a ton of stuff, right? But we can see that that is you\n41:52 know it''s it''s working it''s functional. So let''s go back and see if we can prompt it to be a little more concise.\n41:59 So we come to here ensuring to maintain as much relevant information as possible. However we need to keep\n42:09 our summary concise. The limit\n42:15 is a single short paragraph. Okay. something like this. Let''s try and let''s\n42:22 see what we get with that. Okay, so message one again, nothing to\n42:28 update. See this? So, new summary. You can see it''s a bit shorter. It doesn''t have all those bullet points.\n42:37 Okay, so that seems better. Let''s see. So you can see the first summary is a\n42:44 bit shorter, but then as soon as we get to the second and third summaries, the second summary\n42:50 is actually slightly longer than the third one. Okay, so we''re going to be we''re going to be losing a bit of\n42:56 information in this case, more than we were before, but we''re saving a ton of tokens. So that''s of course a good\n43:03 thing. And of course, we could keep going and adding many interactions here. And we should see that this conversation\n43:09 summary will be it should maintain that sort of length of around one short\n43:14 paragraph. So that is it for this chapter on conversational memory. We''ve\n43:21 seen a few different memory types. We''ve implemented their old deprecated versions so we can see what they were\n43:28 like and then we''ve reimplemented them for the latest versions of lang chain.\n43:33 And to be honest, using logic where we are getting much more into the weeds and\n43:39 that is in some ways okay it complicates things that is true but in other ways it\n43:45 gives us a ton of control. So we can modify those memory types as we did with that final summary buffer memory type.\n43:52 We can modify those to our liking which is incredibly useful when you''re\n43:58 actually building applications for the real world. So that is it for this chapter. We''ll move on to the next one.\n0:00 In this chapter, we''re going to be taking a look at conversational memory in Langchain. We''re going to be taking a\n0:06 look at the core like chat memory components that have already been in\n0:12 line chain since the start, but are essentially no longer in the library.\n0:18 And we''ll be seeing how we actually implement those historic conversational\n0:24 memory utilities in the new versions of line chain. So 0.3. Now, as a\n0:31 pre-warning, this chapter is fairly long, but that is because conversational memory is just such a critical part of\n0:40 chat bots and agents. Conversational memory is what allows them to remember previous interactions. And without it,\n0:47 our chat bots and agents would just be responding to the most recent message\n0:53 without any understanding of previous interactions within a conversations. So they would just not be conversational.\n1:00 And depending on the type of conversation, we might want to go with\n1:05 various approaches to how we remember those interactions within a\n1:11 conversation. Now, throughout this chapter, we''re going to be focusing on these four memory types. We''ll be\n1:18 referring to these, and I''ll be showing you actually how each one of these works. But what we''re really focusing on\n1:24 is rewriting these for the latest version of lang chain using the what''s\n1:29 called the runnable with message\n1:36 history. So we''re going to be essentially taking a look at the original implementations for each of\n1:42 these four original memory types and then we''ll be rewriting them with the the runnable memory history class. So\n1:49 just taking a look at each of these four very quickly. Consential buffer memory\n1:55 is I think the simplest and most intuitive of these memory types. It is\n2:01 literally just you have your messages. They come into this object. They are\n2:08 stored in this object as essentially a list and when you need them again it\n2:13 will return them to you. There''s nothing nothing else to it. It''s super simple. the conversation buffet window memory.\n2:19 Okay, so new word in the middle there, window. This works in pretty much the\n2:25 same way, but those messages that it has stored, it''s not going to return all of them for you. Instead, it''s just going\n2:31 to return the most recent, let''s say, the most recent three, for example.\n2:37 Okay? And that is defined by a parameter K. conversational summary memory. Rather\n2:42 than keeping track of the entire uh interaction memory directly, what it''s\n2:48 doing is as those interactions come in, it''s actually going to take them and it''s going to compress them into a\n2:54 smaller little summary of what has been within that conversation. And as every a\n3:00 new interaction is coming in, it''s going to do that and keep iterating on that summary. And then that is going to be\n3:06 returned to us when we need it. And finally we have the conversational summary buffer memory. So this is it''s\n3:14 taking so the buffer part of this is actually referring to very similar thing\n3:19 to the buffer window memory but rather than it being a you know most K messages\n3:24 it''s looking at the number of tokens within your memory and it''s returning the most recent K tokens. That''s what\n3:33 the buffer part is there. And then it''s also merging that with the summary\n3:39 memory here. So essentially what you''re getting is almost like a list of the most recent messages based on the token\n3:46 length rather than the number of interactions plus a summary which would\n3:51 you know come at the the top here. So you get kind of both. The idea is that obviously this summary here would\n3:58 maintain all of your interactions in a very compressed form. So you''re you''re\n4:04 losing less information and you''re still maintaining you know maybe the very first interaction the user might have\n4:10 introduced themselves given you their name hopefully that would be maintained within the summary and it would not be\n4:17 lost and then you have almost like a higher resolution on the most recent um K or K tokens from your memory. Okay, so\n4:26 let''s jump over to the code. We''re going into the 04 chat memory notebook. Open that in Collab. Okay, now here we are.\n4:33 Let''s go ahead and install the prerequisites. Run all.\n4:39 We again can or cannot use alignmith. It is up to you. Enter that. And let''s come\n4:46 down and start. So firstly just initialize our LM using 40 mini in this\n4:53 example again low temperature and we''re going to start with conversation buffer\n4:58 memory. Okay. So this is the original version of this uh memory type. So let\n5:07 me uh where are we? We''re here. So memory conversation buffer memory and we''re returning messages that needs to\n5:14 be set to true. So the reason that we set return messages to true it mentions\n5:19 up here is if you do not do this, it''s going to be returning your chat history\n5:25 as a string to an LLM. whereas well chat lens nowadays would expect\n5:33 message objects. So yeah, you just want to be returning these as messages rather\n5:38 than as strings. Okay, otherwise yeah, you''re going to get some kind of strange behavior out from your LLMs if you\n5:45 return them strings. So you do want to make sure that it''s true. I think by default it might not be true, but this\n5:51 is coming this is deprecated, right? It does tell you here as deprecation warning. This is coming from older blank\n5:58 chain but it''s a good place to start just to understand this and then we''re going to rewrite this with the runnables which is the recommended way of doing so\n6:05 nowadays. Okay. So adding messages to our memory. We''re going to write this.\n6:12 Okay. So it''s just a it''s just a conversation user AI user AI so on random chat. Main things to note here is\n6:19 I do provide my name. We have the the model''s name right towards the start of those interactions. Okay. Okay. So, I''m\n6:25 just going to add all of those. We''ll do it like this. Okay. Then we can just see\n6:33 we can load our history like so. So, let''s just see what we have there. Okay.\n6:39 So, we have a human message, AI message, human message. Right? This is it exactly what we I showed you just here. It''s\n6:46 just in that message format from Langchain. Okay. So, we can do that.\n6:51 Alternatively, we can actually do this. So we can get our memory. We initialize the constational buffer memory as we did\n6:58 before and we can actually add it directly the message into our memory\n7:03 like that. So we can use this add user message add AI message so on and so on. Load again and it''s going to give us the\n7:09 exact same thing again. There''s multiple ways to do uh the same thing. Cool. So we have that to pass all of this into\n7:16 our LLM. Again this is all deprecated stuff. We''re going to learn how to use properly in a moment. But this is how\n7:23 long chain was doing it in the past. So to pass all of this into our lm, we''d be\n7:28 using this conversation chain, right? Again, this is deprecated. Nowadays, we\n7:33 would be using ll for this. So I I just want to show you, okay, how this would\n7:39 all go together. And then we would invoke, okay, what is my name again? Let''s run that and we''ll see what we\n7:45 get. It''s remembering everything, remember? So this conversation buffer memory, it doesn''t drop messages. It\n7:52 just remembers everything, right? And honestly with the sort of high context\n7:57 windows of many LMs, that might be what you do. It depends on how long you expect a conversation to go on for, but\n8:03 you could you probably in most cases would get away with this. Okay, so what\n8:09 let''s see what we get. Um I say, \"What is my name again?\" Okay, let''s see what it gives me. Says, \"Your name is James.\"\n8:15 Great. Thank you. That works. Now, as I mentioned, all of this that I just showed you is actually\n8:21 deprecated. That''s the old way of doing things. Let''s see how we actually do this in modern or up to-ate blank chain.\n8:28 So, we''re going to be using this runnable with message history. To implement that, we will need to use LSL.\n8:35 And for that, we will need to just define prompt templates, our LM as we usually would. Okay. So, we''re going to\n8:41 set up our system prompt, which is just your helpful assistant called Zeta. Okay, we''re going to put in this\n8:48 messages placeholder. Okay, so that''s important. Essentially, that is where our messages that are\n8:56 coming from our conversational buffer memory is going to be inserted. Right,\n9:01 so it''s going to be that chat history is going to be inserted after our system prompt but before our most recent query\n9:08 which is going to be inserted last here. Okay, so messages placeholder item\n9:13 that''s important and we use that throughout the course as well. So we use it both for chat history and we''ll see\n9:19 later on. We also use it for the intermediate thoughts that a agent would go through as well. So important to\n9:26 remember that little thing. We''ll link our prompt template to our LM again if\n9:32 we would like. We could also add in the I think we only have the query here. Oh,\n9:39 we would probably also want our history as well. Uh, but I''m not going to do that right now. Okay, so we have our\n9:46 pipeline and we can go ahead and actually define our runnable with message history. Now, this class or\n9:53 object when we are initializing it does require a few items. We can see them here. Okay, so we see that we have our\n10:00 pipeline with history. So, it''s basically going to be uh you can you can see here, right? We have that history\n10:05 messages key, right? This here has to align with what we provided as the\n10:10 messages placeholder in our pipeline. Right? So we have our pipeline prompt\n10:17 template here and here. Right? So that''s where it''s coming from. It''s coming from messages placeholder. The variable name\n10:23 is history. Right? That''s important. That links to this. Then for the input\n10:29 messages key here we have query that again links to this. Okay. are both\n10:37 important to have there. The other thing that is important is obviously we''re passing in that pipeline\n10:43 from before. But then we also have this get session history. Basically what this is doing is it''s saying okay I need to\n10:49 get uh the list of messages that make up my chat history that are going to be inserted into this variable. So that is\n10:55 a function that we define. Okay. And within within this function, what we''re trying to do here is actually replicate\n11:03 what we have with the previous conversation buffer memory. Okay, so\n11:09 that''s what we''re doing here. So it''s very simple, right? So we have uh this\n11:15 inmemory chat message history. Okay, so that''s just the object that we''re going to be returning. What this will do is it\n11:22 will set up a session ID. The session I is essentially like a unique identifier so that each conversation or interaction\n11:30 within a single conversation is being mapped to a specific conversation. So you don''t have overlapping let''s say of\n11:35 multiple users using the same system you want to have a unique session ID for each one of those. Okay. And what it''s\n11:41 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here. We are going\n11:48 to initialize that session with an inmemory chat message history. Okay, that''s it.\n11:57 And we return. Okay, and all that''s going to do is it''s going to basically append our messages. They will be\n12:03 appended within this chat map session ID and they''re going to get returned.\n12:08 There''s nothing really there''s nothing else to it to be honest. So, we invoke\n12:14 our runnable. Let''s see what we get. Oh, I need to run this.\n12:20 Okay, note that we do have this config. So, we have the session ID. That''s to again, as I mentioned, keep different\n12:27 conversations separate. Okay, so we''ve run that. Now, let''s run a few more. So,\n12:32 what is my name again? Let''s see if it remembers. Your name is James. How can I help you today, James? Okay, so it''s\n12:41 what we''ve just done there is literally conversation buffer memory but for up\n12:47 to-ate lang chain with L cell with runnables. So you know the recommended way of doing\n12:54 it nowadays. So that''s a very simple example. Okay, there''s really not that\n13:00 much to it. It gets a little more complicated as we start thinking about the different types of memory. Although\n13:06 with that being said, it''s not massively complicated. We''re only rarely going to be changing the way that we''re getting\n13:12 our interactions. So let''s uh let''s dive into that and see how we would do\n13:18 something similar with the conversation buffer window memory. But first, let''s actually just understand okay what is\n13:24 the conversation buffer window memory. So as I mentioned near the start, it''s going to keep track of the last k\n13:30 messages. So there''s a few things to keep in mind here. More messages does\n13:35 mean more tokens sent with each request. And if we have more tokens in each request, it means that we''re increasing\n13:41 the latency of our responses and also the cost. So with the previous memory\n13:46 type, we''re just sending everything. And because we''re sending everything, that is going to be increasing our cost. It''s\n13:52 going to be increasing our latency for every message, especially as a conversation gets longer and longer. And we don''t we might not necessarily want\n13:58 to do that. So with this conversation buffer window memory, we''re going to\n14:03 just say, okay, just return me the most recent messages. Okay, so let''s or let''s\n14:10 see how that would work. Here we''re going to return the most recent four messages. Okay, we are again make sure\n14:17 we''ve turned messages is set to true. Again, this is deprecated. This is just the old way of doing it. In a moment,\n14:23 we''ll see the updated way of doing this. We''ll add all of our messages.\n14:30 Okay, so we have this and just see here, right? So we''ve added in all these\n14:36 messages. There''s more than four messages here and we can actually see that here. So we have human message AI\n14:42 human AI human AI human AI. Right? So we''ve got four pairs of human AI\n14:49 interactions there. But here we don''t have as more than four pairs. So four pairs would take us back all the way to\n14:57 here. I''m researching different types of conversational uh memory. Okay. And if\n15:03 we take a look here, the most the first message we have is I''m researching different types of conversational memory. So it''s cut off these two here\n15:11 which will be a bit problematic when we ask you what our name is. Okay. So let''s just see going to be using conversation\n15:17 chain object again. Again just remember that is deprecated. And I want to say what is my name again?\n15:23 Let''s see. Let''s see what it says. Uh I''m sorry, but I don''t have access to\n15:29 your name or any personal information. If you like, you can tell me your name, right? So it doesn''t actually remember. Uh so that''s kind of like a negative of\n15:38 the conversation buffer window memory. Of course, the uh to fix that in this\n15:43 scenario, we might just want to increase K. Maybe we say remember previous eight interaction pairs and it will actually\n15:50 remember. So where''s my name again? Your name is James. So now it remembers. We''ve just modified how much it is\n15:57 remembering. But of course, you know, there''s pros and cons to this. It really depends on what you''re trying to build.\n16:02 So let''s take a look at how we would actually implement this with the\n16:07 runnable with message history. Okay. So you getting a little more\n16:13 complicated here. Although it it''s it''s not it''s not complicated but well we''ll\n16:19 see. Okay, so we have a buffer window message history. We''re creating a class here. This class is going to inherit\n16:26 from the base chat message history object from lang chain. Okay. And and\n16:31 all of our other message history objects are going to do the same thing before with the in-memory message object that\n16:39 was basically replicating the buffer memory. So we didn''t actually need to do\n16:44 anything. We didn''t need to define our own class here. So in this case we do.\n16:50 So we follow the same pattern that lang chain follows with this base chat\n16:56 message history. And you can see a few of the functions here that are important. So add messages and clear are\n17:01 the ones that we''re going to be focusing on. We also need to have messages which this object attribute here. Okay. So\n17:07 we''re just implementing the synchronous methods here. If we want this to be\n17:14 async, if we want to support async, we would have to add a add messages, um, a\n17:19 get messages and a clear as well. So, let''s go ahead and do that. We have\n17:24 messages. We have K. Again, we''re looking at remembering the top K messages or most recent K messages only.\n17:30 So, it''s important that we have that variable. We are adding messages through this class. This is going to be used by\n17:37 line chain within our runnable. So, we need to make sure that we do have this method. And all we''re going to be doing\n17:42 is extending the self messages uh list here. And then we''re actually just going to be trimming that down so that we''re\n17:49 not remembering anything beyond those, you know, most recent K messages\n17:55 that we have set from here. And then we also have the clear method\n18:00 as well. So we need to include that. That''s just going to clear the history. Okay. So it''s not this isn''t\n18:05 complicated, right? it just gives us this nice default sandal interface for\n18:11 message history and we just need to make sure we''re following that pattern. Okay, I''ve included the uh this print here\n18:17 just so we can see what''s happening. Okay, so we have that and now for that\n18:23 get chat history function that we defined earlier rather than using the built-in method we''re going to be using\n18:30 our own object which is a buffer window message history which we defined just here. Okay, so if session ID is not in\n18:39 the chat map as we did before, we''re going to be initializing our buffer window message history. We''re setting K\n18:45 up here with a default value of four. And then we just return it. Okay. And and and that is it. So uh let''s run\n18:51 this. We have our runnable with message history. We have all of these variables\n18:56 which are exactly the same as before. But then we also have these variables here with this history factory config.\n19:03 And this is where if we have um new variables that we''ve added to our\n19:11 message history, in this case K that we have down here, we need to provide that\n19:17 to line chain and tell it this is a new configurable field. Okay. And we''ve also added it for the session ID here as\n19:23 well. So we''re just being explicit and have everything in there. So we have that and we run. Okay. Now let''s go\n19:32 ahead and invoke and see what we get. Okay, so important here this history\n19:38 factory config that is kind of being fed through into our invoke so that we can actually modify those variables from\n19:45 here. Okay, so we have config configurable session ID. Okay, we just put whatever we want in here and then we\n19:51 also have the number K. Okay, so remember the previous four interactions.\n19:58 I think in this one we''re doing something slightly different. I think we''re remembering the four interactions\n20:03 rather than the previous four interaction pairs. Okay, so my name is James. Uh we''re going to go through I''m\n20:09 just going to actually clear this and then I''m going to start again and we''re going to use the exact same add user\n20:15 message add AI message that we used before. We''re just manually inserting all that into our history so that we can\n20:21 then just see okay what is the result and you can see that k equals 4 is actually unlike before where we were\n20:28 having the uh saving the top four interaction pairs we''re now saving the\n20:35 most recent four interactions not pairs just interactions. And honestly I just\n20:41 think that''s clearer. I think it''s weird that the number four for K would actually save the most recent eight\n20:48 messages, right? I I think that''s odd. So, I''m just not replicating that weirdness. We could if we wanted to. I\n20:56 just don''t like it. So, I''m not doing that. And anyway, we can see from\n21:01 messages that we''re returning just the most four recent messages. Okay, which\n21:06 would be these four. Okay, cool. So we''ve just using the runnable we''ve\n21:11 replicated the old way of having a window memory and okay I''m going to say\n21:18 what is my name again as before it''s not going to remember so we can come to here I''m sorry but I don''t say personal\n21:24 information no so on and so on if you like tell me your name it doesn''t know now let''s try a new one where we\n21:31 initialize a new session okay so we''re going with ID K14 so that''s going to create a new\n21:37 conversation in there and we''re going to say we''re going to set K to 14.\n21:43 Okay, great. I''m going to manually insert the other uh messages as we did before. Okay, and we can see all of\n21:50 those. You can see at the top here, we are still maintaining that hi, my name is James message. Now, let''s see if it\n21:57 remembers my name. Your name is James. Okay, there we go. Cool. So, that is\n22:03 working. We can also see. So, we just added this. What is my name again? Let''s just see if did that get added to our\n22:10 list of messages, right? What is my name again? Nice. And then we also have the response your name is James. So just by\n22:17 invoking this because we''re using the the runnable with message history, it''s\n22:22 just automatically adding all of that into our message history which is nice.\n22:28 Cool. All right. So that is the buffer window memory. Now we are going to take a look\n22:34 at how we might do something a little more complicated which is uh the the summaries. Okay. So when you think about\n22:41 the summary you know what are we doing? We''re actually taking the messages we''re\n22:46 using that lm call to summarize them to compress them and then we''re storing\n22:52 them within messages. So let''s see how we would actually uh do that. So to\n22:57 start with let''s just see how uh it was done in old line chain. So we have conversation summary memory go through\n23:06 that and let''s just see what we get. So again same interactions\n23:13 right I''m just invoking invoking invoking I''m not adding these directly to the messages because it actually\n23:18 needs to go through a um like that summarization process and if we have a\n23:25 look we can see it happening. Okay, current conversation. So, sorry, current\n23:30 conversation. Hello there, my name is James. AI is generating current conversation. The human introduces\n23:36 himself as James. AI greets James warmly and expresses its readiness to chat and assists inquiring about how his day is\n23:43 going, right? So, it''s summarizing the the previous interactions. And then we\n23:49 have, you know, after that summary, we have the most recent human message and then the AI is going to generate its\n23:55 response. Okay? And that continues going continues going and you can see that the the final summary here is going to be a\n24:01 lot longer. Okay. And it''s different that first summary of course as day he mentions stage of researching different\n24:07 types of conversational memory. The AI responds enthusiastically explaining that conversational memory includes\n24:12 short-term memory, long-term memory, contextual memory, personalized memory and then inquires if James is focused on the specific type of memory. Okay, cool.\n24:21 So we get essentially the summary is just getting uh longer and longer as we go. But at some point the idea is that\n24:28 it''s not going to keep just growing and it should actually be shorter than if you were saving every single interaction\n24:33 whilst maintaining as much of the information as possible. But of course\n24:39 you are going to maintain all of the information that you would with for example the the buffer memory. Right?\n24:46 With the summary, you are going to lose information, but hopefully less information than if you''re just cutting\n24:54 interactions. So, you''re trying to reduce your token count whilst maintaining as much information as\n25:00 possible. Now, let''s go and ask uh what is my name again? It should be able to answer\n25:06 because we can see in the summary here that I introduce myself as James.\n25:12 Okay, response. Your name is James. How is your research going? Okay. So, has that cool? Let''s see how we''d implement\n25:19 that. So, again, as before, we''re going to go with that conversation summary\n25:25 message history. We''re going to be importing a system message. Uh we''re going to be using that not for the LM\n25:30 that we''re chatting with, but for the LM that will be generating our summary. So,\n25:37 actually that is not quite correct. There''s create a summary. Not that it matters. It''s just the dock string. So,\n25:43 we have our messages and we also have the llm. So different different attribute here to what we had before.\n25:48 When we initialize a conversation summary message history, we need to passing in our LM. We have the same\n25:55 methods as before. We have add messages and clear. And what we''re doing is as messages coming, we extend with our\n26:02 current messages, but then we''re modifying those. Okay. So we construct\n26:08 our like instructions to make a summary. Okay. So that is here we have the system\n26:14 prompt uh given the existing conversation summary and the new messages generate a new summary of the conversation ensuring to maintain as\n26:21 much relevant information as possible. Okay. Then we have a human message here through that we''re passing the existing\n26:28 summary. Okay. And then we''re passing in the new messages.\n26:33 Okay. Cool. So we format those and invoke the lm\n26:41 here. And then what we''re doing is in the messages we''re actually replacing the existing history that we had before\n26:48 with a new history which is just a single system summary message. Okay,\n26:55 let''s see what we get. As before, we have that get chat history exactly the same as before. The only real difference\n27:01 is that we''re passing in the LM parameter here. And of course, as we''re passing in the lm parameter in here, it\n27:07 does also mean that we''re going to have to include that in the configurable field spec and that we''re going to need\n27:14 to include that when we''re invoking our pipeline. Okay, so we run that pass in\n27:21 the lm. Now, of course, one side effect of uh\n27:26 generating summaries for everything is that we''re actually, you know, we''re generating more. So you are actually\n27:32 using quite a lot of tokens. Whether or not you are saving tokens or not actually depends on the length of a\n27:38 conversation. As a conversation gets longer. If you''re storing everything after a little while that the token\n27:45 usage is actually going to increase. So if in your use case you expect to have\n27:50 shorter conversations, you would be saving money and tokens by just using this standard buffer memory. Whereas if\n27:59 you''re expecting very long conversations, you would be saving tokens and money by using the summary\n28:05 history. Okay, so let''s see what we got from there. We have a summary of the conversation. James introduced himself\n28:11 by saying, \"Hi, my name responded warmly, asking me, \"Hi, James.\" Introduction included details about\n28:16 token usage. Okay, so we actually uh included everything here, which we\n28:22 probably should not have done. Why did we do that? Uh so in here we''re including all of the\n28:35 oh in here so using we''re including all of the content from the messages. So I think maybe if we just do\n28:42 hcontent for x in messages that should resolve\n28:49 that. Okay there we go. So we quickly fix\n28:55 that. So yeah, before we''re passing in the entire message object, which obviously includes all of this information, whereas actually we just\n29:01 want to be passing in the content. So we modified that and now we''re getting what\n29:08 we''d expect. Okay, cool. And then we can keep going, right? So as we as we keep going, the\n29:13 summary should get more like abstract like as we just saw here, it''s literally\n29:19 just giving us the messages directly almost. Okay, so we''re getting bit summary there and we can keep going.\n29:25 We''re going to add just more messages to that. We''ll see the, you know, as we''ll\n29:31 get send those, we''ll get a response, send it again, get response. We''re just adding all of that, invoking all of\n29:38 that, and that will be, of course, adding everything into our message history. Okay, cool. So, we''ve run that.\n29:44 Let''s see what the uh latest summary is.\n29:49 Okay, and then we have this. So this is a summary that we have inside of our our chat history.\n29:55 Okay, cool. Now finally, let''s see what is my name again. We can just double\n30:01 check, you know, has my name in there, so it should be able to tell us.\n30:09 Okay, cool. So your name is James. Pretty interesting. So let''s have a\n30:14 quick look over at Langsmith. So the reason I want to do this is just to point out okay the different essentially\n30:22 token usage that we''re getting with each one of these. Okay. So we can see that we have these runnable message history\n30:27 which probably improved in naming there. But we can see okay how long is each one\n30:33 of these taken? How many tokens are they also using? Come back to here we have\n30:39 this runnable message history. This is we''ll go through a few of these maybe to\n30:45 here I think. And we can see here this is that first interaction where we''re using the buffer memory and we can see\n30:52 how many tokens we used here. So 112 tokens when we''re asking what is my name again. Okay then we modified this to\n31:01 include I think it was like 14 interactions or something along those lines and obviously increases the number\n31:06 of tokens that we''re using. Right? So we can could see that actually happening all in Lang stuff which is quite nice\n31:11 and we can compare okay how many tokens is each one of these using. Now this is\n31:16 looking at the buffer window and then if we come down to here and look at this\n31:22 one. So this is using our summary. Okay so our summary with what is my name again actually use more tokens in this\n31:28 scenario right which is interesting because we''re trying to compress information. The reason there''s more is\n31:34 because there''s not there hasn''t been that many interactions. As the conversation length increases\n31:41 with the summary, this total number of tokens, especially if we prompt it correctly to keep that low, that should\n31:48 remain relatively small. Whereas with the buffer memory, that will just keep\n31:54 increasing and increasing as a as a conversation gets longer. So useful little way of using lang there\n32:02 to just kind of figure out okay in terms of tokens and costs what we looking at for each of these memory types. Okay so\n32:09 our final memory type acts as a mix of the summary memory and the buffer\n32:16 memory. So, what it''s going to do is keep the buffer up until an N number of\n32:23 tokens and then once a message exceeds the number of token limit for the\n32:28 buffer, it is actually going to be added into our summary. So this memory has the\n32:34 benefit of remembering in detail the most recent interactions whilst also not\n32:42 having the limitation of using too many tokens as a conversation gets longer and\n32:49 even potentially exceeding context windows if you try super hard. So this is a very interesting approach. Now as\n32:56 before let''s try the original way of implementing this then we will go ahead\n33:03 and use our update method for implementing this. So we come down to here and we''re going to line chain\n33:09 memory import conversation summary buffer memory. Okay, a few things here.\n33:15 lm for summary. We have the n number of tokens that we can keep before they get\n33:22 added to the summary. and then return messages of course. Okay, you can see again this is deprecated. We use the\n33:28 conversation chain and then we just pass in our memory there and then we can chat. Okay, so super straightforward.\n33:36 First message, we''ll add a few more here. Again, we have to invoke because our\n33:43 memory type here is using MLM to create those summaries as it goes. And let''s\n33:49 see what they look like. Okay. So we can see for the first message here we have a human message and then an AI message.\n33:57 Then we come a little bit lower down again. Same thing human message is the first thing in our history here. Then\n34:04 it''s a system message. So this is at the point where we''ve exceeded that 300 token limit and the memory type here is\n34:11 generating those summaries. So that summary comes in as a message and we can see okay the human named James\n34:18 introduces himself and mentions he''s researching different types of conversational memory and so on and so on right okay cool so we have that then\n34:27 let''s come down a little bit further we can see okay so the summary there okay\n34:34 so that''s what we that''s what we have that is the implementation for the old\n34:40 version of this memory again we can see is deprecated. So, how do we implement this for our more recent versions of\n34:49 lang chain and specifically 0.3? Well, again, we''re using that runnable with message history and it looks a little\n34:57 more complicated than we were getting before, but it''s actually just, you know, it''s nothing too complex. We''re\n35:05 just creating a summary as we did with the previous memory type. But the\n35:10 decision for adding to that summary is based on in this case actually the number of messages. So I didn''t go with\n35:16 the the lang chain version where it''s a number of tokens. I don''t like that. I\n35:22 prefer to go with messages. So what I''m doing is saying okay last k messages.\n35:27 Okay. Once we exceed k messages, the messages beyond that are going to be\n35:33 added to the memory. Okay, cool. So let''s see. We first initialize our\n35:41 conversation summary buffer message history class with lm and k. Okay, so\n35:48 these two here. So lm of course to create summaries and k is just the the limit of the number of messages that we\n35:53 want to keep before adding them to the summary or dropping them from our messages and adding them to the summary.\n36:00 Okay. So we will begin with okay do we have an existing summary. So the reason\n36:07 we set this to none is we can''t extract the summary the existing summary unless\n36:14 it already exists. And the only way we can do that is by checking okay do we\n36:19 have any messages. If yes we want to check if within those messages we have a system message because we''re we''re doing\n36:25 the same structure as what we have up here where the system message that first system message is actually our summary.\n36:32 So that''s what we''re doing here. We''re checking if there is a summary message already stored within our messages.\n36:38 Okay. So we''re checking for that. If we find it, we just do we have this little print\n36:45 statement so we can see that we found something and then we just make our existing summary. I should actually move\n36:53 this to the first instance here. Okay, so that existing summary will be set to\n37:02 the first message. Okay, and this would be a system message\n37:08 rather than a string. Cool. So we have that. Then we want to\n37:14 add any new messages to our history. Okay. So we''re extending the history\n37:20 there. And then we''re saying, okay, if the length of our history is exceeds the\n37:25 K value that we set, we''re going to say, okay, we found that many messages. We''re going to be dropping the latest. It''s\n37:30 going to be the latest two messages. this I will say here one thing or one\n37:36 problem with this is that we''re not going to be saving that many tokens if we''re summarizing every two messages.\n37:43 So, what I would probably do is in in an actual like production setting, I would\n37:50 probably say let''s go up to 20 messages. And once we hit 20\n37:56 messages, let''s take the previous 10, we''re going to summarize them and put them into our summary alongside any, you\n38:02 know, previous summary that already existed. But in in, you know, this is also fine as well. Okay. So, we say we\n38:11 found those measures. We''re going to drop the latest two messages. Okay. So, we pull the the oldest messages out. I\n38:20 should say not the latest. It''s the oldest. Not the latest. Want to keep the latest\n38:26 and drop the oldest. So, we pull out the oldest messages and keep only the most\n38:32 recent messages. Okay. Then I''m saying, okay, if we if we don''t\n38:38 have any old messages to summarize, we don''t do anything. and we just return. Okay, so let''s indicates that this has\n38:44 not been triggered, we would hit this. But in the case this has been triggered\n38:51 and we do have old messages, we''re going to come to here. Okay, so this is we can\n38:59 see we have a system message prompt template saying given the existing conversation summary and the new messages generate a new summary of the\n39:06 conversation ensuring to maintain as much relevant information as possible. So if you want to be more conservative\n39:12 with tokens, we could modify this prompt here to say keep the summary to within\n39:17 the length of a single paragraph for example. And then we have our human\n39:23 message prompt template which can say okay here''s the existing conversation summary and here are new messages. Now\n39:28 new messages here is actually the old messages but the way that we''re framing it to the LM here is that we want to\n39:35 summarize the whole conversation, right? It doesn''t need to have the most recent measures that we''re storing within our\n39:42 buffer. It doesn''t need to know about those. That''s irrelevant to the summary. So, we just tell it that we have these\n39:47 new measures. And as far as this LM is concerned, this is like the full set of interactions. Okay. So, then we would\n39:54 format those and invoke our LM. And then we''ll print out our new summary so we\n40:00 can see what''s going on there. And we would prepend that new summary to our\n40:06 conversation history. Okay. And and this will work. So we can just prepend it\n40:13 like this because we''ve already popped\n40:18 where was it up here. If we have an existing summary, we already popped that from the list. So\n40:24 it''s already been pulled out of that list. So it''s okay for us to just we don''t need to say like we don''t need to\n40:31 do this because we''ve already dropped that initial system message if it existed. Okay. And then we have the\n40:37 clear method as before. So that''s all of the logic for our conversational summary\n40:45 buffer memory. We redefine our get chat history function with the lm and k\n40:53 parameters there. And then we''ll also want to set the configurable fields again. So that is just going to be of\n40:59 course session id lm and k. Okay. So now we can invoke the k value\n41:07 to begin with is going to be four. Okay. So we can see no old messages to\n41:13 update summary with. That''s good. Let''s invoke this a few times and let''s see\n41:18 what we get. Okay. So no old messages to update summary with.\n41:26 found six matches dropping the oldest two. And then we have new summary in the conversation. James and Bruce himself is\n41:33 interested researching different types of conversational memory. Right? So you can see there''s quite a lot in here at\n41:38 the moment. So we would definitely want to prompt the LLM, the summary LLM to keep\n41:45 that short. Otherwise, we''re just getting a ton of stuff, right? But we can see that that is you\n41:52 know it''s it''s working it''s functional. So let''s go back and see if we can prompt it to be a little more concise.\n41:59 So we come to here ensuring to maintain as much relevant information as possible. However we need to keep\n42:09 our summary concise. The limit\n42:15 is a single short paragraph. Okay. something like this. Let''s try and let''s\n42:22 see what we get with that. Okay, so message one again, nothing to\n42:28 update. See this? So, new summary. You can see it''s a bit shorter. It doesn''t have all those bullet points.\n42:37 Okay, so that seems better. Let''s see. So you can see the first summary is a\n42:44 bit shorter, but then as soon as we get to the second and third summaries, the second summary\n42:50 is actually slightly longer than the third one. Okay, so we''re going to be we''re going to be losing a bit of\n42:56 information in this case, more than we were before, but we''re saving a ton of tokens. So that''s of course a good\n43:03 thing. And of course, we could keep going and adding many interactions here. And we should see that this conversation\n43:09 summary will be it should maintain that sort of length of around one short\n43:14 paragraph. So that is it for this chapter on conversational memory. We''ve\n43:21 seen a few different memory types. We''ve implemented their old deprecated versions so we can see what they were\n43:28 like and then we''ve reimplemented them for the latest versions of lang chain.\n43:33 And to be honest, using logic where we are getting much more into the weeds and\n43:39 that is in some ways okay it complicates things that is true but in other ways it\n43:45 gives us a ton of control. So we can modify those memory types as we did with that final summary buffer memory type.\n43:52 We can modify those to our liking which is incredibly useful when you''re\n43:58 actually building applications for the real world. So that is it for this chapter. We''ll move on to the next one.',
  '{"channel": "James Briggs", "video_id": "EtldFS3JbGs", "duration": "44:20", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["Conversational Memory", "ConversationBufferMemory", "ConversationBufferWindowMemory", "ConversationSummaryMemory", "RunnableWithMessageHistory", "LCEL", "LangSmith", "Token Usage"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  1,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '0:00 In this chapter, we are going to 0:02 introduce agents. Now, agents, I think, 0:05 are one of the most important components 0:09 in the world of AI and I don''t see that 0:13 going away anytime so...',
  '0:00 In this chapter, we are going to
0:02 introduce agents. Now, agents, I think,
0:05 are one of the most important components
0:09 in the world of AI and I don''t see that
0:13 going away anytime soon. I think the
0:15 majority of AI applications,
0:19 the intelligent part of those will be
0:22 almost always an implementation of an AI
0:25 agent or multiple AI agents. So in this
0:28 chapter we are just going to introduce
0:30 agents within the context of lang chain.
0:33 We''re going to keep it relatively
0:36 simple. We''re going to go into much more
0:38 depth in agents in the next chapter
0:41 where we''ll do a bit of a deep dive but
0:44 we''ll focus on just introducing the core
0:46 concepts and of course agents within
0:50 line chain here. So, jumping straight
0:53 into our notebook, let''s run our
0:56 prerequisites.
0:58 You''ll see that we do have an additional
1:00 prerequisite here, which is Google
1:02 search results. That''s because we''re
1:03 going to be using the SER API to allow
1:07 our LM as an agent to search the web,
1:10 which is one of the great things about
1:13 agents is that they can do all of these
1:15 additional things and LM by itself
1:17 obviously cannot. So we''ll come down to
1:20 here. We have our linesmith parameters
1:22 again of course. So you enter your line
1:25 chain API key if you have one. And now
1:27 we''re going to take a look at tools
1:30 which is a very essential part of
1:33 agents. So tools are a way for us to
1:36 augment our LMS with essentially
1:39 anything that we can write in code. So
1:42 we mentioned that that we''re going to
1:43 have a Google search tool. That Google
1:45 search tool is some code that gets
1:47 executed by our LLM in order to search
1:51 Google and get some results. So a tool
1:54 can be thought of as any code logic or
1:58 any function in the ca in the case of
2:00 Python any function that has been
2:03 formatted in a way so that our LM can
2:06 understand how to use it and then
2:09 actually use it. Although the the LM
2:12 itself is not using the tool. is more
2:14 our agent execution logic which uses the
2:18 tool for the LM. So we''re going to go
2:21 ahead and actually create a few simple
2:23 tools. We''re going to be using what is
2:25 called the tool decorator from
2:26 Langchain. And there are a few things to
2:30 keep in mind when we''re building tools.
2:33 So for optimal performance, our tool
2:35 needs to be just very readable. And what',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  2,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '2:37 I mean by readable is we need three main 2:40 things. One is a dot string that is 2:43 written in natural language and it is 2:45 going to be used to explain to the LLM 2:48 when and why and how...',
  '2:37 I mean by readable is we need three main
2:40 things. One is a dot string that is
2:43 written in natural language and it is
2:45 going to be used to explain to the LLM
2:48 when and why and how it should use this
2:51 tool. We should also have clear
2:53 parameter names. Those parameter names
2:56 should tell the LM okay what each one of
2:59 these parameters are. They should be
3:02 self-explanatory. If they are not
3:04 self-explanatory,
3:05 we should be including an explanation
3:08 for those parameters within the dock
3:10 string. Then finally, we should have
3:12 type annotations for both our parameters
3:15 and also what we''re returning from the
3:17 tool. So let''s jump in and see how we
3:21 would implement all of that. So we come
3:23 down to here and we have lang chain core
3:25 tools import tool. Okay, so these are
3:28 just four incredibly simple tools. We
3:32 have the addition or add tool, multiply,
3:35 the exponentiate and the subtract tools.
3:38 Okay, so a few calculatores tools. Now
3:42 when we add this tool decorator, it is
3:45 turning each of these tools into what we
3:49 call a structured tool object. So we can
3:52 see that here. We can see we have this
3:56 structured tool. We have a name
3:58 description. Okay. And then we have this
4:00 old schema. We''ll see this in a moment.
4:02 And a function, right? So this function
4:04 is literally just the original function.
4:07 It''s it''s a mapping to the original
4:08 function. So in this case, it it''s the
4:11 add function. Now the description we can
4:13 see is coming from our dock string. And
4:16 of course the name as well is just
4:18 coming from the function name. Okay. And
4:20 then we can also see let''s just print
4:23 the name and description. But then we
4:25 can also see the AGS schema, right? We
4:29 can so this thing here that we can''t
4:31 read at the moment. To read it, we''re
4:33 just going to look at the model JSON
4:36 schema method and then we can see what
4:38 that contains which is all of this
4:40 information. So this actually contains
4:42 everything includes properties. So we
4:44 have the X it title for that and it also
4:48 specifies the type. Okay. So the type
4:51 that we defined is float. Float for
4:54 OpenAI gets mapped to number rather than
4:57 just being float. And then we also see
5:00 that we have this required field. So
5:02 this is telling our LM which parameters
5:05 are required, which ones are optional.',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  3,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '5:07 So we yeah in some cases you would we 5:11 can even do that here. Let''s do Z that 5:14 is going to be float or none. Okay. And 5:19 we''re just going to say it is 0.3. 5:23 Right? I''m going to rem...',
  '5:07 So we yeah in some cases you would we
5:11 can even do that here. Let''s do Z that
5:14 is going to be float or none. Okay. And
5:19 we''re just going to say it is 0.3.
5:23 Right? I''m going to remove this in a
5:25 minute because it''s kind of weird. But
5:27 let''s just see what that looks like. So
5:30 you see that we now have X, Y, and Z.
5:35 But then in Z, we have some additional
5:36 information. Okay. So it can be any of
5:39 it can be a number or it can just be
5:41 nothing. The default value for that is
5:43 0.3.
5:45 Okay. And then if we look here, we can
5:47 see that the required field does not
5:49 include Z. So it''s just X and Y. So it''s
5:52 describing the full function schema for
5:56 us. But let''s remove that.
5:59 Okay. And we can see that again with our
6:01 exponentiate tool. Similar thing. Okay.
6:05 So how how are we going to invoke our
6:08 tool? So the LM the underlying LM is
6:13 actually going to generate a string.
6:15 Okay. So will look something like this.
6:17 This is going to be our LM output. So it
6:21 is it''s a string that is some JSON. And
6:24 of course to load a string into a
6:28 dictionary format, we just use JSON
6:30 loses. Okay, so let''s see that. So this
6:34 could be the alpha from LLM. We load it
6:37 into a dictionary and then we get an
6:39 actual dictionary. And then what we
6:41 would do is we can take our exponentiate
6:44 uh tool. We access the underlying
6:47 function and then we pass it the keyword
6:50 arguments from our dictionary here.
6:54 Okay.
6:56 And that will execute our tool. That is
6:58 the tool execution logic line chain
7:00 implements. And then later on in the
7:03 next chapter we''ll be implementing
7:04 ourselves. Cool. So let''s move on to
7:07 creating an agent. Now we''re going to be
7:10 constructing a simple tool calling
7:12 agent. We''re going to be using linechain
7:14 expression language to do this. Now we
7:17 will be covering line chain expression
7:19 language or L cell more in a upcoming
7:22 chapter. But for now all we need to know
7:25 is that our agent will be constructed
7:29 using syntax and components that look
7:31 like this. So we would start with our
7:33 input parameters that is going to
7:36 include our user query and of course the
7:38 chat history because we need our agent
7:40 to be conversational and remember
7:41 previous interactions within the
7:43 conversation. These input parameters
7:45 will also include a placeholder for what',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  4,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '7:47 we call the agent scratchpad. Now the 7:50 agent stretch pad is essentially where 7:52 we are storing the internal thoughts or 7:55 the internal dialogue of the agent as it 7:57 is using tools ge...',
  '7:47 we call the agent scratchpad. Now the
7:50 agent stretch pad is essentially where
7:52 we are storing the internal thoughts or
7:55 the internal dialogue of the agent as it
7:57 is using tools getting observations from
7:59 those tools and working through those
8:02 multiple internal steps. So in the case
8:05 that we will see it will be using for
8:07 example the addition tool getting the
8:09 result using the multiply tool getting
8:11 the result and then providing a final
8:13 answer to us as a user. So let''s jump in
8:16 and see what that looks like. Okay. So,
8:19 we''ll just start with defining our
8:20 prompt. So, our prompt is going to
8:22 include the system message. That''s
8:24 nothing. We''re not putting anything
8:26 special in there. We''re going to include
8:29 the chat history, which is a messages
8:32 placeholder. Then, we include our human
8:34 message. And then we include a
8:37 placeholder for the agent scratch pad.
8:39 Now, the way that we implement this
8:41 later is going to be slightly different.
8:43 For the scratch pod, we would actually
8:44 use this messages placeholder, but this
8:46 is how we use it with the built-in
8:48 create tool agent from Lang Train. Next,
8:51 we''ll define our LM. We do need our
8:53 opening our API key for that. So, we''ll
8:56 enter that here like so. Okay, so come
9:00 down. Okay, so we''re going to be
9:02 creating this agent. We need
9:04 conversational memory and we are going
9:05 to use the older conversation buffer
9:07 memory class rather than the newer
9:09 runnable with message history class.
9:11 That''s just because we''re also using
9:13 this older create tool calling agent and
9:16 this is this is the older way of doing
9:18 things. In the next chapter, we are
9:21 going to be using the more recent
9:24 basically what we already learned on
9:26 chat history. We''re going to be using
9:28 all of that to implement our chat
9:30 history. But for now, we''re going to be
9:31 using the older method uh which is
9:34 deprecated just as a pre-warning.
9:37 But again as I mentioned at the very
9:39 start of the course we''re starting
9:40 abstract and then we''re getting into the
9:43 details. So we''re going to initialize
9:46 our agent for that we need these four
9:48 things lm as we defined tools as we have
9:51 defined prompt as we have defined and
9:55 then the memory which is our old
9:57 conversation buffer memory. So with all
10:00 of that we are going to go ahead and we',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  5,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '10:02 create a tool calling agent and then we 10:04 just provide it with everything. Okay, 10:07 there we go. 10:09 Now, uh you''ll see here I didn''t pass in 10:11 the the memory. I''m passing it in dow...',
  '10:02 create a tool calling agent and then we
10:04 just provide it with everything. Okay,
10:07 there we go.
10:09 Now, uh you''ll see here I didn''t pass in
10:11 the the memory. I''m passing it in down
10:13 here instead. So, we''re going to start
10:16 with this question, which is what is
10:18 10.7 multiplied by 7.68.
10:22 Okay. So, given the precision of these
10:27 numbers, our L normal LM would not be
10:29 able to answer that or almost definitely
10:33 would not be able to answer that
10:34 correctly. we need a external tool to
10:37 answer that accurately and we''ll see
10:39 that that is exactly what it''s going to
10:41 do. So we can see that the tool agent
10:46 action message here we can see that it
10:49 decided okay I''m going to use the
10:50 multiply tool and here are the
10:52 parameters that I want to use for that
10:53 tool. Okay, we can see X is 10.7 and Y
10:56 is 7.68.
10:58 You can see here that this is already a
11:00 dictionary and that is because lang
11:03 chain has taken the string from our LM
11:07 call and already converted it into a
11:09 dictionary for us. Okay, so that''s just
11:11 it''s happening behind the scenes there.
11:14 And you can actually see if we go into
11:16 the details a little bit, we can see
11:17 that we have these arguments and this is
11:19 the original string that was coming from
11:21 LLM. Okay, which has already been of of
11:23 course processed by lang chain. So we
11:26 have that. Now, the one thing missing
11:30 here is that, okay, we''ve got that the
11:34 LM wants us to use multiply and we''ve
11:36 got what the LM wants us to put into
11:38 multiply, but where''s the answer, right?
11:42 There is no answer because the tool
11:44 itself has not been executed because it
11:46 can''t be executed by the LM. But then,
11:48 okay,
11:50 didn''t we already define our agent here?
11:53 Yes, we define the part of our agent
11:57 that is our LM has our tools and it is
12:00 going to generate which tool to use but
12:03 it actually doesn''t include the agent
12:05 execution part which is okay the agent
12:09 executor is a broader thing. It''s it''s
12:13 broader logic like just code logic which
12:16 acts as a scaffolding within which we
12:19 have the iteration through multiple
12:21 steps of our LLM calls followed by the
12:25 LLM outputting what tool to use followed
12:28 by us actually executing that for the
12:30 LLM and then providing the output back
12:33 into the LM for another decision or',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  6,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '12:36 another step. So the agent itself here 12:39 is not the full agentic 12:42 flow that we might expect. Instead for 12:45 that we need to implement this agent 12:48 executor class. This agent exec...',
  '12:36 another step. So the agent itself here
12:39 is not the full agentic
12:42 flow that we might expect. Instead for
12:45 that we need to implement this agent
12:48 executor class. This agent executor
12:51 includes our agent from before. Then it
12:54 also includes the tools. And one thing
12:56 here is okay we we already passed the
12:58 tools to our agent. Why do we need to
12:59 pass them again? Well the tools being
13:02 passed to our agent up here
13:05 that is being used. So that is
13:07 essentially extracting out those
13:09 function schemers and passing it to our
13:11 L so our LM knows how to use the tools.
13:13 Then we''re down here we''re passing the
13:15 tools again to our agent executor.
13:18 And this is rather than looking at how
13:20 to use those tools. This is just looking
13:22 at okay I want the functions for those
13:24 tools so that I can actually execute
13:25 them for the LM or for the agent. Okay.
13:29 So that''s what is happening there. Now
13:32 we can also pass in our memory directly.
13:34 So you see if we scroll up a little bit
13:36 here, I actually had to pass in the
13:40 memory like this with our agent. That''s
13:43 just because we weren''t using the agent
13:44 executor. Now we have the agent
13:46 executor. It''s going to handle that for
13:47 us.
13:49 And another thing that it''s going to
13:50 handle for us is intermediate steps. So
13:53 you''ll see in a moment that when we
13:55 invoke the agent executor, we don''t
13:56 include the intermediate steps. And
13:58 that''s because it that is already
14:00 handled by the agent executor now. So
14:03 we''ll come down. We''ll set verbose equal
14:05 to true. So we can see what is
14:07 happening. And then we can see here
14:10 there''s no intermediate steps anymore.
14:13 And we we do still pass in the chat
14:15 history like this. But then the addition
14:19 of those new interactions to our memory
14:21 is going to be handled by the executor.
14:24 So fact let me actually show that very
14:27 quickly before we jump in. Okay. So
14:30 that''s currently empty. We''re going to
14:32 execute this.
14:35 Okay, we''re entered that new agent
14:37 execute chain. And let''s just have a
14:39 quick look at our messages again. And
14:41 now you can see that agent executor
14:43 automatically handled the addition of
14:46 our human message and then the
14:48 responding AI message for us. Okay,
14:50 which is useful. Now what happened? So
14:54 we can see that the multiply tool was',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  7,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '14:56 invoked with these parameters and then 14:59 this pink text here that we got that is 15:02 the observation from the tool. So this 15:03 is what the tool output back to us. 15:06 Okay. Then this...',
  '14:56 invoked with these parameters and then
14:59 this pink text here that we got that is
15:02 the observation from the tool. So this
15:03 is what the tool output back to us.
15:06 Okay. Then this final message here is
15:08 not formatted very nicely but this final
15:10 message here is coming from our LM. So
15:12 the green is our LLM output. The pink is
15:16 our tool output. Okay. So the LM after
15:21 seeing this output says 10.7 multiplied
15:26 by 7.68 is approximately 82.18.
15:31 Okay, cool. Useful. And then we can also
15:34 see that the chat history which we we
15:36 already just saw. Great. So that has
15:39 been used correctly. We can just also
15:41 confirm that that is correct. Okay.
15:44 82.1759
15:47 recurring which is exactly what we get
15:49 here. Okay. And we the reason for that
15:51 is obviously our multiply tool is just
15:54 doing this exact operation.
15:57 Cool. So let''s try this with a bit of
16:00 memory. So I''m going to ask or I''m going
16:03 to say to the agent, hello, my name is
16:05 James.
16:08 We''ll leave that as the it''s not
16:10 actually the first interaction because
16:11 we already have these, but it''s an early
16:16 interaction with my name in there. Then
16:19 we''re going to try and perform multiple
16:21 tool calls within a single execution
16:23 loop. And what you''ll see with when it
16:25 is calling these tools is that it can
16:26 actually use multiple tools in parallel.
16:29 So for sure, I think two or three of
16:31 these were used in parallel and then
16:33 define or subtract had to wait for those
16:35 previous results. So it would have been
16:37 executed afterwards and we should
16:40 actually be able to see this in lang.
16:43 So if we go here yeah we can see that we
16:46 have this initial call and then we have
16:48 add and multiply and exponentiate all
16:50 use in parallel. Then we have another
16:52 call which you subtract and then we get
16:54 the response.
16:56 Okay, which is pretty cool. And then the
16:59 final result there is1.
17:03 Now when you look at whether the answer
17:04 is accurate, I think the order here of
17:08 calculations is not quite correct. So if
17:12 we put the actual computation here, it
17:14 gets it right. But otherwise, if I use
17:17 an actual language, it''s like I''m doing
17:19 maybe I''m phrasing it in a in a poor
17:21 way.
17:23 Okay. So I suppose that is pretty
17:25 important. So okay, if we put the
17:28 computation in here, we get the -3. So',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  8,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '17:32 it''s something to be careful with and 17:34 probably requires a little bit of 17:36 prompting to prompting and maybe 17:38 examples in order to get that smooth so 17:41 that it does do things in...',
  '17:32 it''s something to be careful with and
17:34 probably requires a little bit of
17:36 prompting to prompting and maybe
17:38 examples in order to get that smooth so
17:41 that it does do things in the way that
17:43 we might expect or maybe we as humans
17:47 are just bad and misuse the systems. One
17:49 or the other. Okay, so now we''ve gone
17:52 through that a few times. Let''s go and
17:54 see if our agent can still recall our
17:56 name. Okay, and it remembers my name is
17:58 James. Good. So, it still has that
18:00 memory in there as well. That''s good.
18:03 Let''s move on to another quick example
18:06 where we''re just going to use Google
18:07 search. So, we''re going to be using the
18:09 SER API.
18:11 You can Okay, you can get the API key
18:14 that you need from here. So, serapi.com/
18:17 users/sign
18:19 and just enter that in here. So, you
18:22 will get it''s up to 100 searches per
18:25 month for free. So, just be aware of
18:29 that if you overuse it. I don''t think
18:31 they charge you because I I don''t think
18:32 you enter your card details straight
18:35 away, but yeah, just be aware of that
18:38 limit.
18:39 Now, there are certain tools that Lang
18:42 Chain have already built for us. So,
18:44 they''re pre-built tools and we can just
18:45 load them using the load tools function.
18:48 So, we do that like so. We have our load
18:50 tools and we just pass in the set API
18:52 tool only. We could pass in more there
18:54 if we wanted to. And then we also pass
18:56 in our lm.
18:58 Now I''m going to one use that tool, but
19:02 I''m also going to define my own tool
19:03 which is to get the current location
19:06 based on the IP address. Now this is
19:08 we''re in collab at the moment. So it''s
19:09 actually going to get the IP address for
19:11 the collab instance that I''m currently
19:12 on and we''ll find out where that is. So
19:16 that is going to get the IP address and
19:18 then it''s going to provide the data back
19:20 to our LM in this format here. So we''re
19:22 going to latitude, longitude, city, and
19:24 country.
19:25 Okay, we''re also going to get the
19:27 current day and time. So now we''re going
19:31 to redefine our prompt. I''m not going to
19:34 include chat history here. I just want
19:35 this to be like a oneshot thing.
19:39 I''m going to redefine our agent and
19:40 agent executor using our new tools which
19:42 just our sub API plus the get current
19:46 date time and get location from IP. Then',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=Gi7nqB37WEY',
  9,
  'LangChain Agents in 2025 | Full Tutorial for v0.3',
  '19:49 I''m going to invoke our agent executor 19:51 with I have a few questions. What is the 19:53 date and time right now? How is the 19:55 weather where I am? And please give me 19:58 degrees in Cels...',
  '19:49 I''m going to invoke our agent executor
19:51 with I have a few questions. What is the
19:53 date and time right now? How is the
19:55 weather where I am? And please give me
19:58 degrees in Celsius. So when it gives me
20:00 that weather. Okay. And let''s see what
20:02 we get.
20:06 Okay. So apparently we''re in Council
20:08 Bluffs in the US.
20:11 It is 13 degrees Fahrenheit, which I think is
20:14 absolutely freezing. Oh my gosh, it is.
20:17 Yes. - 10. So it''s super cold over
20:20 there.
20:22 And you can see that, okay, it did give
20:24 us Fahrenheit. And that''s that is
20:25 because the tool that we were using
20:27 provided us with Fahrenheit, which is
20:29 fine, but it did translate that over
20:32 into a estimate of Celsius for us, which
20:35 is pretty cool. So, let''s actually
20:37 output that. So, we get this, which I is
20:41 correct.
20:43 We do US approximately this. And we also
20:46 get an description of the conditions as
20:48 well. It''s partly cloudy with 0%
20:51 precipitation, lucky for them, and
20:54 humidity of 66%. Okay, all pretty cool.
20:58 So, that is it for this introduction to
21:00 Langchain agents. As I mentioned, next
21:02 chapter, we''re going to dive much deeper
21:04 into agents and also implement that for
21:07 chain version 0.3. So, we''ll leave this
21:10 chapter here and jump into the next one.',
  '{"channel": "James Briggs", "video_id": "Gi7nqB37WEY", "duration": "21:28", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["AI Agents", "Tool Decorator", "Agent Executor", "Tool Calling", "LangChain Expression Language", "SERP API", "Google Search", "Memory", "Conversational Agents"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=jGg_1h0qzaM',
  1,
  'LangGraph Complete Course for Beginners  Complex AI Agents with Python',
  '0:00 Welcome to this video course on Langraph, the powerful Python library for building advanced conversational AI\n0:07 workflows. In this course, Vbeca will teach you how to design, implement, and\n...',
  '0:00 Welcome to this video course on Langraph, the powerful Python library for building advanced conversational AI\n0:07 workflows. In this course, Vbeca will teach you how to design, implement, and\n0:12 manage complex dialogue systems using a graph-based approach. By the end, you''ll\n0:18 be equipped to build robust, scalable, conversational applications that leverage the full potential of large\n0:25 language models. Hey guys, my name is Vava and I''m a robotics and AI student.\n0:30 In this course, we''re going to be learning all about the fundamentals of Langraph. Now, I assume you''ve heard of\n0:36 Langraph before, hence why you clicked on this course. But I''m also going to assume you have never coded in Langraph\n0:43 before. Now, because of this assumption, I have explained every single thing in as much detail as I possibly can. Now,\n0:51 also this might mean that I might be going slow at times. So if you want you can always speed me up. Now what are we\n0:58 going to be learning in this course? Well to start we''re going to be building a lot of graphs, a lot of AI agents.\n1:05 We''re going to be learning a lot about the theory and I''ve also provided exercises throughout the course in which\n1:11 all of the answers will be provided on the GitHub. With that being said, if you''re\n1:16 ready to start on this journey with me, let''s go to our first section\n1:21 then. All right people. So welcome to the first section of this course. Now in\n1:26 this section we''ll be covering something called as type annotations. Now admittedly this is going to be a\n1:32 completely theoretical section but it will be short and brief. I promise. The reason I''ve kept this specific section\n1:38 in the course is because when we do eventually go on to code uh our AI agents, our graphs and langraph, these\n1:45 will start popping up everywhere. And I don''t really want you to look start coding without ever having seen these\n1:51 before or really not knowing what these actually are. So that''s why I''ve kept it here. But I promise this will be short\n1:57 and brief. Cool. Okay. Let''s begin with dictionaries. Now dictionaries are a\n2:02 data structure. Yes, but there''s a reason I''ve kept it here. So let''s see\n2:07 how a dictionary is described in Python. You should already know this. So in this case, I''ve described a very simple\n2:13 dictionary called uh movie. and it has two keys, the name and the year. And it\n2:18 has two values, Avengers Endgame and 2019. Now, dictionaries are awesome, don''t get me wrong. They''re they allow\n2:25 for efficient data retrieval based on their unique keys. They''re flexible and easy to implement, but there''s a\n2:30 potential problem with them. See, it''s a challenge to ensure that the data is a particular structure. And this could be\n2:38 a huge problem in larger projects. So to put things in simple words, it doesn''t\n2:43 really check if the data is the correct type data type or structure and that could be the source of a lot of logical\n2:50 errors in your project. And if your project is really really large, then this could be quite a headache to\n2:56 identify, right? Cuz it''s quite a small detail. So what is the solution for this? Well, it''s something called a type\n3:03 dictionary. Now, here is an example on how you create a type dictionary in Python. And I just want to uh emphasize\n3:10 that this type annotation is used extensively in langraph. This will be used to define states. Now don''t worry\n3:18 you we haven''t covered states yet. We will cover that in the next section. But just be mindful that this is quite\n3:23 important. So a type dictionary is quite easy to implement. You implement it as a\n3:28 class. In this case, I''ve implemented the same um example I uh uh showed you\n3:34 in the previous section where I described the movie is the same exact keys and values. So, it still has the\n3:40 name and the year. But notice in this class, I have defined the actual uh data\n3:46 type of what that key should be. So, for example, the name is a string and the year is an integer, right? And to\n3:53 initialize a dictionary uh I have done the exact same thing. to have engineet game in 2019. So now there are two main\n4:01 uh benefits of using a type dictionary it''s type safety because we''ve explicitly defined what should be in\n4:08 this data structure and so this will really reduce the runtime errors and obviously the readability is enhanced as\n4:14 well and this will make debugging easier if something goes wrong within this type dictionary. Cool. So we''ve covered type\n4:22 dictionary now. Now we move on to another type of annotation which is union. Now you might have seen these\n4:29 future these later uh types annotations before if you coded in Python but again I''m just giving you a highle overview\n4:35 what these are. So union take a look at this example. So I''ve created a very simple function which takes in a value\n4:42 and it squares it. Now in this case the uh input x could be either an integer or\n4:48 float and union basically says that whatever value you have can be these\n4:54 data types only. So in this case x can only be integer or float. So if I pass in five or 1.23 4 this would be\n5:01 completely fine. It would square the number and everything. But if I passed in a string like I am a string it would\n5:06 completely fail. Now admittedly yes this function is quite easy. If I passed in\n5:12 I''m a string, it would have failed anyway. But in more complicated applications, hopefully you can see how\n5:17 this actually is useful. In fact, the makers of Lang Chain and Langraph used Union quite extensively throughout u\n5:24 making the actual library. So again, it''s flexible and it''s easy to code and\n5:30 it allows for type safety. So because it can provide hints to uh help catch\n5:35 incorrect usage. Now something similar to union is another type annotation which is optional. Now optional is quite\n5:42 similar and in this case I''ve described another function nice message. So you\n5:48 pass in a name. If you pass in a name it will say hi there name. So for example let the name be Bob. If I pass in Bob to\n5:56 this uh function it would say hi there Bob. But what if I don''t pass in\n6:01 anything? Now if I don''t pass anything optional because I''ve used optional says that the name parameter could either be\n6:08 a string or a none value. Now if I pass in nothing it will go in this if\n6:13 statement and say hey random person. But this is also important to emphasize that it cannot be anything else. It can''t be\n6:20 an integer or or a boolean or a float or anything like that. It has to be either a string or a none value because that''s\n6:27 what I''ve defined here. Cool. Now comes another type annotation called any. And\n6:33 any is really the easiest one to understand. It literally means this value could be anything. It could be any\n6:40 data structure. So in this case I''ve created a simple um function called\n6:45 print value where it takes in something and it prints that. And for example I\n6:51 passed in this string and it prints it and anything and everything is allowed.\n6:56 Cool. one last type annotation I promise and it''s the lambda function. So lambda\n7:02 functions are quite useful. For example, in this I''ll give you two examples now. So the first example is this really\n7:10 simple uh example. Now we''ve already I already created a square function before, right? Where it takes in a\n7:16 value, it takes in a number and it squares it. So for example, if I passed in square 10, it would give me 100.\n7:23 Quite an easy example. Now let me give you a second example. this. So if you''ve come from a\n7:29 leaf code background, then you''ve probably seen you''ve either used lambda before and you''ve definitely used math\n7:35 before cuz it''s quite efficient. So for example, if I pass in 1 2 3 4, what this piece of code is saying is that it\n7:42 squares each number in nums. So this map function maps each value uh and performs\n7:49 this function to it. So x * x. So 1 4 9 16 and then converts that back into a\n7:55 list. Now lambda functions really are just a shortcut to writing small functions and they make everything quite\n8:02 efficient. Now obviously this could have been done in one line as well but for example this a beginner programmer could\n8:09 have might have used a for loop but a more advanced programmer could have used this and this is obviously much more\n8:16 efficient. Right? So hopefully you can start to see what how powerful these type annotations are and these will be\n8:23 coming up. So again, no need to memorize this. Just need to have a highle overview what they are. Okay, cool. So\n8:29 now I''ll see you in the next section. See you there. All right, perfect. So let''s\n8:36 continue on. In this section, we will look at the different elements in Langraph. So let''s begin with our first\n8:44 element, one of the most fundamental elements in all of Langraph, the state.\n8:49 So what is a state? Well, it''s a shared data structure that holds the current\n8:54 information or context of the entire application. In simpler terms, it is like the application''s memory where it\n9:01 keeps track of the variables, the data that nodes can access and modify as they execute. Now, don''t worry if you don''t\n9:08 understand what a node is yet. That is what we will be talking in the next slide about. But as a good analogy,\n9:15 think of the whiteboard in a meeting room analogy. Now imagine you''re in a meeting room and\n9:20 there are different participants as well and every time you come up with something new or you want to record some new uh information or update some\n9:27 information you write it on the whiteboard. In this case the whiteboard acts as your state and the participants\n9:34 act as a node. So the state shows us the updated\n9:40 content/in information of your entire application. Hopefully that made a bit of sense.\n9:47 So let''s move on to the node another fundamental element in lang graph. So\n9:52 these are just individual functions or operations that perform specific tasks within the graph. So each of these node\n9:59 receives an input which is often just the current state of your application. It processes it and then produces an\n10:05 output or an updated state. So here''s a good analogy of this.\n10:12 The assembly line station analogy. Now look at this image. Each of these\n10:17 station does one specific job. It could be attaching a part. It could be painting it. It could be inspecting the\n10:24 quality and so on and so on. The point is each of these stations represent a\n10:30 node because they do one specific task. So how do you actually connect\n10:36 these different nodes together? Well, before we go into that, I think it''s important we understand the most\n10:42 important element of them all, the graph. It is so important that it''s even\n10:48 in the name Langraph. So, the graph is just the overarching structure and it\n10:54 maps out how different tasks aka nodes are connected and executed. So it\n11:01 visually represents the workflow showing the sequence and the conditional parts\n11:06 between various operations. Now a graph is quite self-explanatory but you can think of it\n11:11 as a road map. On a road map you can see it display the different routes\n11:16 connecting cities with the different intersections offering choices on which path to take next.\n11:22 Now, here''s a great image of what a graph is, and these are the individual nodes, but you''ll see they''re connected\n11:28 somehow. So, how are these connected? That brings us to the next element,\n11:35 edges. So, edges are just the connection between nodes and these determine the\n11:40 flow of execution. So, they tell us or tell the application which node should be\n11:45 executed next after the current one completes its task. A really good analogy of this is imagining a train\n11:52 track. So this is the train track and think of it as an edge and think of it\n11:57 as connecting two stations one here and one here which represent nodes together\n12:03 in a specific direction. Now the train which will go on the train track that acts as your\n12:10 state. So the state gets updated from one station to another.\n12:16 But there is another type of an edge and it''s called a conditional edge. So this is still not very\n12:24 complicated. It''s quite simple to understand. These are just specialized connections that decide the next node to\n12:30 be executed based on the specific condition or logic applied to the current state. Now a really good analogy\n12:37 for this is the traffic light analogy. So green could mean to go one way, red\n12:42 could mean to stop. yellow could mean to slow down. The point I''m trying to make here is that the condition, in this case\n12:49 the light color, it decides the next step. If you want to think even more\n12:54 simply, you could think about an if else statement. So that being said, we move\n13:01 on to the next element, the start point. So the start point or the node, the\n13:06 start node is a virtual entry point in langraph and this marks where the workflow begins. Now it''s important to\n13:12 note that it doesn''t perform any operations itself but it serves as the designated starting position for the\n13:18 graph''s execution. Now in terms of analogy it is quite simple to understand but if you\n13:25 really want think of it as the starting line of race. Now if you have a start point well\n13:32 you need an end point as well and that''s where the end element comes in. So the\n13:37 end nodes just signifies the conclusion of the workflow in Langraph. So when the\n13:42 application reaches this node, the graph''s execution completely stops and it indicates that all intended processes\n13:49 have been completed. And again, a good analogy for this is just the finish line in a\n13:55 race. So nothing too hard yet. But now let''s look at\n14:01 tools. So tools are specialized functions or utilities that nodes can\n14:06 utilize to perform specific tasks. For example, it could be fetching data from an API. They basically enhance the\n14:14 capabilities of these nodes by providing additional functionalities. Now, one common\n14:19 question could be, well, what''s the difference between a tool and a node? The node is just the part of the graph\n14:25 structure. Whereas the tools, these guys are functionalities used within the\n14:31 nodes. Now, a really good analogy for this is just tools in a toolbox. So\n14:36 imagine a hammer for the nails, a screwdriver for the screws, etc. The\n14:41 point is each tool has a distinct purpose. Again, don''t worry. You will understand the differentiation between\n14:48 tools and nodes in a lot more detail later when we code this, but this is just for a general\n14:53 overview. Now, another question you could be asking is, is there a middleman between a tool and a node? Short answer\n15:01 is yes. That''s where tool node comes in. So a tool node is just a special kind of\n15:07 a node whose main job is to run a tool. So for example, a tool node could\n15:14 be a node where its only job is to use a tool and that tool''s job is to fetch\n15:21 some data from an API. So it connects the tools output back into the state so other nodes can\n15:28 use that information. So think about this analogy going back to the assembly line. In this case,\n15:36 imagine the operator as the tool node and it controls the machine which is the tool and then sends all of these results\n15:43 back into this assembly line. Now if we progress further, let''s\n15:50 look at the state graph. So this is quite an important element as well. This will be one of the first elements you\n15:56 actually interact with and its main purpose is to build and compile the graph structure. So it''s quite\n16:02 important. It manages the nodes, the edges, the overall state and it makes\n16:08 sure that the workflow operates in a unified way and all of the data flows correctly between components. So again\n16:15 it''s quite an important element. You can think about it as a blueprint of a building. So just as a blueprint\n16:22 outlines the design and the connections within a building, the state graph does\n16:28 exactly that, but it just defines the structure and the flow of your workflow or\n16:35 application. Now here''s where the runnable comes in. Now some of you will be coming from a lang chain background\n16:41 and runnable is quite common there and it''s quite similar in langraph as well.\n16:46 A runnable in langraph is just the standardized executable component that performs a specific task within an AI\n16:53 workflow. It basically acts as a fundamental building block allowing for us to create these modular\n17:00 systems. Now a question you could have right now is well what''s the difference between a runnable and a node?\n17:07 Short answer is a runnable can represent various operations whereas a node in\n17:13 lang lang graph typically receives a state performs an action on them and\n17:18 then updates the state. Now don''t worry if you didn''t 100% get that when we go\n17:24 into the coding section you will get it a lot better. But a good analogy is a\n17:29 Lego brick. So just as how Lego bricks can be snapped together to build these complicated structures, runnables can be\n17:37 combined to create sophisticated AI workflows. So now let''s move on to the\n17:42 different types of messages. Now again, if you come from a lang chain background, you''ll be quite\n17:48 familiar with these. If you haven''t, don''t worry. We will look at the five most common message types in Langraph.\n17:57 So to start off, there''s the human message which represents the input from a user. The AI message which represents\n18:03 responses generated by AI models. The system message which is used to provide\n18:08 instructions or context to the model. Tool message which is similar to the function message but specific to\n18:14 tool usage. And the function message represents the tool of a function call.\n18:20 If you''ve used an API like a large language model API before, such as OpenAI''s API, a lot of these will be\n18:26 quite familiar, especially the system message, the AI message, and the human message. And that concludes this\n18:33 section. So, I will see you in the next section. Awesome. So, now this is quite\n18:41 exciting. We''re actually about to start coding in Langra for the very first time. Now that we''ve covered all the theory, admittedly the boring section,\n18:49 we''re now actually going to code up some graphs. And we''re about to code up our very first graph in this sub section.\n18:55 But um for this overall section, I have a slight confession to make, which is\n19:01 we''re not going to be building any AI agents in this section. Why? because I thought that one\n19:08 we haven''t really even seen uh how to actually code in Langraph and combining all of these LLMs APIs and tools and all\n19:15 of that stuff which comes with it combining them together would be quite messy and it could be quite confusing at\n19:21 times especially the fact that we have never coded in Langraph before again like I said at the beginning of the\n19:27 course this course is supposed to be beginner friendly detailed and comprehensive and we''re going to go in\n19:33 steps like little by little so hopefully understand but don''t worry we will be coding AI agents soon we''re just going\n19:39 to be building a couple of graphs right now uh understand lang graph better the syntax better and how to actually code\n19:46 up graphs and get confident with it and then we will actually build AI agents okay cool so what is the graph which\n19:53 we''re going to be building together in this section I call it the uh hello world graph mainly because it''s the most\n19:59 basic form of graph we can actually code in lang graph so the objectives are\n20:04 these So we''re going to be understanding and defining the agent state structure and\n20:11 don''t worry you''ll understand what that is in a few minutes and we''re going to be creating simple node functions nodes\n20:17 like we discussed in the previous section uh and we''re going to be processing them and updating the state.\n20:23 We''re going to be building the first ever basic langraph structure and we will understand how to compile it,\n20:29 invoke it, process it, everything. And really the main goal of this section is\n20:35 to really understand how data flows through a single node in langraph. Now just to give you a bit of a heads up as\n20:42 to what we''ll actually be covering uh what we''re going to be building I should say is this graph. Again like I said\n20:49 this is the most basic form of graph you can build in langraph. It has a start point and an end point and this node\n20:56 sandwiched in between them. All right cool. So hopefully you''ve understood what the objectives are. It''s quite\n21:02 basic and yeah, I''ll see you at the\n21:08 code. Okay, cool. Now let''s actually code this very first graph. So I''ve\n21:13 imported three main things here. The dict, the type dict and the state graph. The dict and type dict is obviously\n21:20 dictionary and type dictionary but um and state graph. These three are\n21:25 elements which we covered in the previous section. So I would highly recommend you going back there if these\n21:30 are completely unfamiliar. But again you don''t need to memorize what these are. Okay. But just to refresh your memory\n21:36 I''ve written in the comment here what the state graph is. So think of the state graph as a framework that helps\n21:43 you design and manage the flow of the tasks in your application. Um again that\n21:48 might sound a bit complicated but it''s not. Once we actually start coding you\n21:53 will it''ll make more sense. So now the first thing we''re going to do after importing everything is create the state\n22:00 of our agent and let''s call it agent state. And just to refresh your memory\n22:06 again what the state is. Think of the state as a shared data structure. And\n22:11 this keeps track of the your all of the information as the application runs. All right cool. So now let''s build the agent\n22:18 state. And the way we do this in Langraph is through a class. So let''s build class agent state and in this in\n22:26 these parenthesis we will try to the the state needs to be in the form of a typed\n22:32 dictionary. So that''s why we specify type dictionary here. Now let''s keep\n22:38 this very very fundamental and basic. Let''s just pass in one input. Let''s call\n22:44 it something like message and obviously we put colon and\n22:51 the we specify the data type of that uh attribute. Now obviously the data type\n22:57 of message will be string right so that''s why we specify strl again this is\n23:02 just normal python so once we''ve done that we are now going to be coding our\n23:08 very first node again another very fundamental element in langraph so how\n23:14 do we actually define a node it''s quite simple it''s just a normal standard\n23:19 python function and this is how you do it so let''s say let''s first try to find\n23:24 The objective um let''s say we are trying to let''s a greeting message a simple\n23:31 greeting message. So we''ll write def greeting node and we need to pass in an\n23:38 input and pass what the output type should be. Now the input type of a node\n23:44 needs to be the state and the output type also has to be the state because\n23:49 remember the state keeps track of all of the information in your application\n23:54 right so obviously you need to pass that as an input and you need to pass out the or return the updated state. So here''s\n24:01 how you do it. You pass in state and what is the state of our application? Well, it''s the agent state which we\n24:08 defined earlier, right? And the output is going to be agent state cuz we need\n24:14 to output the updated state. And our updated state will again just be the agent state once we''ve done all of the\n24:21 um all of the mechanics we do in this function, the actions we perform in this function. All right. Okay. So now we\n24:28 need to do something very very important and it gets annoying sometimes but um\n24:34 it''s really a key habit which I want you to form and it is dog strings. Now dock\n24:39 strings and lang graph is quite important. Why? Because dock strings is what will tell your AI agents when we\n24:46 actually build the AI agents your LLMs what that function actually does what that function''s actions are what it\n24:52 performs. So in this case uh by the way to create a dock string is just three quotation marks three pairs of quotation\n24:59 marks. Uh let''s call the dock string in this case let''s just write simple node\n25:05 that adds a greeting message to the state. Perfect. So now how do we\n25:14 actually refer to this message? Well again this is just normal Python code.\n25:19 So we will pass in state and we will type in message.\n25:25 Now this specific part allows us to actually update the state or the message\n25:31 part of the state. And let''s say let''s come up with something like hey\n25:37 plus state message. Um we can also add something\n25:42 like how is your day going something basic. Now what''s the last thing which I\n25:48 need to do in this function? Think about it. Okay. So now remember in uh a few\n25:56 moments ago I said we have to return the state or the updated state. Well the updated state we''ve already done we''ve\n26:02 just manipulated the state here. So all we have to do is just simply return the state. Cool. And yeah that runs without\n26:10 any errors. Okay. Now let''s actually build the graph uh which is again\n26:16 obviously very important. So how do we build the graph? Remember here I said state graph is a framework that helps us\n26:23 design and manage the flow of tasks as a graph. Well that''s exactly what we''re about to do now. So hopefully it clicks\n26:29 now. So to create a graph in lang graph you use the state graph attribute and\n26:35 you pass in your state. You can see the state schema which VS code has uh asked for what uh the description of what the\n26:42 parameters are. So our state schema in this case is just the agent state which we define right. So we pass an agent\n26:49 state. I will actually also write here our state\n26:54 schema. So uh you can physically see what it is.\n26:59 Okay. And let''s store this in a variable called graph or something. Okay. Now now\n27:07 here comes a very important method. How do we actually add a node to this graph? Cuz this graph is completely like\n27:13 nothing right now. So to add a node we use the inbuilt function graph add node\n27:18 and it requires two main parameters. Now what VS code is suggesting is a god I\n27:25 don''t even know what that all of all of that is right it''s very confusing. So to put things simply you require really two\n27:33 uh parameters the name of your node and what action it will perform. So let''s go\n27:39 with the name. The name could be absolutely anything sensible of course. Um let''s call something like\n27:45 greeter. Cool. And you can see VS Code has also asked us to um input an action.\n27:53 Now what''s the action going to be? Well, the action will just be whatever your node will actually perform. And what\n28:00 action or mechanics will this node actually perform? Well, all of that is defined by this function, right? The\n28:07 greeting node function. So we simply just put that the name of the greeting node function here and that''s it. We''ve\n28:14 successfully added the greeting node to our function to our graph and it will be\n28:21 named as greater. So remember this\n28:27 diagram in this diagram there is supposed to be a start and an end point. We''ve done the node which is sandwiched\n28:33 in between these but we haven''t really added the start and the end point yet. So, how do we do that? Well, there''s\n28:39 actually multiple ways to do that. In this subsection, in this graph, I''m going to teach you one way. Further down\n28:45 the line, I''ll teach you another way. So, but they''re both they''re quite easy. So, you simply just call the inbuilt\n28:51 function set entry point and as the parameter is just one\n28:56 parameter which is the key. Now, the key is the name of your node which you want the start node to connect to. Again,\n29:04 visualize it. The start the start point is here and the node is here. Obviously you need to reference a node for it to\n29:11 create like an edge right. So we simply pass greater and similarly graph dot set\n29:17 finish point. We will again pass greater here as well. Why? Because imagine again\n29:24 the node is here and your finish point is here and you need to connect some sort of connection between these two right and that''s why we use uh greater\n29:30 in this case. Don''t worry, you will solidify this once you complete the exercises and as we go down building\n29:36 more graphs. All right. And one last thing which we need to do is actually compile this graph. So graph compile\n29:43 using the inbuilt uh graph using the inbuilt compile function. And let''s just store this in a\n29:48 variable. Cool. So that run without any errors. But just a word of caution here.\n29:54 Just because the graph compiles without any error doesn''t mean it will successfully run. I mean, God knows once\n30:00 we build like more complicated graphs, there could be so many logical errors. So, that''s just an important thing to\n30:06 know. So, don''t get too happy once it compiles cuz there might be logical errors. Trust me, I know. Okay.\n30:15 So, I want to write some code which will actually help you visualize this. And\n30:20 you can use the IPython library. So, you can use this uh this um piece of code\n30:26 here. This code is awfully familiar with the first ever graph I showed you, right? I\n30:33 I''ll put a picture somewhere here for you to compare. The only difference is really the name of the node which we''ve\n30:39 set. In this case, it''s greater. Why is it greater? Because that''s the name we gave to this node, right? Cool. So\n30:47 that''s looks pretty good. Let''s actually run this. So to run you use the inbuilt\n30:54 method invoke. Um so let''s pass in the message\n31:00 as something like Bob or something and let''s actually store this result in a\n31:09 variable. Okay. Now how can we actually specify uh how can we actually get the\n31:15 value of result? So result we need to actually reference\n31:22 a certain attribute. Now the only attribute we have in uh the entire graph is message right. So we simply just put\n31:29 message and perfect you we get the final answer which is hey Bob how''s your day going now why is it like this because\n31:36 this is exactly how we set our act how we set our function to be what action it performs it says hey then concatenates\n31:44 the uh input message in this case it''s just the name and it says how''s your day\n31:49 going now I could have changed this to absolutely anything else right uh what goes here like these functions are\n31:56 almost endless but That''s the whole flow of how everything works. So hopefully\n32:03 you understood how to build this very first hello world graph. It''s quite\n32:08 simple. But um don''t worry if you didn''t fully 100% understand this. I''m now\n32:14 going to show you what exercise you need to complete uh to be able to solidify this. All right. All right. I''ll see you\n32:20 at the exercise. Okay. So time for your very first exercise. So the exercise for this\n32:27 graph is quite similar to what we just did, but I want you to create a personalized compliment agent. So you\n32:35 should pass in your name as like something like Bob or something and then output something like Bob, you''re doing\n32:42 an amazing job learning langraph. And to give you a hint as to what you need to do again, you again have to concatenate\n32:48 the state, not replace it. All right, it''s very similar to what we just did and it''s quite basic. You should be able\n32:55 to do this, but um this is really just to get your hands dirty. All right. Okay. Once you''ve completed this\n33:01 exercise, join me when we build the second graph. I''ll see you\n33:06 there. Okay. So now we''re about to build our second graph as you can see here.\n33:12 And it''s again quite similar to the first graph we built except now we''re\n33:17 going to be able to pass multiple inputs as you can see here. So again, what are the objectives which you will be\n33:23 learning in this? Well, we''re going to build a more complicated agent state.\n33:28 Uh, and we''re going to be creating a processing node that performs operations on list data. So now we''re about to see\n33:34 how we can really work with different data types apart from just string. And\n33:39 we''re going to set up the entire graph that processes and outputs these and computes these results. And we''re going\n33:45 to be able to invoke the graph with the structured inputs and retrieve the outputs. But the main goal which uh I\n33:53 want you to be able to learn in this specific subsection is really how to handle multiple inputs. All right. Okay.\n34:00 Let''s code this. Okay. So now let''s actually code the second graph up the second\n34:07 application up. So again I''ve just imported the same things again the type dictionary and the state graph. And I''ve\n34:13 also imported the list this time. But list is just a simple data structure which you should know already. So if you\n34:19 remember from the previous graph we made we are supposed to uh implement the state schema first right. So how do we\n34:26 do that? Again we use the class agent state uh type\n34:32 dictionary. Okay before I continue just a heads up I could have named the state schema anything I want. I could have\n34:39 named it uh something arbitrary completely like a bottle for example. In this case I''ve just said agent state\n34:45 because one that''s how I learned it. It''s like a habit for me now. But it also really tells you what it actually\n34:52 is. It''s the state of your agent, right? So that''s why I''ve just kept it like that. But again, just a heads up, you\n34:57 could have named this whatever you want. Cool. Okay. So now let''s the if you\n35:02 remember the main goal for this graph for this uh building this graph was to be able to handle and process multiple\n35:09 different inputs, right? So how do we actually assign and I really do that?\n35:17 Well, the answer is in the state which is here''s what uh which is what we''re about to do now. So you really cuz\n35:24 remember this is just a type dictionary. So you basically have multiple keys now\n35:30 you uh create that. So let''s say something like values list integers.\n35:37 So let''s say one of our input is a list of integers and let''s also pass in a\n35:43 name which will obviously be in a string and let''s have the result in a string\n35:50 something completely random. But now you can see we''re now operating on two different types of data structures uh a\n35:56 a list of integers and a string. And we''re handling three different uh different uh uh inputs values name\n36:04 result. Okay cool. So let''s run this. Perfect. So now let''s actually build our\n36:10 node because in again in this uh graph we''re just going to have a single node to keep things easy. Remember step by\n36:17 step. So let''s call let''s write dev process values and again what was what\n36:24 needs to be here? Yeah. So we need to pass in the state and we need to return the updated\n36:31 state. So how do we do that? Well, we write state agent state and we pass out\n36:37 the agent state. Cool. Now, again, building healthy habits. I know it''s\n36:43 annoying. We have to write the dog string. So, let''s just write something like this\n36:50 function process handles multiple different value in multiple different\n36:57 inputs. Cool. Again, I''m not being super specific here because one, uh, I don''t\n37:03 want to spend too long on writing doctrines and everything, and two, there''s no AI or LLM here, right? So that''s why it doesn''t really matter. I''m\n37:09 just doing this to build healthy habits. Okay, so now let''s do something like whatever values we pass the list of\n37:16 integers. Let''s sum them up. And let''s also concatenate the name as well and\n37:21 store it in the result. Sound cool? Okay, so how do we do that? We pass in\n37:27 state result cuz that''s what we are uh the action we''re performing is on result\n37:32 uh the attribute result and let''s say something like hi there and then we refer to the name\n37:41 um cool and your sum is equal to and let''s just use the inbuilt Python\n37:46 function sum and we pass state values cool and lastly we obviously\n37:54 return the Okay, perfect. And that''s that done.\n38:00 Okay, so now we actually create the graph. Again, this is going to be very very similar to what we did in the\n38:07 previous section because again there''s just a node, there''s a start point and an endpoint. So like last time, we use\n38:14 the state graph to initialize a graph and we pass in our state schema. So agent state and let''s store this in the\n38:21 variable graph. Okay. Uh let''s add our node. So graph add\n38:27 node and again remember it requires two parameters. It requires the name and the\n38:32 action. So in this case the name will be let''s call it processor for example. Again this could be anything you want\n38:40 and your action will be performed by this function right process values. So we can just add that. Okay. Now I''ve\n38:48 already told you how to uh how to initialize a start point and an end point and this is just given by that\n38:54 code. So you attach your entry point to your node. In this case it''s just one node which is the processor node and\n39:00 again same goes with finish and you compile it using\n39:05 graph.compile. Perfect. So take a moment now. How do you think this graph will\n39:10 look like? That again like I said very very\n39:18 similar on how the graph actually looks like but the only difference now is the\n39:24 name of the uh node which we''ve kept this as processor. Okay. So now let''s\n39:30 actually test this. Let''s actually invoke this graph. So how do we do that? Well, we use the invoke function. Now\n39:37 here''s another important part which is quite a common mistake especially like I have done this many times. Make sure to\n39:44 store your compiled graph in a variable cuz if you invoke the graph i.e. if you\n39:50 write something like graph.invoke that won''t make sense cuz you haven''t compiled the graph. That''s why you need\n39:56 to uh invoke using app. That''s why I''ve also done app here. If I did graph get\n40:03 graph. Oh, it''s completely messed up. Right? It says state graph object has no attribute because your graph hasn''t been\n40:09 compiled yet. That''s why when I do appget graph the uh process works. Cool.\n40:16 So now let''s again store this in uh let''s store something like answers is\n40:21 equal to app.invoke. Cool. Let''s pass in some values. Let''s say something like\n40:29 values and let''s have a list of integers. 1 2 3 4. Again, I''m just\n40:35 trying to prove a point. I''m not trying to make a very complicated um graph yet. And let''s pass the name as something\n40:42 like Steve something. Okay. Uh cool. And\n40:48 let''s print let''s print answers. Let''s see what happens. Perfect. So now you can see\n40:55 your values is 1 2 3 4. Your name is Steve. And your result is Hi there\n41:00 Steve. Your sum is equal to 10. Again, why? because that''s exactly what we uh\n41:05 asked the node the action to perform. Hi there, your name which in this case is Steve. Your sum is equal to the sum of\n41:12 the values and 1 + 2 + 3 + 4 is 10. Right? And that''s how you get this\n41:18 answer. Now what if I wanted to just access result? I didn''t want any of this\n41:24 other uh nonsense. Well to do that you can again just specify result and you will get it\n41:31 in a more clean manner. Cool. Okay. Now I want to try one more\n41:38 thing just to build your understanding a bit more. Uh let''s put some print\n41:43 statements here. So let''s have a print state\n41:49 here. Then we perform the action and then we print the state here. This is\n41:55 really just to show you how the state gets updated and it should be easy like\n42:00 interpretable cuz this is quite a basic piece of code. Again, print stated before the action and print state after.\n42:06 So there cool and here you go. So value is equal\n42:12 to 1 2 3 4 name is equal to Steve and these are the inputs we passed. Now notice I didn''t pass results as an input\n42:19 as well. I could have uh done that but Langraph automatically sets that as like\n42:25 a a none value in this case if you don''t pass an input. Now here''s where you need to be\n42:33 cautious. If I had actually used state result here as well to uh update state\n42:39 result like I used state result to update either itself or something else\n42:44 then you would run into a problem because your state result has been initialized as none because you didn''t pass it as an input. So be mindful of\n42:52 that. But in this case it worked because we''re only assigning state result. We''re not using it to assign something. It''s\n42:59 getting assigned. Cool. And you can see after the action has been performed uh\n43:05 your operation has been performed and the thing has been concatenated. You can see result is here and that was exactly\n43:12 what we were getting before we cleaned this up. Cool. So hopefully you understood that. Again it should have\n43:18 been quite intuitive and interpretable but um to solidify your understanding even more complete the exercise. So I''ll\n43:25 see you at the exercise then. Okay. Welcome to the exercise,\n43:30 your second ever exercise. And for this exercise, I want you to create a graph\n43:37 which passes in a single list of integers along with a name and uh an\n43:42 operation this time. And if the operation is a plus, you add the\n43:47 elements. And if a well times, you multiply all the elements all within the\n43:54 same node. So don''t create an extra node yet. So for example your input should\n43:59 could be jack sparrow your values 1 2 3 4 again and then your operation uh uh\n44:04 multiplication and your output should be in the format of hi jack sparrow your answer is 24 so just to give you a hint\n44:11 as to how you would perform something like this uh you would need an if statement in your node so slightly more\n44:17 complicated but the whole concept is the same so once you''ve completed this exercise I will see you in when where we\n44:25 build this third graph All right, see you there. Okay, welcome to your third\n44:33 graph. So, what are we going to do this time? Well, enough processing multiple\n44:39 values and everything. Let''s actually get the graph more complicated. So, that''s why we''re going to be building a\n44:44 sequential graph. So, all it all that basically means is we''re going to be\n44:49 creating and handling multiple nodes that can sequentially process and update different parts of the state. So we will\n44:56 learn how to connect nodes together in a graph through edges of course and we''re going to invoke the graph and really see\n45:02 how the state gets transformed as we uh progress through our graphs step by step. So again your main goal is should\n45:10 be to understand how to create and handle multiple nodes in langraph.\n45:15 Sounds cool. Okay I''ll see you at the code. Cool. So now we''re about to code\n45:22 up the third graph. Uh, and we''re making quite fast progress. So well done on that. So again, the imports are the\n45:30 same. State graph and type dictionary. Perfect. And like we''ve done in the previous two\n45:37 graphs, we''re going to be coding the uh the state schema or the agent state first. So let''s have class agent state.\n45:46 And again, it needs to be in the form of a typed dictionary, right? And in this case, let''s have the three attributes as\n45:53 all strings because we''ve already we already know how to handle multiple data types, right? So, let''s keep it simple.\n45:59 Name string, age string, and final string.\n46:06 Okay. Now, here''s what we''re going to build. Now, we''re about to build our two\n46:11 node functions, uh, which are again the actions. Okay. So again you simply write first well\n46:19 I''ll name it first node in this case and like I mentioned before we pass in the\n46:26 state and we return the updated state. Okay. So again healthy habits doc string\n46:34 again. So this is the first node of our\n46:39 sequence. Okay. And what do we want to do in this specific node? Well, I really\n46:44 just want to manipulate uh the final part. So, let''s say something like\n46:51 state final is equal to state or let''s\n46:56 have an f string\n47:02 f state name. Let''s say something like\n47:08 hi that. Cool. And we''ll just return the state. Perfect. And now again we create\n47:16 a new node. So state agent state. Return\n47:21 that. Perfect. And I''m just going to copy this dock string and just change it. This is the\n47:27 second nerf. Perfect. Okay. To speed things up. And in this case I also want\n47:34 to have state final is equal to you are\n47:43 state age years old. Again quite a simple\n47:50 example easy to follow. That''s why I''ve kept it as quite a basic graph. I mean\n47:55 it''s not going to solve the world''s problems or anything but it will help you understand.\n48:00 There is one logical error which I''ve put deliberately here. I want you to try\n48:07 to identify it. Okay. So the logical error in this\n48:15 case is the that once we''ve built our graph and everything what would have\n48:20 happened is we would have said hi to whoever uh we pass in let''s say Charlie\n48:26 or something. So, hi Charlie. And we store that in the final uh attribute in the state, which is what we want. But\n48:33 here''s where things get like start to be well logically incorrect. Once we\n48:39 finally get to our second node, again, we''re updating state final, which you can do. You can repeat, you can um\n48:46 interact with these attributes at in in any node possible in all of the nodes.\n48:52 And you can do it as many times as you want. But notice this part. What''s\n48:58 happening here is we''ve completely replaced all of the content we had before. So remember how we had hi\n49:03 Charlie? We''ve just completely replaced it with you are age years old. But we\n49:10 want both of them both of those stuff, right? So how do we get both of them?\n49:15 Well, again we just concatenate them. So we can have something like state\n49:21 plus state file. And there we go. Logical error should be now solved,\n49:27 right? Cuz now we have concatenated state final. Uh we''re essentially just like adding on to uh we''re preserving\n49:35 what we had before, right? Okay. Now let''s get to the fun part. How do we\n49:41 actually build this graph? And really it''s quite similar to the previous two\n49:46 graphs except there is one new thing which you''re about to learn. So like always we use state graph to start the\n49:53 framework. So agent state and let''s store it in graph. Again I could have had this name\n50:00 the width variable into anything. I''ve just kept it graph cuz it makes intuitive sense. Okay. Now we add our\n50:07 nodes. So we do graph add node. And for simplicity sake I''m just going to have\n50:13 the name as the same name as the uh function. Okay. So that way it''ll just\n50:19 be easy to follow. So graph add node and second\n50:27 node. Second node. Cool. Okay. Now that we''ve added both nodes, we need to\n50:34 obviously s uh add the entry point and the end point, right? So we set the\n50:40 entry point like this. Again, quite self-explanatory because we wanted to connect to the first node, not the\n50:45 second node, right? So it should be start uh first node, second node, end\n50:51 point. How do we connect the first node and the second node together\n50:59 though? Hopefully you had an answer for that. Uh if you remember or recall from\n51:04 the previous section, theory section, there was an element in Langro called the edge. That''s exactly what we''re\n51:10 about to do right now. We''re about to use edge and that was the new thing which I was talking about a few moments ago which you''re about to learn. So how\n51:17 do we use it? Well you use graph edge add edge and if we can hi perfect. So\n51:25 again it''s quite simple you use a start key and end key. So sim similar to entry point where but your in this case you\n51:33 need to pass two parameters. So the edge we want is between the first node and the second node right? Well that''s\n51:40 exactly what we pass here. So first node and second node and like before we will\n51:48 just set the finish point at second node and we will compile this. Now how will\n51:53 this graph look like? Take a moment to try to think of how it will look\n52:02 like like that. Start point end point and these two notes are sandwiched in\n52:07 between. But now there is a edge. It should be called a directed\n52:12 edge if I''m being like quite picky. But yes, a directed edge cuz the flow of\n52:18 data or your flow of your state updates is from the first node to your second node. Right? Cool. So now that we''ve\n52:24 built that, let''s again invoke this. So I''ve got this code ready here. Uh let''s\n52:30 invoke it. Let''s pass the parameter as Charlie and let''s pass the age as 20.\n52:37 Cool. Print result. Perfect. Apart from the uh\n52:43 misalignment here which I can just change right now. Perfect. Okay. So now you can see\n52:49 it says hi Charlie you are 20 years old. Now obviously we could have performed all of this in one single node which we\n52:56 have been doing in the previous subsection but the obviously the aim was to be able to create multiple nodes\n53:03 right and handle um the state how the state progresses. So yes you one\n53:08 important thing which you''ve learned is obviously how to use the add edge method but another concept which you have\n53:15 solidified here is you can uh change these at these keys of your state at in\n53:22 at any point in time like as long as as however many times you want cuz remember\n53:29 here we''ve passed in state final um we implemented state final here we implemented state final in the second\n53:35 node if we had more nodes in the sequence. We could have done that again and again and again. And we also learned\n53:41 how to like one key logical error is sometimes a lot of people just accidentally replace uh their content in\n53:48 one of the attributes and that leads to a lot of logical errors. So always be mindful of that. And yeah, that again\n53:55 was quite simple, not too hard and hopefully the exercise which I''m about to give you solidifies this. Cool. So I\n54:03 will see you at the exercise then. Awesome. So now we will move on to the\n54:09 exercise for this third graph. And what I want you to do is really build on top\n54:15 of what we just covered. Instead of two nodes, I want you to build three nodes. Again, in a sequence, don''t need to go\n54:22 too fancy yet. We will again three nodes in a sequence. And we will have you will\n54:29 need to accept the user''s name, their age, and a list of their skills. So the first node will be specifically for\n54:35 personalizing the name field with a greeting. The second node will be describing the user''s age. The third\n54:42 node will be listing all of the user skill in a formatted string. And then you''ll need to combine this and uh store\n54:49 it in a result field and output that. And this should be a combined message. And the format I would like you to\n54:55 output is something like this. So let''s say the name was Linda. And let''s say Linda welcome to the system. You are 31\n55:03 years old and you have skills in Python, machine learning and langraph. Okay. And\n55:09 just as a hint for this exercise, I would you''ll need to use the add edge\n55:14 method twice. So this will really solidify your understanding on how to build graphs in general. All right,\n55:22 cool. So once you''ve done that, again, answers will be on GitHub for all of the exercises. Once you have uh cross\n55:28 referenced and checked that you''ve done it right, I will see you in the next section where we build our fourth graph.\n55:34 All right, see you there. Welcome, welcome, welcome. Okay, I''m particularly excited for uh teaching\n55:41 you this graph, graph 4. Why? Because we''re about to learn how to build a conditional graph. So for the very first\n55:49 time, we''re about to implement conditional logic. And obviously we''ve\n55:54 done it in a previous exercise before but that was within a single node. This is how to implement conditional logic in\n56:00 the overall graph structure. And so we will be implementing conditional logic to route the uh flow of data to\n56:07 different nodes. We will be using the start and the end nodes to manage entry and exit points. We will be designing\n56:14 again using multiple nodes to perform different operations such as addition and subtraction. And we will be able to\n56:20 create a router node to handle decision-m and control the graph flow. So the main goal is really to you how we\n56:28 can use this inbuilt function which uh allows you to create conditional edges in langraph. All right, exciting stuff.\n56:35 I''ll see you at the code. Okay, so let''s actually code this\n56:41 up now and you''ll see the imports are slightly modified this time. Again, type\n56:47 dictionary and state graph is there. But now I''ve also imported start and end point. Again, if you remember a few\n56:54 subsections ago, I told you there are multiple ways to be able to initialize the start and the end point. And this is\n56:59 another way you could. Arguably, this is the easier way, but um whatever. I don''t\n57:06 really have a preference, but I''ll teach you both ways regardless. Okay, let''s import these. Successful. Okay. like\n57:13 standard procedure we will design we will um code up the uh the schema the\n57:19 state schema so class agent state and let''s again type dictionary in this case\n57:27 uh uh I want to be able to pass in two numbers and pass in an operation so a\n57:33 plus operation and a minus operation one of those two operations now obviously I\n57:38 could have handled uh all of this within one single node But that''s not the point\n57:43 here. I''ve kept it deliberately very very simple. So the main concept which you learn is how to uh implement\n57:50 conditional logic. Okay. So let''s code the different uh keys\n57:56 which we require. So number one will be an integer. Operation will be in the string a plus or a minus. Uh number two\n58:05 will be an integer and final number will be an integer. the final number will be the result of either adding or\n58:11 subtracting the two numbers. Easy enough. We''ve done this multiple times now. Okay. Now, here''s\n58:18 where things get interesting. Now, just a heads up. Initially, this won''t make\n58:24 sense. But when we look at it from a bird''s eye view and we look back at all the code in this subsection again, uh\n58:32 everything will start to click. So again, it won''t make sense initially, but it will once we look at it. Uh\n58:38 again, don''t worry. All right. So let''s create our first node function. Let''s call it adder. And it''s again still a\n58:46 node. And we input the state schema. And we return the updated state schema and\n58:53 dock string again. But uh this time I''m just going to copy it from here. Uh it''s\n58:59 tells exactly what it does. This node adds the two numbers. Uh and easy enough, we just do state final number is\n59:06 equal to state uh number one plus state number two. Okay. And we just\n59:14 return the state. Quite simple, right? And just\n59:20 like what we did with the addition, we need a node for subtraction as well. So def subtractor. Now uh I already\n59:29 implemented it to uh don''t to not waste time but this node subtracts the two\n59:34 numbers. It''s very similar to the previous uh node function. Uh it just\n59:39 subtracts these two numbers. Again yes you could be saying what if number one is uh smaller than number two it''ll give\n59:46 you a negative result. It that doesn''t matter. The main aim again was to implement the conditional logic not the\n59:53 um inner workings of each node. Okay. Okay. Now we built another type of node.\n1:00:00 Uh and we initialize it the same way but this time let''s call this node decide\n1:00:06 next node. Let''s actually give it a name which actually says what it does. Right.\n1:00:11 So again we use state agent state and we pass like this. Perfect. Okay. Now the\n1:00:19 dock string will be something like so. So this node will select the next phase\n1:00:25 of the graph or well next node of the graph I should say. Okay. Now we use an if statement and\n1:00:33 before I code something let''s just try to map how this will work. This specific\n1:00:39 node will be at the start of our uh graph. So we will have the start node. We will have this uh this specific node\n1:00:46 we''ll call it the router. and this router because it routes uh the next uh\n1:00:52 to the next node depending on what the state schema is at that point. So we will have the uh I will put an image up\n1:01:00 right now so you kind of get what I''m trying to say but we essentially will\n1:01:05 have the router decide whether we uh add the two numbers and subtract the two numbers and obviously this will be\n1:01:11 decided with the operation uh attribute right which you should see from here.\n1:01:16 Okay, let''s code this up now. So this is not the hard part. If state operation,\n1:01:24 if I can spell if state operation is equal to equal to\n1:01:31 plus. Okay, if state operation is equal to equal to plus, we need to do a certain thing to\n1:01:38 pass it to the next node. Okay, now here''s well your first guess could be\n1:01:45 okay. Well, we guess I guess just call this function, right? Not exactly. Not in langraph. You actually return\n1:01:54 uh return the edge. Now, we haven''t described the edge yet, right? But for\n1:02:00 now, I will just say the edg''s name is addition operation. So, addition operation. Similarly, if it''s\n1:02:07 subtraction, we will do this like so. So just to reiterate we will uh you we will\n1:02:15 see what the um value is at the operation in the state schema. If it''s a plus we call we will return the edge\n1:02:23 addition operation and if it''s a subtraction we will use the subtraction operation edge. Again we haven''t\n1:02:30 described or defined these two edges yet. That''s what I was saying earlier. When we look at it from the bird''s eye\n1:02:36 view later on in a few moments once we''ve built everything it will make much more sense. So stick with me for now.\n1:02:42 Okay. And runs perfect. Now we build the graph. And now here''s the exciting part.\n1:02:50 So we again like normal standard procedure we use state graph to create\n1:02:56 the graph framework. So graph is equal to that. And let''s add these nodes uh to\n1:03:02 the uh to our graph. So graph add\n1:03:08 node and let''s say router. Okay. And again we will pass\n1:03:15 this decide next node. Perfect.\n1:03:21 Okay. Now I have another confession to make. Lots of conventions. I know this\n1:03:26 won''t work. I know I haven''t built the rest of the graph yet but this eventually will not work. And there is a\n1:03:34 subtle reason why this won''t work. You know, it''s mainly in this line. Add node\n1:03:40 router decide next node. The problem is with decide next node cuz oh, you can\n1:03:47 see that the dock string appears once we press uh the decide next node. But the\n1:03:52 reason this won''t work is look closely at these three functions. What are we\n1:03:57 doing in these two functions that we''re not doing in this? I''ll give you a moment to try to analyze\n1:04:10 this. Okay. So, doesn''t matter if don''t worry if you didn''t get that. The\n1:04:16 correct answer is we are returning this updated state in this one and this one.\n1:04:21 But in this node, we''re not. We''re just returning the edge. Subtle difference I\n1:04:28 know but that''s how Lang graph works and you will see why they do it like that\n1:04:35 uh right now. So how do we deal with this? Now I obviously could have built this graph and then I would have shown\n1:04:41 you the error but then things would have just gotten messy. That''s why from the get- go I have told you why this wouldn''t work. So now that you know why\n1:04:48 this won''t work, how do you fix this? Simple. You use this code lambda\n1:04:54 state. Now, if you have used lambda functions before, this is quite easy to understand. If you haven''t, don''t worry.\n1:05:00 All this is saying is your input state will be your output state. That''s it. In\n1:05:07 even more simpler words, think of this as a pass through function. So, what it''s saying is your\n1:05:14 input state will be passed, your state will be inputed and your output will be\n1:05:21 the exact same state. Now, why is it the exact same state? because you''re not changing the state at all. You''re\n1:05:28 comparing stuff here, but you''re not assigning anything. There''s a difference between comparison and um and\n1:05:34 assignment. Right? Again, even in this one, you''re just comparing to see whether the operation is a minus, but no\n1:05:41 assignments been made at all. In fact, there''s been no changes to this state\n1:05:47 whatsoever. That''s why we can use this as a pass through function. Now, hopefully that made\n1:05:53 sense. Okay, let''s continue. Again, we will get a lot more practice. Don''t worry, this\n1:05:59 is the first time you''re seeing this. Okay, so now we will add the edge. And\n1:06:05 this is just the normal edge we did last time. So, we will need the start key. And now here''s how you initialize\n1:06:11 differently. Remember how we used to do set entry point and set finish point? We don''t do that anymore. Uh we use start\n1:06:19 the keyword cuz that''s what we imported. Make sure to import it if you do it this way. uh you use start and end. So your\n1:06:26 start will be a start point and your what do you want the start to be connected to? Well, we want it to be\n1:06:32 connected to the router. If I put this in quotation marks, perfect. Now, why not add node or\n1:06:38 subtract node? Well, think again. Refer back to that diagram which I''ll show in\n1:06:43 right here. We if we connected the start point to\n1:06:48 the the add node or the subtract node, well then what''s the point of the router in the first place, right? The whole\n1:06:54 point was the router decides what the inputs are and then from there it branches off to the correct node. So\n1:07:01 that''s why the router needs to be the first node we uh connect our start point to. Cool.\n1:07:07 Okay. Perfect. Now we add the we now implement the main the new thing which\n1:07:14 we are going to learn in this section is graph dot add conditional edge. So graph\n1:07:21 dot add conditional edges. Now again wow looks really\n1:07:26 confusing but it''s actually much more simpler than it looks like. So the first uh part is your\n1:07:33 source which you can see here as well. So the source will just simply be the name of the node. And what''s the name of\n1:07:40 the node which we want the conditional edge to be? It''s the router node, right? So that''s going to be the source part.\n1:07:47 Perfect. Now if you look here, it''s asking for a path. What''s the path you would like it to do? Now before we\n1:07:54 implement the path, we obviously need to per uh imple uh tell it what action what\n1:07:59 what action it needs to do. And that''s where this node will come in the decide next node part. So we pass that as the\n1:08:04 second parameter. So that''s the path. And now we implement something\n1:08:10 called the path map which you should have briefly saw\n1:08:15 here. Uh there path map. So we''ve implemented the source which is the\n1:08:21 router. We''ve implemented the path which is your uh decide next node function. Uh\n1:08:27 again don''t need to worry about hashable runnable any and all of this stuff. Okay, it''s you don''t need to over\n1:08:33 complicate it. Now it''s time for the path map. Okay, so now your path map will be in a form of a dictionary. And\n1:08:40 remember how I said earlier that we had implemented addition operation and subtraction operation. These were edges.\n1:08:46 So now we''re about to implement those only. So we''re about to create two new\n1:08:52 edges here. Let me just write this code up for you and then it will make sense.\n1:08:57 Give me one second. Okay, so there we go. Now what is this\n1:09:05 code actually saying? Well, this is in a format of edge and\n1:09:12 node. Now the starting point of this edge will obviously be this router node and it''s telling us where it will\n1:09:18 connect to. This uh visualization will be it will be it''ll be much easier to\n1:09:24 visualize when I actually show you the graph. Don''t worry. But for now, addition operation and subtraction operation is the edge. And the two nodes\n1:09:31 are add node and subtract node. Right? Okay. Uh lastly, we now we''re now at the\n1:09:38 point where we need to create the end point. But obviously, we if you look back at this diagram which I''ve shown on\n1:09:44 the screen right now, you can see that the we need two edges to connect to the\n1:09:51 end point, right? We need to we need an edge from the and node and we need an edge from the subtract node. So we can\n1:09:58 add two edges like this. graph edge. Uh we uh start at the add node and\n1:10:05 then we end at the endpoint. Again similar subtract node and endpoint. And then we just compile this. So app is\n1:10:11 equal to graph.compile. Cool. No errors. Okay.\n1:10:18 Now here comes the most exciting part. Again try to visualize what this graph\n1:10:23 will actually look like. Okay. So it should look something\n1:10:29 like that. Probably slightly different to what you initially anticipated but\n1:10:36 that''s okay. We again have a start point. We have the router and we have the our two nodes add node subtract\n1:10:42 node. And notice remember when I said addition operation and subtraction operation are the edges names. Well,\n1:10:48 here it is. Addition operation and subtraction operation. It''s telling us uh what the which direction to go into.\n1:10:55 Do we go how do we go to add node? Well, we use the addition operation. How do we go to subtract node? Well, we go to the\n1:11:01 subtract operation. And then obviously we create these two edges, these two to connect to the\n1:11:10 endpoint. Awesome. So, we will once again look at it from a bird''s eye view. But let''s actually invoke this graph to\n1:11:16 see what happens. So let''s use this piece of code. So what\n1:11:23 it''s saying is it''s defining number one as 10, operation as minus and number two as five. So because we''ve used\n1:11:30 subtraction, the final number should be 10 - 5 which is five. And we''ve printed\n1:11:36 the results and the answer is like such. Uh number one is equal to 10, operation\n1:11:41 is equal to minus, number two is equal to 5 and final number is uh five. Obviously the way I''ve invoked it is\n1:11:47 slightly different to what I have done before. Again, this is another way you can invoke. Okay, so not too hard. But let''s\n1:11:56 just go through everything one more time to solidify everything. Okay, so we\n1:12:02 imported everything. We created the state schema using agent state and a type dictionary. Then we created our\n1:12:07 three different nodes which is the add node, subtract node and the decide next node. And this is in within the decide\n1:12:15 next node. You can see that if the operation is a plus, it goes to the addition operation edge which is this\n1:12:22 edge. And if it''s subtraction operation, it goes to this side. And this is how we\n1:12:28 built the graph. We added the nodes. We added the edge from the start point uh to the router. And then we added the\n1:12:34 conditional edge. the new thing which we''ve learned in this section uh which is we uh reference router and we use the\n1:12:40 edge node format. So the edge will be addition operation uh to add node then\n1:12:47 it will be subtraction operation to subtract node. Visually speaking it will be addition operation to add node\n1:12:53 subtraction operation to subtract node. Now, I know this will be quite confusing at first and don''t worry, it took me\n1:12:59 quite a while to understand this myself as well, but hopefully the exercise I''ve given you will really be able to help\n1:13:05 you understand this much better. Okay, so I will see you at the exercise\n1:13:11 then. Awesome. So, let''s actually find out what the exercise is for this graph. So, you need to make this monstrosity.\n1:13:20 Now at first glance it looks terrifying but if you analyze it a little bit closer all it is is what we just coded\n1:13:27 twice. So we coded this and we need to replicate it once more. So in essence\n1:13:33 you need to actually input four numbers and two operations and you need to output their final results. For example\n1:13:39 number one, number two, number three, number four and the respective operation and the respective results. Right? So in\n1:13:44 this case we would have to do 10 - 5 which is 5 and 7 + 7 + 2 aka 9 and those\n1:13:51 two numbers should be outputed. Now the reason I gave you this exercise to do is because this will really solidify your\n1:13:57 understanding about conditional edges which will really be important for the next few next graph and the next AI\n1:14:04 agents we make. Okay. So once you have uh completed it by looking and cross\n1:14:09 referencing the answer on GitHub, I will see you in the next graph.\n1:14:15 Okay. All right. Well done. We''re almost at the end of this section and we''re about to build our final graph aka graph\n1:14:22 5. Now we''ve learned quite a lot about Langraph and its internal mechanisms. And this will really help us in the next\n1:14:29 section where we finally build the AI agents you were looking for. Now in this\n1:14:34 section in this subsection sorry we''re going to be learning an important concept. There''s still one more concept\n1:14:39 we haven''t learned and that''s about looping. So we''re going to be creating well a simple looping graph. Now I kept\n1:14:45 the objectives to be quite small here. There aren''t that many objectives. It''s essentially implementing logic uh which\n1:14:52 involves looping uh to route the flow of data back to the nodes. And we''re going to be creating a single conditional edge\n1:14:58 which you know how to do in the previous section. Regarding the previous section, however, I know the exercise. Please do\n1:15:05 complete that exercise. That exercise will be probably the hardest exercise you would have done until this point.\n1:15:12 So, don''t worry if you didn''t get it. If you did, great job. You''re doing really, really well. But if you didn''t get it,\n1:15:18 look at the GitHub. Try to compare where you went wrong. Remember, in Langraph, there''s more than one way of building\n1:15:24 the graphs. Make sure the graphs are well built and it actually functions.\n1:15:29 And if you want an extension, try to make it even more robust than it is. All right, but back to this now. Final\n1:15:35 graph, I promise. The main goal really is to code up the looping logic. So,\n1:15:40 with that out of the way, let''s build a final code for this section. See you\n1:15:46 there. Awesome. So, final code we have to build for this section. And here we\n1:15:52 go. So, graph 5 squ. Now, I''m going to take a slightly different approach this time. And I''m actually going to show you\n1:15:58 the graph we want to end up building from the get- go. And there''s a reason I''m going to start that from now so we\n1:16:04 get in good practice. The reason is once you finish this course and actually\n1:16:09 start either making your own AI agentic systems for someone else, for your clients or for yourself like make your\n1:16:15 own JavaS system or whatever. You obviously need to plan how it works, right? You need to see okay, what nodes\n1:16:22 do I need? What edges do I need? Does is this does this need to be a conditional edge? where''s the start point going to go, end point going to go etc etc and\n1:16:29 you can either do that via pen and paper or software like I''ve used but point is you need some sort of blueprint and\n1:16:38 that''s how really it works in the industry as well um you can you will obviously have a blueprint and then from\n1:16:44 there you will code up the graph similar to how a UI designer for example uh renders um their UI designs and then\n1:16:52 sends that off to a software developer who uh well develops the application forwards. Right? So that''s the habit I\n1:17:00 want to start uh creating with you. All right. So this is the graph I want to\n1:17:07 build in this section. So there''s obviously going to be a start and end point. And this really should be mostly\n1:17:13 familiar except for this loop. So there we''re going to create a simple greeting node and another node which is called\n1:17:18 the random node. So in the greeting note I essentially want the user to have uh\n1:17:24 stated their name and it should output a simple hi there your name and then the\n1:17:31 graph progresses to the random node and in the random node I essentially want to\n1:17:36 generate five random numbers. Okay, now just as a heads up, yes, this graph in\n1:17:44 industry would be completely useless. I know, but I''ve deliberately kept it simple again so you know the\n1:17:49 fundamentals. Like this loop is could have easily been avoided and transferred\n1:17:55 into a for loop for example, right? Like I could have had a for loop within this node and ran it five times to generate\n1:18:02 the numbers. I get it. But this is again kept deliberately simple so you actually understand the concept. Okay, cool. So\n1:18:09 let''s the usual inputs and the only difference is this time I''ve also imported random but if you have used\n1:18:15 Python before quite a lot you would have come across this library right okay so\n1:18:20 let''s start with our agent state so class agent state type\n1:18:26 dictionary and what''s the first thing we''re going to need well let''s see we\n1:18:31 have the start point do we need anything any keys that no for greeting note what did I say I want uh I wanted the user to\n1:18:38 be able to input their name. So, we need a name attribute or a key. And then for\n1:18:43 the random number, a random node, we need some form of um a list to like\n1:18:51 actually store the numbers. So, we have number and list\n1:18:56 int. Okay, cool. And one more thing, look at this loop. How will we actually\n1:19:02 know when to stop? We need some form of counter, right? So, counter int.\n1:19:08 Perfect. Now, obviously, just as a heads up, when you do go on to make your AI agents and everything, you''re not going\n1:19:15 to know what attributes you need right from the get- go, unless if you planned it like extremely extremely well. But\n1:19:22 chances are you won''t get it. But don''t worry, iteratively well, you''ll obviously be better at speculating what\n1:19:29 attributes you need through practice. But you can obviously do iterative development as well, right? Okay, cool.\n1:19:35 So now let''s actually build these nodes. Okay. So let''s start off with the\n1:19:40 uh greeting node. So how we normally define uh a function. So def greeting\n1:19:46 node, we obviously need the agent states like such. Perfect. And the dock\n1:19:55 string. But uh luckily for me, I''ve already got that here. So I don''t need to do it again. I know it''s boring, but\n1:20:01 habits. Now let''s update update the uh name uh key. So how do we do that? Well,\n1:20:10 you should know by now state name is equal to let''s say something like hi\n1:20:16 there. State name. Perfect. So what will this do? I input a\n1:20:22 name and it''ll replace that name with a string of hi there this person. Now\n1:20:27 let''s also initialize the counter variable here. Now why am I doing that?\n1:20:32 Let me just first write it and think about this. Okay. Now, obviously I''m changing I''m\n1:20:40 setting like the value. So, I will need to have passed in like an valid integer\n1:20:45 when I am passing the uh value when I''m invoking the graph, right? But here''s\n1:20:52 the thing. What if I pass in minus2 for example? Well, as the counter value, as the\n1:20:58 initial counter value, if this line wasn''t there, well, it would have just kept on incrementing until it got got to\n1:21:05 five cuz I want to have five numbers. But if it starts at minus2, well, it would end up giving me seven numbers.\n1:21:12 Now, that''s not robust, right? So, this basically wipes out whatever rubbish\n1:21:18 integer the user even inputs. If they had put zero, well, okay, we replaced it to zero. If they put like minus 20\n1:21:25 because they''re greedy or something, then we have made sure to like set that back to zero. So, so just a way to make\n1:21:31 it robust. That''s all. Uh, return state. Okay, cool. So, now let''s create our\n1:21:38 second node which is a random node. So, we can say random node state agent state\n1:21:46 agent state. Perfect. Dog string. Again the dog strings will be useful. I\n1:21:52 promise in the next section they will. So this generates a number random number\n1:21:57 from 0 to 10. Now this piece of code here\n1:22:03 essentially appends the appends the randomly generated number to the number\n1:22:09 list. Okay, that''s all it does. And what else do we need to do in this node? Well, we need to increment the counter\n1:22:15 value, right? So C uh plus equals to one. So this will\n1:22:22 increment uh the value by one and then we just return the state. Okay, cool. Now here''s where\n1:22:31 we''re how we''re going to implement the looping logic. Now just a warning here and please listen to this. Like in any\n1:22:37 software development uh program or programming language G2, there''s more\n1:22:42 than one way of coding up an application, right? Same goes with Langraph as well. There is multiple multiple different ways of coding like a\n1:22:50 looping code like this graph. I''m going to be showing you one of them. I obviously can''t show you all of them cuz\n1:22:56 there it''s just time constraint, right? But obviously the more you practice the\n1:23:01 uh uh better ways you''ll more efficient ways you''ll find, right? But the way I''m going to show you is pretty efficient as\n1:23:07 well. Don''t worry. Okay. Now, you might have speculated that I''m going to create\n1:23:12 like a router node. It''s close. I''m not going to create another router node. You\n1:23:18 can see in the graph the uh the client let''s say the client wants this graph. The client doesn''t want another router\n1:23:24 node here. So how do we go about that? Well, we could create a conditional edge. How do we do that? Okay, let''s\n1:23:31 begin that. So let''s write a new function say defaf should continue uh state agent\n1:23:38 state agent state and um let''s create this block\n1:23:44 string um function to decide what to do\n1:23:49 next something like that. Okay cool now here''s where we set our looping logic\n1:23:55 and this should look quite familiar to you\n1:24:00 now. Perfect. So let''s run that. Okay. So what have I written here? Well, if\n1:24:05 the counter value is less than five because we''re starting with zero, right? So 0 1 2 3 4. That''ll be five values. Um\n1:24:12 I''ve also written a print statement so like we can keep track of um um the\n1:24:18 progress. Also whenever I''m writing the code as well when you''re uh coding with me or doing the exercise, it''s really\n1:24:24 helpful to print uh statements uh like put in print statements everywhere. Or\n1:24:29 you could also use break points as well. So you know uh where to where the code failed if it\n1:24:36 fails. Okay. So here we return the loop a loop edge and the exit edge. So\n1:24:44 obviously we have the loop edge and this will be the exit edge. So everything is going to plan so far but um so far is\n1:24:50 the key. You never know, right? Okay. Uh just as a heads up though, I want to\n1:24:55 show you this. So this is how the trajectory should follow. We start at the greeting node. Why? Cuz we obviously\n1:25:02 go from the start to the greeting node. And then we enter the random node. And we enter the random node and exit it\n1:25:08 five times. So 1 2 3 4 5. Why five times? Because we want five random\n1:25:14 numbers, right? By then this if statement will uh well it won''t work. It will fail. So we will go to the else\n1:25:20 statement and return exit. And if we return to exit, we''ll go to the end node. Uh okay. endpoint. Okay, so that''s\n1:25:28 how the general gist is. Okay, let''s quickly make this graph. So you should know how to initialize a graph agent\n1:25:35 graph and let''s just add these nodes. So we have our two nodes which are\n1:25:43 here greeting and random which is exactly what we wanted, right? Greeting node and random node. Perfect. Okay. And\n1:25:50 now we''re going to add an edge between greeting and random. Uh why? Because\n1:25:56 well I''ve created this edge. You see this edge greeting node and random node. This edge that''s the edge I''ve created.\n1:26:03 Okay. Now I''m going to create the um the conditional\n1:26:08 edges which is done through here and I''ve written some comments here\n1:26:14 as well. So uh there will be the source node which is the random. So where I\n1:26:21 want the conditional edge to start from and then the routing function or this tree I should have really written action\n1:26:27 here because is the action I want to perform the underlying mechanism or function which is going to which we''re\n1:26:33 going to um determine which edge to use and that''s uh implemented by the should continue function right and notice how\n1:26:40 again these two edges are the same edges here. So if the loop is uh the one which\n1:26:47 um uh is outputed then we need to go back into its random the random node\n1:26:54 which we''ve generate uh which we put there and if it doesn''t we go to the end part. Okay and then obviously we set the\n1:27:01 entry point. Okay. So again you don''t have to\n1:27:07 set the exit point here uh or the finish point because we''ve already done it using end here. Okay, perfect. And then\n1:27:14 we just compile the graph app is equal to\n1:27:20 graph.compile and okay, it compiled. That''s a good sign. But let''s see if we\n1:27:26 have got our graph to be the exact same. Now I''ll put the graph image here\n1:27:32 so I don''t keep scrolling back and forth. But you can see we have the start point and the end point. We have the greeting and the random. And then we\n1:27:39 have our two condition edges. So we have the loop going back into the random node\n1:27:45 as we wanted and the exit which you can see. So take a moment and you can see compare and contrast. Okay, let''s\n1:27:54 continue. Okay, now I have this code. So I''ve given a name my name uh a\n1:28:01 r um a completely brand new list and I''ve set counter to minus one. And as you can see it enters loop\n1:28:08 one, loop two, loop three, loop four because these are print statements we printed. Uh it says hi there v which is\n1:28:15 my name. Uh number 10 21026 just randomly generated and you can see the counter value is five. Now remember what\n1:28:23 I was saying over the counter. We set the counter value to zero here to make it more robust. If we had not done that\n1:28:28 well it would have generated six times. And now I can set this to minus 100. it will still obviously give me different\n1:28:35 random values but um the code is largely the same. So that''s really the way which\n1:28:42 I personally use to create loops in langraph it''s pretty easy right but um\n1:28:49 obviously with practice you might even find some other ways if you do find other ways like obviously uh do let me\n1:28:55 know uh there''s more than one way again you can send me a message on LinkedIn or Instagram or whatever but um yeah so\n1:29:03 this is finally finally uh we have implemented the code for our final graph\n1:29:09 of the section So just complete the graph 5 exercise please and yeah we should be good to go\n1:29:16 to make AI agents. So I''ll see you at this codes exercise. Okay\n1:29:22 cool. Okay good job on that. Now for the exercise for this last graph uh you need\n1:29:28 to implement this graph on the right. So you need to implement an automatic higher or lower gain. So for context,\n1:29:36 you need to set the bounce which we can guess between 1 to 20 integers of course and the graph has to keep guessing where\n1:29:43 the max number of guesses is 7 where if the guess is correct it stops but if not\n1:29:49 we keep looping until we hit the max limit of seven. Now please note we don''t have to pass any inputs the actual graph\n1:29:57 should automatically guess by itself. So there should be no human in the loop human intervention at all. So each time\n1:30:03 a number is guessed the hint node aka this node should say either higher or\n1:30:08 lower and the graph should account for this information and guess the next guess according accordingly. So for\n1:30:15 example the input should be something like the player name student. The guess should just be an empty list cuz we''re\n1:30:20 initializing the list. Attempts should be set to zero and the lower bound and upper bound should be 1 to 20. Now the\n1:30:27 reason I''ve also passed these as inputs is because uh if you wanted to expand\n1:30:32 this to maybe 1 to 50 numbers or whatever you can. It''s quite easy to do that. So just as a hint uh it will need\n1:30:40 to adjust it its bounds after every guess based on the hint provided by the hint though. So once you''ve completed\n1:30:47 this exercise you would have fully reinforced uh your understanding about loops in langraph. So once you''ve\n1:30:54 completed this, cross reference it. Cross reference the answers on GitHub. I will see you in the next section where\n1:31:00 we finally begin AI agents. See you there. Okay people. So welcome back to\n1:31:08 this brand new section where we actually start learning about AI agents. Now we\n1:31:13 finally are upgrading our ability in Langro. I even upgraded my clothing sense. Not really. But this is exciting\n1:31:21 times cuz we actually finally build AI agents. So, we''re going to build a lot of AI agents in this section. And\n1:31:28 starting off with the first agent. Well, technically it''s not really an agent, but I just named it that because it\n1:31:34 sounds cool. But um technically it''s not though. But let''s see what we''re going\n1:31:40 to actually learn in this section in this subsection. So, we''re going to build a simple bot. That''s it. And these\n1:31:46 are the objectives. So we''re going to define a state variable uh state structure which we''re going to have a\n1:31:52 list of human message objects and I briefly uh me uh mentioned what a human\n1:31:58 message was uh a long time ago in the course. Uh what it is it''s well it''s in the name it''s a message prompt which is\n1:32:05 given by the human aka us to the AI. Uh we''re going to initialize the GPD40\n1:32:12 model for this uh using lang chain''s chat open AAI uh uh library. Uh we''re\n1:32:17 going to send and handle different types of messages. We''re going to build and compile the graph of the agent. But the main goal really is how we can integrate\n1:32:25 LLMs into our graphs. So what is this sort of graph we actually going to end\n1:32:31 up building? Now it''s very very simple. It''s going to look like this. And yes, this looks exactly like the graph we\n1:32:38 made in the uh first ever graph we actually ever made. But um the functionality will obviously be\n1:32:44 different cuz now we''re actually integrating LM. So exciting stuff people. Uh okay, I will see you at the\n1:32:50 code then. All right, coding time. So now we first code our well we code up our very\n1:32:58 first AI agent aka the simple what and um I''ve already imported all the\n1:33:03 necessary libraries we''ll need uh to not waste time. So while you''re uh coding these up as well and copying these I''ll\n1:33:10 also briefly explain what these are so we''re at the same level. Okay, so we''ve already imported type dictionary and\n1:33:17 list many times before but um these two we haven''t sorry these two the lang\n1:33:23 chain codon messages import human message so I briefly mentioned this in the intro of this section of the\n1:33:30 subsection what a human message is right and this is the library we get it from and similarly we''re going to be using\n1:33:37 openai''s lms so that''s why we''re going to use chat openai from the lang chain open aai uh library uh the\n1:33:44 langraph.graph. Uh these we were familiar with and this is the env. Now\n1:33:50 just a few points. You could have been saying okay wait hold on I thought we\n1:33:55 were about to do a langraph stuff. Why is the lang chain stuff here? Now you must know that langraph is built on top\n1:34:01 of lang chain and lang chain already has the sophisticated libraries right so why\n1:34:07 not actually use them that''s how langraph is designed it''s designed to use the robust sophisticated libraries\n1:34:14 which lang chain offers right so no I''m not a trader we''re still doing langraph stuff but we''re also using leveraging\n1:34:21 lang chain strengths as well okay and um now this env file now it''s okay if you\n1:34:27 haven''t ever um encountered av file before. Essentially, it''s just a file used to store secret stuff like API keys\n1:34:34 or configuration values. So, it''s really there for security purposes. Now, I have\n1:34:40 my own um file stored in my folder structure uh\n1:34:45 so that you don''t see my API key uh because if you do then I would go bankrupt. So, that''s why. Now, you might\n1:34:53 also be wondering why do we need an API key here? We need the API key because\n1:34:58 we''re doing calls to an external LLM. If we were using our own LLM like through\n1:35:05 OAMA, then we would um not have an API key, right? We would just use like the\n1:35:10 Olama library integration with lang chain. So because we''re using charges,\n1:35:16 we need an API to communicate with the LLM in their cloud servers. Cool. So how\n1:35:21 do we actually load this? So to load our um API, we just use a simple Python uh\n1:35:28 code load. Env. All right. So now that we''re at the same level, let''s actually code up our AI agent. Cool. So let''s\n1:35:36 define the state like we always do. So this time class agent state type\n1:35:42 dictionary. Perfect. Okay. Now what are the attributes we need in this section\n1:35:48 uh in this uh state? Well really just one the messages part right so messages but what form will it be well it will be\n1:35:55 in the form of a a list of human messages right so we''ll have list human\n1:36:01 message why because we when we invoke the graph we''re inputting human messages\n1:36:07 right so to tell the large language model that this is a human message I i.e\n1:36:12 Uh this is a message from me the user aka human right. Um we need to actually\n1:36:18 mention human message that it''s a human message type. Cool. Okay. So now we\n1:36:24 actually initial initialize the large language model. So we just write lm is equal to chat openai. And now we specify\n1:36:32 what model we want. Now I''m going for GPD4er. Now yes there''s also chat uh\n1:36:39 anthropic. I think there''s chat oama. Um there''s a lot of like in-built um\n1:36:45 libraries which lang chain offers which is great. Personally I''ve used chat openai a lot. I''ve also used chat\n1:36:52 anthropic a lot as well. Uh personally I like chat openai cuz it''s just really simple to use. I''ve also used tried well\n1:36:59 tried to use chat oama before but really there''s some difficulty in integrating\n1:37:05 it with lang. So that''s why I''ve opted for openi. And if you''re worried about financial cost, don''t worry, it''s\n1:37:11 extremely cheap. Uh if you want, you could also go for the GPD 40 mini model as well if that''s a concern. But trust\n1:37:18 me, it''s extremely cheap. Like the input tokens, output tokens is like in like\n1:37:24 tens of pennies for like a,000 tokens. So really, really cheap. Okay. So now\n1:37:29 let''s actually define our node through our function. So process and we obviously define the\n1:37:36 state and then return the state like so. Perfect. Now\n1:37:42 how do we actually call the lm? Now lang chain and the langraph team really like\n1:37:48 using the word invoke. You might have noticed that to call a graph or like to make the graph run we''ve used invoke.\n1:37:54 Similarly to run the lm we use invoke as well. So we okay let''s store the\n1:38:00 response we get in a variable. So uh lm.invoke and what do we invoke? Well,\n1:38:06 you can see from the uh hints here that it requires an input of language model input. What''s that basically saying is\n1:38:13 what what what do you want the LM to do? Right? What''s your question? Now what is our question? Well, that''s in the\n1:38:19 messages. So we write state messages. So what will happen here is as soon as I''ve\n1:38:24 written state messages, let''s say I have written hi or whatever uh we will pass\n1:38:30 this to the LLM through the invoke method. The LLM will then generate a response from its cloud server through\n1:38:37 our API and it''ll get it will give us back the um its response and then we''ll\n1:38:43 store it in the response section uh the response variable. Cool. And um let''s\n1:38:48 actually print this like so and return the state like\n1:38:55 such. Okay, done. Now we obviously need to create the graph like such.\n1:39:02 Okay. So uh what is it saying? Well, it''s saying that there is we''ve created\n1:39:08 a node process which is that which where the action is the process function. The\n1:39:14 add we''ve added an edge. We''ve added an edit from the start to the end node end point and we''ve compiled the graph.\n1:39:20 Okay. Um yeah. So let''s now ask the for\n1:39:25 the user input. So user input is equal to input. We''ll say enter\n1:39:32 something. And now we will invoke the agent cuz we need to invoke the agent of\n1:39:39 course because we''re creating a graph, right? And the graph is well like agent\n1:39:44 in this case. Cool. Let''s actually run this code now. So, Python\n1:39:50 agentbot. py and perfect. So, enter. Let''s say\n1:39:57 hi. The AI message was hello, how can I assist you today? Now, I can reassure\n1:40:03 you I did not pre-code this or hardcode this. This is the actual LM. Let''s run it uh one more time.\n1:40:10 Let''s come up with a different message like who are\n1:40:15 you and it''ll say I''m an AI language model created by open AAI called chat GBT. So you can this basically pretty\n1:40:22 much confirms that yes this is GBT uh in the background. Okay, but\n1:40:27 um why why just stick to one message, right? Why not uh be able to run\n1:40:33 multiple message like asking multiple messages kind of like a chatbot, right? So this is the code which does this and\n1:40:39 I''ll walk you through this what''s happening here as well. So uh like before we input our query and now we\n1:40:47 basically say keep iterating through and as soon as the user has said like exit\n1:40:52 or something then uh well you exit the while loop and that basically signifies that well you don''t want to talk to the\n1:40:59 ailm anymore. So let''s have get run this. So python agentbot\n1:41:05 py. Okay let''s say hi again. Hello how are you? But now we can run it again.\n1:41:11 It''s just a simple y loop. It''s nothing groundbreaking. So like who made you? Okay, perfect. What is 2 + 2? 2 + 2\n1:41:20 equals 4. Okay. Uh let''s say now, hi, I\n1:41:25 am Bob. Okay, now watch this carefully. I''m\n1:41:32 about to ask what did I just well or I should say what is my\n1:41:39 name? I''m sorry, but I don''t have the ability to know your name or any personal\n1:41:45 information about you. Why is that? Why didn''t it know what my name\n1:41:52 is? Well, even though I clearly specified it. So, let''s quickly\n1:41:57 exit. Okay, now this is important. Nowhere in the code have we actually\n1:42:02 created some sort of memory. That''s why I called this subsection simple bot. And that''s why I\n1:42:09 kept on saying AI agent because it''s not even an agent yet. Uh it''s just a simple\n1:42:14 like the most basic LLM wrapper you can possibly have. But at least now you know\n1:42:20 how to actually integrate um LLMs in your graphs, right? And it''s\n1:42:26 pretty straightforward. You just you um you just uh embed them within your functions and then your functions\n1:42:33 obviously act as the actions in your notes. And that''s it. It''s quite an easy piece of code. Like it''s only what 29\n1:42:40 lines or 25 lines give or take. Uh but yeah, pretty simple. Um I don''t think\n1:42:46 there''ll be any exercise for this cuz well there really isn''t much to do with this. So I will see you in the\n1:42:53 introduction for the second AI agent we''re going to build. Okay. So I''ll see you\n1:42:58 there. Cool. Cool. Cool. Okay. So now we''re going to build our second AI system. And we''re going to try to fix\n1:43:05 the problems we faced in the last uh AI system we built. And what was the\n1:43:11 problem? Well, the problem was it doesn''t remember what we in what we had said before, right? Why? Cuz we were\n1:43:18 calling separate API calls. So now we''re going to try to create a chatbot with some sort of memory. That''s why I\n1:43:24 included the brain emoji here. So let me walk through the objectives for this uh subsection. So, we''re going to use\n1:43:30 different message types in particular in particular the human message and the AI message. We''re going to maintain a full\n1:43:37 conversation history using both of these message types. We''re going to particularly use the GPD4 model again\n1:43:44 using the lang chains uh chat open AI library and overall we''re going to\n1:43:49 create a sophisticated conversation loop. So, what is the main goal goal of this um subsection? It''s really to\n1:43:56 create a form of memory for our agent. So if you''re ready, let''s go to the\n1:44:02 code. All right, awesome people. So let''s begin coding our simple chatbot\n1:44:07 then. Okay, so like last time, I''ve already imported all of the uh necessary\n1:44:12 libraries and it''s largely this exact same except now I''ve added two more uh\n1:44:17 stuff. So the first is the AI message and I explained this in the introduction of this subsection why we need the AI\n1:44:25 message. And I''ve also imported the union type annotation. Now the union\n1:44:30 type annotation is something we covered in the first chapter. So if you if this is the first time you are looking at it\n1:44:35 or hearing about it, I would recommend you going to the first chapter really understanding and watching the first two\n1:44:41 chapters. They''re quite short to be honest and then coming back. Okay. Now\n1:44:47 that being said, let''s actually begin the uh coding then. Okay. So like always, we define the state. So class\n1:44:55 agent state uh typed dictionary. Perfect. Now last\n1:45:00 time what did we define this as? Again we''re only going to have messages again but uh last time we had list human\n1:45:08 message. Okay so that was what we had defined as our agent state\n1:45:16 previously. Now this time we also want to include the AI message as well. We''re building a more sophisticated chatbot.\n1:45:23 So how do we do that? Well, one way or the naive way is to really have it as\n1:45:30 messages AI list AI message or something like\n1:45:35 that. Something like that. And don''t get me wrong, this is still a valid approach. You can still build your graph\n1:45:41 and everything like that, but um I think it''s a bit longer. So let me tell you\n1:45:47 another way which would actually be better. So remove this. Instead, let''s\n1:45:53 use the type annotation union like this. So, union like so. And let''s include AI\n1:46:02 message. Now, what has this done? Essentially, let me first tell you about a bit about human message and AI\n1:46:09 message. Human message and AI messages and like all of these like different structures are actually data types in\n1:46:14 Langraph and Langchain. That''s what the developers of these libraries have got them to be. And um when I write union\n1:46:23 human message AI message then that basically allows me to store uh either\n1:46:29 human messages or or AI messages in this uh key in the state the messages. So\n1:46:35 that''s what that literally means in a nutshell. Now, one important thing which I want to mention is this. All of these\n1:46:42 AI agentic libraries like Langchain, Langraph, Crew AI, Autogen, they''re all\n1:46:48 great, but um you really can make your own AI agentic system by writing just\n1:46:55 Python functions. You don''t even need to use a library. Now that being said, I\n1:47:01 would still recommend using these libraries, especially Langraph because Langraph, well, because it''s a personal\n1:47:07 favorite, no bias at all, but um it''s Langraph really allows you to control\n1:47:14 much more than other libraries would. Obviously, not as much control as if you\n1:47:19 were writing the Python functions yourself and everything, but I think it''s a good balance of how much control you have and how much uh unnecessary\n1:47:26 jargon you need to write. Because think about it, think about how much of um this unnecessary code which you would\n1:47:32 have had to write else otherwise uh langraph and lang chain support. So that''s why I highly recommend using\n1:47:39 these libraries and everything. So again human message and AI message are data types inbuilt data types within uh lang\n1:47:46 lang graph and lang chain. Cool. Okay. Now let''s again initialize the large\n1:47:52 language model as we did last time. And again we''re only we''re using GP4. Okay, now we''re going to create our\n1:47:58 node. Again, it''s going to be the exact same graph structure, by the way. So,\n1:48:03 state agent state, but obviously the actions we perform will be slightly\n1:48:08 different. Now, um let''s write a dock string. This node will\n1:48:15 um this node will uh do solve the request you input\n1:48:23 something. Dog strings aren''t really needed for this AI agent or this subsection because we''re not going to\n1:48:29 use them. But um again, good habit. Okay, cool. Let''s invoke\n1:48:36 this. So what have I done here? This is exactly the same code which we did in the previous subsection. The l we invoke\n1:48:42 the lm uh with the state messages. And what are the state messages? Well, it''s could be either human message or an AI\n1:48:49 message. It''s a list of those. Awesome. So now we write this piece of\n1:48:56 code. Okay. State messages.appen AI message content equal to\n1:49:02 response.content. What on earth is happening there? Okay. Let''s break this down. Response. Well, that''s just\n1:49:08 extracting only the content part of the response aka the response being the answer or the result after we make the\n1:49:15 uh API call from the large language model. And it only extracts the content. So it only extracts like the important\n1:49:21 stuff, right? It removes all the unnecessary jargon which comes with it like the amount of tokens you use and\n1:49:26 all that. It removes that and that''s uh gets stored in that gets converted to to\n1:49:32 an AI message and that''s appended to our state messages um in the state. Simple.\n1:49:39 Okay. Uh now obviously to make it look pretty in the terminal we''re going to\n1:49:45 print this and then we''re going to return the state. That''s it. That''s how simple it was.\n1:49:52 Okay, so now we''re going to create this exact same graph. And that''s why I''ve just copied and pasted it because it''s a\n1:49:58 time waste of me rewriting the code in front of you again and again. So we can just reuse the same code because it''s\n1:50:04 the exact same graph uh graph structure as the previous subsection. Cool. Okay,\n1:50:09 now we''re going to now here''s where it actually starts working differently.\n1:50:14 See, last time we didn''t have this the conversation history. really this is\n1:50:20 what''s going to be our memory in in um this uh setup. Okay, so we have now in\n1:50:26 initialized conversation history. Now again we''re going to ask the user what they want, right? What''s their request?\n1:50:35 So now we use this Y loop and this Y loop was the exact same loop we had in\n1:50:40 the previous subsection as well. uh it only exits unless if the user has uh\n1:50:46 inputed well exit obviously but now look at this the conversation history is\n1:50:52 appended with the human message and the human message is obviously the user input the reason I''ve kept on writing\n1:50:59 content is because well that''s the parameter in human message as you can see here okay cool uh and we''ve invoked the\n1:51:08 agent what is the agent well the agent is the compiled version of the graph the compiled graph uh with uh the entire\n1:51:15 conversation history. Now this is important. The entire conversation\n1:51:20 history is sent, not just the current human message. So uh this will make more\n1:51:26 sense. Don''t worry, I''m um trying my best to explain it right now, but obviously it will make much much more\n1:51:33 sense as soon as I start running it. Okay? So bear with me if you didn''t fully understand that. Don''t worry.\n1:51:38 Let''s remove that for now. Uh, and then we replace the conversation history completely like wipe it with the\n1:51:46 result messages. Why? Don''t worry, it''s going to make sense as soon as I run it.\n1:51:51 And I think yeah, we should be able to run this now. So, let''s write python memory agent.py, which is\n1:51:59 the name of the file. Cool. Okay, let me just quickly write a hi just to see if the API is\n1:52:04 working. It is perfect. Okay. Hello. How can I assist you today? Uh, now I''ll say\n1:52:10 like, \"Hi, my name is\n1:52:17 Steve.\" Hi Steve, it''s great to meet you. How can I help you today? Okay, now\n1:52:22 remember from last time. Last time if I asked it, hey, who am I? It didn''t know.\n1:52:27 Do you think it will know now? Think about\n1:52:34 it. It does. you are Steve or at least that''s the name you''ve chosen to share with me. Uh and the rest is yes\n1:52:42 whatever. So it does know about what I have said but just looking at this code\n1:52:49 I guess you can try to like pick out okay how does it work and everything like why everything works like that but\n1:52:57 um I think we can add print statements and everything. So let''s try to add print statements now and see well how\n1:53:05 this is actually working. So let''s exit the program. Okay. Uh let''s add a print statement\n1:53:13 here. Let me include this. Cool. So what is this saying? So this print statement\n1:53:20 actually kind think of it like a snapshot of what the current state is. So as soon as it goes into a process\n1:53:27 note as just before it''s about to finish by returning the state, we also print the current state as well. And this will\n1:53:34 literally just output whatever is stored in the messages attribute within our state. Okay. So let''s clear. There we\n1:53:43 go. Let''s run this again. So hi, nice to meet you.\n1:53:49 Something like that. Now take a look at the current state. See it outputed hello, nice to\n1:53:56 meet you. How can I see you today? Why did it output that? Well, because in our process\n1:54:01 function it we''ve asked it we formatted it in a way so it says hey AI which is\n1:54:07 this part and the response or content is this part. Okay. Now this response or\n1:54:13 content is also what was stored remember how I said it''s stored in the um in a\n1:54:18 nice manner and was appended to our state messages. Now was it appended to the state messages? Yes. How do you know\n1:54:25 that? Because look at the human message. The human message was what I wrote which was hi nice to meet you. Uh forget the\n1:54:31 additional keyword arguments and response metadata cuz I didn''t provide any. Uh you don''t need to worry about that. The main part is this part the\n1:54:37 content. And then look at the AI message part. Uh it''s the content is equals to\n1:54:42 hello nice to meet you. How can I see you today? Notice how it''s the exact same thing as it was\n1:54:50 here. Okay. Now, now we''re going to go one step ahead and I''m going to ask it\n1:54:56 to say my name is Steve again. Now, think\n1:55:03 about how will this current state change? Pause the video and try to think\n1:55:08 about this like that. Okay. So, now the second\n1:55:16 uh message I sent was my name is Steve. Its response for the second message was hi Steve. How can I help you today? Now\n1:55:24 look at the current state. It still begins with hi, nice to meet you, which was the first ever message I inputed\n1:55:30 into this conversation and then its respective AI message. And then that gets uh appended. Why appended? Well,\n1:55:37 because we had appended the AI message and appended the human message. So that''s why. Okay. And you\n1:55:45 can see the human message is my name is Steve which is the most recent uh message which I put and then the AI\n1:55:51 message which is hi Steve how can I help you today? Perfect. And we can just keep\n1:55:56 going and going and going. But uh for now I''ll exit. Now here''s the thing. This works\n1:56:03 well relatively well, right? We''ve got it like as a chatbot which is what we\n1:56:08 wanted. it has some recollection of memory or or like of what we what we are who we are and everything. But there''s\n1:56:15 two big problems here. Let''s start with the first uh massive problem which is\n1:56:21 this. You know how I''ve exited the program right now. Yeah. Okay. I''m going\n1:56:28 to run the exact same program again. Now I told it that my name is\n1:56:34 Steve. What is my name?\n1:56:40 and it says uh I''m sorry I don''t have access to\n1:56:45 your personal data. Okay. And look at the current state completely wiped out. Again, that''s pretty self-explanatory as\n1:56:51 to why you exited the program and that''s why obviously all the var the data was stored in the variables, right? The\n1:56:57 state was stored in the variables completely got wiped away. So what is the solution? Think about it.\n1:57:06 Well, obviously one potential solution would be to store it in a database, like a large database, right? Or a vector\n1:57:12 database if you''re trying to do some ragged applications. For now, I''m just going to store it in a very simple text\n1:57:18 file. Why? Cuz it''s really easy. And I''ve got the code as well. And usually, honestly, I just store it in a text file\n1:57:24 when I''m prototyping. Now, yes, obviously, storing it in a vector database or a database is much more robust and sophisticated, and that is\n1:57:31 what you should do. But when you''re prototyping and you want to really see uh try to build it quickly and fast, uh\n1:57:38 I just tend to use a text file. It still works uh still works great and everything. So what is the code for the\n1:57:44 text file? Uh it''s here.\n1:57:50 Okay. So what is this code saying? Well, essentially it''s saying create me a text\n1:57:55 file called logging as a as a text file you see in write mode and a file. Write\n1:58:00 your conversation log. This is just to like make it look better and more aesthetic. But this is the main part of the code which um you should actually\n1:58:07 try and understand for every single message in the conversation history. Okay. Now note the conversation history\n1:58:14 was this variable which we had like initialized the conversation history uh stores the AI messages and the human\n1:58:20 messages. So that''s where all of the um the information outside the graph\n1:58:26 actually is. Right? the state is locked in within the graph now and uh the\n1:58:33 conversation history is just another I guess you can say a duplicate version of the state right because we''ve you we''ve\n1:58:40 appended the exact same human messages and AI messages and kept on updating it\n1:58:45 through this line cool so what it says is that for every single message in the\n1:58:51 conversation history by the way a conversation is the duration between my first message and the last message I\n1:58:58 sent that entire thing is a conversation. A conversation isn''t just a single API call. It''s the entire\n1:59:04 length. Okay? So, just to be mindful of that. So, uh it first checks if it''s a\n1:59:10 human message, it writes that as you and then extracts that content and if it''s the AI message, it uh puts it under the\n1:59:16 AI stuff. So, let''s run this again. So, let''s exit this. Clear this. Okay.\n1:59:25 Who? Okay, let''s say hi, I am Steve. Again, I''m using Steve because a\n1:59:31 Minecraft movie just came out, so that''s why. Okay, now I''m going to intentionally make a spelling mistake\n1:59:37 here. Good morning. It should obviously morning should have been spelled, right? But I''m doing that for a reason. It gets\n1:59:44 no problem. Let''s say tell me a joke. Just another random thing. Sure. Why\n1:59:49 don''t skeletons fight each other? They don''t have the guts. Um, really rubbish. My point is it\n1:59:57 works. Now I''ll exit the program. And now it says conversation saved to login.xt. Now look at\n2:00:04 this. Remove that. Let me remove that. Okay, perfect. So this is the conversation log. The first message I\n2:00:10 ever sent was, \"Hi, I''m Steve.\" There, it''s response. Now, good morning. Now, why did I spell it like wrong? Why did I\n2:00:17 do that? Because I wanted to show you that this is the actual human message, my message being stored unaltered. So\n2:00:25 whatever I pass in the state as a human message that stays there. So it''s unaltered. The AI message cannot or or\n2:00:34 anything can really change the previous human messages at all. Right? So that''s\n2:00:39 why. And you can see tell me a joke. Sure. It''s it''s a rubbish joke is after that. And that''s the end of the login.xt\n2:00:46 file. So that''s a really fast efficient way, not the most robust way of course,\n2:00:52 but it is a fast efficient way to be able to store your data outside um the\n2:00:57 application if it stops. Perfect. Now, what was the second problem that I was mentioning? It''s\n2:01:03 this. Look at how I initially I say, \"Hi, I''m Steve.\" I don''t know why I\n2:01:09 printed twice there. Weird, but whatever. Look at the current state length.\n2:01:15 Okay. Then I pass in another message, it becomes longer. I pass in another\n2:01:20 message, uh it gets longer. It keeps getting longer and longer and\n2:01:25 longer. That''s a problem because think about it, you will use these uh library\n2:01:31 uh you''ll use these like large language models whether it''s for your own AI agentic startup or your own mini Javas\n2:01:36 or your own projects or whatever it is. You would obviously want to minimize cost, right? But using so many tokens uh\n2:01:45 using so many tokens like input tokens especially will really eat away uh your\n2:01:51 uh the amount of money you will uh spend like it will drastically increase it and\n2:01:57 that''s a huge problem right we want to try to be conservative a bit about our money and our financial our finances so\n2:02:04 what is one what is a way to solve this think about\n2:02:10 that Well, right off the bat, I can give you an easy solution to implement which is within the code, write it, write some\n2:02:17 code where if the number of human messages exceeds five or something like\n2:02:23 that, then you remove the first human message in your uh history. Why remove\n2:02:29 the first and not the latest? Well, because the latest is most likely to be more relevant, right? The first message\n2:02:35 could is most likely to be the one where um the one which is well not needed or\n2:02:42 it could have been like it can it has more of a chance to be like a bit more less of an impact to have been removed.\n2:02:51 So why did I pick five? Well, five is just a random number I thought of. You could do 10, 15, 20, 25, three,\n2:02:58 whatever. But that''s a really easy solution to do. Okay, so we learned\n2:03:04 quite a lot there. We learned how to integrate human message and AI message within a thing. And now we''ve created a\n2:03:11 somewhat of a sophisticated chatbot, right? It has a memory. It still talks to us. We if we define uh if we write\n2:03:19 who we are, it remembers that and everything and it works great. Obviously, it has its flaws, but for now, it''s pretty good. Okay, so now\n2:03:26 we''re going to build our third AI agent. And this is going to be a special type of an AI agent.\n2:03:32 The technical term for this type of agent is called a react agent and react\n2:03:37 stands for reasoning and acting. So this is a quite a common type of AI agent\n2:03:43 which you will build. So how does it look like? Well, it looks something like this. So it''s quite simple. There''s a\n2:03:51 start point and then it''s an end point obviously. Then you have your agent and then we use a loop where we attach it to\n2:03:58 tools. Now, this could be one tool, two tools, a lot of tools. And it''s the agent''s job or the LLM in the background\n2:04:06 to be able to decide which tool to select. But not only that, it''s all it''s also its job to be able to decide when\n2:04:13 there''s no more tool calling left to do. And when that happens, it goes to the end part. So that''s the general gist of\n2:04:20 what a React agent is. It''s a very very common and famous type of agent to make in Langraph. And that''s exactly what\n2:04:26 we''re going to be building in this subsection. So what are the objectives? So to build a react agent, the\n2:04:32 objectives are learn how to really create tools in langraph. We''re going to be creating a react graph. Of course,\n2:04:39 we''re going to be working with different types of messages such as tool messages. See, last subsection we''ve we covered AI\n2:04:46 messages, human messages, but now we''re going to look at a lot more types of messages. for example, tool messages,\n2:04:52 system messages, base messages, and we''re obviously going to test our robustness um of our graph. So, the main\n2:04:58 goal is to create a robust React agent. So, if you''re excited, I''ll see you at\n2:05:04 the code. Okay, people. So, now we''re going to code up the React agent. And just a\n2:05:11 heads up, this is going to be quite a long subsection. So, get ready. You can see it''s going to be long because of how\n2:05:17 many imports I''ve done. But because I''ve done so many new imports, I actually want to take some time off and really\n2:05:22 explain each line so that we''re all on the same page. Okay, let''s go. So the first line is from typing import\n2:05:29 annotated sequence and type dictionary. Now we obviously know what a type dictionary is, but we haven''t come\n2:05:35 across annotated or sequence yet. So these are also type annotations. And I''ll start off by explaining what an\n2:05:41 annotated is. So annotated is a type annotation which provides additional\n2:05:47 context to your uh variable or your key without actually affecting the type\n2:05:52 itself, the data type itself. Now what exactly does that mean? Well, I''ll give\n2:05:57 you an example. Let''s say I am trying to create a type dictionary where there is\n2:06:02 an email key in it. Okay? Now, obviously an email is string. So if I was to create a type dictionary, I would have\n2:06:09 written email colon string, right? Sl. That''s how we''ve been doing it. But here''s the thing with certain keys like\n2:06:16 email, they have to be a certain uh format. But like for example, it has to\n2:06:22 be like abcgmail.com for example. But if I pass in\n2:06:28 abcd-gmail.com or something like that, that''s not a valid email format anymore, but it''s still a string technically. So\n2:06:34 it would pass through. So how do we resolve that? Well, that''s where annotated comes in. And I''ll give you an\n2:06:40 example here. Let''s say email is equal to annotated. I''m not going to create the whole type dictionary to save time.\n2:06:46 But for uh the example itself, you first pass in what data type you want it to\n2:06:51 be. So we want email to be a string, right? That''s not changing. But here in quotation marks, I provide some more\n2:06:56 additional information, additional context. And this is basically adding onto the metadata of this key or\n2:07:04 variable. For example, uh let''s say this has to be\n2:07:09 a valid email format. Now, obviously, I should do it\n2:07:15 in more detail, but um that''s how uh for now, that''s fine. So, how how can I\n2:07:22 actually see the metadata? So, if I want to, I would write print email metadata\n2:07:29 uh like that. And then I would press run. And here we go. You can see this\n2:07:35 has to be a valid email format. That''s the exact same thing which is which we wrote here. So that''s annotated done.\n2:07:42 But what about sequence? What does sequence mean? Well, sequence is also a type annotation. And the way I''ve\n2:07:48 described it is here. It basically automatically handles the state updates for sequences such as by adding new\n2:07:54 messages to a chat history. Now what does that mean? Well, it''s really just\n2:08:00 there to avoid any list manipulation to the graph nodes. Obviously, like when we''re using graphs and nodes and all all\n2:08:07 of that stuff and updating the states, there''s a lot of uh list manipulations which we''ll have to do. Sequence really\n2:08:12 handles a lot of that. So, that''s really what it''s there for. You don''t really need to uh worry about it too much.\n2:08:18 Okay. Now, if we continue, we have env uh from import loadenv. From last time,\n2:08:26 we know that this is just to store our API keys and I''ve done that here. That will load the API keys. But you''ll see\n2:08:32 now these three uh imports. We''re importing some new message types here.\n2:08:37 So we''re importing base message, tool message, and system message. I''ll start off with the tool message. So it''s\n2:08:43 essentially a type of message where where the data is passed back to the LM\n2:08:48 after the tool has been called and like the information which is passed is like the content itself, the tool call ID. Uh\n2:08:56 that''s what tool messages. It''s pretty self-explanatory. Now for a system message, it''s the it''s a message for\n2:09:02 providing instructions to the LLM. So like for example, if you''ve used uh LLM\n2:09:07 APIs before, you might have written you are a helpful assistant. That''s exactly what a system message is. And don''t\n2:09:13 worry, we''re going to code this up as well. So you''ll actually see what they are. Now what''s a base message? So in\n2:09:19 the comments, you can see that I''ve written the foundational class for all message types in Langraph. Now here''s\n2:09:25 how this works. Think about the class hierarchy. So you know how you have a parent class and then you have child\n2:09:31 classes as well. Well the base message would be the parent class and these uh\n2:09:37 AI message, human message, tool message, system message and all these other types of message will be like the child\n2:09:42 classes and they will inherit all the properties of the base message cuz that''s the parent one. I guess you can\n2:09:48 say the uh all father or something but the AI message, human message and all\n2:09:53 these child uh classes obviously they''ll have their own properties, right? For example, the tool message has its own\n2:09:58 content and tool call ID and all that stuff. So that''s what the base message is. It''s really the foundational class\n2:10:04 for all the message types in Langraph. Cool. Okay. So now if we\n2:10:10 continue, you can see we''ve imported chat, openAI. Uh we''ve done state graph and M. These we''ve come across. We know\n2:10:16 what they are. And we''ve imported tool and tool nodes which we cover in the second chapter or second section of this\n2:10:23 course. uh these are different elements which we''re going to be uh using in langraph. Now what about this line from\n2:10:29 langraph dossage import add messages. Now what does that mean? So this is a\n2:10:34 little bit um different. This add messages is a reducer function. Now if\n2:10:41 this is the first time you''re hearing that don''t panic. It''s not that hard. So let''s let me copy this one second. Okay.\n2:10:49 So a reducer function is essentially just a rule that controls how updates from nodes are combined with the\n2:10:56 existing state. In simpler words, it really just tells us how to merge new data into the current state. Now here''s\n2:11:03 the thing. If we didn''t have some sort of a reducer function, uh updates would have just replaced the existing value or\n2:11:09 state entirely. And I''ll give you an example for this.\n2:11:15 So let''s say I had a state where it was just high. I had one attribute messages\n2:11:21 and high. Now obviously I should have created the type dictionary and everything and formalize it but just for\n2:11:26 simpler um for times saving purposes I''ve done it like this. Now what if I\n2:11:32 had an update which says nice to meet you. If I didn''t have a reducer function that would completely overwrite it. Now\n2:11:38 in the previous uh graphs and agents we''ve made we''ve appended it but now that we''re using so many different\n2:11:45 messages and calls and tool calling and what whatever we can''t really always append\n2:11:50 everything like it will get far too complicated. So that''s why we need to leverage reducer function. So if we\n2:11:56 didn''t use a reducer function it would just overwrite it completely. But if we did like hi nice to meet you it would\n2:12:03 append it. That''s the key. So in a nutshell, the reducer function really\n2:12:08 just aggregates the uh data in the state. This reducer function uh and the reducer function which I''m talking about\n2:12:13 is add messages. So once again, add messages is a reducer function that will\n2:12:19 really just allow us to append everything into the state without any overring happening cuz so we want to\n2:12:26 preserve the state. Okay, cool. So now let''s actually code this uh react agent.\n2:12:32 Okay. All right. Okay. Okay, I''ve cleared the screen now and let''s actually begin like we how we always\n2:12:38 begin with the uh creation of our state of our agent. So, type dictionary like\n2:12:44 such. Okay. And now we we''ll only have one key in this uh in this example which\n2:12:52 is just messages. And now let''s use the new type annotations we''ve learned. So,\n2:12:58 sequence base message and reducer function add messages. So again this\n2:13:05 piece of code is saying to preserve the state by actually appending it rather\n2:13:10 than overwriting. That''s what this reducer function does. Okay. All right.\n2:13:15 Okay. Oh, and the sequence of base messages is the data type and this\n2:13:20 provides the metadata. That''s why we have the annotated keyword here. That''s really it. Okay. Uh now let''s create our\n2:13:28 first ever tool. Now how do we do this? Some of you who have a um who have come\n2:13:33 from lang chain might know how to do this already. We use a decorator and we\n2:13:39 define like this. Now this decorator basically tells Python that this function is quite is special. It does\n2:13:45 something and well it is special because it''s a tool which we''re going to use. So let''s define our tool as def. Let''s\n2:13:52 create a simple addition tool. Okay. So we''ll say a integer b integer.\n2:13:59 It''s basically going to add two numbers. And this is where doc strings actually come now. And I''ll show you how important they are. For now, let''s say\n2:14:06 uh this is an addition function that adds two numbers together.\n2:14:15 Okay. All right. And we just return a plus b. Simple. Now, how can we actually\n2:14:23 infuse these tools to our large language model? Well, first let''s create a list.\n2:14:30 Add like such. Now, yes, at this current moment, I only have one tool, but in a\n2:14:36 few moments, we''ll have multiple tools. That''s why I''m adding this uh list for now. And let''s actually create our\n2:14:41 model. So, model is equal to chat openai. Model is equal to\n2:14:48 GPT40. Again, I''m using GPD40 because I''ve never had a problem with it to be honest. So how can we tell our GPD40\n2:14:56 large language model that these are the tools you can use? Well, we can use this inbuilt Python um inbuilt function\n2:15:03 called bind tools. Bind tools like that. And we pass in the list of tools we\n2:15:08 have. So that''s tools. Pretty simple, right? Okay. So now large language model\n2:15:14 will have access to all of our tools. Okay. So now we need to create a\n2:15:19 node which actually acts as the agent within our graph. So how do we do that?\n2:15:24 Let me create just a simple function like def model call. We pass in the state agent state. Again it needs to\n2:15:31 return the agent state. Okay. Now I''m going to quickly copy this\n2:15:37 piece of code. Give me a\n2:15:42 second. Okay. So what is this code doing? uh you can see that we''re invoking the\n2:15:49 model aka running the model and this is the system message which we are asking.\n2:15:54 So we''re explicitly saying the large language model that you are my a system please answer my query to the best of\n2:16:01 your ability. So that''s what the large language model''s task is to do. Now if\n2:16:07 we want to get technical here, you could have written it in a slightly different way and that way is through\n2:16:15 this. So remove this. We could have said system\n2:16:22 prompt. Okay. So what''s going on here? Remember how I said system message is also something which we imported. Uh so\n2:16:29 the system message like I said is this line of uh is this line. You are my AS system. Please answer my query to the\n2:16:35 best of your ability. Now, either way would have worked if we if I had just straight up\n2:16:40 passed this string into here. That would have worked as well. Personally, I think this way is better. Even though they\n2:16:46 achieve the exact same thing, I think this way is better cuz it''s more readable. Okay? And you''re only adding\n2:16:51 just one more import. Okay? So, I would prefer you to I would really recommend\n2:16:56 you doing like uh like this so even the large language model knows that this is a system message. Okay, cool. And this\n2:17:04 is uh just another way of writing like the updated state. You know how we''ve been writing state uh brackets messages\n2:17:12 is equal to something something something. Well, this is a more compact way of updating the state as well. So\n2:17:17 return messages response. So we update the messages with the response. No plus\n2:17:22 equal to this this this or adding something we just we can simply just write it with the updated state. Why?\n2:17:29 because the add messages aka the reducer function handles the appending uh for us. It doesn''t overwrite it. Now, if I\n2:17:37 ran this code and I built the graph and everything, would it work? No. Why it wouldn''t work? Because\n2:17:45 think about it, the response when we''ve invoked the model and we store it in the response when we actually invoked it, we\n2:17:52 didn''t actually pass in the query. How do we pass in the query? Think about it.\n2:17:58 All I passed is my system message. Where does the query go? So to be able to add\n2:18:03 the query, I actually have to add like this. So state messages. The query it\n2:18:10 will be in the form of a human message. And the human message will be stored in the messages attribute, right? And now\n2:18:16 that we''ve passed that into our model as well and we can invoke it. And now this\n2:18:21 should work. Okay. Okay. Okay, so now we define the\n2:18:28 conditional edge. Now why do we need the conditional edge here? I''ll put the picture of the react agent. Again here\n2:18:34 you can see that the looping part even like in the last one in the graph when we made the loops for the first time you\n2:18:40 saw that was it was a conditional edge which we had to use and now that''s actually going to come in play here.\n2:18:46 That''s why I took so time to build those graphs because now the concept is coming. So how do we define the\n2:18:53 conditional edge def should\n2:18:59 continue. Okay. So again like always we pass in the state and let''s do it like\n2:19:09 this like such else\n2:19:14 return continue. Okay. So as you know as you um might have guessed end and\n2:19:21 continue will be edges which I''ll define later in the graph. But what is going on here? Well essentially when I''ll pass in\n2:19:28 the query uh when we''ve invoked the actual model you will know that we''ll create a list of tools right so what\n2:19:36 we''re going to be doing is we''re going to be uh getting the last message and we''re going to see if there''s any more\n2:19:42 tools needed to be ran. If there are then we''ll go into the continue edge aka\n2:19:48 we''ll go to the tool node and we''ll select the tool and we''ll do all this uh actions and then come back. If there''s\n2:19:54 no more tool calling left we will just end and we''ll just exit the uh graph and\n2:19:59 that''ll be the case. You''ll get uh uh you''ll understand more what I what I mean when we''ve actually test we''re\n2:20:05 testing and running the graph. Okay. Now let''s just define the graph. So like\n2:20:11 always we create the graph we initialize the graph through the state graph and let''s call the node R agent. So the\n2:20:17 action will be the model call aka the underlying function will be this. Okay. Now we create something called a\n2:20:25 tool node which is also what we covered in the previous uh in the second section\n2:20:30 or second chapter in this course. The tool node essentially is just a singular\n2:20:35 node which contains all of the different tools. So we only have one tool. If you see what this variable is tools, we only\n2:20:41 have one tool which is add. Don''t worry, we''ll add some more tools like subtracts and multiply in a bit. But I just want\n2:20:47 to uh like really solidify your concept of how we can add these tools and how the graph will work. Okay. Now we\n2:20:55 obviously set our entry point and point it to the R agent. Now let''s add our conditional\n2:21:01 edge. So remember remember how I said there''s two edges, continue and end. again continue and end and if it goes to\n2:21:09 the end we end it. If it goes to tools then we go to tool node which is tools. Okay. Okay. So yeah this is\n2:21:17 pretty straightforward still. Right now we also need to add an edge which goes back from our tool to our agent cuz\n2:21:24 that''s how we''re going to create a circular connection. Right? You can see that the conditional\n2:21:30 edge only provides a one-way directed edge from the from either the agent to\n2:21:36 the tool node or the agent to the endpoint. But we need another edge which will go back from the uh tool node back\n2:21:43 to the agent. And that''s what this um that''s what this edge does. And lastly, we need to uh obviously compile it. So\n2:21:50 we''ll just say app is equal to graph dompile. Perfect. That''s it. Now I''ve\n2:21:57 just created a a new helper function here which this isn''t part of langraph.\n2:22:03 I''ve just written this code because it will make every like the tool calling and everything uh like output in a much\n2:22:09 better way. So you''ll see what I mean in just one second. Okay. So now we\n2:22:16 actually can begin. So let''s say the input is uh something like this.\n2:22:24 Let''s say I want to add 3 + 4. Okay, simple. And this line of code basically\n2:22:31 streams the data. That''s all it does. So, let''s actually run this. Clear. And\n2:22:36 let''s do it. Okay. So, remember we wrote add\n2:22:41 3+4. Wow, look at that. So, we''ve added 3+4. It calls the tool and it even knows\n2:22:47 what tool to pick. Add. Um, and it gives us the result. The tool message as you\n2:22:52 can see is seven and the AI message the final AI message is the sum of three and four is seven. That''s it. Let''s try\n2:23:00 something harder now. Let''s write add 34 + 21. So if we run\n2:23:08 this you can see 55 cuz 34 + 21 is 55. And you can also see again all the tool\n2:23:14 calls and everything that''s done right. I want to show you one two more things actually before we add some more tools\n2:23:20 which is this. If I remove this dock string here by commenting out for\n2:23:25 now. Let''s clear and let''s run that\n2:23:30 again. Error. Why? Because the function must have a dock string if description\n2:23:36 is not provided. The dock string is necessary. That''s why included otherwise the graph won''t work. It''s remember it\n2:23:43 tells the LM what that tool is for. Okay. So now that we''ve have uh we''ve\n2:23:49 got that there, let''s try this as well. Add uh 3+ 4. Again, this time I want both of\n2:23:58 them to be executed. So clear now. Do you think this will work? Let''s\n2:24:05 see. Add 34 + 21. Add 3+ 4. Perfect. Brilliant. Okay. So you can see the\n2:24:11 result of adding 34 + 21 is 55. The result of adding 3+ 4 is 7. You can see how the tool was called twice this time\n2:24:18 and that''s the power of the loop which we created. Remember we created the conditional edge and then we also\n2:24:24 created that directed edge back from the tool node to the agent. Let''s let''s try to make it even uh give more um\n2:24:31 complicated stuff. Let''s say add add 12 + 12 something. So let me\n2:24:38 clear this. Clear. Let''s see what happens.\n2:24:46 Wow, look at this. If I press enter, sorry, I messed up there. But you can\n2:24:53 see the results of the addition as well as 34 + 21 is 55 7 24 or and you can\n2:24:58 also see that I called the tool the sorry the AI called the tool three times.\n2:25:04 Now these tool calls is also an indication that the LLM didn''t use its own like information inbuilt information\n2:25:11 which it was trained on to come up with an answer. Right? Remember an LLM doesn''t know how to do maths. It just\n2:25:16 guesses the next output like through probability. But through this we were\n2:25:21 able to actually add the two numbers. So an important concept here is\n2:25:27 the LLM actually decides what should be passed as the arguments to each tool. So\n2:25:33 3 + 4 like if I said add 3 + 4 it will actually uh create it will actually uh\n2:25:39 input the numbers 3 and four and then this tool will handle the uh information return it and it will go back to the\n2:25:44 agent and then the AI agent will decide what''s the answer and everything. So that''s how it works. Awesome. Okay. Now\n2:25:50 let''s make this even more complicated. Let''s add some more tools. Let''s add\n2:25:58 subtract and multiply. Okay. And the only re change we have to do is instead\n2:26:05 of this one line we just now include subtract and multiply as well. That''s\n2:26:11 it. Otherwise this line this code largely stays the same. Now let''s\n2:26:16 actually run this same command and see if it gets confused with the different\n2:26:21 um different tools we have it has access to. Now let''s see. Okay.\n2:26:29 You can see again that 55 724. Okay, it didn''t get confused. Perfect. Let''s now give it a\n2:26:36 different command. Let''s say something like this. One\n2:26:44 second. Add 40 + 12 and then multiply the result by 6. So now it has to make\n2:26:49 use of two different tools. Let''s see if it gets that.\n2:26:55 Okay. Wow. Brilliant. So it first used the add tool and then used the multiplication tool and you can see all\n2:27:02 the queries or all the tool called and everything and the final answer is 312. So 52 * 6 yes it is 312. Okay. Wow that\n2:27:10 works like brilliantly. So now that we know that this is robust what about if I\n2:27:16 add this let''s say also tell me a joke\n2:27:22 please. What do you think will happen? Do you think this will break? Let''s see.\n2:27:28 If I play this and run this. Let''s\n2:27:37 see. Wow. Look at this. The result of 40 adding 14 and 12\n2:27:43 is 52 multiplying that is by 6 is 312. And here''s a joke for you. Why don''t skeletons fight each other? They don''t\n2:27:49 have the gun. I swear to God, it''s always the same dead joke. But you get the point. This is so robust. It can\n2:27:55 handle even queries where it doesn''t even need a tool and that ladies and gentlemen is the power of langraph. So\n2:28:01 it''s it''s so robust even if we don''t need to use a tool it will still give us an answer and the reason it was able to\n2:28:09 do that is once all the tool calling is done it passes it to the agent the agent checks again oh I need to tell it I need\n2:28:16 to tell the user a joke as well and adds that to the final information and then ends it that''s the power of\n2:28:22 lang okay so after all of that we finally now know how to create a react\n2:28:27 agent yes it was a simple react agent but the concepts the same. You can create your own external tools from now\n2:28:33 onwards and you can create your own graph. And that was the whole point of this course, right? For you to actually\n2:28:38 understand how we can uh create these um how we can use different tools and then the rest is up to you. It''s up to your\n2:28:45 imagination. Okay, perfect. So now I will see you at the next subsection. All\n2:28:51 right, see you there. Okay, people. So we''ve made great progress so far. So\n2:28:57 well done on that. But now we make a fourth AI agent. And this time we''ll do\n2:29:02 things again slightly differently. Well, this time we''re going to be making a mini project together. So the project''s\n2:29:10 name is going to be called Drafter. And you''ll see why in a minute. So picture\n2:29:15 this. Me and you are working in a company together. And our boss comes up\n2:29:20 to us and she has a problem and some orders for us. So the problem is this.\n2:29:26 Our company is not working efficiently. We spend way too much time drafting documents and this needs to be fixed.\n2:29:33 Again, a valid problem. So, what are her orders? She says you need to create an\n2:29:38 AI agentic system that can speed up drafting documents, emails, etc. The AI\n2:29:44 agentic system should have human AI collaboration, meaning the human should be able to provide continuous feedback\n2:29:49 and the AI agent should stop when the human is happy with the draft. The system should also be fast and be\n2:29:56 able to save the drafts. Okay. So then me and you start discussing and we are\n2:30:02 going to use land graph obviously and we come up with a sketch. Now the sketch of\n2:30:08 our graph is something like this. It obviously is going to have a start and an end point and it''s going to have our\n2:30:14 agent and the agent will have access to tools aka the tool node. Now this looks\n2:30:20 similar to a react agent which we covered in the last subsection. But there''s a reason we don''t we haven''t\n2:30:26 chosen to do that. See we realize that one of the tools is the save tool. It\n2:30:31 will save the draft, right? That was one of our requirements. But obviously when we once we''ve saved it, the process\n2:30:37 should end, right? But if you remember from a React agent, the tools always goes back to the AI agent, not directly\n2:30:45 to the endpoint. And we don''t want that anymore. So that''s why as soon as the save tool is used because the save tool\n2:30:52 will be within tools right it ends. So that is the structure we have chosen to\n2:30:58 go with. So the only thing left is to actually code this graph. So let''s code\n2:31:04 this together then. Okay. So let''s actually code up this drafter project then. So you can\n2:31:11 see I''ve already done all the imports and I''ve loaded up my env file. Now all of these imports are imports which\n2:31:17 you''ve already uh encountered before. So there''s no point in looking at them again. But the first thing which I''m\n2:31:23 going to do is I''m going to be defining the a global variable. Now this global variable yes it''s a bit\n2:31:31 odd defining global variables and there''s a reason which I''ve done it and this will become more apparent as I go\n2:31:37 through the code but just as a heads up the reason I''ve defined a global variable in this case is to actually\n2:31:44 pass in a state in tools the the correct way to do it in langraph is through\n2:31:50 something called injected state now injected state is beyond the scope of this uh course so the workound on that\n2:31:58 is to use a global variable and what will happen is our tools will uh\n2:32:04 whatever updates are made uh we''ll update the global variable and then when we go on to save it uh the save tool\n2:32:12 will use the contents in this global variable and save that into a text file.\n2:32:17 So that''s why this is included. Okay. So now let''s define our agent state again.\n2:32:23 And the way that''s done is the exact same way we did last time. Uh class\n2:32:28 agent state messages annotated sequence base message add message as the reducer function. So now we define the tools and\n2:32:37 there will be two tools for this. The first tool will be the update tool and the second tool will be the save tool.\n2:32:42 So let''s start off with the uh update tool and I will obviously use the decorator and then create def\n2:32:50 update and then we need to pass in\n2:32:55 uh pass in a parameter content. Now just as a refresher whatever parameters you\n2:33:01 pass or you request who actually gets those parameters? Well, the LLM or your\n2:33:08 model in the background that''s uh what will automatically pass the parameters\n2:33:13 for this model uh for this tool. So, uh in this case the content parameter will be uh that will be provided by the lm in\n2:33:21 the background. So, you don''t need to worry about that. Okay. So, now we need the dock string obviously and I''ve just\n2:33:27 created a simple dock string which just updates the document with the provided content because that is exactly what it does.\n2:33:33 So now we define to interact with the global variable in Python, you obviously need to uh define it uh you need to code\n2:33:40 it like this and then you need to update your document content aka the global\n2:33:47 variable with your current content and then you just return again a statement\n2:33:53 to the like the large language model telling it that we have successfully updated it. So I''ve written document has\n2:33:58 been updated successfully. The current content is this which is the content which we store in the thing. Okay. So\n2:34:07 now we define our second tool which is the save tool. So again same decorator we use uh like\n2:34:14 this. And now we request the llm to give us a file name as well. So it will it\n2:34:22 should give us a suitable file name uh which will be a suitable file name for the text file and uh it will and now\n2:34:28 this save tool will automatically handle all the save logic. So uh as a dock\n2:34:35 string I pass like this. So saves the current document to a text file and finishes the process. And the arguments\n2:34:42 are file name which is the name for the text file. Now, I''ve specifically mentioned that we''re going to be using a\n2:34:48 text file so that the uh uh the LLM knows that the file name which it needs\n2:34:54 to pass has to have a txt in the end of it. Now, if it doesn''t uh by any means\n2:35:01 to make the uh graph to make the entire code more robust, I''ve also written this if statement such that if this file name\n2:35:07 doesn''t end with a txt, just put a txt there just uh as robust as measure. Now\n2:35:13 again we need to uh call the global variable again. So global document\n2:35:19 content. Okay. Now this next bit of code that''s this is not langraph. This is just uh whatever you put in the tool. Uh\n2:35:26 it it doesn''t have to it''s not going to be langraph related. Right. So this piece of code is just uh some code which\n2:35:34 allows you to save the uh contents a the the content store in in the global\n2:35:40 variable under the file name uh and as a text file and I''ve also added this\n2:35:45 exception uh which is a good thing for debugging purposes where it if there''s\n2:35:51 an error it will tell me exactly what the error is and then we can fix it. Okay. So hopefully there won''t be any\n2:35:57 errors. Now we create a list of tools which uh again will be update and save\n2:36:04 because we only have two tools. And now we actually call the uh model and how do\n2:36:10 we call the model like such? Now let me ask you a question. Is this it for the model definition or do\n2:36:18 we need something else? Well, there''s a reason I asked that question, right? We forgotten to do\n2:36:24 bind tools. So bind tools and tools. So that will do. Okay. Now we actually\n2:36:31 initialize the agent itself or the function which will cuz remember the agent will be a node in our graph. And\n2:36:39 what will be the function behind that? It will be this function which we''re about to define. So let''s write this as\n2:36:46 def r agent. And again we need to pass in the state the agent state and it''ll\n2:36:51 return the agent state. And okay so now this doc uh not doc\n2:36:58 string this we need to pass in a system message to our llm right now this llm\n2:37:03 this system prompt will be quite large so get ready uh like such so in this system prompt I\n2:37:11 have specifically said this is a system message and the content is this you are drafter a helpful writing assistant\n2:37:18 you''re going to help the user aka us to update and modify documents and I''ve also written some more stuff about what\n2:37:25 the uh update or what what to do if the user wants to update. We use the update tool. Uh we need to use the save tool to\n2:37:33 save it and to always show the current document say after modifications and all that stuff. Cool.\n2:37:40 Okay. So, oops. There we go. And now it''s time for some robustness\n2:37:48 measures. So when we''re first initializing the graph like when it''s the first message we''re writing\n2:37:54 obviously we''re not going to straight up say uh how would you like to change the document right because we haven''t passed in a document yet. So if messages uh\n2:38:03 this part if that if there''s nothing in it we will have to say something like an\n2:38:08 introduction message right. So this is how you can do that. So we can\n2:38:15 say if not state messages aka if there''s nothing in the state messages then we\n2:38:20 can say uh I''m ready to help you update a document. What would you like to create? and then it collects the user\n2:38:26 input and passes it as a put stores it as a a human message in this user\n2:38:32 message variable. Now what if I''ve already passed it uh passed in a message\n2:38:37 or like we are on the process of updating our draft or drafting it. Well to do that we need this else statement\n2:38:45 and what does this say? Well it says what would you like to do with the document? So this assume this says that\n2:38:50 there''s already stuff in the messages uh state a messages key in the state how do you want to update it further and then\n2:38:57 we also print it uh under this emoji uh in the terminal so the user can also see\n2:39:03 what they''ve inputed and then this is also stored in the user message. All right. Okay.\n2:39:10 Now we combine all of this uh all messages aka the system prompt which was\n2:39:16 the system message uh and we create a list of uh list of the uh state messages\n2:39:22 and the user message the new message which we want the aka the update and\n2:39:27 then we just invoke the model and how do we invoke the model you just use the uh\n2:39:33 model invoke okay so pretty basic code so far there''s nothing hard or nothing\n2:39:39 uh extraordinary or something we haven''t seen before. All of this we have seen before. And now the rest of this\n2:39:47 function is just a print statement which I''ve included. This print statement is\n2:39:52 just for uh um making things look prettier on the terminal. That''s all it is. You can see the true print\n2:39:58 statements. There''s the AI response which will be printed and then there''ll be the tools uh whatever the tool\n2:40:04 messages are that''s also printed. So that''s the whole point of it. there''s nothing like to really like talk about\n2:40:09 it here. Uh and then we also need to obviously return the updated state. Now remember\n2:40:16 last time I showed you that this is also a really convenient concise way to uh\n2:40:22 update the states. So from now onwards we''re only going to update the states like this. Okay. Now we create our\n2:40:29 conditional edge function or the function behind the conditional edge cuz remember let me open this up. So the\n2:40:36 conditional edge which I''m talking about will be this this this conditional edge.\n2:40:42 So there from tools there will be either the select the uh the choice of going to\n2:40:47 back to the agent or the choice of ending it. So we need to create the underlying function behind that. So\n2:40:54 let''s create that now uh under this. So should continue. We''ve\n2:41:02 done this many times before. it det will determine if we should continue or end the conversation and remember continue\n2:41:08 or end the conversation. Okay, makes total sense. Okay, so\n2:41:16 now we do this. So we get the messages and if there''s nothing in the messages,\n2:41:22 well obviously we''ll need to continue, right? It won''t go to the end part. Uh\n2:41:28 and this is just as like a robustness measure to be honest. Okay. So now this piece of code\n2:41:35 is basically saying look at the most recent tool message or the uh recent\n2:41:41 tool we''ve used and we need to check if this tool uh has used the save tool. Now\n2:41:48 why remember how we have two tools we have either the uh update tool or the\n2:41:54 save tool. If we use the update tool, well, we will obviously need to use the continue branch, right? But if we use\n2:42:01 the uh save tool, well, after you saved it, there''s nothing else to do, right?\n2:42:07 You finished your draft, you finished everything, so might as well end the program. That''s why this end tool. So,\n2:42:14 for the continue, uh if to go to the continue um through the continue edge,\n2:42:19 we have to use the update tool. And to go to the end uh edge you need to use the uh the save tool. So should make\n2:42:28 sense now but don''t worry if it doesn''t we will do some more print statements so you see the workflow. Don''t worry. And\n2:42:35 lastly we need to obviously return continue because by default it''s checked here that it''s used\n2:42:42 the save tool. The only other tool left is the right tool and uh sorry the\n2:42:47 update tool. And the update tool means that we have to go to the continue edge, right? Okay. And that''s that uh function\n2:42:54 done as well. So pretty easy still. Now this next function is again I only coded\n2:43:01 this just to make the print statements in a more readable message format uh\n2:43:06 when we printed on the terminal. So you will see where this comes in play when we actually start uh invoking the graph\n2:43:12 and seeing how our process is going. Okay, cool. Okay. So now we actually\n2:43:19 init uh create the graph. So how do we create the graph? We''ve done this many times. We will initialize it through a\n2:43:26 state graph. And now we will add the nodes. So agent and tools. And the tools\n2:43:33 will be a tool node. And again if you notice back we had one node, two node,\n2:43:38 the agent node and the tools node. I''m keep I''m like reflecting back and forth between this diagram and the code\n2:43:45 so I can show you exactly what we''re coding. Okay. So again agent and tools node uh we''ve\n2:43:51 done now we will set the entry point at agent which is the start point aka this\n2:43:57 part right and now we''re going to add an edge\n2:44:02 between agent and tools. Now we need to obviously create this edge because the agent needs to go to the tools right and\n2:44:08 then this edge this directed edge and this conditional edge creates the loop\n2:44:14 uh which will allow for the human AI collaboration. All right. Okay. So now\n2:44:20 we add the conditional edge and that''s the conditional edge which I was talking about the continue at the end aka this\n2:44:27 condition this conditional edge from tools. Okay. And now the last thing we need to do is\n2:44:35 just compile it because we''ve finished the graph completely, right? There''s nothing left. We''ve done the start point, we''ve done the end point, the\n2:44:41 end, this conditional edges done, the nodes done, and then this directed edge is done and the start point is obviously\n2:44:47 done because we''ve uh created a directed. So you can see the entire graph we have created just like that. So\n2:44:53 again, nothing too hard. Cool. Okay. So now we actually run\n2:44:59 the program. And to run it, I have just written this um function so that\n2:45:04 everything is in a more compact way. This is just to invoke the graph. Okay.\n2:45:09 And let''s do that. So that was the entire code. And this code will allow\n2:45:15 for human AI collaboration. Now, yes, we used a global variable. And again, there\n2:45:21 is nothing wrong with using a global variable. I know some of you might frown upon it, but um again, there''s nothing\n2:45:27 wrong with it. If we wanted to use more complicated uh form uh complicated uh\n2:45:32 stuff from langraph like the injected state or even using something like commands and interrupts uh we would have\n2:45:39 to write the code slightly differently but because this is a beginner level course uh we''ve just disregarded that\n2:45:46 completely and we found another way of performing human AI collaboration. Awesome. So let''s actually run this now.\n2:45:53 So let''s write python draft. py and you should be able to see all of\n2:46:00 the things. I made my face cam slightly smaller so you can hopefully see everything. Perfect. So you currently\n2:46:05 have an empty document. Could you let me know what you like to add or create in the document? So what would you like? So\n2:46:11 let''s say we are writing an email to our colleague Tom saying that we can''t make\n2:46:16 it to the meeting. So let''s say write me an email. Let''s say uh write me an email\n2:46:22 to Tom saying we I cannot make it to the\n2:46:28 meeting. Let''s see what it says. So it says\n2:46:34 uh hi Tom I hope this message find you well. Please let me know. Okay let''s now\n2:46:40 give it some feedback on how we how we can improve. So and also you can also see that it''s used the update tool as\n2:46:46 well. Perfect. So, let''s say um make sure to also have specified that\n2:46:56 the meeting was supposed to be at 1000 a.m. at some random negation.\n2:47:05 Canary Wolf. Okay. Okay. Let''s see the updated thing.\n2:47:11 Hi, Tom. Uh you can see message meeting at 10. Uh, can I wolf due to unforeseen\n2:47:18 circumstances? Uh, let''s I don''t like this uh your name part though. So, let''s say my name is\n2:47:26 V and it will update that as you can see. Perfect. Uh, what do what else do\n2:47:32 we want to change? We can also say something like uh let''s say but tell him\n2:47:40 that I can make it at 12 p.m. in\n2:47:47 um New York, some random location. Okay, I''m making this up, but you you get the\n2:47:53 uh plan. Uh the next day. So, let''s\n2:47:59 see. And perfect. It''s updated it. However, I am available to meet at 12:00\n2:48:04 p.m. in New York the next day. Obviously, it''s complete like rubbish like the timings of the location I''ve written. But you can see how we can just\n2:48:13 uh use human AI collaboration here. Uh one more thing which I don''t like is\n2:48:18 this part. I don''t like the fact that it''s not a new line. So, I mean I''m being a bit picky here. We can say\n2:48:24 something like uh put the II hope this message finds you\n2:48:36 well. Awesome. And as you can see that''s done as well. So now let''s say I like it. Save it please. And what\n2:48:45 happens? You can see that uh it used the save tool. The tool results is document\n2:48:52 has been updated successfully. The current content is this and the document has been saved to unable to attend\n2:48:58 meeting email. Now remember we never passed in the file name at all. That was all generated by the by the agent\n2:49:05 itself. And to check we need to go on unable to attend meeting. So let''s see\n2:49:10 here it is. And you can see it''s the exact same\n2:49:15 meeting uh exact same email we said. So, subject, oops, best regards me. All the\n2:49:22 exact same content. Perfect. And we don''t even need to uh make it so that\n2:49:27 we''re drafting emails. We can even drop short stories. We can drop whatever we want. In fact, we can also pass in uh a\n2:49:35 previous message. So, the reason it started off like with nothing is because\n2:49:40 we pass in an empty list. But if you wanted to, we could have written something over here uh with our pre with\n2:49:48 a already existing email or already existing document and then it could the model the agentic system would know that\n2:49:55 this is what the content is the current content how would you like to uh change that and that is exactly how uh we will\n2:50:02 be able to operate on our existing ones. So you can see that this is quite a robust thing. If we want another\n2:50:09 example, for example, uh let''s say python drafter.\n2:50:16 py. Okay, now watch this as well. Look how robust this is. If I say something\n2:50:22 like write an email, it actually gives back questions.\n2:50:28 So sure, what would you like the email to say? D. So remember, it didn''t even go through any tool here. uh using\n2:50:35 langraph we can really make the agents quite robust and that''s the thing which I wanted to show you it doesn''t always\n2:50:42 have to pick a tool its own like LLM like the agent itself cuz remember the\n2:50:47 agent node has an LLM in the background back end the bind tools function allows\n2:50:52 it allows it scope like it increases the scope of it uh by providing some tools\n2:50:57 but that doesn''t mean it has to use those tools if it doesn''t feel like the need to use the tools it won''t and in\n2:51:04 this case it wanted to ask us more questions about it. So it would say show what would you like this email to say\n2:51:10 because to be fair I only wrote three words. Uh but that was what I was trying to show you. So let''s just clear this\n2:51:16 now cuz we don''t need to. And yeah you can see perfectly works human AI\n2:51:21 collaboration in langraph and this is actually somewhat useful as well. Now yes of course you can use GPT4 canvas\n2:51:29 and all of that stuff of course but um this is how you would do it in Lagraph. All right. So, if you would like an\n2:51:36 extension to this, what you could do is add a voice feature as well. So, maybe\n2:51:42 you could add use OpenAI whisper for uh speech to text conversion or add 11 laps\n2:51:48 for text to speech conversion and maybe you can make it voice based cuz right now I''m giving it I''m how am I\n2:51:55 communicating it with text mode? What about voice mode? You could also include a GUI to this. There''s a lot of stuff\n2:52:02 which you can do on you can even have your own knowledge base as well and include that. So a lot of potential with\n2:52:08 this if you want a homework for this uh specific project that there you go. All right. Okay. Cool. So that''s the end of\n2:52:16 this subsection. Awesome. So now let''s build our fifth AI agent. And some of you\n2:52:22 might have been looking forward to this. It''s retrieval augmented generation rag.\n2:52:27 So what will the graph look like? It will look something like this. Again,\n2:52:32 start point, end point, really similar to what a react agent was, right? But uh\n2:52:38 we have two agents in this case. We have a retriever agent and we have our main agent LLM, right? So, and it will have\n2:52:44 obviously a conditional edge, a loop, and everything. Again, we''re bringing everything we''ve learned so far and\n2:52:49 merging them into one. And we''re also going to be learning about a little bit about rag. Now, I''ll assume you know\n2:52:55 what rag is. I''m not going to go too much in detail into like the nitty-gritty of it. But again, in the\n2:53:01 surface level, I will obviously explain what rag is about and everything. Okay. So, if you''re excited, let''s uh let''s\n2:53:08 jump to the code. Okay. So, now you can see that I''ve already done all of the imports\n2:53:13 which we''ll need. But you''ll notice how there are these four imports which we haven''t come across yet. Now, rather\n2:53:20 than explain them from the get- go, I will explain them as they come because it''ll make more sense. uh it''ll make\n2:53:25 more intuitive sense that way. Okay. So now I''m going to be loading our uh ENV\n2:53:31 file which contains all the API keys. And this time I''m going to be\n2:53:37 initializing our LLM differently. Well, slightly differently. It''s the same LLM, but why did I say differently? Because\n2:53:44 I''ve passed in a new parameter called temperature. Now for those of you who do not know what temperature is, it''s\n2:53:49 essentially a parameter which depicts how stochastic the model outputs how\n2:53:54 stoastic you want the model outputs to be. So because I''ve set it to be zero, temperature equal to zero makes the\n2:54:00 model output more deterministic. Similarly, if I had set the temperature to be one, the model output would have\n2:54:07 been more stochastic. Okay. So now we create the embedding\n2:54:13 model. And the embedding model uh is what''s going to convert our text into vector embeddings. Right? Uh so this\n2:54:20 will be the layout for it. Now please note one important thing which is the embedding model has to be compatible\n2:54:27 with the LLM we''re using. You can use whatever LLM you want but make sure the embedding model uh is compatible with\n2:54:35 it. For example uh let''s say we''re using GBD40 uh an open model but the embedding\n2:54:40 model we''re using is from Olama some random model. Now that they wouldn''t most likely they''re not going to be\n2:54:46 compatible. Why? Because there''s so many differences between them. One potential difference could be the vector dimension. So just a rule of thumb. Make\n2:54:54 sure the LLM and the embedding model is compatible. Okay. Awesome. So now we''re\n2:55:01 going to specify the PDF part. So this is the stock market performance 2024 PDF. And essentially this is just a\n2:55:07 document which I created which contains um a lot about the stock market\n2:55:12 performance. Okay. Uh I can show you that right now actually. So this contains nine pages and is just a\n2:55:20 document containing about some stock market details in 2024. Okay. Awesome.\n2:55:26 So now um in case you''ve specified the\n2:55:31 uh you have put the PDF in a wrong directory or if it can''t find it uh this error will pop up. So again I''ve just\n2:55:37 put this for debugging purposes if you use the code which I provided on GitHub. Okay. Now this will load the PDF and you\n2:55:46 can see pi PDF loader is one of the imports which we made here. So again\n2:55:51 it''s in the name and the common. It just simply loads the PDF. Okay. Uh and this\n2:55:58 try and accept command uh just checks if the PDF is there. And pages is equal to\n2:56:04 PDF loader.load. So this essentially says how many pages are there in the document. So you can see there''s nine\n2:56:12 pages in our document. So if I run this command, if I run this, so clear\n2:56:20 python rag agent.py py it should say nine\n2:56:25 pages. So there we go. PDF has been loaded and has nine pages as expected. Right? Okay.\n2:56:33 Now it''s time for the chunking process. Now what is chunking? First look at this. There are two parameters which\n2:56:39 I''ve specified. Chunk size which is a,000 and chunk overlap which is 200. So\n2:56:45 let''s break this down a bit. Going back to our document. So chunk size was 1,000 tokens. So let''s say that this was a\n2:56:54 chunk for example. Okay, obviously that''s not going to be a thousand tokens, but just as like uh\n2:57:00 demonstration purposes, let''s assume it is. So this is saying as soon as you''ve reached 1,000 tokens, you create a new\n2:57:06 chunk. So let''s say 1,000 tokens ended here. So this would be the start of a new chunk like such. Okay. And you keep\n2:57:12 going and going and going until the end of the document. But what if the what about the second parameter? The second\n2:57:19 parameter is specified overlap and that''s essentially saying let me use it in a different color that your chunks\n2:57:25 consecutive chunks should have some tokens um which are which exist in both\n2:57:30 for example because it was 200 the second chunk is not going to start from here. It''s actually going to start\n2:57:37 something like this. They''re obviously going to be the same length uh in terms of tokens but\n2:57:43 they will have some tokens which will be in both chunks. So for example, this part will be in both cuz that''s the\n2:57:50 overlap. 200 tokens to be precise. Okay, so that was just a brief overview of what chunks are in uh rag. Okay, so\n2:57:59 that''s that part done. And again, this recursive character text splitter is one of the imports we did. Okay,\n2:58:08 so this text splitter um chunking process, we now apply it to all of the\n2:58:14 pages, all of our nine pages in our document. Okay. And this piece of code essentially\n2:58:22 saying this, the chroma vector database, we''re going to be using a chroma vector database to store all of our vector\n2:58:27 embeddings, by the way. But the uh the place where we want our chroma vector\n2:58:33 database to be will be specified in this file path. And the collection''s name will be called stock market. Now, you\n2:58:39 can specify it wherever you want obviously, but I''ve just specified it to be in the same folder. Okay.\n2:58:47 So this is just an if statement to make sure that uh if this is the first time\n2:58:53 we''re running this command uh if we''re running this file uh if this collection\n2:58:58 doesn''t exist we will create the um collection in the specified directory.\n2:59:05 Okay, again not too hard yet. Now here comes a try except command u try accept\n2:59:11 block. So this is where we actually create the vector embedding uh where we create the chroma vector um\n2:59:19 database and these are just parameters which I specify. So for example, how I want the pages to be split, what\n2:59:25 embeddings to use, where to store it and the collection name. The collection name being stock market, right? And if there\n2:59:30 is an error, it will throw an error and if it''s successful, it''ll print on the terminal. Okay, awesome. So now we\n2:59:37 create something called a retriever. So the retriever is quite important in rag.\n2:59:42 It''s well obviously the first part of rag retrieval augmented generation. So the retriever is what actually well\n2:59:49 retrieves the chunks the most similar chunks. Um the search type which we''re going to use similarity. It''s just the\n2:59:56 default anyway. Uh you don''t really need to know how that works to be honest. But what you do need to know is this part.\n3:00:03 So in this code I have made sure that every time uh it goes the amount of\n3:00:09 chunks it uh outputs back is five. Why? Because k here is the amount of chunks\n3:00:16 to be returned. So I''ve set it as five. Now I''m pretty sure if we go to the\n3:00:21 actual documents here the default the default is four. Okay. So uh this is\n3:00:28 just a parameter which you can uh set. Now you don''t want it to be too high of course or too low. So you want like a\n3:00:34 good middle ground and 405 is a good middle ground in my opinion. Okay. So now let''s create our tool. So again we\n3:00:42 use decorator tool. And the tool''s name is going to be this retriever tool. It will input it will take in a query and\n3:00:49 it''ll output a string. So the dock string is as follows. This tool searches\n3:00:54 and returns the information from our document. Okay self-explanatory.\n3:01:00 uh and obviously we need to invoke it to the retriever. So whatever query we ask\n3:01:06 for example uh what was Apple''s performance in 2024 that will be the query and that will be passed to our\n3:01:13 retriever which will grab all the chunks the most the top five most similar chunks. Okay. Now if we don''t if there''s\n3:01:20 nothing similar uh which it finds for example if I say something like uh who''s\n3:01:26 Bob the builder something like that right obviously Bob the builder is not in this document uh so it will return as\n3:01:33 I found no relevance information in the document and uh this will be passed to our LLM agent okay if it does find it\n3:01:41 though what we''ll do is we''ll create an empty list and we will store all of the\n3:01:46 similarity um us the all of the chunks which it found and then return those results uh\n3:01:53 through this. Okay, still it''s quite easy still uh and this piece of code\n3:02:00 we''ve already come across. There is only one tool. So we just bind that tool to our\n3:02:05 LLM. And this also code we have also we''ve uh done many times. It''s the uh\n3:02:14 creation of the agent state. And again we''re using our add messages reducer function. All of this we''ve covered many\n3:02:20 times so you should be quite familiar with it. Okay. So now we create the should continue function and the should\n3:02:26 continue function uh is going to be the underlying function between our conditional edge behind our conditional\n3:02:32 edge. So it will check if the last message contains any tool calls. If it\n3:02:37 does then we um proceed. If it doesn''t then we''ll just end\n3:02:44 right. Okay. So now we specify the system prompt. Now this system prompt is\n3:02:49 going to be quite big. So let me copy and paste it here. Now the reason is quite big is I want to specify as much\n3:02:56 information to the LLM so that it knows what to do. Right? So I''ve just said you''re an intelligent AI assistant who\n3:03:02 answers questions about the document uh loaded into your knowledge base. Uh you\n3:03:07 can read the rest if you would like. But I''ve also written this. Please always site the specific parts of the document\n3:03:13 you use in your answers. This is really just to make sure it''s not hallucinating. Right? because as we know\n3:03:19 hallucination is quite a big problem with LLMs. So this is just to make sure um hallucinations are kept to a bare\n3:03:27 minimum. Okay. All right. So now we create a dictionary of our tools and we\n3:03:35 now create the underlying function which will be our LLM agent. So this function will call the LLM with the current state\n3:03:42 and you can see it converts the messages to a list passes the system messages and passes it to our LLM which is defined\n3:03:49 like this and it will just return the messages aka the updated state. Okay, this should be like such.\n3:03:59 Okay, awesome. So now we create our second agent which will be the retriever agent which you saw on the in the graph\n3:04:06 which I showed you in the introduction. So the retriever agent executes the tool calls from the LLM response. So what is\n3:04:13 this code actually saying? Well, all in all, this massive piece of code really\n3:04:20 just says if there is a tool, if the tool name is within the is a proper\n3:04:26 specified tool, aka if it''s retriever tool, then actually run it. If it''s not,\n3:04:32 then we will output the result as input in incorrect tool name. Please retry and select the tool from list of available\n3:04:39 tools. It''s just for checking if a if the tool which is decided from the LLM\n3:04:44 is valid or not. So that''s all what this is doing. If it is valid, it will invoke it and we will store the results uh like\n3:04:52 this and we will return that. Okay. Again this should be agent state like\n3:04:59 such. Okay. So we''ve created all of our our two um AI agents now and now we''re\n3:05:07 going to create the graph itself. So like how we''ve done initialize it through state graph and then we''re going\n3:05:14 to add our two AI agents as nodes with their respective\n3:05:19 actions and we are now going to add the conditional edge. So which will be llm\n3:05:26 which will be start from l lm and the should continue function is the function which will be um the underlying function\n3:05:34 and this is a uh true false statement and this is the edge the set entry point\n3:05:39 all of this we''ve covered many times so again should be quite familiar to you and last but not least compile the graph\n3:05:48 and store it in a ragation okay one last thing though uh I''ve created\n3:05:55 this function and this function is just a function which allows us to keep asking questions to our graph and keep\n3:06:02 receiving qu uh answers back and if you want to exit we can write either exit or quit um and it''s just a simple while\n3:06:10 loop that''s all it is okay and it prints the answer okay so that''s the actual code\n3:06:17 complete now we''re going to test it and see uh if the if this is reliable or\n3:06:23 not. Okay. Okay. So, let''s actually test this now um by doing python rag agent.\n3:06:31 py. Let''s run this. Okay. PDF has been loaded and has nine\n3:06:38 pages. Created chromo vector data uh chroma database vector store. So, where\n3:06:43 is this stored? Well, you can see that this is uh by the way, this will all be on GitHub as well. But this is the\n3:06:49 Chroma database and its respective um bin bin files. Okay. And we can even\n3:06:56 view it. But it''ll look something like that. Okay. But because this has been created,\n3:07:02 this is a good sign that everything is working. Okay. So, let''s ask a simple question. Uh let''s ask something\n3:07:11 like how was the S&P 500\n3:07:16 uh performing in 2024? Enter. So it''s\n3:07:22 calling the retriever tool uh with the query this uh its result then puts that\n3:07:27 complete back to the model and the model has given us this. In 2024, the S&P 500 delivered a total return of this with a\n3:07:34 23% increase late 1990s and all of that stuff uh magnificent 7 and has given us\n3:07:39 the uh respective uh citations as well. Now, how can we verify this is uh correct? Let''s\n3:07:45 see. So, notice how if you remember this part, the total\n3:07:52 return of approximately 25%. Well, the reason I prom I asked it for this is because that''s exactly what uh\n3:08:00 over here it stated the benchmark roughly at 25% 23%. Uh remember this\n3:08:05 late 1990s part that''s exactly what this is saying here as well and this was\n3:08:12 correctly defined in the first document. So this is clearly working now right it\n3:08:17 can''t have made up this information. So that means our rag is successfully set up. Now I can ask as many questions I\n3:08:22 possib as I want now but now let''s see if there is something which is not\n3:08:28 included in the rag. So for example we can say something like how did open AI\n3:08:34 perform in 2024 retrieve a tool called back to the\n3:08:40 model. Okay, now look at this. Uh if I\n3:08:46 do it like that, the documents do not provide specific information about OpenAI stock performance, which is true\n3:08:51 cuz OpenAI is not a publicly traded company. Uh and yeah, it got that\n3:08:56 correct. So no hallucination there. So you can clearly see that this is working\n3:09:02 uh completely fine. And that ladies and gentlemen is how you create a retrieval\n3:09:07 augmented generation graph in Langraph. Okay.\n3:09:12 Awesome. All right people. So that brings us to the end of this course and I hope you liked it and I hope you\n3:09:19 learned a lot about Langraph. Now although this course is finishing here, your journey in Langraph\n3:09:25 is just beginning. Just think about how many cool AI projects, AI agent systems you can make now. Maybe your own Javis\n3:09:31 as well. Now, if you have any further questions related to the course material\n3:09:37 or just things in general or just want to say hi, you can always message me on LinkedIn. With that being said, thank\n3:09:43 you so much for watching this course and I hope to see you in another course. Take care.\n0:00 Welcome to this video course on Langraph, the powerful Python library for building advanced conversational AI\n0:07 workflows. In this course, Vbeca will teach you how to design, implement, and\n0:12 manage complex dialogue systems using a graph-based approach. By the end, you''ll\n0:18 be equipped to build robust, scalable, conversational applications that leverage the full potential of large\n0:25 language models. Hey guys, my name is Vava and I''m a robotics and AI student.\n0:30 In this course, we''re going to be learning all about the fundamentals of Langraph. Now, I assume you''ve heard of\n0:36 Langraph before, hence why you clicked on this course. But I''m also going to assume you have never coded in Langraph\n0:43 before. Now, because of this assumption, I have explained every single thing in as much detail as I possibly can. Now,\n0:51 also this might mean that I might be going slow at times. So if you want you can always speed me up. Now what are we\n0:58 going to be learning in this course? Well to start we''re going to be building a lot of graphs, a lot of AI agents.\n1:05 We''re going to be learning a lot about the theory and I''ve also provided exercises throughout the course in which\n1:11 all of the answers will be provided on the GitHub. With that being said, if you''re\n1:16 ready to start on this journey with me, let''s go to our first section\n1:21 then. All right people. So welcome to the first section of this course. Now in\n1:26 this section we''ll be covering something called as type annotations. Now admittedly this is going to be a\n1:32 completely theoretical section but it will be short and brief. I promise. The reason I''ve kept this specific section\n1:38 in the course is because when we do eventually go on to code uh our AI agents, our graphs and langraph, these\n1:45 will start popping up everywhere. And I don''t really want you to look start coding without ever having seen these\n1:51 before or really not knowing what these actually are. So that''s why I''ve kept it here. But I promise this will be short\n1:57 and brief. Cool. Okay. Let''s begin with dictionaries. Now dictionaries are a\n2:02 data structure. Yes, but there''s a reason I''ve kept it here. So let''s see\n2:07 how a dictionary is described in Python. You should already know this. So in this case, I''ve described a very simple\n2:13 dictionary called uh movie. and it has two keys, the name and the year. And it\n2:18 has two values, Avengers Endgame and 2019. Now, dictionaries are awesome, don''t get me wrong. They''re they allow\n2:25 for efficient data retrieval based on their unique keys. They''re flexible and easy to implement, but there''s a\n2:30 potential problem with them. See, it''s a challenge to ensure that the data is a particular structure. And this could be\n2:38 a huge problem in larger projects. So to put things in simple words, it doesn''t\n2:43 really check if the data is the correct type data type or structure and that could be the source of a lot of logical\n2:50 errors in your project. And if your project is really really large, then this could be quite a headache to\n2:56 identify, right? Cuz it''s quite a small detail. So what is the solution for this? Well, it''s something called a type\n3:03 dictionary. Now, here is an example on how you create a type dictionary in Python. And I just want to uh emphasize\n3:10 that this type annotation is used extensively in langraph. This will be used to define states. Now don''t worry\n3:18 you we haven''t covered states yet. We will cover that in the next section. But just be mindful that this is quite\n3:23 important. So a type dictionary is quite easy to implement. You implement it as a\n3:28 class. In this case, I''ve implemented the same um example I uh uh showed you\n3:34 in the previous section where I described the movie is the same exact keys and values. So, it still has the\n3:40 name and the year. But notice in this class, I have defined the actual uh data\n3:46 type of what that key should be. So, for example, the name is a string and the year is an integer, right? And to\n3:53 initialize a dictionary uh I have done the exact same thing. to have engineet game in 2019. So now there are two main\n4:01 uh benefits of using a type dictionary it''s type safety because we''ve explicitly defined what should be in\n4:08 this data structure and so this will really reduce the runtime errors and obviously the readability is enhanced as\n4:14 well and this will make debugging easier if something goes wrong within this type dictionary. Cool. So we''ve covered type\n4:22 dictionary now. Now we move on to another type of annotation which is union. Now you might have seen these\n4:29 future these later uh types annotations before if you coded in Python but again I''m just giving you a highle overview\n4:35 what these are. So union take a look at this example. So I''ve created a very simple function which takes in a value\n4:42 and it squares it. Now in this case the uh input x could be either an integer or\n4:48 float and union basically says that whatever value you have can be these\n4:54 data types only. So in this case x can only be integer or float. So if I pass in five or 1.23 4 this would be\n5:01 completely fine. It would square the number and everything. But if I passed in a string like I am a string it would\n5:06 completely fail. Now admittedly yes this function is quite easy. If I passed in\n5:12 I''m a string, it would have failed anyway. But in more complicated applications, hopefully you can see how\n5:17 this actually is useful. In fact, the makers of Lang Chain and Langraph used Union quite extensively throughout u\n5:24 making the actual library. So again, it''s flexible and it''s easy to code and\n5:30 it allows for type safety. So because it can provide hints to uh help catch\n5:35 incorrect usage. Now something similar to union is another type annotation which is optional. Now optional is quite\n5:42 similar and in this case I''ve described another function nice message. So you\n5:48 pass in a name. If you pass in a name it will say hi there name. So for example let the name be Bob. If I pass in Bob to\n5:56 this uh function it would say hi there Bob. But what if I don''t pass in\n6:01 anything? Now if I don''t pass anything optional because I''ve used optional says that the name parameter could either be\n6:08 a string or a none value. Now if I pass in nothing it will go in this if\n6:13 statement and say hey random person. But this is also important to emphasize that it cannot be anything else. It can''t be\n6:20 an integer or or a boolean or a float or anything like that. It has to be either a string or a none value because that''s\n6:27 what I''ve defined here. Cool. Now comes another type annotation called any. And\n6:33 any is really the easiest one to understand. It literally means this value could be anything. It could be any\n6:40 data structure. So in this case I''ve created a simple um function called\n6:45 print value where it takes in something and it prints that. And for example I\n6:51 passed in this string and it prints it and anything and everything is allowed.\n6:56 Cool. one last type annotation I promise and it''s the lambda function. So lambda\n7:02 functions are quite useful. For example, in this I''ll give you two examples now. So the first example is this really\n7:10 simple uh example. Now we''ve already I already created a square function before, right? Where it takes in a\n7:16 value, it takes in a number and it squares it. So for example, if I passed in square 10, it would give me 100.\n7:23 Quite an easy example. Now let me give you a second example. this. So if you''ve come from a\n7:29 leaf code background, then you''ve probably seen you''ve either used lambda before and you''ve definitely used math\n7:35 before cuz it''s quite efficient. So for example, if I pass in 1 2 3 4, what this piece of code is saying is that it\n7:42 squares each number in nums. So this map function maps each value uh and performs\n7:49 this function to it. So x * x. So 1 4 9 16 and then converts that back into a\n7:55 list. Now lambda functions really are just a shortcut to writing small functions and they make everything quite\n8:02 efficient. Now obviously this could have been done in one line as well but for example this a beginner programmer could\n8:09 have might have used a for loop but a more advanced programmer could have used this and this is obviously much more\n8:16 efficient. Right? So hopefully you can start to see what how powerful these type annotations are and these will be\n8:23 coming up. So again, no need to memorize this. Just need to have a highle overview what they are. Okay, cool. So\n8:29 now I''ll see you in the next section. See you there. All right, perfect. So let''s\n8:36 continue on. In this section, we will look at the different elements in Langraph. So let''s begin with our first\n8:44 element, one of the most fundamental elements in all of Langraph, the state.\n8:49 So what is a state? Well, it''s a shared data structure that holds the current\n8:54 information or context of the entire application. In simpler terms, it is like the application''s memory where it\n9:01 keeps track of the variables, the data that nodes can access and modify as they execute. Now, don''t worry if you don''t\n9:08 understand what a node is yet. That is what we will be talking in the next slide about. But as a good analogy,\n9:15 think of the whiteboard in a meeting room analogy. Now imagine you''re in a meeting room and\n9:20 there are different participants as well and every time you come up with something new or you want to record some new uh information or update some\n9:27 information you write it on the whiteboard. In this case the whiteboard acts as your state and the participants\n9:34 act as a node. So the state shows us the updated\n9:40 content/in information of your entire application. Hopefully that made a bit of sense.\n9:47 So let''s move on to the node another fundamental element in lang graph. So\n9:52 these are just individual functions or operations that perform specific tasks within the graph. So each of these node\n9:59 receives an input which is often just the current state of your application. It processes it and then produces an\n10:05 output or an updated state. So here''s a good analogy of this.\n10:12 The assembly line station analogy. Now look at this image. Each of these\n10:17 station does one specific job. It could be attaching a part. It could be painting it. It could be inspecting the\n10:24 quality and so on and so on. The point is each of these stations represent a\n10:30 node because they do one specific task. So how do you actually connect\n10:36 these different nodes together? Well, before we go into that, I think it''s important we understand the most\n10:42 important element of them all, the graph. It is so important that it''s even\n10:48 in the name Langraph. So, the graph is just the overarching structure and it\n10:54 maps out how different tasks aka nodes are connected and executed. So it\n11:01 visually represents the workflow showing the sequence and the conditional parts\n11:06 between various operations. Now a graph is quite self-explanatory but you can think of it\n11:11 as a road map. On a road map you can see it display the different routes\n11:16 connecting cities with the different intersections offering choices on which path to take next.\n11:22 Now, here''s a great image of what a graph is, and these are the individual nodes, but you''ll see they''re connected\n11:28 somehow. So, how are these connected? That brings us to the next element,\n11:35 edges. So, edges are just the connection between nodes and these determine the\n11:40 flow of execution. So, they tell us or tell the application which node should be\n11:45 executed next after the current one completes its task. A really good analogy of this is imagining a train\n11:52 track. So this is the train track and think of it as an edge and think of it\n11:57 as connecting two stations one here and one here which represent nodes together\n12:03 in a specific direction. Now the train which will go on the train track that acts as your\n12:10 state. So the state gets updated from one station to another.\n12:16 But there is another type of an edge and it''s called a conditional edge. So this is still not very\n12:24 complicated. It''s quite simple to understand. These are just specialized connections that decide the next node to\n12:30 be executed based on the specific condition or logic applied to the current state. Now a really good analogy\n12:37 for this is the traffic light analogy. So green could mean to go one way, red\n12:42 could mean to stop. yellow could mean to slow down. The point I''m trying to make here is that the condition, in this case\n12:49 the light color, it decides the next step. If you want to think even more\n12:54 simply, you could think about an if else statement. So that being said, we move\n13:01 on to the next element, the start point. So the start point or the node, the\n13:06 start node is a virtual entry point in langraph and this marks where the workflow begins. Now it''s important to\n13:12 note that it doesn''t perform any operations itself but it serves as the designated starting position for the\n13:18 graph''s execution. Now in terms of analogy it is quite simple to understand but if you\n13:25 really want think of it as the starting line of race. Now if you have a start point well\n13:32 you need an end point as well and that''s where the end element comes in. So the\n13:37 end nodes just signifies the conclusion of the workflow in Langraph. So when the\n13:42 application reaches this node, the graph''s execution completely stops and it indicates that all intended processes\n13:49 have been completed. And again, a good analogy for this is just the finish line in a\n13:55 race. So nothing too hard yet. But now let''s look at\n14:01 tools. So tools are specialized functions or utilities that nodes can\n14:06 utilize to perform specific tasks. For example, it could be fetching data from an API. They basically enhance the\n14:14 capabilities of these nodes by providing additional functionalities. Now, one common\n14:19 question could be, well, what''s the difference between a tool and a node? The node is just the part of the graph\n14:25 structure. Whereas the tools, these guys are functionalities used within the\n14:31 nodes. Now, a really good analogy for this is just tools in a toolbox. So\n14:36 imagine a hammer for the nails, a screwdriver for the screws, etc. The\n14:41 point is each tool has a distinct purpose. Again, don''t worry. You will understand the differentiation between\n14:48 tools and nodes in a lot more detail later when we code this, but this is just for a general\n14:53 overview. Now, another question you could be asking is, is there a middleman between a tool and a node? Short answer\n15:01 is yes. That''s where tool node comes in. So a tool node is just a special kind of\n15:07 a node whose main job is to run a tool. So for example, a tool node could\n15:14 be a node where its only job is to use a tool and that tool''s job is to fetch\n15:21 some data from an API. So it connects the tools output back into the state so other nodes can\n15:28 use that information. So think about this analogy going back to the assembly line. In this case,\n15:36 imagine the operator as the tool node and it controls the machine which is the tool and then sends all of these results\n15:43 back into this assembly line. Now if we progress further, let''s\n15:50 look at the state graph. So this is quite an important element as well. This will be one of the first elements you\n15:56 actually interact with and its main purpose is to build and compile the graph structure. So it''s quite\n16:02 important. It manages the nodes, the edges, the overall state and it makes\n16:08 sure that the workflow operates in a unified way and all of the data flows correctly between components. So again\n16:15 it''s quite an important element. You can think about it as a blueprint of a building. So just as a blueprint\n16:22 outlines the design and the connections within a building, the state graph does\n16:28 exactly that, but it just defines the structure and the flow of your workflow or\n16:35 application. Now here''s where the runnable comes in. Now some of you will be coming from a lang chain background\n16:41 and runnable is quite common there and it''s quite similar in langraph as well.\n16:46 A runnable in langraph is just the standardized executable component that performs a specific task within an AI\n16:53 workflow. It basically acts as a fundamental building block allowing for us to create these modular\n17:00 systems. Now a question you could have right now is well what''s the difference between a runnable and a node?\n17:07 Short answer is a runnable can represent various operations whereas a node in\n17:13 lang lang graph typically receives a state performs an action on them and\n17:18 then updates the state. Now don''t worry if you didn''t 100% get that when we go\n17:24 into the coding section you will get it a lot better. But a good analogy is a\n17:29 Lego brick. So just as how Lego bricks can be snapped together to build these complicated structures, runnables can be\n17:37 combined to create sophisticated AI workflows. So now let''s move on to the\n17:42 different types of messages. Now again, if you come from a lang chain background, you''ll be quite\n17:48 familiar with these. If you haven''t, don''t worry. We will look at the five most common message types in Langraph.\n17:57 So to start off, there''s the human message which represents the input from a user. The AI message which represents\n18:03 responses generated by AI models. The system message which is used to provide\n18:08 instructions or context to the model. Tool message which is similar to the function message but specific to\n18:14 tool usage. And the function message represents the tool of a function call.\n18:20 If you''ve used an API like a large language model API before, such as OpenAI''s API, a lot of these will be\n18:26 quite familiar, especially the system message, the AI message, and the human message. And that concludes this\n18:33 section. So, I will see you in the next section. Awesome. So, now this is quite\n18:41 exciting. We''re actually about to start coding in Langra for the very first time. Now that we''ve covered all the theory, admittedly the boring section,\n18:49 we''re now actually going to code up some graphs. And we''re about to code up our very first graph in this sub section.\n18:55 But um for this overall section, I have a slight confession to make, which is\n19:01 we''re not going to be building any AI agents in this section. Why? because I thought that one\n19:08 we haven''t really even seen uh how to actually code in Langraph and combining all of these LLMs APIs and tools and all\n19:15 of that stuff which comes with it combining them together would be quite messy and it could be quite confusing at\n19:21 times especially the fact that we have never coded in Langraph before again like I said at the beginning of the\n19:27 course this course is supposed to be beginner friendly detailed and comprehensive and we''re going to go in\n19:33 steps like little by little so hopefully understand but don''t worry we will be coding AI agents soon we''re just going\n19:39 to be building a couple of graphs right now uh understand lang graph better the syntax better and how to actually code\n19:46 up graphs and get confident with it and then we will actually build AI agents okay cool so what is the graph which\n19:53 we''re going to be building together in this section I call it the uh hello world graph mainly because it''s the most\n19:59 basic form of graph we can actually code in lang graph so the objectives are\n20:04 these So we''re going to be understanding and defining the agent state structure and\n20:11 don''t worry you''ll understand what that is in a few minutes and we''re going to be creating simple node functions nodes\n20:17 like we discussed in the previous section uh and we''re going to be processing them and updating the state.\n20:23 We''re going to be building the first ever basic langraph structure and we will understand how to compile it,\n20:29 invoke it, process it, everything. And really the main goal of this section is\n20:35 to really understand how data flows through a single node in langraph. Now just to give you a bit of a heads up as\n20:42 to what we''ll actually be covering uh what we''re going to be building I should say is this graph. Again like I said\n20:49 this is the most basic form of graph you can build in langraph. It has a start point and an end point and this node\n20:56 sandwiched in between them. All right cool. So hopefully you''ve understood what the objectives are. It''s quite\n21:02 basic and yeah, I''ll see you at the\n21:08 code. Okay, cool. Now let''s actually code this very first graph. So I''ve\n21:13 imported three main things here. The dict, the type dict and the state graph. The dict and type dict is obviously\n21:20 dictionary and type dictionary but um and state graph. These three are\n21:25 elements which we covered in the previous section. So I would highly recommend you going back there if these\n21:30 are completely unfamiliar. But again you don''t need to memorize what these are. Okay. But just to refresh your memory\n21:36 I''ve written in the comment here what the state graph is. So think of the state graph as a framework that helps\n21:43 you design and manage the flow of the tasks in your application. Um again that\n21:48 might sound a bit complicated but it''s not. Once we actually start coding you\n21:53 will it''ll make more sense. So now the first thing we''re going to do after importing everything is create the state\n22:00 of our agent and let''s call it agent state. And just to refresh your memory\n22:06 again what the state is. Think of the state as a shared data structure. And\n22:11 this keeps track of the your all of the information as the application runs. All right cool. So now let''s build the agent\n22:18 state. And the way we do this in Langraph is through a class. So let''s build class agent state and in this in\n22:26 these parenthesis we will try to the the state needs to be in the form of a typed\n22:32 dictionary. So that''s why we specify type dictionary here. Now let''s keep\n22:38 this very very fundamental and basic. Let''s just pass in one input. Let''s call\n22:44 it something like message and obviously we put colon and\n22:51 the we specify the data type of that uh attribute. Now obviously the data type\n22:57 of message will be string right so that''s why we specify strl again this is\n23:02 just normal python so once we''ve done that we are now going to be coding our\n23:08 very first node again another very fundamental element in langraph so how\n23:14 do we actually define a node it''s quite simple it''s just a normal standard\n23:19 python function and this is how you do it so let''s say let''s first try to find\n23:24 The objective um let''s say we are trying to let''s a greeting message a simple\n23:31 greeting message. So we''ll write def greeting node and we need to pass in an\n23:38 input and pass what the output type should be. Now the input type of a node\n23:44 needs to be the state and the output type also has to be the state because\n23:49 remember the state keeps track of all of the information in your application\n23:54 right so obviously you need to pass that as an input and you need to pass out the or return the updated state. So here''s\n24:01 how you do it. You pass in state and what is the state of our application? Well, it''s the agent state which we\n24:08 defined earlier, right? And the output is going to be agent state cuz we need\n24:14 to output the updated state. And our updated state will again just be the agent state once we''ve done all of the\n24:21 um all of the mechanics we do in this function, the actions we perform in this function. All right. Okay. So now we\n24:28 need to do something very very important and it gets annoying sometimes but um\n24:34 it''s really a key habit which I want you to form and it is dog strings. Now dock\n24:39 strings and lang graph is quite important. Why? Because dock strings is what will tell your AI agents when we\n24:46 actually build the AI agents your LLMs what that function actually does what that function''s actions are what it\n24:52 performs. So in this case uh by the way to create a dock string is just three quotation marks three pairs of quotation\n24:59 marks. Uh let''s call the dock string in this case let''s just write simple node\n25:05 that adds a greeting message to the state. Perfect. So now how do we\n25:14 actually refer to this message? Well again this is just normal Python code.\n25:19 So we will pass in state and we will type in message.\n25:25 Now this specific part allows us to actually update the state or the message\n25:31 part of the state. And let''s say let''s come up with something like hey\n25:37 plus state message. Um we can also add something\n25:42 like how is your day going something basic. Now what''s the last thing which I\n25:48 need to do in this function? Think about it. Okay. So now remember in uh a few\n25:56 moments ago I said we have to return the state or the updated state. Well the updated state we''ve already done we''ve\n26:02 just manipulated the state here. So all we have to do is just simply return the state. Cool. And yeah that runs without\n26:10 any errors. Okay. Now let''s actually build the graph uh which is again\n26:16 obviously very important. So how do we build the graph? Remember here I said state graph is a framework that helps us\n26:23 design and manage the flow of tasks as a graph. Well that''s exactly what we''re about to do now. So hopefully it clicks\n26:29 now. So to create a graph in lang graph you use the state graph attribute and\n26:35 you pass in your state. You can see the state schema which VS code has uh asked for what uh the description of what the\n26:42 parameters are. So our state schema in this case is just the agent state which we define right. So we pass an agent\n26:49 state. I will actually also write here our state\n26:54 schema. So uh you can physically see what it is.\n26:59 Okay. And let''s store this in a variable called graph or something. Okay. Now now\n27:07 here comes a very important method. How do we actually add a node to this graph? Cuz this graph is completely like\n27:13 nothing right now. So to add a node we use the inbuilt function graph add node\n27:18 and it requires two main parameters. Now what VS code is suggesting is a god I\n27:25 don''t even know what that all of all of that is right it''s very confusing. So to put things simply you require really two\n27:33 uh parameters the name of your node and what action it will perform. So let''s go\n27:39 with the name. The name could be absolutely anything sensible of course. Um let''s call something like\n27:45 greeter. Cool. And you can see VS Code has also asked us to um input an action.\n27:53 Now what''s the action going to be? Well, the action will just be whatever your node will actually perform. And what\n28:00 action or mechanics will this node actually perform? Well, all of that is defined by this function, right? The\n28:07 greeting node function. So we simply just put that the name of the greeting node function here and that''s it. We''ve\n28:14 successfully added the greeting node to our function to our graph and it will be\n28:21 named as greater. So remember this\n28:27 diagram in this diagram there is supposed to be a start and an end point. We''ve done the node which is sandwiched\n28:33 in between these but we haven''t really added the start and the end point yet. So, how do we do that? Well, there''s\n28:39 actually multiple ways to do that. In this subsection, in this graph, I''m going to teach you one way. Further down\n28:45 the line, I''ll teach you another way. So, but they''re both they''re quite easy. So, you simply just call the inbuilt\n28:51 function set entry point and as the parameter is just one\n28:56 parameter which is the key. Now, the key is the name of your node which you want the start node to connect to. Again,\n29:04 visualize it. The start the start point is here and the node is here. Obviously you need to reference a node for it to\n29:11 create like an edge right. So we simply pass greater and similarly graph dot set\n29:17 finish point. We will again pass greater here as well. Why? Because imagine again\n29:24 the node is here and your finish point is here and you need to connect some sort of connection between these two right and that''s why we use uh greater\n29:30 in this case. Don''t worry, you will solidify this once you complete the exercises and as we go down building\n29:36 more graphs. All right. And one last thing which we need to do is actually compile this graph. So graph compile\n29:43 using the inbuilt uh graph using the inbuilt compile function. And let''s just store this in a\n29:48 variable. Cool. So that run without any errors. But just a word of caution here.\n29:54 Just because the graph compiles without any error doesn''t mean it will successfully run. I mean, God knows once\n30:00 we build like more complicated graphs, there could be so many logical errors. So, that''s just an important thing to\n30:06 know. So, don''t get too happy once it compiles cuz there might be logical errors. Trust me, I know. Okay.\n30:15 So, I want to write some code which will actually help you visualize this. And\n30:20 you can use the IPython library. So, you can use this uh this um piece of code\n30:26 here. This code is awfully familiar with the first ever graph I showed you, right? I\n30:33 I''ll put a picture somewhere here for you to compare. The only difference is really the name of the node which we''ve\n30:39 set. In this case, it''s greater. Why is it greater? Because that''s the name we gave to this node, right? Cool. So\n30:47 that''s looks pretty good. Let''s actually run this. So to run you use the inbuilt\n30:54 method invoke. Um so let''s pass in the message\n31:00 as something like Bob or something and let''s actually store this result in a\n31:09 variable. Okay. Now how can we actually specify uh how can we actually get the\n31:15 value of result? So result we need to actually reference\n31:22 a certain attribute. Now the only attribute we have in uh the entire graph is message right. So we simply just put\n31:29 message and perfect you we get the final answer which is hey Bob how''s your day going now why is it like this because\n31:36 this is exactly how we set our act how we set our function to be what action it performs it says hey then concatenates\n31:44 the uh input message in this case it''s just the name and it says how''s your day\n31:49 going now I could have changed this to absolutely anything else right uh what goes here like these functions are\n31:56 almost endless but That''s the whole flow of how everything works. So hopefully\n32:03 you understood how to build this very first hello world graph. It''s quite\n32:08 simple. But um don''t worry if you didn''t fully 100% understand this. I''m now\n32:14 going to show you what exercise you need to complete uh to be able to solidify this. All right. All right. I''ll see you\n32:20 at the exercise. Okay. So time for your very first exercise. So the exercise for this\n32:27 graph is quite similar to what we just did, but I want you to create a personalized compliment agent. So you\n32:35 should pass in your name as like something like Bob or something and then output something like Bob, you''re doing\n32:42 an amazing job learning langraph. And to give you a hint as to what you need to do again, you again have to concatenate\n32:48 the state, not replace it. All right, it''s very similar to what we just did and it''s quite basic. You should be able\n32:55 to do this, but um this is really just to get your hands dirty. All right. Okay. Once you''ve completed this\n33:01 exercise, join me when we build the second graph. I''ll see you\n33:06 there. Okay. So now we''re about to build our second graph as you can see here.\n33:12 And it''s again quite similar to the first graph we built except now we''re\n33:17 going to be able to pass multiple inputs as you can see here. So again, what are the objectives which you will be\n33:23 learning in this? Well, we''re going to build a more complicated agent state.\n33:28 Uh, and we''re going to be creating a processing node that performs operations on list data. So now we''re about to see\n33:34 how we can really work with different data types apart from just string. And\n33:39 we''re going to set up the entire graph that processes and outputs these and computes these results. And we''re going\n33:45 to be able to invoke the graph with the structured inputs and retrieve the outputs. But the main goal which uh I\n33:53 want you to be able to learn in this specific subsection is really how to handle multiple inputs. All right. Okay.\n34:00 Let''s code this. Okay. So now let''s actually code the second graph up the second\n34:07 application up. So again I''ve just imported the same things again the type dictionary and the state graph. And I''ve\n34:13 also imported the list this time. But list is just a simple data structure which you should know already. So if you\n34:19 remember from the previous graph we made we are supposed to uh implement the state schema first right. So how do we\n34:26 do that? Again we use the class agent state uh type\n34:32 dictionary. Okay before I continue just a heads up I could have named the state schema anything I want. I could have\n34:39 named it uh something arbitrary completely like a bottle for example. In this case I''ve just said agent state\n34:45 because one that''s how I learned it. It''s like a habit for me now. But it also really tells you what it actually\n34:52 is. It''s the state of your agent, right? So that''s why I''ve just kept it like that. But again, just a heads up, you\n34:57 could have named this whatever you want. Cool. Okay. So now let''s the if you\n35:02 remember the main goal for this graph for this uh building this graph was to be able to handle and process multiple\n35:09 different inputs, right? So how do we actually assign and I really do that?\n35:17 Well, the answer is in the state which is here''s what uh which is what we''re about to do now. So you really cuz\n35:24 remember this is just a type dictionary. So you basically have multiple keys now\n35:30 you uh create that. So let''s say something like values list integers.\n35:37 So let''s say one of our input is a list of integers and let''s also pass in a\n35:43 name which will obviously be in a string and let''s have the result in a string\n35:50 something completely random. But now you can see we''re now operating on two different types of data structures uh a\n35:56 a list of integers and a string. And we''re handling three different uh different uh uh inputs values name\n36:04 result. Okay cool. So let''s run this. Perfect. So now let''s actually build our\n36:10 node because in again in this uh graph we''re just going to have a single node to keep things easy. Remember step by\n36:17 step. So let''s call let''s write dev process values and again what was what\n36:24 needs to be here? Yeah. So we need to pass in the state and we need to return the updated\n36:31 state. So how do we do that? Well, we write state agent state and we pass out\n36:37 the agent state. Cool. Now, again, building healthy habits. I know it''s\n36:43 annoying. We have to write the dog string. So, let''s just write something like this\n36:50 function process handles multiple different value in multiple different\n36:57 inputs. Cool. Again, I''m not being super specific here because one, uh, I don''t\n37:03 want to spend too long on writing doctrines and everything, and two, there''s no AI or LLM here, right? So that''s why it doesn''t really matter. I''m\n37:09 just doing this to build healthy habits. Okay, so now let''s do something like whatever values we pass the list of\n37:16 integers. Let''s sum them up. And let''s also concatenate the name as well and\n37:21 store it in the result. Sound cool? Okay, so how do we do that? We pass in\n37:27 state result cuz that''s what we are uh the action we''re performing is on result\n37:32 uh the attribute result and let''s say something like hi there and then we refer to the name\n37:41 um cool and your sum is equal to and let''s just use the inbuilt Python\n37:46 function sum and we pass state values cool and lastly we obviously\n37:54 return the Okay, perfect. And that''s that done.\n38:00 Okay, so now we actually create the graph. Again, this is going to be very very similar to what we did in the\n38:07 previous section because again there''s just a node, there''s a start point and an endpoint. So like last time, we use\n38:14 the state graph to initialize a graph and we pass in our state schema. So agent state and let''s store this in the\n38:21 variable graph. Okay. Uh let''s add our node. So graph add\n38:27 node and again remember it requires two parameters. It requires the name and the\n38:32 action. So in this case the name will be let''s call it processor for example. Again this could be anything you want\n38:40 and your action will be performed by this function right process values. So we can just add that. Okay. Now I''ve\n38:48 already told you how to uh how to initialize a start point and an end point and this is just given by that\n38:54 code. So you attach your entry point to your node. In this case it''s just one node which is the processor node and\n39:00 again same goes with finish and you compile it using\n39:05 graph.compile. Perfect. So take a moment now. How do you think this graph will\n39:10 look like? That again like I said very very\n39:18 similar on how the graph actually looks like but the only difference now is the\n39:24 name of the uh node which we''ve kept this as processor. Okay. So now let''s\n39:30 actually test this. Let''s actually invoke this graph. So how do we do that? Well, we use the invoke function. Now\n39:37 here''s another important part which is quite a common mistake especially like I have done this many times. Make sure to\n39:44 store your compiled graph in a variable cuz if you invoke the graph i.e. if you\n39:50 write something like graph.invoke that won''t make sense cuz you haven''t compiled the graph. That''s why you need\n39:56 to uh invoke using app. That''s why I''ve also done app here. If I did graph get\n40:03 graph. Oh, it''s completely messed up. Right? It says state graph object has no attribute because your graph hasn''t been\n40:09 compiled yet. That''s why when I do appget graph the uh process works. Cool.\n40:16 So now let''s again store this in uh let''s store something like answers is\n40:21 equal to app.invoke. Cool. Let''s pass in some values. Let''s say something like\n40:29 values and let''s have a list of integers. 1 2 3 4. Again, I''m just\n40:35 trying to prove a point. I''m not trying to make a very complicated um graph yet. And let''s pass the name as something\n40:42 like Steve something. Okay. Uh cool. And\n40:48 let''s print let''s print answers. Let''s see what happens. Perfect. So now you can see\n40:55 your values is 1 2 3 4. Your name is Steve. And your result is Hi there\n41:00 Steve. Your sum is equal to 10. Again, why? because that''s exactly what we uh\n41:05 asked the node the action to perform. Hi there, your name which in this case is Steve. Your sum is equal to the sum of\n41:12 the values and 1 + 2 + 3 + 4 is 10. Right? And that''s how you get this\n41:18 answer. Now what if I wanted to just access result? I didn''t want any of this\n41:24 other uh nonsense. Well to do that you can again just specify result and you will get it\n41:31 in a more clean manner. Cool. Okay. Now I want to try one more\n41:38 thing just to build your understanding a bit more. Uh let''s put some print\n41:43 statements here. So let''s have a print state\n41:49 here. Then we perform the action and then we print the state here. This is\n41:55 really just to show you how the state gets updated and it should be easy like\n42:00 interpretable cuz this is quite a basic piece of code. Again, print stated before the action and print state after.\n42:06 So there cool and here you go. So value is equal\n42:12 to 1 2 3 4 name is equal to Steve and these are the inputs we passed. Now notice I didn''t pass results as an input\n42:19 as well. I could have uh done that but Langraph automatically sets that as like\n42:25 a a none value in this case if you don''t pass an input. Now here''s where you need to be\n42:33 cautious. If I had actually used state result here as well to uh update state\n42:39 result like I used state result to update either itself or something else\n42:44 then you would run into a problem because your state result has been initialized as none because you didn''t pass it as an input. So be mindful of\n42:52 that. But in this case it worked because we''re only assigning state result. We''re not using it to assign something. It''s\n42:59 getting assigned. Cool. And you can see after the action has been performed uh\n43:05 your operation has been performed and the thing has been concatenated. You can see result is here and that was exactly\n43:12 what we were getting before we cleaned this up. Cool. So hopefully you understood that. Again it should have\n43:18 been quite intuitive and interpretable but um to solidify your understanding even more complete the exercise. So I''ll\n43:25 see you at the exercise then. Okay. Welcome to the exercise,\n43:30 your second ever exercise. And for this exercise, I want you to create a graph\n43:37 which passes in a single list of integers along with a name and uh an\n43:42 operation this time. And if the operation is a plus, you add the\n43:47 elements. And if a well times, you multiply all the elements all within the\n43:54 same node. So don''t create an extra node yet. So for example your input should\n43:59 could be jack sparrow your values 1 2 3 4 again and then your operation uh uh\n44:04 multiplication and your output should be in the format of hi jack sparrow your answer is 24 so just to give you a hint\n44:11 as to how you would perform something like this uh you would need an if statement in your node so slightly more\n44:17 complicated but the whole concept is the same so once you''ve completed this exercise I will see you in when where we\n44:25 build this third graph All right, see you there. Okay, welcome to your third\n44:33 graph. So, what are we going to do this time? Well, enough processing multiple\n44:39 values and everything. Let''s actually get the graph more complicated. So, that''s why we''re going to be building a\n44:44 sequential graph. So, all it all that basically means is we''re going to be\n44:49 creating and handling multiple nodes that can sequentially process and update different parts of the state. So we will\n44:56 learn how to connect nodes together in a graph through edges of course and we''re going to invoke the graph and really see\n45:02 how the state gets transformed as we uh progress through our graphs step by step. So again your main goal is should\n45:10 be to understand how to create and handle multiple nodes in langraph.\n45:15 Sounds cool. Okay I''ll see you at the code. Cool. So now we''re about to code\n45:22 up the third graph. Uh, and we''re making quite fast progress. So well done on that. So again, the imports are the\n45:30 same. State graph and type dictionary. Perfect. And like we''ve done in the previous two\n45:37 graphs, we''re going to be coding the uh the state schema or the agent state first. So let''s have class agent state.\n45:46 And again, it needs to be in the form of a typed dictionary, right? And in this case, let''s have the three attributes as\n45:53 all strings because we''ve already we already know how to handle multiple data types, right? So, let''s keep it simple.\n45:59 Name string, age string, and final string.\n46:06 Okay. Now, here''s what we''re going to build. Now, we''re about to build our two\n46:11 node functions, uh, which are again the actions. Okay. So again you simply write first well\n46:19 I''ll name it first node in this case and like I mentioned before we pass in the\n46:26 state and we return the updated state. Okay. So again healthy habits doc string\n46:34 again. So this is the first node of our\n46:39 sequence. Okay. And what do we want to do in this specific node? Well, I really\n46:44 just want to manipulate uh the final part. So, let''s say something like\n46:51 state final is equal to state or let''s\n46:56 have an f string\n47:02 f state name. Let''s say something like\n47:08 hi that. Cool. And we''ll just return the state. Perfect. And now again we create\n47:16 a new node. So state agent state. Return\n47:21 that. Perfect. And I''m just going to copy this dock string and just change it. This is the\n47:27 second nerf. Perfect. Okay. To speed things up. And in this case I also want\n47:34 to have state final is equal to you are\n47:43 state age years old. Again quite a simple\n47:50 example easy to follow. That''s why I''ve kept it as quite a basic graph. I mean\n47:55 it''s not going to solve the world''s problems or anything but it will help you understand.\n48:00 There is one logical error which I''ve put deliberately here. I want you to try\n48:07 to identify it. Okay. So the logical error in this\n48:15 case is the that once we''ve built our graph and everything what would have\n48:20 happened is we would have said hi to whoever uh we pass in let''s say Charlie\n48:26 or something. So, hi Charlie. And we store that in the final uh attribute in the state, which is what we want. But\n48:33 here''s where things get like start to be well logically incorrect. Once we\n48:39 finally get to our second node, again, we''re updating state final, which you can do. You can repeat, you can um\n48:46 interact with these attributes at in in any node possible in all of the nodes.\n48:52 And you can do it as many times as you want. But notice this part. What''s\n48:58 happening here is we''ve completely replaced all of the content we had before. So remember how we had hi\n49:03 Charlie? We''ve just completely replaced it with you are age years old. But we\n49:10 want both of them both of those stuff, right? So how do we get both of them?\n49:15 Well, again we just concatenate them. So we can have something like state\n49:21 plus state file. And there we go. Logical error should be now solved,\n49:27 right? Cuz now we have concatenated state final. Uh we''re essentially just like adding on to uh we''re preserving\n49:35 what we had before, right? Okay. Now let''s get to the fun part. How do we\n49:41 actually build this graph? And really it''s quite similar to the previous two\n49:46 graphs except there is one new thing which you''re about to learn. So like always we use state graph to start the\n49:53 framework. So agent state and let''s store it in graph. Again I could have had this name\n50:00 the width variable into anything. I''ve just kept it graph cuz it makes intuitive sense. Okay. Now we add our\n50:07 nodes. So we do graph add node. And for simplicity sake I''m just going to have\n50:13 the name as the same name as the uh function. Okay. So that way it''ll just\n50:19 be easy to follow. So graph add node and second\n50:27 node. Second node. Cool. Okay. Now that we''ve added both nodes, we need to\n50:34 obviously s uh add the entry point and the end point, right? So we set the\n50:40 entry point like this. Again, quite self-explanatory because we wanted to connect to the first node, not the\n50:45 second node, right? So it should be start uh first node, second node, end\n50:51 point. How do we connect the first node and the second node together\n50:59 though? Hopefully you had an answer for that. Uh if you remember or recall from\n51:04 the previous section, theory section, there was an element in Langro called the edge. That''s exactly what we''re\n51:10 about to do right now. We''re about to use edge and that was the new thing which I was talking about a few moments ago which you''re about to learn. So how\n51:17 do we use it? Well you use graph edge add edge and if we can hi perfect. So\n51:25 again it''s quite simple you use a start key and end key. So sim similar to entry point where but your in this case you\n51:33 need to pass two parameters. So the edge we want is between the first node and the second node right? Well that''s\n51:40 exactly what we pass here. So first node and second node and like before we will\n51:48 just set the finish point at second node and we will compile this. Now how will\n51:53 this graph look like? Take a moment to try to think of how it will look\n52:02 like like that. Start point end point and these two notes are sandwiched in\n52:07 between. But now there is a edge. It should be called a directed\n52:12 edge if I''m being like quite picky. But yes, a directed edge cuz the flow of\n52:18 data or your flow of your state updates is from the first node to your second node. Right? Cool. So now that we''ve\n52:24 built that, let''s again invoke this. So I''ve got this code ready here. Uh let''s\n52:30 invoke it. Let''s pass the parameter as Charlie and let''s pass the age as 20.\n52:37 Cool. Print result. Perfect. Apart from the uh\n52:43 misalignment here which I can just change right now. Perfect. Okay. So now you can see\n52:49 it says hi Charlie you are 20 years old. Now obviously we could have performed all of this in one single node which we\n52:56 have been doing in the previous subsection but the obviously the aim was to be able to create multiple nodes\n53:03 right and handle um the state how the state progresses. So yes you one\n53:08 important thing which you''ve learned is obviously how to use the add edge method but another concept which you have\n53:15 solidified here is you can uh change these at these keys of your state at in\n53:22 at any point in time like as long as as however many times you want cuz remember\n53:29 here we''ve passed in state final um we implemented state final here we implemented state final in the second\n53:35 node if we had more nodes in the sequence. We could have done that again and again and again. And we also learned\n53:41 how to like one key logical error is sometimes a lot of people just accidentally replace uh their content in\n53:48 one of the attributes and that leads to a lot of logical errors. So always be mindful of that. And yeah, that again\n53:55 was quite simple, not too hard and hopefully the exercise which I''m about to give you solidifies this. Cool. So I\n54:03 will see you at the exercise then. Awesome. So now we will move on to the\n54:09 exercise for this third graph. And what I want you to do is really build on top\n54:15 of what we just covered. Instead of two nodes, I want you to build three nodes. Again, in a sequence, don''t need to go\n54:22 too fancy yet. We will again three nodes in a sequence. And we will have you will\n54:29 need to accept the user''s name, their age, and a list of their skills. So the first node will be specifically for\n54:35 personalizing the name field with a greeting. The second node will be describing the user''s age. The third\n54:42 node will be listing all of the user skill in a formatted string. And then you''ll need to combine this and uh store\n54:49 it in a result field and output that. And this should be a combined message. And the format I would like you to\n54:55 output is something like this. So let''s say the name was Linda. And let''s say Linda welcome to the system. You are 31\n55:03 years old and you have skills in Python, machine learning and langraph. Okay. And\n55:09 just as a hint for this exercise, I would you''ll need to use the add edge\n55:14 method twice. So this will really solidify your understanding on how to build graphs in general. All right,\n55:22 cool. So once you''ve done that, again, answers will be on GitHub for all of the exercises. Once you have uh cross\n55:28 referenced and checked that you''ve done it right, I will see you in the next section where we build our fourth graph.\n55:34 All right, see you there. Welcome, welcome, welcome. Okay, I''m particularly excited for uh teaching\n55:41 you this graph, graph 4. Why? Because we''re about to learn how to build a conditional graph. So for the very first\n55:49 time, we''re about to implement conditional logic. And obviously we''ve\n55:54 done it in a previous exercise before but that was within a single node. This is how to implement conditional logic in\n56:00 the overall graph structure. And so we will be implementing conditional logic to route the uh flow of data to\n56:07 different nodes. We will be using the start and the end nodes to manage entry and exit points. We will be designing\n56:14 again using multiple nodes to perform different operations such as addition and subtraction. And we will be able to\n56:20 create a router node to handle decision-m and control the graph flow. So the main goal is really to you how we\n56:28 can use this inbuilt function which uh allows you to create conditional edges in langraph. All right, exciting stuff.\n56:35 I''ll see you at the code. Okay, so let''s actually code this\n56:41 up now and you''ll see the imports are slightly modified this time. Again, type\n56:47 dictionary and state graph is there. But now I''ve also imported start and end point. Again, if you remember a few\n56:54 subsections ago, I told you there are multiple ways to be able to initialize the start and the end point. And this is\n56:59 another way you could. Arguably, this is the easier way, but um whatever. I don''t\n57:06 really have a preference, but I''ll teach you both ways regardless. Okay, let''s import these. Successful. Okay. like\n57:13 standard procedure we will design we will um code up the uh the schema the\n57:19 state schema so class agent state and let''s again type dictionary in this case\n57:27 uh uh I want to be able to pass in two numbers and pass in an operation so a\n57:33 plus operation and a minus operation one of those two operations now obviously I\n57:38 could have handled uh all of this within one single node But that''s not the point\n57:43 here. I''ve kept it deliberately very very simple. So the main concept which you learn is how to uh implement\n57:50 conditional logic. Okay. So let''s code the different uh keys\n57:56 which we require. So number one will be an integer. Operation will be in the string a plus or a minus. Uh number two\n58:05 will be an integer and final number will be an integer. the final number will be the result of either adding or\n58:11 subtracting the two numbers. Easy enough. We''ve done this multiple times now. Okay. Now, here''s\n58:18 where things get interesting. Now, just a heads up. Initially, this won''t make\n58:24 sense. But when we look at it from a bird''s eye view and we look back at all the code in this subsection again, uh\n58:32 everything will start to click. So again, it won''t make sense initially, but it will once we look at it. Uh\n58:38 again, don''t worry. All right. So let''s create our first node function. Let''s call it adder. And it''s again still a\n58:46 node. And we input the state schema. And we return the updated state schema and\n58:53 dock string again. But uh this time I''m just going to copy it from here. Uh it''s\n58:59 tells exactly what it does. This node adds the two numbers. Uh and easy enough, we just do state final number is\n59:06 equal to state uh number one plus state number two. Okay. And we just\n59:14 return the state. Quite simple, right? And just\n59:20 like what we did with the addition, we need a node for subtraction as well. So def subtractor. Now uh I already\n59:29 implemented it to uh don''t to not waste time but this node subtracts the two\n59:34 numbers. It''s very similar to the previous uh node function. Uh it just\n59:39 subtracts these two numbers. Again yes you could be saying what if number one is uh smaller than number two it''ll give\n59:46 you a negative result. It that doesn''t matter. The main aim again was to implement the conditional logic not the\n59:53 um inner workings of each node. Okay. Okay. Now we built another type of node.\n1:00:00 Uh and we initialize it the same way but this time let''s call this node decide\n1:00:06 next node. Let''s actually give it a name which actually says what it does. Right.\n1:00:11 So again we use state agent state and we pass like this. Perfect. Okay. Now the\n1:00:19 dock string will be something like so. So this node will select the next phase\n1:00:25 of the graph or well next node of the graph I should say. Okay. Now we use an if statement and\n1:00:33 before I code something let''s just try to map how this will work. This specific\n1:00:39 node will be at the start of our uh graph. So we will have the start node. We will have this uh this specific node\n1:00:46 we''ll call it the router. and this router because it routes uh the next uh\n1:00:52 to the next node depending on what the state schema is at that point. So we will have the uh I will put an image up\n1:01:00 right now so you kind of get what I''m trying to say but we essentially will\n1:01:05 have the router decide whether we uh add the two numbers and subtract the two numbers and obviously this will be\n1:01:11 decided with the operation uh attribute right which you should see from here.\n1:01:16 Okay, let''s code this up now. So this is not the hard part. If state operation,\n1:01:24 if I can spell if state operation is equal to equal to\n1:01:31 plus. Okay, if state operation is equal to equal to plus, we need to do a certain thing to\n1:01:38 pass it to the next node. Okay, now here''s well your first guess could be\n1:01:45 okay. Well, we guess I guess just call this function, right? Not exactly. Not in langraph. You actually return\n1:01:54 uh return the edge. Now, we haven''t described the edge yet, right? But for\n1:02:00 now, I will just say the edg''s name is addition operation. So, addition operation. Similarly, if it''s\n1:02:07 subtraction, we will do this like so. So just to reiterate we will uh you we will\n1:02:15 see what the um value is at the operation in the state schema. If it''s a plus we call we will return the edge\n1:02:23 addition operation and if it''s a subtraction we will use the subtraction operation edge. Again we haven''t\n1:02:30 described or defined these two edges yet. That''s what I was saying earlier. When we look at it from the bird''s eye\n1:02:36 view later on in a few moments once we''ve built everything it will make much more sense. So stick with me for now.\n1:02:42 Okay. And runs perfect. Now we build the graph. And now here''s the exciting part.\n1:02:50 So we again like normal standard procedure we use state graph to create\n1:02:56 the graph framework. So graph is equal to that. And let''s add these nodes uh to\n1:03:02 the uh to our graph. So graph add\n1:03:08 node and let''s say router. Okay. And again we will pass\n1:03:15 this decide next node. Perfect.\n1:03:21 Okay. Now I have another confession to make. Lots of conventions. I know this\n1:03:26 won''t work. I know I haven''t built the rest of the graph yet but this eventually will not work. And there is a\n1:03:34 subtle reason why this won''t work. You know, it''s mainly in this line. Add node\n1:03:40 router decide next node. The problem is with decide next node cuz oh, you can\n1:03:47 see that the dock string appears once we press uh the decide next node. But the\n1:03:52 reason this won''t work is look closely at these three functions. What are we\n1:03:57 doing in these two functions that we''re not doing in this? I''ll give you a moment to try to analyze\n1:04:10 this. Okay. So, doesn''t matter if don''t worry if you didn''t get that. The\n1:04:16 correct answer is we are returning this updated state in this one and this one.\n1:04:21 But in this node, we''re not. We''re just returning the edge. Subtle difference I\n1:04:28 know but that''s how Lang graph works and you will see why they do it like that\n1:04:35 uh right now. So how do we deal with this? Now I obviously could have built this graph and then I would have shown\n1:04:41 you the error but then things would have just gotten messy. That''s why from the get- go I have told you why this wouldn''t work. So now that you know why\n1:04:48 this won''t work, how do you fix this? Simple. You use this code lambda\n1:04:54 state. Now, if you have used lambda functions before, this is quite easy to understand. If you haven''t, don''t worry.\n1:05:00 All this is saying is your input state will be your output state. That''s it. In\n1:05:07 even more simpler words, think of this as a pass through function. So, what it''s saying is your\n1:05:14 input state will be passed, your state will be inputed and your output will be\n1:05:21 the exact same state. Now, why is it the exact same state? because you''re not changing the state at all. You''re\n1:05:28 comparing stuff here, but you''re not assigning anything. There''s a difference between comparison and um and\n1:05:34 assignment. Right? Again, even in this one, you''re just comparing to see whether the operation is a minus, but no\n1:05:41 assignments been made at all. In fact, there''s been no changes to this state\n1:05:47 whatsoever. That''s why we can use this as a pass through function. Now, hopefully that made\n1:05:53 sense. Okay, let''s continue. Again, we will get a lot more practice. Don''t worry, this\n1:05:59 is the first time you''re seeing this. Okay, so now we will add the edge. And\n1:06:05 this is just the normal edge we did last time. So, we will need the start key. And now here''s how you initialize\n1:06:11 differently. Remember how we used to do set entry point and set finish point? We don''t do that anymore. Uh we use start\n1:06:19 the keyword cuz that''s what we imported. Make sure to import it if you do it this way. uh you use start and end. So your\n1:06:26 start will be a start point and your what do you want the start to be connected to? Well, we want it to be\n1:06:32 connected to the router. If I put this in quotation marks, perfect. Now, why not add node or\n1:06:38 subtract node? Well, think again. Refer back to that diagram which I''ll show in\n1:06:43 right here. We if we connected the start point to\n1:06:48 the the add node or the subtract node, well then what''s the point of the router in the first place, right? The whole\n1:06:54 point was the router decides what the inputs are and then from there it branches off to the correct node. So\n1:07:01 that''s why the router needs to be the first node we uh connect our start point to. Cool.\n1:07:07 Okay. Perfect. Now we add the we now implement the main the new thing which\n1:07:14 we are going to learn in this section is graph dot add conditional edge. So graph\n1:07:21 dot add conditional edges. Now again wow looks really\n1:07:26 confusing but it''s actually much more simpler than it looks like. So the first uh part is your\n1:07:33 source which you can see here as well. So the source will just simply be the name of the node. And what''s the name of\n1:07:40 the node which we want the conditional edge to be? It''s the router node, right? So that''s going to be the source part.\n1:07:47 Perfect. Now if you look here, it''s asking for a path. What''s the path you would like it to do? Now before we\n1:07:54 implement the path, we obviously need to per uh imple uh tell it what action what\n1:07:59 what action it needs to do. And that''s where this node will come in the decide next node part. So we pass that as the\n1:08:04 second parameter. So that''s the path. And now we implement something\n1:08:10 called the path map which you should have briefly saw\n1:08:15 here. Uh there path map. So we''ve implemented the source which is the\n1:08:21 router. We''ve implemented the path which is your uh decide next node function. Uh\n1:08:27 again don''t need to worry about hashable runnable any and all of this stuff. Okay, it''s you don''t need to over\n1:08:33 complicate it. Now it''s time for the path map. Okay, so now your path map will be in a form of a dictionary. And\n1:08:40 remember how I said earlier that we had implemented addition operation and subtraction operation. These were edges.\n1:08:46 So now we''re about to implement those only. So we''re about to create two new\n1:08:52 edges here. Let me just write this code up for you and then it will make sense.\n1:08:57 Give me one second. Okay, so there we go. Now what is this\n1:09:05 code actually saying? Well, this is in a format of edge and\n1:09:12 node. Now the starting point of this edge will obviously be this router node and it''s telling us where it will\n1:09:18 connect to. This uh visualization will be it will be it''ll be much easier to\n1:09:24 visualize when I actually show you the graph. Don''t worry. But for now, addition operation and subtraction operation is the edge. And the two nodes\n1:09:31 are add node and subtract node. Right? Okay. Uh lastly, we now we''re now at the\n1:09:38 point where we need to create the end point. But obviously, we if you look back at this diagram which I''ve shown on\n1:09:44 the screen right now, you can see that the we need two edges to connect to the\n1:09:51 end point, right? We need to we need an edge from the and node and we need an edge from the subtract node. So we can\n1:09:58 add two edges like this. graph edge. Uh we uh start at the add node and\n1:10:05 then we end at the endpoint. Again similar subtract node and endpoint. And then we just compile this. So app is\n1:10:11 equal to graph.compile. Cool. No errors. Okay.\n1:10:18 Now here comes the most exciting part. Again try to visualize what this graph\n1:10:23 will actually look like. Okay. So it should look something\n1:10:29 like that. Probably slightly different to what you initially anticipated but\n1:10:36 that''s okay. We again have a start point. We have the router and we have the our two nodes add node subtract\n1:10:42 node. And notice remember when I said addition operation and subtraction operation are the edges names. Well,\n1:10:48 here it is. Addition operation and subtraction operation. It''s telling us uh what the which direction to go into.\n1:10:55 Do we go how do we go to add node? Well, we use the addition operation. How do we go to subtract node? Well, we go to the\n1:11:01 subtract operation. And then obviously we create these two edges, these two to connect to the\n1:11:10 endpoint. Awesome. So, we will once again look at it from a bird''s eye view. But let''s actually invoke this graph to\n1:11:16 see what happens. So let''s use this piece of code. So what\n1:11:23 it''s saying is it''s defining number one as 10, operation as minus and number two as five. So because we''ve used\n1:11:30 subtraction, the final number should be 10 - 5 which is five. And we''ve printed\n1:11:36 the results and the answer is like such. Uh number one is equal to 10, operation\n1:11:41 is equal to minus, number two is equal to 5 and final number is uh five. Obviously the way I''ve invoked it is\n1:11:47 slightly different to what I have done before. Again, this is another way you can invoke. Okay, so not too hard. But let''s\n1:11:56 just go through everything one more time to solidify everything. Okay, so we\n1:12:02 imported everything. We created the state schema using agent state and a type dictionary. Then we created our\n1:12:07 three different nodes which is the add node, subtract node and the decide next node. And this is in within the decide\n1:12:15 next node. You can see that if the operation is a plus, it goes to the addition operation edge which is this\n1:12:22 edge. And if it''s subtraction operation, it goes to this side. And this is how we\n1:12:28 built the graph. We added the nodes. We added the edge from the start point uh to the router. And then we added the\n1:12:34 conditional edge. the new thing which we''ve learned in this section uh which is we uh reference router and we use the\n1:12:40 edge node format. So the edge will be addition operation uh to add node then\n1:12:47 it will be subtraction operation to subtract node. Visually speaking it will be addition operation to add node\n1:12:53 subtraction operation to subtract node. Now, I know this will be quite confusing at first and don''t worry, it took me\n1:12:59 quite a while to understand this myself as well, but hopefully the exercise I''ve given you will really be able to help\n1:13:05 you understand this much better. Okay, so I will see you at the exercise\n1:13:11 then. Awesome. So, let''s actually find out what the exercise is for this graph. So, you need to make this monstrosity.\n1:13:20 Now at first glance it looks terrifying but if you analyze it a little bit closer all it is is what we just coded\n1:13:27 twice. So we coded this and we need to replicate it once more. So in essence\n1:13:33 you need to actually input four numbers and two operations and you need to output their final results. For example\n1:13:39 number one, number two, number three, number four and the respective operation and the respective results. Right? So in\n1:13:44 this case we would have to do 10 - 5 which is 5 and 7 + 7 + 2 aka 9 and those\n1:13:51 two numbers should be outputed. Now the reason I gave you this exercise to do is because this will really solidify your\n1:13:57 understanding about conditional edges which will really be important for the next few next graph and the next AI\n1:14:04 agents we make. Okay. So once you have uh completed it by looking and cross\n1:14:09 referencing the answer on GitHub, I will see you in the next graph.\n1:14:15 Okay. All right. Well done. We''re almost at the end of this section and we''re about to build our final graph aka graph\n1:14:22 5. Now we''ve learned quite a lot about Langraph and its internal mechanisms. And this will really help us in the next\n1:14:29 section where we finally build the AI agents you were looking for. Now in this\n1:14:34 section in this subsection sorry we''re going to be learning an important concept. There''s still one more concept\n1:14:39 we haven''t learned and that''s about looping. So we''re going to be creating well a simple looping graph. Now I kept\n1:14:45 the objectives to be quite small here. There aren''t that many objectives. It''s essentially implementing logic uh which\n1:14:52 involves looping uh to route the flow of data back to the nodes. And we''re going to be creating a single conditional edge\n1:14:58 which you know how to do in the previous section. Regarding the previous section, however, I know the exercise. Please do\n1:15:05 complete that exercise. That exercise will be probably the hardest exercise you would have done until this point.\n1:15:12 So, don''t worry if you didn''t get it. If you did, great job. You''re doing really, really well. But if you didn''t get it,\n1:15:18 look at the GitHub. Try to compare where you went wrong. Remember, in Langraph, there''s more than one way of building\n1:15:24 the graphs. Make sure the graphs are well built and it actually functions.\n1:15:29 And if you want an extension, try to make it even more robust than it is. All right, but back to this now. Final\n1:15:35 graph, I promise. The main goal really is to code up the looping logic. So,\n1:15:40 with that out of the way, let''s build a final code for this section. See you\n1:15:46 there. Awesome. So, final code we have to build for this section. And here we\n1:15:52 go. So, graph 5 squ. Now, I''m going to take a slightly different approach this time. And I''m actually going to show you\n1:15:58 the graph we want to end up building from the get- go. And there''s a reason I''m going to start that from now so we\n1:16:04 get in good practice. The reason is once you finish this course and actually\n1:16:09 start either making your own AI agentic systems for someone else, for your clients or for yourself like make your\n1:16:15 own JavaS system or whatever. You obviously need to plan how it works, right? You need to see okay, what nodes\n1:16:22 do I need? What edges do I need? Does is this does this need to be a conditional edge? where''s the start point going to go, end point going to go etc etc and\n1:16:29 you can either do that via pen and paper or software like I''ve used but point is you need some sort of blueprint and\n1:16:38 that''s how really it works in the industry as well um you can you will obviously have a blueprint and then from\n1:16:44 there you will code up the graph similar to how a UI designer for example uh renders um their UI designs and then\n1:16:52 sends that off to a software developer who uh well develops the application forwards. Right? So that''s the habit I\n1:17:00 want to start uh creating with you. All right. So this is the graph I want to\n1:17:07 build in this section. So there''s obviously going to be a start and end point. And this really should be mostly\n1:17:13 familiar except for this loop. So there we''re going to create a simple greeting node and another node which is called\n1:17:18 the random node. So in the greeting note I essentially want the user to have uh\n1:17:24 stated their name and it should output a simple hi there your name and then the\n1:17:31 graph progresses to the random node and in the random node I essentially want to\n1:17:36 generate five random numbers. Okay, now just as a heads up, yes, this graph in\n1:17:44 industry would be completely useless. I know, but I''ve deliberately kept it simple again so you know the\n1:17:49 fundamentals. Like this loop is could have easily been avoided and transferred\n1:17:55 into a for loop for example, right? Like I could have had a for loop within this node and ran it five times to generate\n1:18:02 the numbers. I get it. But this is again kept deliberately simple so you actually understand the concept. Okay, cool. So\n1:18:09 let''s the usual inputs and the only difference is this time I''ve also imported random but if you have used\n1:18:15 Python before quite a lot you would have come across this library right okay so\n1:18:20 let''s start with our agent state so class agent state type\n1:18:26 dictionary and what''s the first thing we''re going to need well let''s see we\n1:18:31 have the start point do we need anything any keys that no for greeting note what did I say I want uh I wanted the user to\n1:18:38 be able to input their name. So, we need a name attribute or a key. And then for\n1:18:43 the random number, a random node, we need some form of um a list to like\n1:18:51 actually store the numbers. So, we have number and list\n1:18:56 int. Okay, cool. And one more thing, look at this loop. How will we actually\n1:19:02 know when to stop? We need some form of counter, right? So, counter int.\n1:19:08 Perfect. Now, obviously, just as a heads up, when you do go on to make your AI agents and everything, you''re not going\n1:19:15 to know what attributes you need right from the get- go, unless if you planned it like extremely extremely well. But\n1:19:22 chances are you won''t get it. But don''t worry, iteratively well, you''ll obviously be better at speculating what\n1:19:29 attributes you need through practice. But you can obviously do iterative development as well, right? Okay, cool.\n1:19:35 So now let''s actually build these nodes. Okay. So let''s start off with the\n1:19:40 uh greeting node. So how we normally define uh a function. So def greeting\n1:19:46 node, we obviously need the agent states like such. Perfect. And the dock\n1:19:55 string. But uh luckily for me, I''ve already got that here. So I don''t need to do it again. I know it''s boring, but\n1:20:01 habits. Now let''s update update the uh name uh key. So how do we do that? Well,\n1:20:10 you should know by now state name is equal to let''s say something like hi\n1:20:16 there. State name. Perfect. So what will this do? I input a\n1:20:22 name and it''ll replace that name with a string of hi there this person. Now\n1:20:27 let''s also initialize the counter variable here. Now why am I doing that?\n1:20:32 Let me just first write it and think about this. Okay. Now, obviously I''m changing I''m\n1:20:40 setting like the value. So, I will need to have passed in like an valid integer\n1:20:45 when I am passing the uh value when I''m invoking the graph, right? But here''s\n1:20:52 the thing. What if I pass in minus2 for example? Well, as the counter value, as the\n1:20:58 initial counter value, if this line wasn''t there, well, it would have just kept on incrementing until it got got to\n1:21:05 five cuz I want to have five numbers. But if it starts at minus2, well, it would end up giving me seven numbers.\n1:21:12 Now, that''s not robust, right? So, this basically wipes out whatever rubbish\n1:21:18 integer the user even inputs. If they had put zero, well, okay, we replaced it to zero. If they put like minus 20\n1:21:25 because they''re greedy or something, then we have made sure to like set that back to zero. So, so just a way to make\n1:21:31 it robust. That''s all. Uh, return state. Okay, cool. So, now let''s create our\n1:21:38 second node which is a random node. So, we can say random node state agent state\n1:21:46 agent state. Perfect. Dog string. Again the dog strings will be useful. I\n1:21:52 promise in the next section they will. So this generates a number random number\n1:21:57 from 0 to 10. Now this piece of code here\n1:22:03 essentially appends the appends the randomly generated number to the number\n1:22:09 list. Okay, that''s all it does. And what else do we need to do in this node? Well, we need to increment the counter\n1:22:15 value, right? So C uh plus equals to one. So this will\n1:22:22 increment uh the value by one and then we just return the state. Okay, cool. Now here''s where\n1:22:31 we''re how we''re going to implement the looping logic. Now just a warning here and please listen to this. Like in any\n1:22:37 software development uh program or programming language G2, there''s more\n1:22:42 than one way of coding up an application, right? Same goes with Langraph as well. There is multiple multiple different ways of coding like a\n1:22:50 looping code like this graph. I''m going to be showing you one of them. I obviously can''t show you all of them cuz\n1:22:56 there it''s just time constraint, right? But obviously the more you practice the\n1:23:01 uh uh better ways you''ll more efficient ways you''ll find, right? But the way I''m going to show you is pretty efficient as\n1:23:07 well. Don''t worry. Okay. Now, you might have speculated that I''m going to create\n1:23:12 like a router node. It''s close. I''m not going to create another router node. You\n1:23:18 can see in the graph the uh the client let''s say the client wants this graph. The client doesn''t want another router\n1:23:24 node here. So how do we go about that? Well, we could create a conditional edge. How do we do that? Okay, let''s\n1:23:31 begin that. So let''s write a new function say defaf should continue uh state agent\n1:23:38 state agent state and um let''s create this block\n1:23:44 string um function to decide what to do\n1:23:49 next something like that. Okay cool now here''s where we set our looping logic\n1:23:55 and this should look quite familiar to you\n1:24:00 now. Perfect. So let''s run that. Okay. So what have I written here? Well, if\n1:24:05 the counter value is less than five because we''re starting with zero, right? So 0 1 2 3 4. That''ll be five values. Um\n1:24:12 I''ve also written a print statement so like we can keep track of um um the\n1:24:18 progress. Also whenever I''m writing the code as well when you''re uh coding with me or doing the exercise, it''s really\n1:24:24 helpful to print uh statements uh like put in print statements everywhere. Or\n1:24:29 you could also use break points as well. So you know uh where to where the code failed if it\n1:24:36 fails. Okay. So here we return the loop a loop edge and the exit edge. So\n1:24:44 obviously we have the loop edge and this will be the exit edge. So everything is going to plan so far but um so far is\n1:24:50 the key. You never know, right? Okay. Uh just as a heads up though, I want to\n1:24:55 show you this. So this is how the trajectory should follow. We start at the greeting node. Why? Cuz we obviously\n1:25:02 go from the start to the greeting node. And then we enter the random node. And we enter the random node and exit it\n1:25:08 five times. So 1 2 3 4 5. Why five times? Because we want five random\n1:25:14 numbers, right? By then this if statement will uh well it won''t work. It will fail. So we will go to the else\n1:25:20 statement and return exit. And if we return to exit, we''ll go to the end node. Uh okay. endpoint. Okay, so that''s\n1:25:28 how the general gist is. Okay, let''s quickly make this graph. So you should know how to initialize a graph agent\n1:25:35 graph and let''s just add these nodes. So we have our two nodes which are\n1:25:43 here greeting and random which is exactly what we wanted, right? Greeting node and random node. Perfect. Okay. And\n1:25:50 now we''re going to add an edge between greeting and random. Uh why? Because\n1:25:56 well I''ve created this edge. You see this edge greeting node and random node. This edge that''s the edge I''ve created.\n1:26:03 Okay. Now I''m going to create the um the conditional\n1:26:08 edges which is done through here and I''ve written some comments here\n1:26:14 as well. So uh there will be the source node which is the random. So where I\n1:26:21 want the conditional edge to start from and then the routing function or this tree I should have really written action\n1:26:27 here because is the action I want to perform the underlying mechanism or function which is going to which we''re\n1:26:33 going to um determine which edge to use and that''s uh implemented by the should continue function right and notice how\n1:26:40 again these two edges are the same edges here. So if the loop is uh the one which\n1:26:47 um uh is outputed then we need to go back into its random the random node\n1:26:54 which we''ve generate uh which we put there and if it doesn''t we go to the end part. Okay and then obviously we set the\n1:27:01 entry point. Okay. So again you don''t have to\n1:27:07 set the exit point here uh or the finish point because we''ve already done it using end here. Okay, perfect. And then\n1:27:14 we just compile the graph app is equal to\n1:27:20 graph.compile and okay, it compiled. That''s a good sign. But let''s see if we\n1:27:26 have got our graph to be the exact same. Now I''ll put the graph image here\n1:27:32 so I don''t keep scrolling back and forth. But you can see we have the start point and the end point. We have the greeting and the random. And then we\n1:27:39 have our two condition edges. So we have the loop going back into the random node\n1:27:45 as we wanted and the exit which you can see. So take a moment and you can see compare and contrast. Okay, let''s\n1:27:54 continue. Okay, now I have this code. So I''ve given a name my name uh a\n1:28:01 r um a completely brand new list and I''ve set counter to minus one. And as you can see it enters loop\n1:28:08 one, loop two, loop three, loop four because these are print statements we printed. Uh it says hi there v which is\n1:28:15 my name. Uh number 10 21026 just randomly generated and you can see the counter value is five. Now remember what\n1:28:23 I was saying over the counter. We set the counter value to zero here to make it more robust. If we had not done that\n1:28:28 well it would have generated six times. And now I can set this to minus 100. it will still obviously give me different\n1:28:35 random values but um the code is largely the same. So that''s really the way which\n1:28:42 I personally use to create loops in langraph it''s pretty easy right but um\n1:28:49 obviously with practice you might even find some other ways if you do find other ways like obviously uh do let me\n1:28:55 know uh there''s more than one way again you can send me a message on LinkedIn or Instagram or whatever but um yeah so\n1:29:03 this is finally finally uh we have implemented the code for our final graph\n1:29:09 of the section So just complete the graph 5 exercise please and yeah we should be good to go\n1:29:16 to make AI agents. So I''ll see you at this codes exercise. Okay\n1:29:22 cool. Okay good job on that. Now for the exercise for this last graph uh you need\n1:29:28 to implement this graph on the right. So you need to implement an automatic higher or lower gain. So for context,\n1:29:36 you need to set the bounce which we can guess between 1 to 20 integers of course and the graph has to keep guessing where\n1:29:43 the max number of guesses is 7 where if the guess is correct it stops but if not\n1:29:49 we keep looping until we hit the max limit of seven. Now please note we don''t have to pass any inputs the actual graph\n1:29:57 should automatically guess by itself. So there should be no human in the loop human intervention at all. So each time\n1:30:03 a number is guessed the hint node aka this node should say either higher or\n1:30:08 lower and the graph should account for this information and guess the next guess according accordingly. So for\n1:30:15 example the input should be something like the player name student. The guess should just be an empty list cuz we''re\n1:30:20 initializing the list. Attempts should be set to zero and the lower bound and upper bound should be 1 to 20. Now the\n1:30:27 reason I''ve also passed these as inputs is because uh if you wanted to expand\n1:30:32 this to maybe 1 to 50 numbers or whatever you can. It''s quite easy to do that. So just as a hint uh it will need\n1:30:40 to adjust it its bounds after every guess based on the hint provided by the hint though. So once you''ve completed\n1:30:47 this exercise you would have fully reinforced uh your understanding about loops in langraph. So once you''ve\n1:30:54 completed this, cross reference it. Cross reference the answers on GitHub. I will see you in the next section where\n1:31:00 we finally begin AI agents. See you there. Okay people. So welcome back to\n1:31:08 this brand new section where we actually start learning about AI agents. Now we\n1:31:13 finally are upgrading our ability in Langro. I even upgraded my clothing sense. Not really. But this is exciting\n1:31:21 times cuz we actually finally build AI agents. So, we''re going to build a lot of AI agents in this section. And\n1:31:28 starting off with the first agent. Well, technically it''s not really an agent, but I just named it that because it\n1:31:34 sounds cool. But um technically it''s not though. But let''s see what we''re going\n1:31:40 to actually learn in this section in this subsection. So, we''re going to build a simple bot. That''s it. And these\n1:31:46 are the objectives. So we''re going to define a state variable uh state structure which we''re going to have a\n1:31:52 list of human message objects and I briefly uh me uh mentioned what a human\n1:31:58 message was uh a long time ago in the course. Uh what it is it''s well it''s in the name it''s a message prompt which is\n1:32:05 given by the human aka us to the AI. Uh we''re going to initialize the GPD40\n1:32:12 model for this uh using lang chain''s chat open AAI uh uh library. Uh we''re\n1:32:17 going to send and handle different types of messages. We''re going to build and compile the graph of the agent. But the main goal really is how we can integrate\n1:32:25 LLMs into our graphs. So what is this sort of graph we actually going to end\n1:32:31 up building? Now it''s very very simple. It''s going to look like this. And yes, this looks exactly like the graph we\n1:32:38 made in the uh first ever graph we actually ever made. But um the functionality will obviously be\n1:32:44 different cuz now we''re actually integrating LM. So exciting stuff people. Uh okay, I will see you at the\n1:32:50 code then. All right, coding time. So now we first code our well we code up our very\n1:32:58 first AI agent aka the simple what and um I''ve already imported all the\n1:33:03 necessary libraries we''ll need uh to not waste time. So while you''re uh coding these up as well and copying these I''ll\n1:33:10 also briefly explain what these are so we''re at the same level. Okay, so we''ve already imported type dictionary and\n1:33:17 list many times before but um these two we haven''t sorry these two the lang\n1:33:23 chain codon messages import human message so I briefly mentioned this in the intro of this section of the\n1:33:30 subsection what a human message is right and this is the library we get it from and similarly we''re going to be using\n1:33:37 openai''s lms so that''s why we''re going to use chat openai from the lang chain open aai uh library uh the\n1:33:44 langraph.graph. Uh these we were familiar with and this is the env. Now\n1:33:50 just a few points. You could have been saying okay wait hold on I thought we\n1:33:55 were about to do a langraph stuff. Why is the lang chain stuff here? Now you must know that langraph is built on top\n1:34:01 of lang chain and lang chain already has the sophisticated libraries right so why\n1:34:07 not actually use them that''s how langraph is designed it''s designed to use the robust sophisticated libraries\n1:34:14 which lang chain offers right so no I''m not a trader we''re still doing langraph stuff but we''re also using leveraging\n1:34:21 lang chain strengths as well okay and um now this env file now it''s okay if you\n1:34:27 haven''t ever um encountered av file before. Essentially, it''s just a file used to store secret stuff like API keys\n1:34:34 or configuration values. So, it''s really there for security purposes. Now, I have\n1:34:40 my own um file stored in my folder structure uh\n1:34:45 so that you don''t see my API key uh because if you do then I would go bankrupt. So, that''s why. Now, you might\n1:34:53 also be wondering why do we need an API key here? We need the API key because\n1:34:58 we''re doing calls to an external LLM. If we were using our own LLM like through\n1:35:05 OAMA, then we would um not have an API key, right? We would just use like the\n1:35:10 Olama library integration with lang chain. So because we''re using charges,\n1:35:16 we need an API to communicate with the LLM in their cloud servers. Cool. So how\n1:35:21 do we actually load this? So to load our um API, we just use a simple Python uh\n1:35:28 code load. Env. All right. So now that we''re at the same level, let''s actually code up our AI agent. Cool. So let''s\n1:35:36 define the state like we always do. So this time class agent state type\n1:35:42 dictionary. Perfect. Okay. Now what are the attributes we need in this section\n1:35:48 uh in this uh state? Well really just one the messages part right so messages but what form will it be well it will be\n1:35:55 in the form of a a list of human messages right so we''ll have list human\n1:36:01 message why because we when we invoke the graph we''re inputting human messages\n1:36:07 right so to tell the large language model that this is a human message I i.e\n1:36:12 Uh this is a message from me the user aka human right. Um we need to actually\n1:36:18 mention human message that it''s a human message type. Cool. Okay. So now we\n1:36:24 actually initial initialize the large language model. So we just write lm is equal to chat openai. And now we specify\n1:36:32 what model we want. Now I''m going for GPD4er. Now yes there''s also chat uh\n1:36:39 anthropic. I think there''s chat oama. Um there''s a lot of like in-built um\n1:36:45 libraries which lang chain offers which is great. Personally I''ve used chat openai a lot. I''ve also used chat\n1:36:52 anthropic a lot as well. Uh personally I like chat openai cuz it''s just really simple to use. I''ve also used tried well\n1:36:59 tried to use chat oama before but really there''s some difficulty in integrating\n1:37:05 it with lang. So that''s why I''ve opted for openi. And if you''re worried about financial cost, don''t worry, it''s\n1:37:11 extremely cheap. Uh if you want, you could also go for the GPD 40 mini model as well if that''s a concern. But trust\n1:37:18 me, it''s extremely cheap. Like the input tokens, output tokens is like in like\n1:37:24 tens of pennies for like a,000 tokens. So really, really cheap. Okay. So now\n1:37:29 let''s actually define our node through our function. So process and we obviously define the\n1:37:36 state and then return the state like so. Perfect. Now\n1:37:42 how do we actually call the lm? Now lang chain and the langraph team really like\n1:37:48 using the word invoke. You might have noticed that to call a graph or like to make the graph run we''ve used invoke.\n1:37:54 Similarly to run the lm we use invoke as well. So we okay let''s store the\n1:38:00 response we get in a variable. So uh lm.invoke and what do we invoke? Well,\n1:38:06 you can see from the uh hints here that it requires an input of language model input. What''s that basically saying is\n1:38:13 what what what do you want the LM to do? Right? What''s your question? Now what is our question? Well, that''s in the\n1:38:19 messages. So we write state messages. So what will happen here is as soon as I''ve\n1:38:24 written state messages, let''s say I have written hi or whatever uh we will pass\n1:38:30 this to the LLM through the invoke method. The LLM will then generate a response from its cloud server through\n1:38:37 our API and it''ll get it will give us back the um its response and then we''ll\n1:38:43 store it in the response section uh the response variable. Cool. And um let''s\n1:38:48 actually print this like so and return the state like\n1:38:55 such. Okay, done. Now we obviously need to create the graph like such.\n1:39:02 Okay. So uh what is it saying? Well, it''s saying that there is we''ve created\n1:39:08 a node process which is that which where the action is the process function. The\n1:39:14 add we''ve added an edge. We''ve added an edit from the start to the end node end point and we''ve compiled the graph.\n1:39:20 Okay. Um yeah. So let''s now ask the for\n1:39:25 the user input. So user input is equal to input. We''ll say enter\n1:39:32 something. And now we will invoke the agent cuz we need to invoke the agent of\n1:39:39 course because we''re creating a graph, right? And the graph is well like agent\n1:39:44 in this case. Cool. Let''s actually run this code now. So, Python\n1:39:50 agentbot. py and perfect. So, enter. Let''s say\n1:39:57 hi. The AI message was hello, how can I assist you today? Now, I can reassure\n1:40:03 you I did not pre-code this or hardcode this. This is the actual LM. Let''s run it uh one more time.\n1:40:10 Let''s come up with a different message like who are\n1:40:15 you and it''ll say I''m an AI language model created by open AAI called chat GBT. So you can this basically pretty\n1:40:22 much confirms that yes this is GBT uh in the background. Okay, but\n1:40:27 um why why just stick to one message, right? Why not uh be able to run\n1:40:33 multiple message like asking multiple messages kind of like a chatbot, right? So this is the code which does this and\n1:40:39 I''ll walk you through this what''s happening here as well. So uh like before we input our query and now we\n1:40:47 basically say keep iterating through and as soon as the user has said like exit\n1:40:52 or something then uh well you exit the while loop and that basically signifies that well you don''t want to talk to the\n1:40:59 ailm anymore. So let''s have get run this. So python agentbot\n1:41:05 py. Okay let''s say hi again. Hello how are you? But now we can run it again.\n1:41:11 It''s just a simple y loop. It''s nothing groundbreaking. So like who made you? Okay, perfect. What is 2 + 2? 2 + 2\n1:41:20 equals 4. Okay. Uh let''s say now, hi, I\n1:41:25 am Bob. Okay, now watch this carefully. I''m\n1:41:32 about to ask what did I just well or I should say what is my\n1:41:39 name? I''m sorry, but I don''t have the ability to know your name or any personal\n1:41:45 information about you. Why is that? Why didn''t it know what my name\n1:41:52 is? Well, even though I clearly specified it. So, let''s quickly\n1:41:57 exit. Okay, now this is important. Nowhere in the code have we actually\n1:42:02 created some sort of memory. That''s why I called this subsection simple bot. And that''s why I\n1:42:09 kept on saying AI agent because it''s not even an agent yet. Uh it''s just a simple\n1:42:14 like the most basic LLM wrapper you can possibly have. But at least now you know\n1:42:20 how to actually integrate um LLMs in your graphs, right? And it''s\n1:42:26 pretty straightforward. You just you um you just uh embed them within your functions and then your functions\n1:42:33 obviously act as the actions in your notes. And that''s it. It''s quite an easy piece of code. Like it''s only what 29\n1:42:40 lines or 25 lines give or take. Uh but yeah, pretty simple. Um I don''t think\n1:42:46 there''ll be any exercise for this cuz well there really isn''t much to do with this. So I will see you in the\n1:42:53 introduction for the second AI agent we''re going to build. Okay. So I''ll see you\n1:42:58 there. Cool. Cool. Cool. Okay. So now we''re going to build our second AI system. And we''re going to try to fix\n1:43:05 the problems we faced in the last uh AI system we built. And what was the\n1:43:11 problem? Well, the problem was it doesn''t remember what we in what we had said before, right? Why? Cuz we were\n1:43:18 calling separate API calls. So now we''re going to try to create a chatbot with some sort of memory. That''s why I\n1:43:24 included the brain emoji here. So let me walk through the objectives for this uh subsection. So, we''re going to use\n1:43:30 different message types in particular in particular the human message and the AI message. We''re going to maintain a full\n1:43:37 conversation history using both of these message types. We''re going to particularly use the GPD4 model again\n1:43:44 using the lang chains uh chat open AI library and overall we''re going to\n1:43:49 create a sophisticated conversation loop. So, what is the main goal goal of this um subsection? It''s really to\n1:43:56 create a form of memory for our agent. So if you''re ready, let''s go to the\n1:44:02 code. All right, awesome people. So let''s begin coding our simple chatbot\n1:44:07 then. Okay, so like last time, I''ve already imported all of the uh necessary\n1:44:12 libraries and it''s largely this exact same except now I''ve added two more uh\n1:44:17 stuff. So the first is the AI message and I explained this in the introduction of this subsection why we need the AI\n1:44:25 message. And I''ve also imported the union type annotation. Now the union\n1:44:30 type annotation is something we covered in the first chapter. So if you if this is the first time you are looking at it\n1:44:35 or hearing about it, I would recommend you going to the first chapter really understanding and watching the first two\n1:44:41 chapters. They''re quite short to be honest and then coming back. Okay. Now\n1:44:47 that being said, let''s actually begin the uh coding then. Okay. So like always, we define the state. So class\n1:44:55 agent state uh typed dictionary. Perfect. Now last\n1:45:00 time what did we define this as? Again we''re only going to have messages again but uh last time we had list human\n1:45:08 message. Okay so that was what we had defined as our agent state\n1:45:16 previously. Now this time we also want to include the AI message as well. We''re building a more sophisticated chatbot.\n1:45:23 So how do we do that? Well, one way or the naive way is to really have it as\n1:45:30 messages AI list AI message or something like\n1:45:35 that. Something like that. And don''t get me wrong, this is still a valid approach. You can still build your graph\n1:45:41 and everything like that, but um I think it''s a bit longer. So let me tell you\n1:45:47 another way which would actually be better. So remove this. Instead, let''s\n1:45:53 use the type annotation union like this. So, union like so. And let''s include AI\n1:46:02 message. Now, what has this done? Essentially, let me first tell you about a bit about human message and AI\n1:46:09 message. Human message and AI messages and like all of these like different structures are actually data types in\n1:46:14 Langraph and Langchain. That''s what the developers of these libraries have got them to be. And um when I write union\n1:46:23 human message AI message then that basically allows me to store uh either\n1:46:29 human messages or or AI messages in this uh key in the state the messages. So\n1:46:35 that''s what that literally means in a nutshell. Now, one important thing which I want to mention is this. All of these\n1:46:42 AI agentic libraries like Langchain, Langraph, Crew AI, Autogen, they''re all\n1:46:48 great, but um you really can make your own AI agentic system by writing just\n1:46:55 Python functions. You don''t even need to use a library. Now that being said, I\n1:47:01 would still recommend using these libraries, especially Langraph because Langraph, well, because it''s a personal\n1:47:07 favorite, no bias at all, but um it''s Langraph really allows you to control\n1:47:14 much more than other libraries would. Obviously, not as much control as if you\n1:47:19 were writing the Python functions yourself and everything, but I think it''s a good balance of how much control you have and how much uh unnecessary\n1:47:26 jargon you need to write. Because think about it, think about how much of um this unnecessary code which you would\n1:47:32 have had to write else otherwise uh langraph and lang chain support. So that''s why I highly recommend using\n1:47:39 these libraries and everything. So again human message and AI message are data types inbuilt data types within uh lang\n1:47:46 lang graph and lang chain. Cool. Okay. Now let''s again initialize the large\n1:47:52 language model as we did last time. And again we''re only we''re using GP4. Okay, now we''re going to create our\n1:47:58 node. Again, it''s going to be the exact same graph structure, by the way. So,\n1:48:03 state agent state, but obviously the actions we perform will be slightly\n1:48:08 different. Now, um let''s write a dock string. This node will\n1:48:15 um this node will uh do solve the request you input\n1:48:23 something. Dog strings aren''t really needed for this AI agent or this subsection because we''re not going to\n1:48:29 use them. But um again, good habit. Okay, cool. Let''s invoke\n1:48:36 this. So what have I done here? This is exactly the same code which we did in the previous subsection. The l we invoke\n1:48:42 the lm uh with the state messages. And what are the state messages? Well, it''s could be either human message or an AI\n1:48:49 message. It''s a list of those. Awesome. So now we write this piece of\n1:48:56 code. Okay. State messages.appen AI message content equal to\n1:49:02 response.content. What on earth is happening there? Okay. Let''s break this down. Response. Well, that''s just\n1:49:08 extracting only the content part of the response aka the response being the answer or the result after we make the\n1:49:15 uh API call from the large language model. And it only extracts the content. So it only extracts like the important\n1:49:21 stuff, right? It removes all the unnecessary jargon which comes with it like the amount of tokens you use and\n1:49:26 all that. It removes that and that''s uh gets stored in that gets converted to to\n1:49:32 an AI message and that''s appended to our state messages um in the state. Simple.\n1:49:39 Okay. Uh now obviously to make it look pretty in the terminal we''re going to\n1:49:45 print this and then we''re going to return the state. That''s it. That''s how simple it was.\n1:49:52 Okay, so now we''re going to create this exact same graph. And that''s why I''ve just copied and pasted it because it''s a\n1:49:58 time waste of me rewriting the code in front of you again and again. So we can just reuse the same code because it''s\n1:50:04 the exact same graph uh graph structure as the previous subsection. Cool. Okay,\n1:50:09 now we''re going to now here''s where it actually starts working differently.\n1:50:14 See, last time we didn''t have this the conversation history. really this is\n1:50:20 what''s going to be our memory in in um this uh setup. Okay, so we have now in\n1:50:26 initialized conversation history. Now again we''re going to ask the user what they want, right? What''s their request?\n1:50:35 So now we use this Y loop and this Y loop was the exact same loop we had in\n1:50:40 the previous subsection as well. uh it only exits unless if the user has uh\n1:50:46 inputed well exit obviously but now look at this the conversation history is\n1:50:52 appended with the human message and the human message is obviously the user input the reason I''ve kept on writing\n1:50:59 content is because well that''s the parameter in human message as you can see here okay cool uh and we''ve invoked the\n1:51:08 agent what is the agent well the agent is the compiled version of the graph the compiled graph uh with uh the entire\n1:51:15 conversation history. Now this is important. The entire conversation\n1:51:20 history is sent, not just the current human message. So uh this will make more\n1:51:26 sense. Don''t worry, I''m um trying my best to explain it right now, but obviously it will make much much more\n1:51:33 sense as soon as I start running it. Okay? So bear with me if you didn''t fully understand that. Don''t worry.\n1:51:38 Let''s remove that for now. Uh, and then we replace the conversation history completely like wipe it with the\n1:51:46 result messages. Why? Don''t worry, it''s going to make sense as soon as I run it.\n1:51:51 And I think yeah, we should be able to run this now. So, let''s write python memory agent.py, which is\n1:51:59 the name of the file. Cool. Okay, let me just quickly write a hi just to see if the API is\n1:52:04 working. It is perfect. Okay. Hello. How can I assist you today? Uh, now I''ll say\n1:52:10 like, \"Hi, my name is\n1:52:17 Steve.\" Hi Steve, it''s great to meet you. How can I help you today? Okay, now\n1:52:22 remember from last time. Last time if I asked it, hey, who am I? It didn''t know.\n1:52:27 Do you think it will know now? Think about\n1:52:34 it. It does. you are Steve or at least that''s the name you''ve chosen to share with me. Uh and the rest is yes\n1:52:42 whatever. So it does know about what I have said but just looking at this code\n1:52:49 I guess you can try to like pick out okay how does it work and everything like why everything works like that but\n1:52:57 um I think we can add print statements and everything. So let''s try to add print statements now and see well how\n1:53:05 this is actually working. So let''s exit the program. Okay. Uh let''s add a print statement\n1:53:13 here. Let me include this. Cool. So what is this saying? So this print statement\n1:53:20 actually kind think of it like a snapshot of what the current state is. So as soon as it goes into a process\n1:53:27 note as just before it''s about to finish by returning the state, we also print the current state as well. And this will\n1:53:34 literally just output whatever is stored in the messages attribute within our state. Okay. So let''s clear. There we\n1:53:43 go. Let''s run this again. So hi, nice to meet you.\n1:53:49 Something like that. Now take a look at the current state. See it outputed hello, nice to\n1:53:56 meet you. How can I see you today? Why did it output that? Well, because in our process\n1:54:01 function it we''ve asked it we formatted it in a way so it says hey AI which is\n1:54:07 this part and the response or content is this part. Okay. Now this response or\n1:54:13 content is also what was stored remember how I said it''s stored in the um in a\n1:54:18 nice manner and was appended to our state messages. Now was it appended to the state messages? Yes. How do you know\n1:54:25 that? Because look at the human message. The human message was what I wrote which was hi nice to meet you. Uh forget the\n1:54:31 additional keyword arguments and response metadata cuz I didn''t provide any. Uh you don''t need to worry about that. The main part is this part the\n1:54:37 content. And then look at the AI message part. Uh it''s the content is equals to\n1:54:42 hello nice to meet you. How can I see you today? Notice how it''s the exact same thing as it was\n1:54:50 here. Okay. Now, now we''re going to go one step ahead and I''m going to ask it\n1:54:56 to say my name is Steve again. Now, think\n1:55:03 about how will this current state change? Pause the video and try to think\n1:55:08 about this like that. Okay. So, now the second\n1:55:16 uh message I sent was my name is Steve. Its response for the second message was hi Steve. How can I help you today? Now\n1:55:24 look at the current state. It still begins with hi, nice to meet you, which was the first ever message I inputed\n1:55:30 into this conversation and then its respective AI message. And then that gets uh appended. Why appended? Well,\n1:55:37 because we had appended the AI message and appended the human message. So that''s why. Okay. And you\n1:55:45 can see the human message is my name is Steve which is the most recent uh message which I put and then the AI\n1:55:51 message which is hi Steve how can I help you today? Perfect. And we can just keep\n1:55:56 going and going and going. But uh for now I''ll exit. Now here''s the thing. This works\n1:56:03 well relatively well, right? We''ve got it like as a chatbot which is what we\n1:56:08 wanted. it has some recollection of memory or or like of what we what we are who we are and everything. But there''s\n1:56:15 two big problems here. Let''s start with the first uh massive problem which is\n1:56:21 this. You know how I''ve exited the program right now. Yeah. Okay. I''m going\n1:56:28 to run the exact same program again. Now I told it that my name is\n1:56:34 Steve. What is my name?\n1:56:40 and it says uh I''m sorry I don''t have access to\n1:56:45 your personal data. Okay. And look at the current state completely wiped out. Again, that''s pretty self-explanatory as\n1:56:51 to why you exited the program and that''s why obviously all the var the data was stored in the variables, right? The\n1:56:57 state was stored in the variables completely got wiped away. So what is the solution? Think about it.\n1:57:06 Well, obviously one potential solution would be to store it in a database, like a large database, right? Or a vector\n1:57:12 database if you''re trying to do some ragged applications. For now, I''m just going to store it in a very simple text\n1:57:18 file. Why? Cuz it''s really easy. And I''ve got the code as well. And usually, honestly, I just store it in a text file\n1:57:24 when I''m prototyping. Now, yes, obviously, storing it in a vector database or a database is much more robust and sophisticated, and that is\n1:57:31 what you should do. But when you''re prototyping and you want to really see uh try to build it quickly and fast, uh\n1:57:38 I just tend to use a text file. It still works uh still works great and everything. So what is the code for the\n1:57:44 text file? Uh it''s here.\n1:57:50 Okay. So what is this code saying? Well, essentially it''s saying create me a text\n1:57:55 file called logging as a as a text file you see in write mode and a file. Write\n1:58:00 your conversation log. This is just to like make it look better and more aesthetic. But this is the main part of the code which um you should actually\n1:58:07 try and understand for every single message in the conversation history. Okay. Now note the conversation history\n1:58:14 was this variable which we had like initialized the conversation history uh stores the AI messages and the human\n1:58:20 messages. So that''s where all of the um the information outside the graph\n1:58:26 actually is. Right? the state is locked in within the graph now and uh the\n1:58:33 conversation history is just another I guess you can say a duplicate version of the state right because we''ve you we''ve\n1:58:40 appended the exact same human messages and AI messages and kept on updating it\n1:58:45 through this line cool so what it says is that for every single message in the\n1:58:51 conversation history by the way a conversation is the duration between my first message and the last message I\n1:58:58 sent that entire thing is a conversation. A conversation isn''t just a single API call. It''s the entire\n1:59:04 length. Okay? So, just to be mindful of that. So, uh it first checks if it''s a\n1:59:10 human message, it writes that as you and then extracts that content and if it''s the AI message, it uh puts it under the\n1:59:16 AI stuff. So, let''s run this again. So, let''s exit this. Clear this. Okay.\n1:59:25 Who? Okay, let''s say hi, I am Steve. Again, I''m using Steve because a\n1:59:31 Minecraft movie just came out, so that''s why. Okay, now I''m going to intentionally make a spelling mistake\n1:59:37 here. Good morning. It should obviously morning should have been spelled, right? But I''m doing that for a reason. It gets\n1:59:44 no problem. Let''s say tell me a joke. Just another random thing. Sure. Why\n1:59:49 don''t skeletons fight each other? They don''t have the guts. Um, really rubbish. My point is it\n1:59:57 works. Now I''ll exit the program. And now it says conversation saved to login.xt. Now look at\n2:00:04 this. Remove that. Let me remove that. Okay, perfect. So this is the conversation log. The first message I\n2:00:10 ever sent was, \"Hi, I''m Steve.\" There, it''s response. Now, good morning. Now, why did I spell it like wrong? Why did I\n2:00:17 do that? Because I wanted to show you that this is the actual human message, my message being stored unaltered. So\n2:00:25 whatever I pass in the state as a human message that stays there. So it''s unaltered. The AI message cannot or or\n2:00:34 anything can really change the previous human messages at all. Right? So that''s\n2:00:39 why. And you can see tell me a joke. Sure. It''s it''s a rubbish joke is after that. And that''s the end of the login.xt\n2:00:46 file. So that''s a really fast efficient way, not the most robust way of course,\n2:00:52 but it is a fast efficient way to be able to store your data outside um the\n2:00:57 application if it stops. Perfect. Now, what was the second problem that I was mentioning? It''s\n2:01:03 this. Look at how I initially I say, \"Hi, I''m Steve.\" I don''t know why I\n2:01:09 printed twice there. Weird, but whatever. Look at the current state length.\n2:01:15 Okay. Then I pass in another message, it becomes longer. I pass in another\n2:01:20 message, uh it gets longer. It keeps getting longer and longer and\n2:01:25 longer. That''s a problem because think about it, you will use these uh library\n2:01:31 uh you''ll use these like large language models whether it''s for your own AI agentic startup or your own mini Javas\n2:01:36 or your own projects or whatever it is. You would obviously want to minimize cost, right? But using so many tokens uh\n2:01:45 using so many tokens like input tokens especially will really eat away uh your\n2:01:51 uh the amount of money you will uh spend like it will drastically increase it and\n2:01:57 that''s a huge problem right we want to try to be conservative a bit about our money and our financial our finances so\n2:02:04 what is one what is a way to solve this think about\n2:02:10 that Well, right off the bat, I can give you an easy solution to implement which is within the code, write it, write some\n2:02:17 code where if the number of human messages exceeds five or something like\n2:02:23 that, then you remove the first human message in your uh history. Why remove\n2:02:29 the first and not the latest? Well, because the latest is most likely to be more relevant, right? The first message\n2:02:35 could is most likely to be the one where um the one which is well not needed or\n2:02:42 it could have been like it can it has more of a chance to be like a bit more less of an impact to have been removed.\n2:02:51 So why did I pick five? Well, five is just a random number I thought of. You could do 10, 15, 20, 25, three,\n2:02:58 whatever. But that''s a really easy solution to do. Okay, so we learned\n2:03:04 quite a lot there. We learned how to integrate human message and AI message within a thing. And now we''ve created a\n2:03:11 somewhat of a sophisticated chatbot, right? It has a memory. It still talks to us. We if we define uh if we write\n2:03:19 who we are, it remembers that and everything and it works great. Obviously, it has its flaws, but for now, it''s pretty good. Okay, so now\n2:03:26 we''re going to build our third AI agent. And this is going to be a special type of an AI agent.\n2:03:32 The technical term for this type of agent is called a react agent and react\n2:03:37 stands for reasoning and acting. So this is a quite a common type of AI agent\n2:03:43 which you will build. So how does it look like? Well, it looks something like this. So it''s quite simple. There''s a\n2:03:51 start point and then it''s an end point obviously. Then you have your agent and then we use a loop where we attach it to\n2:03:58 tools. Now, this could be one tool, two tools, a lot of tools. And it''s the agent''s job or the LLM in the background\n2:04:06 to be able to decide which tool to select. But not only that, it''s all it''s also its job to be able to decide when\n2:04:13 there''s no more tool calling left to do. And when that happens, it goes to the end part. So that''s the general gist of\n2:04:20 what a React agent is. It''s a very very common and famous type of agent to make in Langraph. And that''s exactly what\n2:04:26 we''re going to be building in this subsection. So what are the objectives? So to build a react agent, the\n2:04:32 objectives are learn how to really create tools in langraph. We''re going to be creating a react graph. Of course,\n2:04:39 we''re going to be working with different types of messages such as tool messages. See, last subsection we''ve we covered AI\n2:04:46 messages, human messages, but now we''re going to look at a lot more types of messages. for example, tool messages,\n2:04:52 system messages, base messages, and we''re obviously going to test our robustness um of our graph. So, the main\n2:04:58 goal is to create a robust React agent. So, if you''re excited, I''ll see you at\n2:05:04 the code. Okay, people. So, now we''re going to code up the React agent. And just a\n2:05:11 heads up, this is going to be quite a long subsection. So, get ready. You can see it''s going to be long because of how\n2:05:17 many imports I''ve done. But because I''ve done so many new imports, I actually want to take some time off and really\n2:05:22 explain each line so that we''re all on the same page. Okay, let''s go. So the first line is from typing import\n2:05:29 annotated sequence and type dictionary. Now we obviously know what a type dictionary is, but we haven''t come\n2:05:35 across annotated or sequence yet. So these are also type annotations. And I''ll start off by explaining what an\n2:05:41 annotated is. So annotated is a type annotation which provides additional\n2:05:47 context to your uh variable or your key without actually affecting the type\n2:05:52 itself, the data type itself. Now what exactly does that mean? Well, I''ll give\n2:05:57 you an example. Let''s say I am trying to create a type dictionary where there is\n2:06:02 an email key in it. Okay? Now, obviously an email is string. So if I was to create a type dictionary, I would have\n2:06:09 written email colon string, right? Sl. That''s how we''ve been doing it. But here''s the thing with certain keys like\n2:06:16 email, they have to be a certain uh format. But like for example, it has to\n2:06:22 be like abcgmail.com for example. But if I pass in\n2:06:28 abcd-gmail.com or something like that, that''s not a valid email format anymore, but it''s still a string technically. So\n2:06:34 it would pass through. So how do we resolve that? Well, that''s where annotated comes in. And I''ll give you an\n2:06:40 example here. Let''s say email is equal to annotated. I''m not going to create the whole type dictionary to save time.\n2:06:46 But for uh the example itself, you first pass in what data type you want it to\n2:06:51 be. So we want email to be a string, right? That''s not changing. But here in quotation marks, I provide some more\n2:06:56 additional information, additional context. And this is basically adding onto the metadata of this key or\n2:07:04 variable. For example, uh let''s say this has to be\n2:07:09 a valid email format. Now, obviously, I should do it\n2:07:15 in more detail, but um that''s how uh for now, that''s fine. So, how how can I\n2:07:22 actually see the metadata? So, if I want to, I would write print email metadata\n2:07:29 uh like that. And then I would press run. And here we go. You can see this\n2:07:35 has to be a valid email format. That''s the exact same thing which is which we wrote here. So that''s annotated done.\n2:07:42 But what about sequence? What does sequence mean? Well, sequence is also a type annotation. And the way I''ve\n2:07:48 described it is here. It basically automatically handles the state updates for sequences such as by adding new\n2:07:54 messages to a chat history. Now what does that mean? Well, it''s really just\n2:08:00 there to avoid any list manipulation to the graph nodes. Obviously, like when we''re using graphs and nodes and all all\n2:08:07 of that stuff and updating the states, there''s a lot of uh list manipulations which we''ll have to do. Sequence really\n2:08:12 handles a lot of that. So, that''s really what it''s there for. You don''t really need to uh worry about it too much.\n2:08:18 Okay. Now, if we continue, we have env uh from import loadenv. From last time,\n2:08:26 we know that this is just to store our API keys and I''ve done that here. That will load the API keys. But you''ll see\n2:08:32 now these three uh imports. We''re importing some new message types here.\n2:08:37 So we''re importing base message, tool message, and system message. I''ll start off with the tool message. So it''s\n2:08:43 essentially a type of message where where the data is passed back to the LM\n2:08:48 after the tool has been called and like the information which is passed is like the content itself, the tool call ID. Uh\n2:08:56 that''s what tool messages. It''s pretty self-explanatory. Now for a system message, it''s the it''s a message for\n2:09:02 providing instructions to the LLM. So like for example, if you''ve used uh LLM\n2:09:07 APIs before, you might have written you are a helpful assistant. That''s exactly what a system message is. And don''t\n2:09:13 worry, we''re going to code this up as well. So you''ll actually see what they are. Now what''s a base message? So in\n2:09:19 the comments, you can see that I''ve written the foundational class for all message types in Langraph. Now here''s\n2:09:25 how this works. Think about the class hierarchy. So you know how you have a parent class and then you have child\n2:09:31 classes as well. Well the base message would be the parent class and these uh\n2:09:37 AI message, human message, tool message, system message and all these other types of message will be like the child\n2:09:42 classes and they will inherit all the properties of the base message cuz that''s the parent one. I guess you can\n2:09:48 say the uh all father or something but the AI message, human message and all\n2:09:53 these child uh classes obviously they''ll have their own properties, right? For example, the tool message has its own\n2:09:58 content and tool call ID and all that stuff. So that''s what the base message is. It''s really the foundational class\n2:10:04 for all the message types in Langraph. Cool. Okay. So now if we\n2:10:10 continue, you can see we''ve imported chat, openAI. Uh we''ve done state graph and M. These we''ve come across. We know\n2:10:16 what they are. And we''ve imported tool and tool nodes which we cover in the second chapter or second section of this\n2:10:23 course. uh these are different elements which we''re going to be uh using in langraph. Now what about this line from\n2:10:29 langraph dossage import add messages. Now what does that mean? So this is a\n2:10:34 little bit um different. This add messages is a reducer function. Now if\n2:10:41 this is the first time you''re hearing that don''t panic. It''s not that hard. So let''s let me copy this one second. Okay.\n2:10:49 So a reducer function is essentially just a rule that controls how updates from nodes are combined with the\n2:10:56 existing state. In simpler words, it really just tells us how to merge new data into the current state. Now here''s\n2:11:03 the thing. If we didn''t have some sort of a reducer function, uh updates would have just replaced the existing value or\n2:11:09 state entirely. And I''ll give you an example for this.\n2:11:15 So let''s say I had a state where it was just high. I had one attribute messages\n2:11:21 and high. Now obviously I should have created the type dictionary and everything and formalize it but just for\n2:11:26 simpler um for times saving purposes I''ve done it like this. Now what if I\n2:11:32 had an update which says nice to meet you. If I didn''t have a reducer function that would completely overwrite it. Now\n2:11:38 in the previous uh graphs and agents we''ve made we''ve appended it but now that we''re using so many different\n2:11:45 messages and calls and tool calling and what whatever we can''t really always append\n2:11:50 everything like it will get far too complicated. So that''s why we need to leverage reducer function. So if we\n2:11:56 didn''t use a reducer function it would just overwrite it completely. But if we did like hi nice to meet you it would\n2:12:03 append it. That''s the key. So in a nutshell, the reducer function really\n2:12:08 just aggregates the uh data in the state. This reducer function uh and the reducer function which I''m talking about\n2:12:13 is add messages. So once again, add messages is a reducer function that will\n2:12:19 really just allow us to append everything into the state without any overring happening cuz so we want to\n2:12:26 preserve the state. Okay, cool. So now let''s actually code this uh react agent.\n2:12:32 Okay. All right. Okay. Okay, I''ve cleared the screen now and let''s actually begin like we how we always\n2:12:38 begin with the uh creation of our state of our agent. So, type dictionary like\n2:12:44 such. Okay. And now we we''ll only have one key in this uh in this example which\n2:12:52 is just messages. And now let''s use the new type annotations we''ve learned. So,\n2:12:58 sequence base message and reducer function add messages. So again this\n2:13:05 piece of code is saying to preserve the state by actually appending it rather\n2:13:10 than overwriting. That''s what this reducer function does. Okay. All right.\n2:13:15 Okay. Oh, and the sequence of base messages is the data type and this\n2:13:20 provides the metadata. That''s why we have the annotated keyword here. That''s really it. Okay. Uh now let''s create our\n2:13:28 first ever tool. Now how do we do this? Some of you who have a um who have come\n2:13:33 from lang chain might know how to do this already. We use a decorator and we\n2:13:39 define like this. Now this decorator basically tells Python that this function is quite is special. It does\n2:13:45 something and well it is special because it''s a tool which we''re going to use. So let''s define our tool as def. Let''s\n2:13:52 create a simple addition tool. Okay. So we''ll say a integer b integer.\n2:13:59 It''s basically going to add two numbers. And this is where doc strings actually come now. And I''ll show you how important they are. For now, let''s say\n2:14:06 uh this is an addition function that adds two numbers together.\n2:14:15 Okay. All right. And we just return a plus b. Simple. Now, how can we actually\n2:14:23 infuse these tools to our large language model? Well, first let''s create a list.\n2:14:30 Add like such. Now, yes, at this current moment, I only have one tool, but in a\n2:14:36 few moments, we''ll have multiple tools. That''s why I''m adding this uh list for now. And let''s actually create our\n2:14:41 model. So, model is equal to chat openai. Model is equal to\n2:14:48 GPT40. Again, I''m using GPD40 because I''ve never had a problem with it to be honest. So how can we tell our GPD40\n2:14:56 large language model that these are the tools you can use? Well, we can use this inbuilt Python um inbuilt function\n2:15:03 called bind tools. Bind tools like that. And we pass in the list of tools we\n2:15:08 have. So that''s tools. Pretty simple, right? Okay. So now large language model\n2:15:14 will have access to all of our tools. Okay. So now we need to create a\n2:15:19 node which actually acts as the agent within our graph. So how do we do that?\n2:15:24 Let me create just a simple function like def model call. We pass in the state agent state. Again it needs to\n2:15:31 return the agent state. Okay. Now I''m going to quickly copy this\n2:15:37 piece of code. Give me a\n2:15:42 second. Okay. So what is this code doing? uh you can see that we''re invoking the\n2:15:49 model aka running the model and this is the system message which we are asking.\n2:15:54 So we''re explicitly saying the large language model that you are my a system please answer my query to the best of\n2:16:01 your ability. So that''s what the large language model''s task is to do. Now if\n2:16:07 we want to get technical here, you could have written it in a slightly different way and that way is through\n2:16:15 this. So remove this. We could have said system\n2:16:22 prompt. Okay. So what''s going on here? Remember how I said system message is also something which we imported. Uh so\n2:16:29 the system message like I said is this line of uh is this line. You are my AS system. Please answer my query to the\n2:16:35 best of your ability. Now, either way would have worked if we if I had just straight up\n2:16:40 passed this string into here. That would have worked as well. Personally, I think this way is better. Even though they\n2:16:46 achieve the exact same thing, I think this way is better cuz it''s more readable. Okay? And you''re only adding\n2:16:51 just one more import. Okay? So, I would prefer you to I would really recommend\n2:16:56 you doing like uh like this so even the large language model knows that this is a system message. Okay, cool. And this\n2:17:04 is uh just another way of writing like the updated state. You know how we''ve been writing state uh brackets messages\n2:17:12 is equal to something something something. Well, this is a more compact way of updating the state as well. So\n2:17:17 return messages response. So we update the messages with the response. No plus\n2:17:22 equal to this this this or adding something we just we can simply just write it with the updated state. Why?\n2:17:29 because the add messages aka the reducer function handles the appending uh for us. It doesn''t overwrite it. Now, if I\n2:17:37 ran this code and I built the graph and everything, would it work? No. Why it wouldn''t work? Because\n2:17:45 think about it, the response when we''ve invoked the model and we store it in the response when we actually invoked it, we\n2:17:52 didn''t actually pass in the query. How do we pass in the query? Think about it.\n2:17:58 All I passed is my system message. Where does the query go? So to be able to add\n2:18:03 the query, I actually have to add like this. So state messages. The query it\n2:18:10 will be in the form of a human message. And the human message will be stored in the messages attribute, right? And now\n2:18:16 that we''ve passed that into our model as well and we can invoke it. And now this\n2:18:21 should work. Okay. Okay. Okay, so now we define the\n2:18:28 conditional edge. Now why do we need the conditional edge here? I''ll put the picture of the react agent. Again here\n2:18:34 you can see that the looping part even like in the last one in the graph when we made the loops for the first time you\n2:18:40 saw that was it was a conditional edge which we had to use and now that''s actually going to come in play here.\n2:18:46 That''s why I took so time to build those graphs because now the concept is coming. So how do we define the\n2:18:53 conditional edge def should\n2:18:59 continue. Okay. So again like always we pass in the state and let''s do it like\n2:19:09 this like such else\n2:19:14 return continue. Okay. So as you know as you um might have guessed end and\n2:19:21 continue will be edges which I''ll define later in the graph. But what is going on here? Well essentially when I''ll pass in\n2:19:28 the query uh when we''ve invoked the actual model you will know that we''ll create a list of tools right so what\n2:19:36 we''re going to be doing is we''re going to be uh getting the last message and we''re going to see if there''s any more\n2:19:42 tools needed to be ran. If there are then we''ll go into the continue edge aka\n2:19:48 we''ll go to the tool node and we''ll select the tool and we''ll do all this uh actions and then come back. If there''s\n2:19:54 no more tool calling left we will just end and we''ll just exit the uh graph and\n2:19:59 that''ll be the case. You''ll get uh uh you''ll understand more what I what I mean when we''ve actually test we''re\n2:20:05 testing and running the graph. Okay. Now let''s just define the graph. So like\n2:20:11 always we create the graph we initialize the graph through the state graph and let''s call the node R agent. So the\n2:20:17 action will be the model call aka the underlying function will be this. Okay. Now we create something called a\n2:20:25 tool node which is also what we covered in the previous uh in the second section\n2:20:30 or second chapter in this course. The tool node essentially is just a singular\n2:20:35 node which contains all of the different tools. So we only have one tool. If you see what this variable is tools, we only\n2:20:41 have one tool which is add. Don''t worry, we''ll add some more tools like subtracts and multiply in a bit. But I just want\n2:20:47 to uh like really solidify your concept of how we can add these tools and how the graph will work. Okay. Now we\n2:20:55 obviously set our entry point and point it to the R agent. Now let''s add our conditional\n2:21:01 edge. So remember remember how I said there''s two edges, continue and end. again continue and end and if it goes to\n2:21:09 the end we end it. If it goes to tools then we go to tool node which is tools. Okay. Okay. So yeah this is\n2:21:17 pretty straightforward still. Right now we also need to add an edge which goes back from our tool to our agent cuz\n2:21:24 that''s how we''re going to create a circular connection. Right? You can see that the conditional\n2:21:30 edge only provides a one-way directed edge from the from either the agent to\n2:21:36 the tool node or the agent to the endpoint. But we need another edge which will go back from the uh tool node back\n2:21:43 to the agent. And that''s what this um that''s what this edge does. And lastly, we need to uh obviously compile it. So\n2:21:50 we''ll just say app is equal to graph dompile. Perfect. That''s it. Now I''ve\n2:21:57 just created a a new helper function here which this isn''t part of langraph.\n2:22:03 I''ve just written this code because it will make every like the tool calling and everything uh like output in a much\n2:22:09 better way. So you''ll see what I mean in just one second. Okay. So now we\n2:22:16 actually can begin. So let''s say the input is uh something like this.\n2:22:24 Let''s say I want to add 3 + 4. Okay, simple. And this line of code basically\n2:22:31 streams the data. That''s all it does. So, let''s actually run this. Clear. And\n2:22:36 let''s do it. Okay. So, remember we wrote add\n2:22:41 3+4. Wow, look at that. So, we''ve added 3+4. It calls the tool and it even knows\n2:22:47 what tool to pick. Add. Um, and it gives us the result. The tool message as you\n2:22:52 can see is seven and the AI message the final AI message is the sum of three and four is seven. That''s it. Let''s try\n2:23:00 something harder now. Let''s write add 34 + 21. So if we run\n2:23:08 this you can see 55 cuz 34 + 21 is 55. And you can also see again all the tool\n2:23:14 calls and everything that''s done right. I want to show you one two more things actually before we add some more tools\n2:23:20 which is this. If I remove this dock string here by commenting out for\n2:23:25 now. Let''s clear and let''s run that\n2:23:30 again. Error. Why? Because the function must have a dock string if description\n2:23:36 is not provided. The dock string is necessary. That''s why included otherwise the graph won''t work. It''s remember it\n2:23:43 tells the LM what that tool is for. Okay. So now that we''ve have uh we''ve\n2:23:49 got that there, let''s try this as well. Add uh 3+ 4. Again, this time I want both of\n2:23:58 them to be executed. So clear now. Do you think this will work? Let''s\n2:24:05 see. Add 34 + 21. Add 3+ 4. Perfect. Brilliant. Okay. So you can see the\n2:24:11 result of adding 34 + 21 is 55. The result of adding 3+ 4 is 7. You can see how the tool was called twice this time\n2:24:18 and that''s the power of the loop which we created. Remember we created the conditional edge and then we also\n2:24:24 created that directed edge back from the tool node to the agent. Let''s let''s try to make it even uh give more um\n2:24:31 complicated stuff. Let''s say add add 12 + 12 something. So let me\n2:24:38 clear this. Clear. Let''s see what happens.\n2:24:46 Wow, look at this. If I press enter, sorry, I messed up there. But you can\n2:24:53 see the results of the addition as well as 34 + 21 is 55 7 24 or and you can\n2:24:58 also see that I called the tool the sorry the AI called the tool three times.\n2:25:04 Now these tool calls is also an indication that the LLM didn''t use its own like information inbuilt information\n2:25:11 which it was trained on to come up with an answer. Right? Remember an LLM doesn''t know how to do maths. It just\n2:25:16 guesses the next output like through probability. But through this we were\n2:25:21 able to actually add the two numbers. So an important concept here is\n2:25:27 the LLM actually decides what should be passed as the arguments to each tool. So\n2:25:33 3 + 4 like if I said add 3 + 4 it will actually uh create it will actually uh\n2:25:39 input the numbers 3 and four and then this tool will handle the uh information return it and it will go back to the\n2:25:44 agent and then the AI agent will decide what''s the answer and everything. So that''s how it works. Awesome. Okay. Now\n2:25:50 let''s make this even more complicated. Let''s add some more tools. Let''s add\n2:25:58 subtract and multiply. Okay. And the only re change we have to do is instead\n2:26:05 of this one line we just now include subtract and multiply as well. That''s\n2:26:11 it. Otherwise this line this code largely stays the same. Now let''s\n2:26:16 actually run this same command and see if it gets confused with the different\n2:26:21 um different tools we have it has access to. Now let''s see. Okay.\n2:26:29 You can see again that 55 724. Okay, it didn''t get confused. Perfect. Let''s now give it a\n2:26:36 different command. Let''s say something like this. One\n2:26:44 second. Add 40 + 12 and then multiply the result by 6. So now it has to make\n2:26:49 use of two different tools. Let''s see if it gets that.\n2:26:55 Okay. Wow. Brilliant. So it first used the add tool and then used the multiplication tool and you can see all\n2:27:02 the queries or all the tool called and everything and the final answer is 312. So 52 * 6 yes it is 312. Okay. Wow that\n2:27:10 works like brilliantly. So now that we know that this is robust what about if I\n2:27:16 add this let''s say also tell me a joke\n2:27:22 please. What do you think will happen? Do you think this will break? Let''s see.\n2:27:28 If I play this and run this. Let''s\n2:27:37 see. Wow. Look at this. The result of 40 adding 14 and 12\n2:27:43 is 52 multiplying that is by 6 is 312. And here''s a joke for you. Why don''t skeletons fight each other? They don''t\n2:27:49 have the gun. I swear to God, it''s always the same dead joke. But you get the point. This is so robust. It can\n2:27:55 handle even queries where it doesn''t even need a tool and that ladies and gentlemen is the power of langraph. So\n2:28:01 it''s it''s so robust even if we don''t need to use a tool it will still give us an answer and the reason it was able to\n2:28:09 do that is once all the tool calling is done it passes it to the agent the agent checks again oh I need to tell it I need\n2:28:16 to tell the user a joke as well and adds that to the final information and then ends it that''s the power of\n2:28:22 lang okay so after all of that we finally now know how to create a react\n2:28:27 agent yes it was a simple react agent but the concepts the same. You can create your own external tools from now\n2:28:33 onwards and you can create your own graph. And that was the whole point of this course, right? For you to actually\n2:28:38 understand how we can uh create these um how we can use different tools and then the rest is up to you. It''s up to your\n2:28:45 imagination. Okay, perfect. So now I will see you at the next subsection. All\n2:28:51 right, see you there. Okay, people. So we''ve made great progress so far. So\n2:28:57 well done on that. But now we make a fourth AI agent. And this time we''ll do\n2:29:02 things again slightly differently. Well, this time we''re going to be making a mini project together. So the project''s\n2:29:10 name is going to be called Drafter. And you''ll see why in a minute. So picture\n2:29:15 this. Me and you are working in a company together. And our boss comes up\n2:29:20 to us and she has a problem and some orders for us. So the problem is this.\n2:29:26 Our company is not working efficiently. We spend way too much time drafting documents and this needs to be fixed.\n2:29:33 Again, a valid problem. So, what are her orders? She says you need to create an\n2:29:38 AI agentic system that can speed up drafting documents, emails, etc. The AI\n2:29:44 agentic system should have human AI collaboration, meaning the human should be able to provide continuous feedback\n2:29:49 and the AI agent should stop when the human is happy with the draft. The system should also be fast and be\n2:29:56 able to save the drafts. Okay. So then me and you start discussing and we are\n2:30:02 going to use land graph obviously and we come up with a sketch. Now the sketch of\n2:30:08 our graph is something like this. It obviously is going to have a start and an end point and it''s going to have our\n2:30:14 agent and the agent will have access to tools aka the tool node. Now this looks\n2:30:20 similar to a react agent which we covered in the last subsection. But there''s a reason we don''t we haven''t\n2:30:26 chosen to do that. See we realize that one of the tools is the save tool. It\n2:30:31 will save the draft, right? That was one of our requirements. But obviously when we once we''ve saved it, the process\n2:30:37 should end, right? But if you remember from a React agent, the tools always goes back to the AI agent, not directly\n2:30:45 to the endpoint. And we don''t want that anymore. So that''s why as soon as the save tool is used because the save tool\n2:30:52 will be within tools right it ends. So that is the structure we have chosen to\n2:30:58 go with. So the only thing left is to actually code this graph. So let''s code\n2:31:04 this together then. Okay. So let''s actually code up this drafter project then. So you can\n2:31:11 see I''ve already done all the imports and I''ve loaded up my env file. Now all of these imports are imports which\n2:31:17 you''ve already uh encountered before. So there''s no point in looking at them again. But the first thing which I''m\n2:31:23 going to do is I''m going to be defining the a global variable. Now this global variable yes it''s a bit\n2:31:31 odd defining global variables and there''s a reason which I''ve done it and this will become more apparent as I go\n2:31:37 through the code but just as a heads up the reason I''ve defined a global variable in this case is to actually\n2:31:44 pass in a state in tools the the correct way to do it in langraph is through\n2:31:50 something called injected state now injected state is beyond the scope of this uh course so the workound on that\n2:31:58 is to use a global variable and what will happen is our tools will uh\n2:32:04 whatever updates are made uh we''ll update the global variable and then when we go on to save it uh the save tool\n2:32:12 will use the contents in this global variable and save that into a text file.\n2:32:17 So that''s why this is included. Okay. So now let''s define our agent state again.\n2:32:23 And the way that''s done is the exact same way we did last time. Uh class\n2:32:28 agent state messages annotated sequence base message add message as the reducer function. So now we define the tools and\n2:32:37 there will be two tools for this. The first tool will be the update tool and the second tool will be the save tool.\n2:32:42 So let''s start off with the uh update tool and I will obviously use the decorator and then create def\n2:32:50 update and then we need to pass in\n2:32:55 uh pass in a parameter content. Now just as a refresher whatever parameters you\n2:33:01 pass or you request who actually gets those parameters? Well, the LLM or your\n2:33:08 model in the background that''s uh what will automatically pass the parameters\n2:33:13 for this model uh for this tool. So, uh in this case the content parameter will be uh that will be provided by the lm in\n2:33:21 the background. So, you don''t need to worry about that. Okay. So, now we need the dock string obviously and I''ve just\n2:33:27 created a simple dock string which just updates the document with the provided content because that is exactly what it does.\n2:33:33 So now we define to interact with the global variable in Python, you obviously need to uh define it uh you need to code\n2:33:40 it like this and then you need to update your document content aka the global\n2:33:47 variable with your current content and then you just return again a statement\n2:33:53 to the like the large language model telling it that we have successfully updated it. So I''ve written document has\n2:33:58 been updated successfully. The current content is this which is the content which we store in the thing. Okay. So\n2:34:07 now we define our second tool which is the save tool. So again same decorator we use uh like\n2:34:14 this. And now we request the llm to give us a file name as well. So it will it\n2:34:22 should give us a suitable file name uh which will be a suitable file name for the text file and uh it will and now\n2:34:28 this save tool will automatically handle all the save logic. So uh as a dock\n2:34:35 string I pass like this. So saves the current document to a text file and finishes the process. And the arguments\n2:34:42 are file name which is the name for the text file. Now, I''ve specifically mentioned that we''re going to be using a\n2:34:48 text file so that the uh uh the LLM knows that the file name which it needs\n2:34:54 to pass has to have a txt in the end of it. Now, if it doesn''t uh by any means\n2:35:01 to make the uh graph to make the entire code more robust, I''ve also written this if statement such that if this file name\n2:35:07 doesn''t end with a txt, just put a txt there just uh as robust as measure. Now\n2:35:13 again we need to uh call the global variable again. So global document\n2:35:19 content. Okay. Now this next bit of code that''s this is not langraph. This is just uh whatever you put in the tool. Uh\n2:35:26 it it doesn''t have to it''s not going to be langraph related. Right. So this piece of code is just uh some code which\n2:35:34 allows you to save the uh contents a the the content store in in the global\n2:35:40 variable under the file name uh and as a text file and I''ve also added this\n2:35:45 exception uh which is a good thing for debugging purposes where it if there''s\n2:35:51 an error it will tell me exactly what the error is and then we can fix it. Okay. So hopefully there won''t be any\n2:35:57 errors. Now we create a list of tools which uh again will be update and save\n2:36:04 because we only have two tools. And now we actually call the uh model and how do\n2:36:10 we call the model like such? Now let me ask you a question. Is this it for the model definition or do\n2:36:18 we need something else? Well, there''s a reason I asked that question, right? We forgotten to do\n2:36:24 bind tools. So bind tools and tools. So that will do. Okay. Now we actually\n2:36:31 initialize the agent itself or the function which will cuz remember the agent will be a node in our graph. And\n2:36:39 what will be the function behind that? It will be this function which we''re about to define. So let''s write this as\n2:36:46 def r agent. And again we need to pass in the state the agent state and it''ll\n2:36:51 return the agent state. And okay so now this doc uh not doc\n2:36:58 string this we need to pass in a system message to our llm right now this llm\n2:37:03 this system prompt will be quite large so get ready uh like such so in this system prompt I\n2:37:11 have specifically said this is a system message and the content is this you are drafter a helpful writing assistant\n2:37:18 you''re going to help the user aka us to update and modify documents and I''ve also written some more stuff about what\n2:37:25 the uh update or what what to do if the user wants to update. We use the update tool. Uh we need to use the save tool to\n2:37:33 save it and to always show the current document say after modifications and all that stuff. Cool.\n2:37:40 Okay. So, oops. There we go. And now it''s time for some robustness\n2:37:48 measures. So when we''re first initializing the graph like when it''s the first message we''re writing\n2:37:54 obviously we''re not going to straight up say uh how would you like to change the document right because we haven''t passed in a document yet. So if messages uh\n2:38:03 this part if that if there''s nothing in it we will have to say something like an\n2:38:08 introduction message right. So this is how you can do that. So we can\n2:38:15 say if not state messages aka if there''s nothing in the state messages then we\n2:38:20 can say uh I''m ready to help you update a document. What would you like to create? and then it collects the user\n2:38:26 input and passes it as a put stores it as a a human message in this user\n2:38:32 message variable. Now what if I''ve already passed it uh passed in a message\n2:38:37 or like we are on the process of updating our draft or drafting it. Well to do that we need this else statement\n2:38:45 and what does this say? Well it says what would you like to do with the document? So this assume this says that\n2:38:50 there''s already stuff in the messages uh state a messages key in the state how do you want to update it further and then\n2:38:57 we also print it uh under this emoji uh in the terminal so the user can also see\n2:39:03 what they''ve inputed and then this is also stored in the user message. All right. Okay.\n2:39:10 Now we combine all of this uh all messages aka the system prompt which was\n2:39:16 the system message uh and we create a list of uh list of the uh state messages\n2:39:22 and the user message the new message which we want the aka the update and\n2:39:27 then we just invoke the model and how do we invoke the model you just use the uh\n2:39:33 model invoke okay so pretty basic code so far there''s nothing hard or nothing\n2:39:39 uh extraordinary or something we haven''t seen before. All of this we have seen before. And now the rest of this\n2:39:47 function is just a print statement which I''ve included. This print statement is\n2:39:52 just for uh um making things look prettier on the terminal. That''s all it is. You can see the true print\n2:39:58 statements. There''s the AI response which will be printed and then there''ll be the tools uh whatever the tool\n2:40:04 messages are that''s also printed. So that''s the whole point of it. there''s nothing like to really like talk about\n2:40:09 it here. Uh and then we also need to obviously return the updated state. Now remember\n2:40:16 last time I showed you that this is also a really convenient concise way to uh\n2:40:22 update the states. So from now onwards we''re only going to update the states like this. Okay. Now we create our\n2:40:29 conditional edge function or the function behind the conditional edge cuz remember let me open this up. So the\n2:40:36 conditional edge which I''m talking about will be this this this conditional edge.\n2:40:42 So there from tools there will be either the select the uh the choice of going to\n2:40:47 back to the agent or the choice of ending it. So we need to create the underlying function behind that. So\n2:40:54 let''s create that now uh under this. So should continue. We''ve\n2:41:02 done this many times before. it det will determine if we should continue or end the conversation and remember continue\n2:41:08 or end the conversation. Okay, makes total sense. Okay, so\n2:41:16 now we do this. So we get the messages and if there''s nothing in the messages,\n2:41:22 well obviously we''ll need to continue, right? It won''t go to the end part. Uh\n2:41:28 and this is just as like a robustness measure to be honest. Okay. So now this piece of code\n2:41:35 is basically saying look at the most recent tool message or the uh recent\n2:41:41 tool we''ve used and we need to check if this tool uh has used the save tool. Now\n2:41:48 why remember how we have two tools we have either the uh update tool or the\n2:41:54 save tool. If we use the update tool, well, we will obviously need to use the continue branch, right? But if we use\n2:42:01 the uh save tool, well, after you saved it, there''s nothing else to do, right?\n2:42:07 You finished your draft, you finished everything, so might as well end the program. That''s why this end tool. So,\n2:42:14 for the continue, uh if to go to the continue um through the continue edge,\n2:42:19 we have to use the update tool. And to go to the end uh edge you need to use the uh the save tool. So should make\n2:42:28 sense now but don''t worry if it doesn''t we will do some more print statements so you see the workflow. Don''t worry. And\n2:42:35 lastly we need to obviously return continue because by default it''s checked here that it''s used\n2:42:42 the save tool. The only other tool left is the right tool and uh sorry the\n2:42:47 update tool. And the update tool means that we have to go to the continue edge, right? Okay. And that''s that uh function\n2:42:54 done as well. So pretty easy still. Now this next function is again I only coded\n2:43:01 this just to make the print statements in a more readable message format uh\n2:43:06 when we printed on the terminal. So you will see where this comes in play when we actually start uh invoking the graph\n2:43:12 and seeing how our process is going. Okay, cool. Okay. So now we actually\n2:43:19 init uh create the graph. So how do we create the graph? We''ve done this many times. We will initialize it through a\n2:43:26 state graph. And now we will add the nodes. So agent and tools. And the tools\n2:43:33 will be a tool node. And again if you notice back we had one node, two node,\n2:43:38 the agent node and the tools node. I''m keep I''m like reflecting back and forth between this diagram and the code\n2:43:45 so I can show you exactly what we''re coding. Okay. So again agent and tools node uh we''ve\n2:43:51 done now we will set the entry point at agent which is the start point aka this\n2:43:57 part right and now we''re going to add an edge\n2:44:02 between agent and tools. Now we need to obviously create this edge because the agent needs to go to the tools right and\n2:44:08 then this edge this directed edge and this conditional edge creates the loop\n2:44:14 uh which will allow for the human AI collaboration. All right. Okay. So now\n2:44:20 we add the conditional edge and that''s the conditional edge which I was talking about the continue at the end aka this\n2:44:27 condition this conditional edge from tools. Okay. And now the last thing we need to do is\n2:44:35 just compile it because we''ve finished the graph completely, right? There''s nothing left. We''ve done the start point, we''ve done the end point, the\n2:44:41 end, this conditional edges done, the nodes done, and then this directed edge is done and the start point is obviously\n2:44:47 done because we''ve uh created a directed. So you can see the entire graph we have created just like that. So\n2:44:53 again, nothing too hard. Cool. Okay. So now we actually run\n2:44:59 the program. And to run it, I have just written this um function so that\n2:45:04 everything is in a more compact way. This is just to invoke the graph. Okay.\n2:45:09 And let''s do that. So that was the entire code. And this code will allow\n2:45:15 for human AI collaboration. Now, yes, we used a global variable. And again, there\n2:45:21 is nothing wrong with using a global variable. I know some of you might frown upon it, but um again, there''s nothing\n2:45:27 wrong with it. If we wanted to use more complicated uh form uh complicated uh\n2:45:32 stuff from langraph like the injected state or even using something like commands and interrupts uh we would have\n2:45:39 to write the code slightly differently but because this is a beginner level course uh we''ve just disregarded that\n2:45:46 completely and we found another way of performing human AI collaboration. Awesome. So let''s actually run this now.\n2:45:53 So let''s write python draft. py and you should be able to see all of\n2:46:00 the things. I made my face cam slightly smaller so you can hopefully see everything. Perfect. So you currently\n2:46:05 have an empty document. Could you let me know what you like to add or create in the document? So what would you like? So\n2:46:11 let''s say we are writing an email to our colleague Tom saying that we can''t make\n2:46:16 it to the meeting. So let''s say write me an email. Let''s say uh write me an email\n2:46:22 to Tom saying we I cannot make it to the\n2:46:28 meeting. Let''s see what it says. So it says\n2:46:34 uh hi Tom I hope this message find you well. Please let me know. Okay let''s now\n2:46:40 give it some feedback on how we how we can improve. So and also you can also see that it''s used the update tool as\n2:46:46 well. Perfect. So, let''s say um make sure to also have specified that\n2:46:56 the meeting was supposed to be at 1000 a.m. at some random negation.\n2:47:05 Canary Wolf. Okay. Okay. Let''s see the updated thing.\n2:47:11 Hi, Tom. Uh you can see message meeting at 10. Uh, can I wolf due to unforeseen\n2:47:18 circumstances? Uh, let''s I don''t like this uh your name part though. So, let''s say my name is\n2:47:26 V and it will update that as you can see. Perfect. Uh, what do what else do\n2:47:32 we want to change? We can also say something like uh let''s say but tell him\n2:47:40 that I can make it at 12 p.m. in\n2:47:47 um New York, some random location. Okay, I''m making this up, but you you get the\n2:47:53 uh plan. Uh the next day. So, let''s\n2:47:59 see. And perfect. It''s updated it. However, I am available to meet at 12:00\n2:48:04 p.m. in New York the next day. Obviously, it''s complete like rubbish like the timings of the location I''ve written. But you can see how we can just\n2:48:13 uh use human AI collaboration here. Uh one more thing which I don''t like is\n2:48:18 this part. I don''t like the fact that it''s not a new line. So, I mean I''m being a bit picky here. We can say\n2:48:24 something like uh put the II hope this message finds you\n2:48:36 well. Awesome. And as you can see that''s done as well. So now let''s say I like it. Save it please. And what\n2:48:45 happens? You can see that uh it used the save tool. The tool results is document\n2:48:52 has been updated successfully. The current content is this and the document has been saved to unable to attend\n2:48:58 meeting email. Now remember we never passed in the file name at all. That was all generated by the by the agent\n2:49:05 itself. And to check we need to go on unable to attend meeting. So let''s see\n2:49:10 here it is. And you can see it''s the exact same\n2:49:15 meeting uh exact same email we said. So, subject, oops, best regards me. All the\n2:49:22 exact same content. Perfect. And we don''t even need to uh make it so that\n2:49:27 we''re drafting emails. We can even drop short stories. We can drop whatever we want. In fact, we can also pass in uh a\n2:49:35 previous message. So, the reason it started off like with nothing is because\n2:49:40 we pass in an empty list. But if you wanted to, we could have written something over here uh with our pre with\n2:49:48 a already existing email or already existing document and then it could the model the agentic system would know that\n2:49:55 this is what the content is the current content how would you like to uh change that and that is exactly how uh we will\n2:50:02 be able to operate on our existing ones. So you can see that this is quite a robust thing. If we want another\n2:50:09 example, for example, uh let''s say python drafter.\n2:50:16 py. Okay, now watch this as well. Look how robust this is. If I say something\n2:50:22 like write an email, it actually gives back questions.\n2:50:28 So sure, what would you like the email to say? D. So remember, it didn''t even go through any tool here. uh using\n2:50:35 langraph we can really make the agents quite robust and that''s the thing which I wanted to show you it doesn''t always\n2:50:42 have to pick a tool its own like LLM like the agent itself cuz remember the\n2:50:47 agent node has an LLM in the background back end the bind tools function allows\n2:50:52 it allows it scope like it increases the scope of it uh by providing some tools\n2:50:57 but that doesn''t mean it has to use those tools if it doesn''t feel like the need to use the tools it won''t and in\n2:51:04 this case it wanted to ask us more questions about it. So it would say show what would you like this email to say\n2:51:10 because to be fair I only wrote three words. Uh but that was what I was trying to show you. So let''s just clear this\n2:51:16 now cuz we don''t need to. And yeah you can see perfectly works human AI\n2:51:21 collaboration in langraph and this is actually somewhat useful as well. Now yes of course you can use GPT4 canvas\n2:51:29 and all of that stuff of course but um this is how you would do it in Lagraph. All right. So, if you would like an\n2:51:36 extension to this, what you could do is add a voice feature as well. So, maybe\n2:51:42 you could add use OpenAI whisper for uh speech to text conversion or add 11 laps\n2:51:48 for text to speech conversion and maybe you can make it voice based cuz right now I''m giving it I''m how am I\n2:51:55 communicating it with text mode? What about voice mode? You could also include a GUI to this. There''s a lot of stuff\n2:52:02 which you can do on you can even have your own knowledge base as well and include that. So a lot of potential with\n2:52:08 this if you want a homework for this uh specific project that there you go. All right. Okay. Cool. So that''s the end of\n2:52:16 this subsection. Awesome. So now let''s build our fifth AI agent. And some of you\n2:52:22 might have been looking forward to this. It''s retrieval augmented generation rag.\n2:52:27 So what will the graph look like? It will look something like this. Again,\n2:52:32 start point, end point, really similar to what a react agent was, right? But uh\n2:52:38 we have two agents in this case. We have a retriever agent and we have our main agent LLM, right? So, and it will have\n2:52:44 obviously a conditional edge, a loop, and everything. Again, we''re bringing everything we''ve learned so far and\n2:52:49 merging them into one. And we''re also going to be learning about a little bit about rag. Now, I''ll assume you know\n2:52:55 what rag is. I''m not going to go too much in detail into like the nitty-gritty of it. But again, in the\n2:53:01 surface level, I will obviously explain what rag is about and everything. Okay. So, if you''re excited, let''s uh let''s\n2:53:08 jump to the code. Okay. So, now you can see that I''ve already done all of the imports\n2:53:13 which we''ll need. But you''ll notice how there are these four imports which we haven''t come across yet. Now, rather\n2:53:20 than explain them from the get- go, I will explain them as they come because it''ll make more sense. uh it''ll make\n2:53:25 more intuitive sense that way. Okay. So now I''m going to be loading our uh ENV\n2:53:31 file which contains all the API keys. And this time I''m going to be\n2:53:37 initializing our LLM differently. Well, slightly differently. It''s the same LLM, but why did I say differently? Because\n2:53:44 I''ve passed in a new parameter called temperature. Now for those of you who do not know what temperature is, it''s\n2:53:49 essentially a parameter which depicts how stochastic the model outputs how\n2:53:54 stoastic you want the model outputs to be. So because I''ve set it to be zero, temperature equal to zero makes the\n2:54:00 model output more deterministic. Similarly, if I had set the temperature to be one, the model output would have\n2:54:07 been more stochastic. Okay. So now we create the embedding\n2:54:13 model. And the embedding model uh is what''s going to convert our text into vector embeddings. Right? Uh so this\n2:54:20 will be the layout for it. Now please note one important thing which is the embedding model has to be compatible\n2:54:27 with the LLM we''re using. You can use whatever LLM you want but make sure the embedding model uh is compatible with\n2:54:35 it. For example uh let''s say we''re using GBD40 uh an open model but the embedding\n2:54:40 model we''re using is from Olama some random model. Now that they wouldn''t most likely they''re not going to be\n2:54:46 compatible. Why? Because there''s so many differences between them. One potential difference could be the vector dimension. So just a rule of thumb. Make\n2:54:54 sure the LLM and the embedding model is compatible. Okay. Awesome. So now we''re\n2:55:01 going to specify the PDF part. So this is the stock market performance 2024 PDF. And essentially this is just a\n2:55:07 document which I created which contains um a lot about the stock market\n2:55:12 performance. Okay. Uh I can show you that right now actually. So this contains nine pages and is just a\n2:55:20 document containing about some stock market details in 2024. Okay. Awesome.\n2:55:26 So now um in case you''ve specified the\n2:55:31 uh you have put the PDF in a wrong directory or if it can''t find it uh this error will pop up. So again I''ve just\n2:55:37 put this for debugging purposes if you use the code which I provided on GitHub. Okay. Now this will load the PDF and you\n2:55:46 can see pi PDF loader is one of the imports which we made here. So again\n2:55:51 it''s in the name and the common. It just simply loads the PDF. Okay. Uh and this\n2:55:58 try and accept command uh just checks if the PDF is there. And pages is equal to\n2:56:04 PDF loader.load. So this essentially says how many pages are there in the document. So you can see there''s nine\n2:56:12 pages in our document. So if I run this command, if I run this, so clear\n2:56:20 python rag agent.py py it should say nine\n2:56:25 pages. So there we go. PDF has been loaded and has nine pages as expected. Right? Okay.\n2:56:33 Now it''s time for the chunking process. Now what is chunking? First look at this. There are two parameters which\n2:56:39 I''ve specified. Chunk size which is a,000 and chunk overlap which is 200. So\n2:56:45 let''s break this down a bit. Going back to our document. So chunk size was 1,000 tokens. So let''s say that this was a\n2:56:54 chunk for example. Okay, obviously that''s not going to be a thousand tokens, but just as like uh\n2:57:00 demonstration purposes, let''s assume it is. So this is saying as soon as you''ve reached 1,000 tokens, you create a new\n2:57:06 chunk. So let''s say 1,000 tokens ended here. So this would be the start of a new chunk like such. Okay. And you keep\n2:57:12 going and going and going until the end of the document. But what if the what about the second parameter? The second\n2:57:19 parameter is specified overlap and that''s essentially saying let me use it in a different color that your chunks\n2:57:25 consecutive chunks should have some tokens um which are which exist in both\n2:57:30 for example because it was 200 the second chunk is not going to start from here. It''s actually going to start\n2:57:37 something like this. They''re obviously going to be the same length uh in terms of tokens but\n2:57:43 they will have some tokens which will be in both chunks. So for example, this part will be in both cuz that''s the\n2:57:50 overlap. 200 tokens to be precise. Okay, so that was just a brief overview of what chunks are in uh rag. Okay, so\n2:57:59 that''s that part done. And again, this recursive character text splitter is one of the imports we did. Okay,\n2:58:08 so this text splitter um chunking process, we now apply it to all of the\n2:58:14 pages, all of our nine pages in our document. Okay. And this piece of code essentially\n2:58:22 saying this, the chroma vector database, we''re going to be using a chroma vector database to store all of our vector\n2:58:27 embeddings, by the way. But the uh the place where we want our chroma vector\n2:58:33 database to be will be specified in this file path. And the collection''s name will be called stock market. Now, you\n2:58:39 can specify it wherever you want obviously, but I''ve just specified it to be in the same folder. Okay.\n2:58:47 So this is just an if statement to make sure that uh if this is the first time\n2:58:53 we''re running this command uh if we''re running this file uh if this collection\n2:58:58 doesn''t exist we will create the um collection in the specified directory.\n2:59:05 Okay, again not too hard yet. Now here comes a try except command u try accept\n2:59:11 block. So this is where we actually create the vector embedding uh where we create the chroma vector um\n2:59:19 database and these are just parameters which I specify. So for example, how I want the pages to be split, what\n2:59:25 embeddings to use, where to store it and the collection name. The collection name being stock market, right? And if there\n2:59:30 is an error, it will throw an error and if it''s successful, it''ll print on the terminal. Okay, awesome. So now we\n2:59:37 create something called a retriever. So the retriever is quite important in rag.\n2:59:42 It''s well obviously the first part of rag retrieval augmented generation. So the retriever is what actually well\n2:59:49 retrieves the chunks the most similar chunks. Um the search type which we''re going to use similarity. It''s just the\n2:59:56 default anyway. Uh you don''t really need to know how that works to be honest. But what you do need to know is this part.\n3:00:03 So in this code I have made sure that every time uh it goes the amount of\n3:00:09 chunks it uh outputs back is five. Why? Because k here is the amount of chunks\n3:00:16 to be returned. So I''ve set it as five. Now I''m pretty sure if we go to the\n3:00:21 actual documents here the default the default is four. Okay. So uh this is\n3:00:28 just a parameter which you can uh set. Now you don''t want it to be too high of course or too low. So you want like a\n3:00:34 good middle ground and 405 is a good middle ground in my opinion. Okay. So now let''s create our tool. So again we\n3:00:42 use decorator tool. And the tool''s name is going to be this retriever tool. It will input it will take in a query and\n3:00:49 it''ll output a string. So the dock string is as follows. This tool searches\n3:00:54 and returns the information from our document. Okay self-explanatory.\n3:01:00 uh and obviously we need to invoke it to the retriever. So whatever query we ask\n3:01:06 for example uh what was Apple''s performance in 2024 that will be the query and that will be passed to our\n3:01:13 retriever which will grab all the chunks the most the top five most similar chunks. Okay. Now if we don''t if there''s\n3:01:20 nothing similar uh which it finds for example if I say something like uh who''s\n3:01:26 Bob the builder something like that right obviously Bob the builder is not in this document uh so it will return as\n3:01:33 I found no relevance information in the document and uh this will be passed to our LLM agent okay if it does find it\n3:01:41 though what we''ll do is we''ll create an empty list and we will store all of the\n3:01:46 similarity um us the all of the chunks which it found and then return those results uh\n3:01:53 through this. Okay, still it''s quite easy still uh and this piece of code\n3:02:00 we''ve already come across. There is only one tool. So we just bind that tool to our\n3:02:05 LLM. And this also code we have also we''ve uh done many times. It''s the uh\n3:02:14 creation of the agent state. And again we''re using our add messages reducer function. All of this we''ve covered many\n3:02:20 times so you should be quite familiar with it. Okay. So now we create the should continue function and the should\n3:02:26 continue function uh is going to be the underlying function between our conditional edge behind our conditional\n3:02:32 edge. So it will check if the last message contains any tool calls. If it\n3:02:37 does then we um proceed. If it doesn''t then we''ll just end\n3:02:44 right. Okay. So now we specify the system prompt. Now this system prompt is\n3:02:49 going to be quite big. So let me copy and paste it here. Now the reason is quite big is I want to specify as much\n3:02:56 information to the LLM so that it knows what to do. Right? So I''ve just said you''re an intelligent AI assistant who\n3:03:02 answers questions about the document uh loaded into your knowledge base. Uh you\n3:03:07 can read the rest if you would like. But I''ve also written this. Please always site the specific parts of the document\n3:03:13 you use in your answers. This is really just to make sure it''s not hallucinating. Right? because as we know\n3:03:19 hallucination is quite a big problem with LLMs. So this is just to make sure um hallucinations are kept to a bare\n3:03:27 minimum. Okay. All right. So now we create a dictionary of our tools and we\n3:03:35 now create the underlying function which will be our LLM agent. So this function will call the LLM with the current state\n3:03:42 and you can see it converts the messages to a list passes the system messages and passes it to our LLM which is defined\n3:03:49 like this and it will just return the messages aka the updated state. Okay, this should be like such.\n3:03:59 Okay, awesome. So now we create our second agent which will be the retriever agent which you saw on the in the graph\n3:04:06 which I showed you in the introduction. So the retriever agent executes the tool calls from the LLM response. So what is\n3:04:13 this code actually saying? Well, all in all, this massive piece of code really\n3:04:20 just says if there is a tool, if the tool name is within the is a proper\n3:04:26 specified tool, aka if it''s retriever tool, then actually run it. If it''s not,\n3:04:32 then we will output the result as input in incorrect tool name. Please retry and select the tool from list of available\n3:04:39 tools. It''s just for checking if a if the tool which is decided from the LLM\n3:04:44 is valid or not. So that''s all what this is doing. If it is valid, it will invoke it and we will store the results uh like\n3:04:52 this and we will return that. Okay. Again this should be agent state like\n3:04:59 such. Okay. So we''ve created all of our our two um AI agents now and now we''re\n3:05:07 going to create the graph itself. So like how we''ve done initialize it through state graph and then we''re going\n3:05:14 to add our two AI agents as nodes with their respective\n3:05:19 actions and we are now going to add the conditional edge. So which will be llm\n3:05:26 which will be start from l lm and the should continue function is the function which will be um the underlying function\n3:05:34 and this is a uh true false statement and this is the edge the set entry point\n3:05:39 all of this we''ve covered many times so again should be quite familiar to you and last but not least compile the graph\n3:05:48 and store it in a ragation okay one last thing though uh I''ve created\n3:05:55 this function and this function is just a function which allows us to keep asking questions to our graph and keep\n3:06:02 receiving qu uh answers back and if you want to exit we can write either exit or quit um and it''s just a simple while\n3:06:10 loop that''s all it is okay and it prints the answer okay so that''s the actual code\n3:06:17 complete now we''re going to test it and see uh if the if this is reliable or\n3:06:23 not. Okay. Okay. So, let''s actually test this now um by doing python rag agent.\n3:06:31 py. Let''s run this. Okay. PDF has been loaded and has nine\n3:06:38 pages. Created chromo vector data uh chroma database vector store. So, where\n3:06:43 is this stored? Well, you can see that this is uh by the way, this will all be on GitHub as well. But this is the\n3:06:49 Chroma database and its respective um bin bin files. Okay. And we can even\n3:06:56 view it. But it''ll look something like that. Okay. But because this has been created,\n3:07:02 this is a good sign that everything is working. Okay. So, let''s ask a simple question. Uh let''s ask something\n3:07:11 like how was the S&P 500\n3:07:16 uh performing in 2024? Enter. So it''s\n3:07:22 calling the retriever tool uh with the query this uh its result then puts that\n3:07:27 complete back to the model and the model has given us this. In 2024, the S&P 500 delivered a total return of this with a\n3:07:34 23% increase late 1990s and all of that stuff uh magnificent 7 and has given us\n3:07:39 the uh respective uh citations as well. Now, how can we verify this is uh correct? Let''s\n3:07:45 see. So, notice how if you remember this part, the total\n3:07:52 return of approximately 25%. Well, the reason I prom I asked it for this is because that''s exactly what uh\n3:08:00 over here it stated the benchmark roughly at 25% 23%. Uh remember this\n3:08:05 late 1990s part that''s exactly what this is saying here as well and this was\n3:08:12 correctly defined in the first document. So this is clearly working now right it\n3:08:17 can''t have made up this information. So that means our rag is successfully set up. Now I can ask as many questions I\n3:08:22 possib as I want now but now let''s see if there is something which is not\n3:08:28 included in the rag. So for example we can say something like how did open AI\n3:08:34 perform in 2024 retrieve a tool called back to the\n3:08:40 model. Okay, now look at this. Uh if I\n3:08:46 do it like that, the documents do not provide specific information about OpenAI stock performance, which is true\n3:08:51 cuz OpenAI is not a publicly traded company. Uh and yeah, it got that\n3:08:56 correct. So no hallucination there. So you can clearly see that this is working\n3:09:02 uh completely fine. And that ladies and gentlemen is how you create a retrieval\n3:09:07 augmented generation graph in Langraph. Okay.\n3:09:12 Awesome. All right people. So that brings us to the end of this course and I hope you liked it and I hope you\n3:09:19 learned a lot about Langraph. Now although this course is finishing here, your journey in Langraph\n3:09:25 is just beginning. Just think about how many cool AI projects, AI agent systems you can make now. Maybe your own Javis\n3:09:31 as well. Now, if you have any further questions related to the course material\n3:09:37 or just things in general or just want to say hi, you can always message me on LinkedIn. With that being said, thank\n3:09:43 you so much for watching this course and I hope to see you in another course. Take care.',
  '{"channel": "freeCodeCamp.org", "video_id": "jGg_1h0qzaM", "duration": "3:09:51", "level": "INTERMEDIATE", "application": "LangGraph", "topics": ["LangGraph", "Python", "AI Agents", "Conversational AI", "Graph-based Workflows", "Type Annotations", "RAG Agent", "LangChain"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=o126p1QN_RI',
  1,
  'Complete RAG Crash Course With Langchain In 2 Hours',
  '0:00 Hello all, my name is Krishna and I am super excited to announce this amazing crash course on rag that is retrieval\n0:07 augmented generation. Uh in this specific crash course it''ll be somewhere...',
  '0:00 Hello all, my name is Krishna and I am super excited to announce this amazing crash course on rag that is retrieval\n0:07 augmented generation. Uh in this specific crash course it''ll be somewhere around 2.5 to 3 hours but we are going\n0:14 to discuss everything that is related to rack completely from scratch. Uh we''ll\n0:20 be talking about the entire pipeline from data injection to retrieval pipeline to output generation. how to\n0:26 use LLM models, how to use embedding models in this uh along with this uh what should be the right strategy of\n0:32 using chunkings and many more things right so we will be deep diving into\n0:38 both the theoretical understanding along with the practical implementation and we will initially go ahead step by step\n0:44 we''ll start with the basic implementation and then as we go ahead in the advanced section we''ll also implement the modular coding right the\n0:51 main aim of the modular coding is to link the entire pipeline in a way so that you should be able to understand\n0:56 how rag actually works and also implement it in your company use cases. Let me tell you one very important\n1:03 thing. 90%age of the use cases that are currently been worked in all the companies are specifically related to\n1:09 rag. So this crash course will be an amazing one for you all of you. We''ll keep a simple like target of thousand uh\n1:16 try to complete it as soon as possible and we''ll also keep a like target to some uh comments target of 500. So\n1:23 please try to complete it and yes go ahead and enjoy this particular crash course. Thank you. So this is a simple\n1:29 definition that uh I have put up over here and uh in this definition first of\n1:35 all we''ll try to understand rag. Okay. So first of all let''s go through the definition and then I will give you a\n1:41 brief idea what exactly rag is all about you know. So here you can clearly see that\n1:48 rag is the process of optimizing the output of a large language model. Okay.\n1:55 So it references an authorative knowledge base outside of his training\n2:01 data set source before get generating a response. LLMs are trained on vast\n2:08 volume of data as we all know and use billions of parameters to generally original output for task like question\n2:15 answering, translating and completing sentences. Rag extends the already powerful capabilities of LLM to specific\n2:22 domain or an organizational internal knowledge base all without the need to\n2:28 retrain the model. Okay, it is cost effective approach to improve LLM output. So it''s relevant, accurate and\n2:35 useful in various context. So this is just a basic definition. You can refer to this particular definition. So guys,\n2:40 now let''s go ahead and understand about rag. So let''s consider that I have a\n2:46 generative AI application. And as you all know in a generative AI application, usually let''s say that I have an LLM. So\n2:52 this is my LLM. Now usually whenever we have a LLM what happens is that let''s\n2:58 consider that I have a user a user is asking a query. So this is a\n3:05 my query from the user and before it is sent to the LLM we do add a prompt right\n3:13 we do add a prompt and this prompt is just like an instruction to the LLM like how the LLM should work okay and then\n3:22 based on this we actually get an output now this is a simple generative AI\n3:27 application wherein the LLM is used to generate the content\n3:34 Okay, generate the content. So obviously by using this specific technique we give\n3:40 a query and this LLM you know that it has been trained with billions of data\n3:47 okay different kind of data that is available in the internet and based on this it will be able to generate the\n3:53 output. One of the disadvantage of this, let me talk about the disadvantage of\n3:59 this particular approach. As you know that every LLM that is trained, you know, it will be trained for a specific\n4:07 set of data. So let''s say right now it is 31st August. Okay, 31st August.\n4:15 Let''s say this is my LLM model and this is basically GPT5 which is the recent model from OpenAI.\n4:22 Now as you know that when this model was launched this model may be trained\n4:27 by may be trained with data till 1st August. Okay. So this LLM will not have\n4:35 any idea what has basically happened in the current world between 1st to 31st\n4:40 August. Right? And let''s say if I go ahead and ask a specific question to the\n4:45 LLM which is between this specific dates for any kind of events the LLM will\n4:52 start hallucinating. So one of the major disadvantages of only using the LLM is\n4:59 that it will hallucinate. Okay. When we say hallucinating what does this basically mean? It means that even\n5:07 though it does not have the knowledge what has happened between 1st August to 31st August any events even though we\n5:13 ask any question the LLM will try to generate it own answer because it does\n5:19 not want to look like a fool. Okay, that is the best example. It does not want to\n5:24 look like a fool. So it will try to generate some answers and it will make sure that it''ll it''ll show you answer\n5:31 that you may also have to believe it. that is how it will be written you know in terms of the output that we get so\n5:38 usually this condition is basically called as hallucinating okay so this is one of the major disadvantage the second\n5:45 disadvantage that you have so let''s say that I''m using this LLM and you know this LLM has been trained with huge\n5:51 amount of data now what happens is that I''m running a startup\n5:57 let''s say now in my startup I''m solving a specific use case and I have some data\n6:05 which again I need to use this particular data along with my LLM. Okay.\n6:10 So let''s say that I have some other data like you know um policies policies of my\n6:17 company I have HR policies of my company I have finance policies you know and\n6:24 this policies all will not be available in the it will not be available publicly\n6:29 because it is my startup so these all data has been protected now I also want\n6:34 to use this specific data and probably create a chatbot okay now how do I do this Now one way is that many people\n6:41 will say hey kish we can take this particular data and we can fine-tune the\n6:46 model right we can simply fine-tune the model yes this is a very good solution but\n6:54 understand fine-tuning a model is a very expensive process very tedious process\n7:00 because this LLM whichever LLM we are using it has billions of parameter and tweaking this billions of parameter\n7:06 usually takes a lot of time Right? So obviously this is a solution but this is\n7:12 a very expensive solution. Okay. Now do we have any other way? Any other way and\n7:19 remember these all policies and these all data will also keep on getting updated as we run the startup. Right? So\n7:28 every time we cannot just go ahead and finetune it like every day we not fine-tune it. Right? So we should try to\n7:33 find out a solution like how do we prevent this? So this can again be\n7:38 prevented with the help of rag right now how it will be prevented with\n7:45 the help of rag I will talk about it okay so here instead of fine-tuning I''m\n7:50 saying that hey I will go ahead and implement the rag now you''ll understand only when we understand the pipeline of\n7:56 the rag which I will discuss in this specific video okay now these are the\n8:01 major two disadvantages that you see right over here and yes they are some\n8:07 more disadvantages which we''ll just deep dive more as we go ahead. Okay. Now what\n8:12 happens in uh if we use rag and how we are preventing it. See rag is nothing but it\n8:18 is it is saying that is a process of optimizing the output of a large language model. So it references an\n8:24 authorative knowledge base outside of his training data. Now how do we solve\n8:30 this hallucinating and this problem that we have. Okay. So let me just go ahead and draw the diagram again. Okay. So\n8:37 here is my LLM. Okay. And here is my query. So let''s say that uh I am coming\n8:44 up with an user query. So let''s consider it over here. Okay. And here I''m drawing\n8:50 a user I''m user. Okay. And this user\n8:55 will first of all give a query. Okay. Now what happens is that there\n9:03 will be two important pipelines that will be created. As I said over here we\n9:09 are trying to optimize the output of a large language model. So it references\n9:15 an authorative knowledge base outside of it training data source. So as you all\n9:20 know this is my LLM right? This LLM is already trained with huge amount of data. Now along with this I will be\n9:27 having an external database and this database we basically\n9:33 say it as vector database okay external vector database now you you know that\n9:39 this LLM is already trained with some amount of data and any additional data let''s say my startup data my policies HR\n9:47 finance whatever data is there we will try to create a data injection pipeline\n9:54 over here data injection pipeline over here. Now\n10:00 what will be this data injection pipeline? So let''s say I have my data from this data we will do some kind of\n10:09 parsing and from this parsing we will do\n10:14 embeddings embeddings and then we finally store it\n10:20 into the vector store. Okay. Now whenever we talk about the specific data this data can be in any format. It can\n10:27 be in PDF format. It can be in HTML format. It can be in Excel format. It\n10:33 can be even in SQL database format or unstructured format. Any format. So what\n10:39 we do initially we take this data and we do data parsing. Now here data parsing\n10:44 is a very important step. I think if you crack this step then developing a rag\n10:52 application becomes very easy. Data parsing is all about how do you read the\n10:57 unstructured data or the structured data that is present inside this and how do\n11:03 you chunk this data right? How do you chunk? How do you divide the specific\n11:09 data into chunks? Chunking is very important because you need to save this data inside some kind of vector store.\n11:16 This is nothing but vector store or vector DB. Okay. Now vector store and vector DB is nothing but it will\n11:23 actually help you to save vectors inside this. Okay. So once you do the chunking\n11:29 after doing the chunking you pass it to the embedding models. Now here in the embedding models you basically convert\n11:36 text to vectors. Okay, vectors is just like a numerical\n11:42 representation for text so that you will be able to apply algorithms like\n11:49 similarity search, cosine similarity techniques that are already available, right? Wherein similar kind of results\n11:57 based on a specific query can be retrieved from this particular databases. Okay, so here whenever I talk\n12:03 about vector DB, this is my vector DB or vector store. Here we are storing embeddings. Okay. And this embeddings\n12:10 will get applied to every chunks. Embeddings is nothing but we basically use we convert text into vectors. Here\n12:19 we can use different different embeddings like Google gemin models. We can use openi embedding models. We can\n12:25 use hugging phase embedding models and each and every embedding models exist with different different cost and there\n12:32 are also open source embedding models which will actually help you to convert the text into vectors. Now this is one\n12:37 specific pipeline which we call it as data injection pipeline. At the end of the data injection pipeline, you are\n12:43 able to store the text into vectors inside your vector DB. Now how rag is\n12:51 different from the previous one, right? So initially you had this data injection pipeline where you are converting all\n12:56 your data into vectors, right? And this data is specifically for this particular\n13:02 startup. And now I have created a knowledge base. So this is my knowledge\n13:08 base. External knowledge base or internal knowledge base whatever knowledge base I have. And this\n13:14 knowledge base does not exist with this LLM. Right? Yes, some amount of information may be available but not the\n13:20 entire part. Now see the definition. It is a process of\n13:25 optimizing the output of a large language so that it references an authorative knowledge base outside of\n13:32 this training data. Now what will happen when user gives a query? Now this query\n13:37 instead of directly going to the LLM will go to this vector database right\n13:43 and before going here also we need to go ahead and apply embedding right because\n13:48 this query will be converted into vectors right why we need to convert\n13:55 into vectors so that when we are hitting this query to the vector DB this similarity search is basically applied\n14:03 and based on this we get some kind of\n14:09 context we get some information from the vector DB and now whatever query I''m asking\n14:16 okay if I ask hey what is the leave policy of my company\n14:22 right now what will happen first of all it will go to the vector store it will gather all the related information that\n14:29 is available over here and that information when it is sending it to the lm it is called as context\n14:35 Now we use this context along with we go ahead and write a specific prompt.\n14:42 Now this prompt is an instruction to the LLM and it says that you can use this\n14:47 context to answer the question and finally you get a output.\n14:53 This is the entire pipeline. This pipeline is basically called as retrieval pipeline.\n15:01 Retrieval pipeline. And this is a very good example of a traditional rag.\n15:08 Now you may be thinking kish what about other types of rag. Don''t worry thumb don''t worry I will explain it completely\n15:14 from basic to advanc with implementation each and everything because later on we''ll be discussing about agentic rags.\n15:20 We''ll be discussing how agentic rags actually work each and everything. But I hope you got an idea with respect to\n15:26 this. Now here you will even not be seeing this particular problem like you''ll not completely remove\n15:32 hallucination but some amount of hallucination if any queries that is asked related to the data that is\n15:38 present in the vector DB I will definitely get some kind of context and\n15:43 my LLM will give me the output as let''s say that if that data is not present over here then LLM can hallucinate right\n15:51 but here we are doing this see one best example that you can do is that you can use perfectly Perplexity.\n15:58 Perplexity is nothing but it is based on rag. It is completely developed based on\n16:05 rag applications. Okay. Rag it is it is a kind of a rag application. In\n16:10 perplexity you have connected to various retrievers. You are connected to tools.\n16:17 You are connected to web search right and then it is summarizing the\n16:22 output and giving by the LLM. Right? and it also uses various LLMs itself. I''m\n16:27 also planning to mostly start a startup soon enough within couple of weeks I\n16:33 guess and the kind of application that I''m developing is a rag application only\n16:38 and it solves a very good problem for a developer. Okay. So that is the reason I''m not even able to upload a lot of\n16:45 videos because I''m pretty much involved in those startups and working and developing a product that India can\n16:51 definitely remember. Okay. And this is how you know this is this is this is how\n16:58 things are and you can basically see how good uh you know the pipeline actually\n17:04 works and this is basically a traditional rack. Now you may be thinking what all things we''ll be discussing. Okay fine we have discussed\n17:09 about a traditional rack in the future classes what coding we''ll be doing. Okay so let''s go ahead and talk about it. As\n17:16 I said two important pipelines we''ll go ahead and create one is a data injection pipeline and one is a retrieval\n17:22 pipeline. Okay. Now in the data injection pipeline you''ll be seeing that\n17:28 we will be performing data injection. Along with the data injection we will go ahead and do data parsing. Then we''ll\n17:34 perform embeddings. Then uh we will store everything into the vector store.\n17:40 Then we will create a ve retriever for this. And whenever a user ask any queries, it will be able to give the\n17:47 context to the LLM. And then finally we will be generating the output. So here\n17:52 this is retrieval. This is auggmentation right? This is augumentation over here.\n17:58 Augmentation basically means what? You''re giving a context to the LLM along with the prompt to generate the output.\n18:04 Right? So this is basically called as augumentation and finally you''re generating the output right which is\n18:09 nothing but generation. So here you are basically generating. Now\n18:16 in the next session how we are going to implement it. First of all I will show you how to perform this two steps in a\n18:24 very efficient way. Okay sorry not these two steps. I will show you how we can perform these all steps right data in\n18:32 data parsing and embedding. Here we are going to consider different different files like PDF, HTML.\n18:39 Okay. Um PDF, HTML, you can consider Excel, you can consider SQL database,\n18:44 you can consider any kind of files. Then we''ll do document parsing and we will try to convert this into document. So\n18:50 document is an amazing data structure which you can basically use it and you\n18:56 can even parse this do the chunking and store it in the vector embeddings sorry vector store then we''ll perform\n19:02 embeddings here we will use both open source and we are going to use paid embeddings\n19:08 for the same okay and then finally we go to the vector store then based on a user\n19:13 query how do we go ahead and apply the same embeddings we are going to see that okay and then finally we''ll be\n19:19 developing this So mostly I really want I''m I''m focusing more on making bigger videos so that you don''t just follow a\n19:26 playlist. Okay, I want to basically cover a lot of stuff in one video so\n19:31 that uh you should also be able to efficiently cover it instead of covering 50 different videos. Right now when we\n19:38 are doing data injection and data parsing right there are various techniques. See we are going to see about optimization.\n19:45 We are going to see about various chunking strategies, context engineering, these all kind of topics\n19:50 will be coming up when we talk about data parsing you know u what is semantic chunker you know how do we go ahead and\n19:57 do the chunking in those strategies and all everything we''ll try to discuss as we go ahead but I hope you got a very\n20:03 super cool idea about what exactly is rag hello guys so we are going to continue the discussion with respect to\n20:08 rag already till now we have understood what is rag then what are the main\n20:15 drawbacks we are fixing with rag and along with that we have also understood how the rag pipeline is right it usually\n20:22 consists of two important pipeline one is the data injection pipeline and one is the retrieval pipeline which includes\n20:27 this two box okay now we are going to go ahead with some kind of practical implementation\n20:34 now the major thing that usually comes in my mind right whenever we go ahead and start any new series that is how\n20:42 should we cover a specific topic you know so that we can understand the coding from basics and we move towards\n20:49 modular coding so that is how I''m going to implement this entire pipeline\n20:54 initially we will go ahead with some basic code we''ll try to understand the fundamentals and then we will start\n21:01 writing more complex code we''ll be using modular coding also so initially we will\n21:06 write all the code in Jupyter notebook then we''ll increase the complexity we''ll write uh code in terms of class reus\n21:13 reus usability and then we''ll try to see that how we can actually create the pipeline. So that is how the agenda will\n21:20 probably go ahead as we go ahead right. So two important things that we''ll think about. The first important thing is to\n21:26 understand about the document structure. Now whenever we work with any external\n21:32 knowledge database any data that needs to be feeded into the vector DB you\n21:38 definitely need to know about this document structure. Why? Because inside this data injection pipeline the first\n21:44 step is data injection. Now whenever we talk about data injection here we can have any kind of files right we can have\n21:50 PDF files, HTML file, DB file, Excel file. Our main aim is to read all this\n21:56 particular file content and probably convert into a structure wherein we can\n22:02 additionally do uh we can apply strategies like chunking embedding and store it into the vector DB. That is\n22:08 what this entire pipeline is all about. So for that you really need to understand this document structure. So\n22:14 if you see this diagram right so since uh these two are the main topics that we\n22:19 are going to cover in this particular video initially we will go ahead with document structure understanding this and then we''ll try to build our complete\n22:26 rag pipeline in our complete rag pipeline we have two important step one is the data injection pipeline and the\n22:33 other one is the query retrieval pipeline now whenever we talk about the data injection pipeline let''s let''s talk\n22:40 about this in complete depth right so initially you have this data injection pipeline Right? In the data injection\n22:45 pipeline, the first step is data injection. That basically means let''s say that you have you may have different\n22:51 kind of files like PDF, HTML, right? Excel, you may have uh DB file, you may\n22:59 have unstructured file, any kind of file format. So in data injection what is our main strategy is that how to proceed\n23:06 with reading this particular file. How to perform data parsing.\n23:12 How to perform data parsing and then finally how to convert this\n23:18 into a document structure. Document structure. So that is the\n23:24 reason in this video right as I said we''re going to first of all understand\n23:29 about document structure. how to build this document structure, what is metadata? Now, inside this document\n23:35 structure, uh you will be learning about important components like metadata.\n23:40 You''ll be learning about content. You''ll be learning about how the structure of the metadata exist each and everything,\n23:47 right? So, we will be covering completely in depth like how these things actually work. Okay? Once you\n23:55 understand this that and this data parsing is really really important step\n24:00 because of this you know later in the retrieval pipeline that is the query retrieval pipeline based on this parsing\n24:07 it can become much more efficient right you''ll be able to get the results much more accuracy much more accurate so that\n24:14 is the reason you need to really focus on the data parsing now after doing the data parsing the next step usually is\n24:21 something called as chunking right so Here in the chunking we we convert this\n24:28 entire data into chunks multiple chunks. So this chunks is like let''s say this is\n24:35 my chunk one this is my chunk two this is my chunk three this is my chunk four\n24:44 okay then as we go ahead after applying chunking. So chunking basically means\n24:50 and why do we apply chunking? Chunking strategy is very simple. Whatever documents we have, we are just dividing\n24:56 this into smaller parts or smaller chunks. The reason we do this because\n25:02 whenever we consider with respect to any LLM model or any L embedding models,\n25:08 let''s say here the next step is all about embeddings. Okay. In embedding\n25:14 with respect to every LMA model, there is a fixed context size. Okay.\n25:21 Let''s say if I take the complete 100 pages PDF and I directly try to give it to a L model for performing the\n25:28 embeddings like uh if I give it directly to an embedding model for performing the embeddings and embedding basically means\n25:33 you convert text to vectors. It will not be possible. It will say that hey you\n25:39 have you you are providing data more than the context size and that will not\n25:44 be possible in order to convert the text into vectors. So within the limit of the context size you really need to give the\n25:51 data and this is for both embedding models and even in the later stages whenever we use any kind of LLM model\n25:57 because for every LLM model there is a fixed context size. Yeah different LLM model may have different different\n26:03 context size. So that is the reason and it is always a good strategy that we try to divide our data into chunks so that\n26:10 we fit them in a way that we uh in the later stages we''ll be able to efficiently put them into the vector\n26:15 database which is this. So after chunking for every chunk we go ahead and apply embeddings. Okay. So we go ahead\n26:23 and apply embeddings and from the embeddings we finally store that into our vector DB. Now inside this vector DB\n26:30 all this will be stored in the form of vectors. Like let''s say this is my record one record two record three\n26:37 record four like that right so this is one record two record this is my third record then fourth record fifth record\n26:44 this you have right now from this particular vector DB you will definitely\n26:50 be able to apply any kind of similarity search similarity search now in this\n26:57 specific video what we are going to do is that I will be using any of this file\n27:02 and I''ll create this entire pipeline. Okay, I will I''ll just create this\n27:07 entire pipeline and you also need to probably work along with me later on.\n27:14 For any other files, I will give you an assignment. Okay, I will show you with couple of files. Let''s say I''ll take PDF\n27:20 file and I''ll show you this entire data injection. Then what you do is that as an assignment you use any of the other\n27:27 file format let''s say Excel, CSV whatever file format you want and you try to complete the same pipeline. Okay.\n27:34 So that is what is my strategy and please make sure to complete the assignment also and we will go step by\n27:40 step completely from scratch so that everybody will be able to follow. So first of all I will go ahead and open my\n27:46 empty folder and in this remember I will be using lang chain uh and this is just a traditional rag right now in the later\n27:54 stages we will move towards aentic rag. So from this particular command I will just go ahead and open my command\n27:59 prompt. I will open my VS code. So let me quickly go ahead and open the VS\n28:05 code. Now from the VS code the next step will be that I will\n28:11 quickly open my terminal terminal and let me just go ahead and\n28:17 write uv uh I''ll just go ahead and initialize this particular workspace as my repository. So yt rag is my\n28:24 workspace. Now I will just go ahead and also go ahead and create my environment.\n28:31 So if you''re using uv package so you can just write uv env. So my Python 3.13.2\n28:37 will be the recent uh Python version that I''m specifically using for this particular project. And then I will go\n28:44 ahead and create activate this particular environment. Okay, perfect. Till here we are good enough. Now I will\n28:50 go ahead and create my requirement.txt. Now from this requirement.txt txt. Let\n28:56 me quickly go ahead and install some of the packages like lang chain lang chain\n29:01 core uh core lang chain dash community\n29:08 uh the all things are there. Let''s me quickly go ahead and install these\n29:13 packages. So uv add minus r requirement\n29:18 txt. Okay, txt. So this is done and along with this I\n29:26 will also go ahead and install some of the libraries like pi pdf pi mu\n29:32 m new pdf. Okay so these are all libraries I''ll be using. I''ll talk about why I''m using pi pdfd pi mu pdf right.\n29:39 This is specifically to read my pdf documents. So one example that I''m actually going to show you is with\n29:45 respect to PDF and then you should also try to create the same pipeline with the help of any other uh data types. Okay,\n29:53 data formats types like let''s say it will be it can be JSON, it can be anything as such. So uh my requirement\n30:00 txt is filled. Now what I will do is that I''ll quickly go ahead and create my data folder and here I will also go\n30:06 ahead and create my notebook folder quickly so that I can start working on it and then along with this I will also\n30:14 go ahead and add UV add ipi kernel. Okay so that I will be able to work along\n30:19 with my Jupyter notebook. So ipi kernel has got executed. Now quickly I will\n30:25 first of all start with my Jupyter notebook and at the first thing that I told you it''s related to document data\n30:31 structure right document what is document and what is how document can be very very helpful if you are using in\n30:38 the document data uh in the data injection pipeline okay so I''ll quickly select my kernel\n30:45 and these all things you really need to be a good at Python programming language see there cannot be anything that you uh\n30:52 you can skip Python programming programming language. So my suggestion would be never do that. Okay. So Python\n30:57 is must and this time I''m just going to use some more advanced coding and it will not be possible for me to write\n31:03 line by line. So definitely I''ll go a little bit fast to in order to explain you. Okay.\n31:08 Now as I told you if I go back over here in the data injection our main aim is to\n31:14 load some data apply some chunking then convert into embeddings and finally\n31:19 store it into the vector DB. That is what my entire data injection pipeline is all about. Right? For understanding\n31:25 this, we need to understand a document structure because all this chunking that is done, you know, the final output will\n31:31 be documents. Now, what exactly is a document data structure? So here I will\n31:37 go ahead and write what exactly is a document data structure. So for this I\n31:42 will go ahead and import from lang chain or to probably show you this. I will be\n31:50 showing you some kind of uh file so that you''ll be able to understand it. Okay,\n31:56 let me put this file over here. Okay, I have some file over here and\n32:02 then we''ll try to understand. Okay, what exactly is a document structure? See lang chin document structure. So\n32:08 langchen uh document is a kind of a data structure which will be able to save\n32:15 some data in some format where we have two important things. One is the page content and one is the metadata.\n32:23 The page content will basically have the content that is present inside that\n32:29 particular file. Okay. So if you are reading the file inside my page content\n32:34 all those detail all those content that is present inside the file will be available over here and metadata will be\n32:41 some more additional information of the file like it can be the file name it can be how many number of pages are there\n32:47 how what is the time stamp of the file each and everything. So this way whenever you read any kind of data and\n32:54 you convert them right in a document data structure this format will be very very important because at the end of the\n33:00 day we will be doing the embedding on this particular data and pushing it into the vector DB and when we do that\n33:07 specific task pushing it to the vector DB we will be able to apply different different uh algorithms like similarity\n33:15 search cosine similarity and we''ll be able to retrieve the results. So here you can see that all the information\n33:21 regarding this is given over here. So usually langchen document structure it has two important core components. One\n33:28 is page underscore content and one is metadata. And here page content will be the actual text uh content where all it\n33:36 will be very very handy in research papers if you want to probably create a rag application or research papers\n33:41 product manual. So you can specifically use this in lang chain you definitely\n33:46 have different different loaders. Okay, loaders like you have something like PDF loader, you have CSV loader, you have\n33:53 web- based loader, you have directory loader. Now see all these loaders what it does is that for PDF loader will be\n33:59 used to load the PDF files and once it loads the PDF file right it will be\n34:04 giving you the output of the documents in the form of a document structure. Okay, I will show you practically also\n34:11 why I''m specifically saying and stressing on this. Okay, it will definitely give you all the output in\n34:16 the form of a document structure. Similarly, in the case of CSV loader, here we are giving the CSV file, but it\n34:22 will try to convert the entire content that is present inside that CSV into a document data structure. Similarly, with\n34:28 respect to web brace loader, clarity loader. Similarly, there are so many different different loaders over here,\n34:34 right? You can use any of this particular loader to load the data and at the end of the day uh this loader\n34:41 will finally give you the output in the form of document structure. Okay. So I\n34:47 hope you got an idea about what exactly is document structure itself. Okay. So now quickly what I will do I will go\n34:53 ahead and uh start explaining you about like how we can start with the document structure. So for the document we need\n35:00 to import from langin. langchen dot there''s something called as text\n35:07 splitter and uh sorry langchen core it is present inside core dot documents\n35:15 import document okay now this document you will be able to see that if you just\n35:21 hover over here you''ll be able to the class for storing a piece of text and associated metadata okay now if you\n35:30 really want to understand a document structure so first of all I will go ahead create one document let''s say\n35:35 manually I''ll go ahead and create so I will use this document and inside this we will be using two parameters one is\n35:41 the page content let''s say this page content I''m writing this is the main\n35:46 text content uh content uh I''m using to create rag okay so I\n35:55 I''ve just basically written some some basic content over here let''s consider\n36:00 that this particular content is coming from a txt file Okay, but along with this content, if you really want to\n36:07 improve the search query retrieval from the vector DB, you need to also go ahead and write metadata. So the second\n36:13 parameter that you''ll be able to see is something called as metadata. Now inside this metadata, you can write different\n36:20 different information because at the end of the day this is text. You can write like okay fine this is my source. The\n36:25 source is basically coming from example.txt file. Okay. Then let''s say\n36:30 the number of pages are uh equal to one. Okay. Total number of pages are like\n36:36 one. Uh I can also go ahead and write some more information like okay who is the author for this? Author is nothing\n36:42 but crush nayak. So this is the additional details that you''ll be able to see it. Okay fine. Let''s go ahead and\n36:48 write date created. So date created. Right. Date created. And here I can go\n36:54 ahead and write 24 -01 - 0 like it''s like first 2024 or first 2025. Now why\n37:02 these all metadata will be really really important because once we consider this document right once we do the chunking\n37:09 once we do the embedding and once we store into the vector DB when you''re doing the similarity search you can also\n37:15 apply filters that is the most important thing of this and when you apply filters\n37:20 let''s say that I am applying a filter uh I''m searching what is the main text content for building the rag some\n37:26 information is there let''s say there''s some information related to the rag if I ask that particular question and I say\n37:32 by author Krishnaak I just had that particular filter then it knows from which document to probably pick up\n37:39 because it is going to apply a filter by using the name of author right and that is why this metadata will definitely\n37:46 play a very important role now if I just go ahead and execute this doc you''ll be able to see that fine I''m getting this\n37:53 particular document here you can see metadata is there and as you go ahead you''ll also be able to see page_content\n37:59 right so these are the two main important parameters with respect to this which everybody can probably go\n38:05 ahead and use it. Okay. Now I hope you got a very clear idea about it. Uh now\n38:11 what I''ll do I will just go ahead and create a simple simple create a simple\n38:17 txt file. Okay. Now for creating a simple txt file what I will do I will\n38:23 just go ahead and import OS. Okay. And I''m saying OS domake directory data /\n38:29 text file. So I''m trying to create this particular inside this f folder I''m creating this particular folder name\n38:35 okay and if it already exist I''ll say that don''t do anything right so as soon as I go ahead and execute it you''ll be\n38:40 able to see that okay it is going inside the notebook file I''ll remove this and\n38:46 let me go ahead and write double dot slash let''s see now you can see over here text file is present okay so text\n38:53 file I''m I''ve just done that inside this now let me go ahead and manually create a text file with the help of Python\n38:59 code. Okay. So I will just go ahead and use a Python code. See guys, these are all our basic Python code. I don''t want\n39:06 to write each and every line of code and make it very very big. Our main aim should be that understand concepts\n39:12 quickly show you multiple use cases and then try to implement this. Okay. So now\n39:18 you will be able to see I have created this simple text. I''ve given the file name something like this. So let me go\n39:23 ahead and write this to it. Data text files python intro.txt. And this is some\n39:29 content that is present inside that particular key name. Okay. So this is my file name. You can see\n39:35 this is key is my file name. And then here I have specifically my Python content. Okay. Here I''m saying for file\n39:43 content in sample text do items. I''m telling to open the file name. I''m\n39:48 saying that write the content. Okay. So this file path is nothing but my file name. Okay. So if file is not there, it\n39:55 will try to create python intro.txt. So now if I go ahead and execute this.\n40:01 So it is saying me no directory. Okay, let me just go ahead and create one file. Okay, python intro\n40:09 um text file. Okay, I have to give the path because there are two files that is over here. One is okay, one file is also\n40:16 over here. Okay, so I''ll just go ahead and write dot. Okay. So now here you can see my sample files has got created\n40:23 machine_arning.txt and python intro.txt. Now what I will do see I''ve created some\n40:31 sample file. I could have also manually created it instead of doing the code. Okay. But I really wanted to show you\n40:36 all the things. Now what I will do I will show you how to read this particular text using text loader. So\n40:43 one of the loader that is present inside langin is something called as text loader. So here I will go ahead and\n40:49 write from langchain dot document loaders import text loader.\n40:56 Okay text loader. So here we have imported text loader and uh along with\n41:02 this uh see if you don''t want to also use this if I execute this this is also there before if I talk about it right\n41:10 when langchain keeps on changing its library here and there. So there we used\n41:15 to use langun community.d document loaders. This also we used to use import\n41:20 text loader. So any of them you can actually use unless and until you get a deprecated\n41:26 warning. Okay. Now the question is that how do we go ahead and read the text. So\n41:31 I''ll write loader is equal to I will initialize text loader. Give let''s give the path. The\n41:37 path is nothing but parent folder. We go to the parent folder data /ext files\n41:44 /ython intro.txt. So here I have actually given\n41:50 my file name whatever file name we have actually created and we can also go ahead and use encoding UTF8. Okay,\n41:56 encoding UTF8. So once I do this okay and now once I go\n42:04 ahead and read this loader now what it is giving it is giving me an object of\n42:10 um text loader right now in order to get the content inside this I will be using loader.load load. Okay. And here you''ll\n42:18 be able to see that I will be getting the document. Okay.\n42:24 Now let''s go ahead and print the document. So I will write print document. So let''s say this is my\n42:30 document. I''m going to print it. So here you can see in the document you are getting metadata. You''re getting the\n42:35 entire information and this is your page content. Now this is what it is doing, right? This text loader is by default\n42:42 giving you the data in the document structure. as soon as it is reading. And here the best part is that you can also\n42:48 see some of the metadata information has also got updated like what is the source right you can still go ahead and and\n42:55 manually change more information inside the metadata but by default the best part is that whenever you''re using this\n43:02 all libraries then also it will be able to give you the content in the document structure which is really really good\n43:09 because in the document structure you have two important things. one is the metadata and one is the page content. So\n43:16 this is with respect to text loader right I have just read the text loader and I''m able to get this in this way.\n43:22 Okay. Now one more way what I will do I will show you with the help of directory loader like if I have all the important\n43:31 files in my directory. Can I read it like that also or not? Okay. So for doing this let''s use uh one more library\n43:39 which is called as directory loader. Right. So here you can see lang community.document document loader\n43:45 import directory loader now inside my directory loader you can see that I''m giving this particular file again this\n43:51 file should be uh parent folder does this and here I given the pattern to\n43:56 match see this function basically you can give a pattern to match all the files then you can use loaderclass\n44:03 loaderclass basically means which file you are planning to load if it is a PDF one you can directly go ahead and use\n44:09 PDF okay so what I can actually do is that I can also go ahead and insert PDF\n44:14 files over here. I can also provide this in the form of list so that it will be\n44:19 able to read both the content. Okay. So once I go ahead and execute this, you can see here also I''m using the encoding\n44:26 and all these things. And here you can see uh once I go ahead and write directory\n44:32 loader dot load okay and here you will be able\n44:38 to see documents. Okay. And then now if you just go ahead\n44:43 and print the documents you should be able to see this. Okay. I''m getting an error to log the progress please install\n44:49 pip install tdk. Okay. So here we have enabled the parameter show progress is equal to true. Let me make it as false.\n44:56 So that I don''t need to probably go ahead and install this. Now here clearly you can see that there were two text txt\n45:01 file. I got two documents. Yes. Now further you can do chunking and all right based on the number of documents\n45:08 over there I was able to get it. Right. So this is the most amazing part uh\n45:14 about this. Now what I will uh quickly do is that let me go ahead and create uh\n45:19 a PDF file also. Okay. So here I have some examples of the PDF file. Okay. So\n45:25 let me quickly go ahead and copy this and paste it over here. Reveal explorer\n45:32 data. I have text files. I have PDF files. Now inside this PDF file now my main aim is to read both the text and\n45:39 PDF files. Let''s see. So here I have attention PDF, this PDF, this PDF. Okay,\n45:44 so this is my one document. Okay, let me go ahead and write the same code. Copy and paste it over here. And this will\n45:51 basically be for the PDFs. So for PDF I will be having from langchain\n45:57 lang core dot document loaders import\n46:03 pipdf. I think pi pdf is not available over\n46:08 here. Let''s see where is this specific library. I''m just checking out the documentation. Uh PI PDF. Oh yeah, it\n46:15 should be there. So it should be here in the inside my community dod document loaders. I have two different types of\n46:22 library. PI PDF and PIMU PDF. PIMU PDF is better when compared to PIP PDF. You\n46:28 can see uh PI PDF shows load and parse a PDF file using PI PDF library. And similarly if you go ahead and see py mu\n46:35 pdf it loads and parse pdf file using this provides method to load this this this is there all the information you\n46:41 can see the differences which one is better which one is not better in the later stages. Okay now\n46:47 what I''m doing is that I will give the path over here. So from data / data and\n46:53 here you can see the path is nothing but PDF here I will go ahead and write PDF\n47:00 instead of writing text loader I will go ahead and write pi mu PDF let''s go ahead and use pi mu PDF I can also include\n47:07 encoding in this and here what I will do I will quickly write PDF documents is\n47:16 equal to directory loader dot load Okay. And then if I just go ahead and\n47:23 see PDF documents, you should be able to see there are so many different PDFs. Okay. I''m getting an error. Uh get text\n47:30 got an unexpected argument. Okay. Let''s remove this. I will not be requiring anything. We don''t need to apply any\n47:36 encoding by default. Okay. So here you can see I have got all my documents. Yes. So how many different files were\n47:44 there inside PDF folder? One is attention. PDF, embedding, PDF, object detection. These are some of the\n47:49 research paper and with respect to this all we are able to see this and now the best part is that when you''re using Pymo\n47:55 PDF here the metadata information is completely different seeation date source file path total pages\n48:04 right format see total pages is 15 for the first one then 27 then 21 see you\n48:10 can see it so beautifully it is there see I have also created some of the PDFs\n48:16 there also you''ll be able to see some kind of author''s name also right it tries to bring up all the entire\n48:23 source information and this is your page content right so beautifully you are able to see the entire content quickly\n48:29 right so that is what this all PDF is all about and here at the end of the day\n48:35 even though we use this specific libraries we are getting this in the form of a document structure it is a\n48:41 list of documents so if I go ahead and say what is type of PDF document of zero\n48:47 You''ll be able to see okay it is of a document type right now that is the most\n48:53 important thing if you now see that we have understood about document structure\n48:58 we know how to read PDF and txt now don''t you think you can actually easily find out how to probably go ahead and\n49:05 read the Excel DB any kind of files and this is the task that you really need to do how you''ll do it just go to lang\n49:11 chain document loaders right and you will be able to find out everything over\n49:17 here. Just go ahead and try it out. Try it out. Try it out. Try to see if the document structure that you''re getting\n49:23 is good or not. So here there are so many different things you can go just go ahead and try it out. If you want from a\n49:29 AWS S3 you you want from AWS S3 directory go ahead and just install this particular library give this but before\n49:36 that you have to do the authentication and all right. Once you do this and uh once you''re able to do it, you can use\n49:42 any kind of document loaders as you add but at the end of the day what is what\n49:47 is the best thing about this at the end of the day you are able to convert everything into a document data\n49:53 structure right now if you see with respect to data injection here you have actually completed now the next step is\n49:59 that I will move towards chunking okay I''ll move and show you how the chunking can be specifically done what are the\n50:06 different ways of chunking um that you can actually do you know and then finally we''ll see that how we can even\n50:12 convert into embeddings we''ll try to use an open source embeddings for this and then finally a vector DB so yes I hope\n50:18 you have understood about the data injection part now let''s move towards the chunking part where we will\n50:23 understand uh how we can actually performing chunking and I have also told you what is the importance of chunking\n50:30 so guys till now we have already discussed about the entire document structure and uh I''ve also shown you how\n50:36 with the help of PI PDF loader PI MUD MU PDF loader and how with the help of text\n50:42 loader you will be able to read the txt file and PDF file. All the other files again you can go ahead and see the\n50:48 langun documentation you have different different document loaders which I have already discussed right and these are\n50:53 some of the document loaders that you can specifically use uh which I have already shown you um from the\n51:00 documentation page now we going to go ahead one step ahead you know um because\n51:05 we have just started with this we understood about data parsing and we were able to create the document\n51:10 structure itself now I really want to probably go ahead and do the chunking uh then after the chunking I also want\n51:18 to probably go ahead and do the embedding and finally whatever text to vectors is basically converted this\n51:26 vectors will be stored in some kind of vector store DB okay so let''s go ahead and start building this entire pipeline\n51:32 okay so uh and this pipeline will initially build it we''ll start from complete basics since this entire rack\n51:38 series we are learning from basic stuff right so definitely you''ll love it\n51:43 you''ll love to expl explanation that what I''m doing you know so here uh what I will do I will go ahead and create one\n51:48 more file quickly and I''ll say hey this is nothing but PDF loader ipnb okay and\n51:56 uh here I will go ahead and select my kernel this is my kernel and let''s go ahead and start the entire rag pipeline\n52:04 and this pipeline is nothing but data injection to vector DB pipeline okay\n52:11 vector DB pipeline we are going to go ahead and build this quickly.\n52:16 So, uh first step as you know that I already have one data folder over here.\n52:23 So, this is what is my data folder and I definitely have a lot of PDF files inside this PDF folder itself.\n52:30 So first thing first uh what I will do I will go ahead and create a function you\n52:35 know uh saying that uh where in I will try to read all the documents from this\n52:42 and I will try to uh read the data inside this particular document that is PDF file and then uh we may use pi PDF\n52:49 folder PI PDF loader and then finally convert that into a document. Okay. So for this what I will do I will quickly\n52:56 go ahead and create a function and this function will be nothing but uh this is a markdown. Let me just go ahead and\n53:02 make a code cell. So uh before I go ahead I go I want to import all the\n53:08 important libraries that are available. Uh some of the libraries that I will be noting down over here is nothing but\n53:15 import OS. Then you have something called langin document langen community\n53:20 langun community document loaders. I''m using pi pdfd loader and all then you also have this langchen textsplitter and\n53:28 recursive character textplitter. Okay so u otherwise instead of writing in a new file I will let''s go ahead and use okay\n53:35 this file is fine so I will just go ahead and execute this I will I don''t require the path library. So once I\n53:41 execute this these all libraries will get executed now we will be able to use\n53:47 this. Now since my first step is related to data injection. Now whenever I really\n53:53 want to specifically do data injection, what I will do is that I will try to read all the PDFs. So we will read all\n54:00 the PDFs inside the directory. Okay, directory.\n54:06 Now guys, uh you need to have some knowledge with respect to coding. So\n54:11 otherwise if I keep on writing line by line, it''ll definitely take a lot of time. So here we are going to create a\n54:17 function which is called as process all PDFs. Here we need to give the PDF directory. Once you give the PDF\n54:24 directory uh we will probably go ahead and take the path. So for this also I\n54:29 will be requiring the path library over here. So once we get the path based on the workspace location here we are going\n54:36 to get the PDF directory path. Then we''ll list of all we''ll go ahead and apply this regular expression to get all\n54:42 the PDF files. Then here I''m printing what is the length of the PDF file and we are processing every PDF files. So\n54:49 here you can see that I''m using pi pdf loader str of pdf file name whatever file name then I''m doing documents is\n54:55 equal to loader.load load here I get the document okay here what I''m doing I''m adding some more information related to\n55:02 metadata so here you can see doc metadata of source file I''m giving the pdf file name I''m also saying that hey\n55:09 what is the metadata file type so this is my new keys inside my metadata to some put some more additional\n55:14 information and finally you get a PDF I''m just mentioning some more metadata information so along with this I''ve put\n55:21 up this metadata information like file type source file now you can add keep on adding any number of metadata\n55:27 information like you want right and once we read this entire documents we are going to go ahead and store in this\n55:33 particular variable that is called as all documents which is nothing but it is a list of it is a list it is an empty\n55:39 list okay so once we do this here we''ll be able to see it is returning this all documents so this function what it does\n55:45 is that from inside a folder it reads all the all the uh PDF files it reads\n55:52 the content inside this it adds this kind of metadata information and finally it is basically storing in this\n55:58 particular variable. Okay. Now we call this particular function process all PDFs. I''m giving the data folder over\n56:04 here. So once I execute this you''ll be able to see that it has found out four PDF files and attention. PDF had 15\n56:11 pages. My embedding PDF had 27 pages and object detection PDF had 21 pages. And\n56:18 this is proposal one page. Okay. So all the information I have it over here. Now\n56:23 if I go ahead and check my all documents.\n56:28 So if I go ahead and check just this particular v variable all PDF documents you should be able to see that this is\n56:36 my list of documents right and the best part is that for every PDF you''ll be able to see by default some of the\n56:41 metadata information along with this you can see there is an author metadata keywords mode date all this modified\n56:48 date right all these information are basically present in the metadata information now here what we have added\n56:54 we have added source along with the source you can see we have also uh total pages is also added at source file is\n57:00 also added and these are my text which is present inside my page content right\n57:06 so for every PDF whatever is the possibility size of the document we have we are able to read it now this is a\n57:12 step that we have done right now we have to go to the next step and perform the chunking now how do I go ahead and\n57:19 perform the chunking now I have my all my list of documents so what I will do I will just go ahead and quickly create a\n57:25 function and this will be specifically text splitting\n57:32 get into chunks. Okay, chunks I have over here. Right. So, first of all, I\n57:37 will go ahead and create a function which is called as split documents. Split documents. And inside this\n57:43 documents, I will be giving my parameters. The first parameter is nothing but documents. Then I have my\n57:50 chunk size is equal to,000. then I have chunk underscore\n57:56 overlap is equal to 200. Okay. So I have given all these things. Now you know how\n58:02 to do the chunking. It is very simple. You go ahead and directly use the recursive character text.\n58:09 And for this we we definitely require recursive character text which we have already imported I think right. So on\n58:16 the top you''ll be able to see that we have imported this which is present in langin.extplitter. So inside we are taking this text\n58:22 splitter which is nothing but recursive character text splitter. Now this is recursively split all the document size\n58:28 based on the chunk size that is 1,000 chunk overlap 200. Chunk overlap basically means some number of text will\n58:35 be able to get overlapped between two different documents right when we are doing the splitting. And uh here you can\n58:41 see we are also using separators right this is just like an empty space like a blank uh sorry this is an empty space\n58:48 this is one more separator this is a new line separator now you tell me in the comment section what separator is this\n58:53 okay so we can use different different separators you can also use comma um we''ll be seeing different types of\n58:59 chunking strategies in the later stages but let''s let''s start creating this one pipeline then you''ll be getting a clear\n59:06 idea about it like how this entire pipeline works Okay, then you have this text splitter. Uh once you uh\n59:13 specifically have this text splitter, you can actually use this to do the splitting. Right. So now what I will do,\n59:18 I will create a variable inside this and I will write textplitter.split documents. So we are using the split\n59:25 documents and we are giving the documents and these all are the default parameters that we are giving over here. Now once we do the split, you''ll also be\n59:31 able to see what is the page content. I''ll just try to display 200 characters from the page content and you can also\n59:37 see the metadata right so once we go ahead and execute this this is going to return the entire split documents now\n59:44 let''s go ahead and use this split let''s say here I''m just going to go ahead and\n59:49 get all my chunks I will be using this function split documents and let''s give\n59:55 the documents here we are going to give the list of documents right uh like uh\n1:00:00 what are the list of documents so list of documents is nothing but all PDF document. So I will give it over here\n1:00:06 and let''s see the chunks. Okay. So now if I go ahead and just go ahead and print the chunks, you should be able to\n1:00:13 see that my all my data is basically chunked, right? And uh you can see that\n1:00:18 we have splitted 64 documents into 359 chunks. So these are all my chunks that\n1:00:23 we have done it, right? That basically means we have converted all our text into smaller chunks, right? Based on the\n1:00:30 uh chunk size and the overlap. So like this kind of chunks we have how much 359\n1:00:35 I guess how much it is 359. Initially we had only 64 documents right for every page there will be a separate document\n1:00:42 structure. Perfect. So we have done this and uh we have done the splitting part.\n1:00:48 Now let''s go to the next step. The next step will be quite interesting because now if you see from this particular\n1:00:56 pipeline right what are we doing right? So here we have done the chunking but these two are the most important steps.\n1:01:02 One is the embedding right we need to perform some kind of embeddings over here right embedding uh generation\n1:01:09 embedding generation and vector store DB right embedding you can use any kind of models but I will try to focus on using\n1:01:16 open source model so that everybody will be able to just try it out you know uh\n1:01:21 for this what I will do I will just try to use some kind of modular coding so I will try to create some classes you know\n1:01:27 for embedding I will create a separate class and inside this we will try to define different different function\n1:01:32 Because in embedding uh you know that you are converting text into vectors right so for converting text into\n1:01:38 vectors I may define different functions like loading the model generating embeddings you know that kind of and in\n1:01:44 vector DB like again we''ll try to create this as a separate class. So let''s go ahead and probably go ahead and discuss\n1:01:51 about this uh wherein we work on the embedding part\n1:01:57 quickly let''s go ahead and see the embedding part. So for the embedding I will just go ahead and write a markdown.\n1:02:04 So let me quickly write embedding and vector store DB right. So we are going\n1:02:10 to specifically go ahead and implement these two important modules. Now first of all what I do do is that I I\n1:02:16 definitely require some kind of libraries over here right for embeddings. So for embedding uh we are\n1:02:21 going to use sentence transformer. uh we are going to use a model that is available in hugging face and for that I\n1:02:27 will be using the sentence transformers library along with this uh I also want\n1:02:32 to use some kind of uh you know vector store so this is the vector store I may\n1:02:39 use that is fire CPU you can use fires or you can also go ahead and use chromb\n1:02:44 so these are some very good open-source vector store that is available um now\n1:02:50 these all libraries will be more than sufficient to get started with. So quickly let me go ahead and install it.\n1:02:55 So I will write uvad minus r requirement.txt. So once I do the installation you''ll be\n1:03:02 able to see that. Okay the installation will get completed.\n1:03:09 So once the installation gets completed it''ll take some amount of time because we are loading the entire transformers.\n1:03:14 So here you can see that quickly it has got installed. Now I''ll go again back to over here. Now once I go over here what\n1:03:21 is the first step that I''m actually going to do is that I will quickly go ahead and import some of the libraries\n1:03:26 that I require like this right so I''m importing numpy from sentence transformer I''m importing sentence\n1:03:32 transformer my embedding model right will be available inside this then I''m\n1:03:37 importing chromadb then uh we also importing the settings from this we are importing uyu ID the reason of creating\n1:03:44 this uyu ID is that because every record that we specifically insert into the vector dv we''ll have some kind of id\n1:03:51 over there we''ll generate that then along with this we will also be importing list dictionary ne and t pupil\n1:03:57 and uh since we are going to apply cosign similarity while doing the retrieval from the vector db I also will be importing this and this is available\n1:04:04 in skyitler so let''s quickly execute this okay and till then I will go ahead\n1:04:10 and create more number of cells now as I said for embedding I will go ahead and\n1:04:16 write one different class So I will say embedding manager. So this will be\n1:04:22 responsible in doing the embedding part. So first first thing is that once I am creating this uh for every class that we\n1:04:30 specifically create, we need to write an init function. Okay. So init. So this is\n1:04:35 my constructor you''ll be seeing that it handles document embedding generation using transformer. Here we are\n1:04:40 initializing the embedding manager and the model name that we are giving is all mini LM L6 V2. So this is available uh\n1:04:49 in uh hugging face this specific model all mini L6 V2 and this is responsible\n1:04:55 in specifically converting a text into vectors and you get somewhere around 384 dimensions. Okay. Then uh we initialize\n1:05:02 the embedding manager. Then model name is nothing but hugging fist model name for sentence embeddings. We are going to\n1:05:08 use this. Okay. So here we are initializing the model name. Uh we are saying self domodel is equal to none.\n1:05:15 Okay. Because here uh later on we''ll initialize this value. This function is very important load model. So that\n1:05:21 basically means my next function will be load model. And this model work is very simple. This function work is very\n1:05:27 simple. It is going to load this model that is all mini L6 V2. Okay. So I will\n1:05:32 create another function which is nothing but underscore load model. Why we write underscore? Uh this is just like a\n1:05:37 protected function. Uh if you know about classes, we use something called as a protected function. And within this\n1:05:44 protected function within this class only it''ll be accessible. So here uh what we are doing we using the sentence\n1:05:49 transformer and whatever model name we have we are loading it. Okay we are loading it. So selfro model of sentence\n1:05:56 transformer model self model name then this will be modeled uh loaded and here you''ll also be able to get the\n1:06:01 dimension. For that we use a function called as get sentence embedding dimension and by default it will be uh\n1:06:08 somewhere around 384 dimensions. Okay, that basically means every text will be converted into 384 dimensions. So once\n1:06:15 we have this init function, we have the load model. Now one more function that we require is generate embeddings,\n1:06:20 right? So here uh you''ll be able to see that I will be seeing this generate\n1:06:26 embeddings function. Okay. So generate embedding is nothing but it takes the\n1:06:32 text that is nothing but list of string and it returns a numpy array. Okay. So\n1:06:37 here it generates the embedding for list of text very simple. So here what we are doing we are basically using this self\n1:06:42 domodel dot encode is the function that we have to use on text whatever text list of text we give and we also giving\n1:06:49 show progress bar is equal to true so that we should be able to see the progress bar and we return the embeddings. Okay. Now generate embedding\n1:06:56 is one function. Load model is one function. We have al also used get sentence embedding dimension just to get\n1:07:02 the dimension. Okay. Now for this you can either get I can you can either\n1:07:07 create this particular function or you can also remove this it is not necessary but what I did is that to show you much\n1:07:13 more in a better way we will create this function get sentence embedding dimension. So here is my get embedding\n1:07:19 dimension self. So here what we are doing we just written model get sentence embedding dimension. See instead of\n1:07:25 doing like this also I can write like this only over here. Okay I can just quickly write this particular function\n1:07:32 over here. Okay. So sometime it is not required you can also. So I will just go ahead and remove it if you want. Okay I\n1:07:38 will just remove it. Perfect. So I have these two three important function. Now\n1:07:44 we can initialize the embeddings. Okay.\n1:07:49 Uh sorry we can initialize the embedding manager. So here I will write embedding\n1:07:55 manager is equal to embedding\n1:08:00 manager. So I hope this is the class name\n1:08:06 should not be underscore it should be like this. Okay now once I go ahead and write this and once I execute it this\n1:08:12 will just go ahead and initialize the constructor. Right. So here you can see it is loading the embedding model. All\n1:08:18 mini LM V62 model loaded successfully and here you can see the dimension is\n1:08:24 384 right so it has been loaded so when we calling this particular function this is basically getting loaded right so my\n1:08:31 embedding manager now has the model information over here great so I have my\n1:08:36 model ready so if you see from this particular graph this entire class has been created now we go to the next step\n1:08:43 and create this specific class that basically means over here we have our model embedding ready we just need to\n1:08:48 use it. Now, similarly, we''ll go ahead and create it for the vector store also. Okay, vector store is just like a vector\n1:08:54 DB database where you can store all the vectors that has been converted by the embedding layer inside it so that you\n1:09:00 can apply any kind of similarity search into it. Right? So, first of all, let me\n1:09:05 quickly go ahead and define a class for this also. So, here I will go ahead and\n1:09:12 write vector store. Okay, vector store.\n1:09:17 Uh remember guys the code that I''m showing you is very simple if you just see you need to have some coding\n1:09:24 knowledge if you really want to become better in rag. Okay now we''ll go to the next step with respect to the vector\n1:09:30 store. Now in the vector store we are creating a class vector store. Again here we are using a init method. We are\n1:09:37 giving a collection name. What should be the collection name for the vector store itself. And uh here the collection name\n1:09:43 we giving it as PDF documents. We are also giving the persistent directory which will be this particular directory\n1:09:49 that is inside my data folder. Persistent directory means whatever vector store is basically created we are going to save it that in the hard disk.\n1:09:56 So here uh first of all I''m giving the collection name I''m giving the person directory collection is none. Self\n1:10:01 docolction is equal to none. Okay. And then we are initializing the store. Now whenever we initialize the store that\n1:10:07 basically means this function will be initializing the vector store itself. Right. So for this we need to create\n1:10:13 another function again and see the code. Okay, just observe the code. Here we are initializing chromab client and\n1:10:19 collection. So here we have written osmake directory of self.persistent directory whatever directory path is there. If it already exist we are just\n1:10:26 going to keep it like that otherwise it is going to create a new directory. Then we create a client self.client wherein\n1:10:33 we are using chromadv.persistentclient function and we are given the persistent directory over here. So what it is going\n1:10:38 to do? It is basically going to create a client which will be having a reference to the chrom vector store. Okay. Then we\n1:10:46 go ahead and create a collection. So here we write self.colction. Then self.client dot get or create\n1:10:52 collections. We''re giving the collection name and we''re giving some metadata information like what is the collection information. And here we basically\n1:10:59 create a collection uh collection basically means it''s just like uh where we are going to store the uh vector uh\n1:11:05 where we are going to store the uh vectors inside my vector store. So it''ll be stored inside this particular\n1:11:11 collection name. Then we are initializing this with the collection name dot collection count. Okay. So as\n1:11:18 soon as we execute this that basically means my chromb client will be ready and my collection will be created. Okay. Now\n1:11:24 the next function is that usually whenever we create a collection we need to add the documents right. So for\n1:11:30 documents we will be creating another function. So quickly let''s go ahead and\n1:11:35 create this because whenever I have a document I will go ahead and create this particular connection. Okay. So here you\n1:11:42 can see I''ve created another function which is called as add document. Here we give the list of document. We apply the\n1:11:47 embeddings. Very simple add documents and the embeddings to the vector store. And here\n1:11:52 you can see if length of documents is not equal to length of embeddings. Here you can actually see this. Now we are\n1:11:57 preparing the data for chromb. We require ids, metadata, document text and embedding list. So now whatever\n1:12:04 documents I have over here. Whatever documents I''m getting, I will be zipping it means I I''m creating a pupil with\n1:12:11 embeddings and then I am creating a UYU ID. Why I require UU ID? because it''s\n1:12:17 just like a id for a specific record, right? And that will be my doc id. Okay,\n1:12:24 doc id variable and I''m appending it over there. Then we are preparing the metadata. Whatever doc metadata we get.\n1:12:31 Remember we are iterating through this documents. So we have all the information. So that all metadata we are\n1:12:36 putting it over here. Doc index content length. We are just adding some more metadata information to put it inside my\n1:12:43 vector db. Then we get the document content from doc.page_content.\n1:12:49 And we also get the embedding where we are converting this embedding to list. Okay. See two information is basically\n1:12:55 required right over here. If you see uh from this particular function one is embedding which is my MP. ND array right\n1:13:02 and this embedding is coming from where from the previous function right generate embeddings where we have done it. So it''s all linkage. See the reason\n1:13:10 of creating this particular in the form of class because I want to link each and every pipeline right. So here we are\n1:13:16 writing embedding list.append embedding.2 two list. So we have the page content, we have this list. So what\n1:13:21 I''m doing I''m adding that entirely in the collection. So for this we require ids, we required emitting list, we\n1:13:28 require metadata, we require document text. So whatever we have prepared,\n1:13:33 we''re just adding it over here based on the parameters, right? And finally you''ll be able to see the how many number of documents has been inserted.\n1:13:40 Now quickly let''s go ahead and initialize.\n1:13:47 Let''s go ahead and initialize my vector store. So I''ll write vector store is equal to\n1:13:55 uh vector store and I''ll initialize this. Okay. So\n1:14:01 quickly I will go ahead and write vector store. So now this is basically going to initialize the entire vector store\n1:14:08 itself. Right. So here you can see this is my collection name and existing document in collection is zero since we\n1:14:14 did not add any number of records. Okay. Now, if we want to add any number of records, we have to call this function\n1:14:22 add documents, right? So, let''s uh go ahead and do that and let''s call it. Okay. Now, first of all, uh you know\n1:14:29 that I have already done the splitting of the chunks, right? So, here if you go ahead and see this, this is my split\n1:14:36 chunks, right? Uh sorry, that was the variable. Let''s see which variable it has got saved. Okay, it should be\n1:14:42 chunks, right? So these are my chunks right\n1:14:47 now chunks what I am actually going to do is that I will extract all the text from that particular chunk and we''ll\n1:14:54 generate an embedding. Okay. So for that what I will do I will say I will put a list comprehension. So here now let''s\n1:15:03 convert the text to embeddings. Okay we''re going to\n1:15:11 go ahead and do this. And here we are basically going to write\n1:15:16 chunks. First of all, I''ll iterate. Okay, I will say that hey for doc in chunks.\n1:15:25 Okay, and we are just going to take this doc dot page content. Okay, so we are\n1:15:31 going to take all this page content and basically go ahead and create my text\n1:15:37 text variable. Okay. So once I go ahead and do this, you should be able to see this is my text, right? All the text\n1:15:44 that I have and this text I will pass it to my embedding manager, right?\n1:15:49 Embedding manager which I have actually created. So what I will do quickly, I will just go ahead and execute this once\n1:15:55 again. I have all my text. Okay, I have all my text. Now from this\n1:16:01 we will go ahead and generate the embeddings. Now once we generate the\n1:16:06 embedding how do we generate the embeddings very simple we use this embedding manager which object we have\n1:16:13 actually created what object we have created earlier if you see over here this is my embedding manager right so we\n1:16:20 are using this embedding manager dot generate embedding and here I have to give the text in the form of a list list\n1:16:26 of strings right so here quickly I will call this particular function dot uh dot\n1:16:34 generate generate generate\n1:16:40 underscore embeddings. Okay. And here you will be able to see that\n1:16:46 I''ll be giving my text. Then let''s store\n1:16:51 store in the vector database. So after we convert that into an embedding, we\n1:16:57 store everything in the vector database. Right? So here I will use vector store.\n1:17:02 vector store the variable that we have created dot add\n1:17:08 documents and this is a small letter add documents this is a function that we\n1:17:15 have used and inside this if you remember we have to give our we have to give our entire\n1:17:23 chunks okay whatever embeddings we are specifically applying okay so once we do\n1:17:31 this You can see this embeddings whatever we have got and the chunks the documents\n1:17:37 the entire documents we''re going to do this okay so let''s quickly execute this and I think now my embedding will happen\n1:17:43 now you can see that for 359 text this is happening and it has got converted into so many number of batches\n1:17:51 uh vector store is not defined why it is not defined let''s see what I have defined over there okay it should be\n1:17:56 vector store so this should be the spelling of my vector store instead of that. Okay. So\n1:18:03 now let me quickly go ahead and execute this. Now inside that same vector store\n1:18:08 it''ll get it''ll get executed. Okay\n1:18:13 perfect. Now you can see that the total document in the collection is 359. So if you see over here uh inside my u\n1:18:22 notebook file inside my data file here there is something called as vector store and we have done the persistent\n1:18:27 over here right. So persistent basically means the now now f the it is saved in\n1:18:32 this particular hard disk. We can just load this hard disk and we can probably go ahead and execute anything as such.\n1:18:38 Okay. Now perfect. Now you can see that we have completed this entire pipeline.\n1:18:43 Now we have all the data available over here in the vector store DB right in the\n1:18:48 form of vectors. But now the main thing is that how do we perform the retrieval? Because retrieval\n1:18:55 see in retrieval what happens is that whenever we have a user query we have to\n1:19:02 take this query we have to convert that into embeddings again okay and then we\n1:19:09 basically go ahead and hit the vector store in the form of a retriever and then only we get the context. So in our\n1:19:15 example first of all we''ll try to get till here. Okay, we have a user query.\n1:19:20 We convert that query into embeddings. Then we hit this particular vector store and we get the context. So let''s go\n1:19:26 ahead and create this specific pipeline now. Okay. And for this pipeline, we will try to create a rag retriever.\n1:19:33 Okay. So we will try to create a rag retriever. So let''s quickly go ahead and do that particular thing. Till now we\n1:19:40 have created all the amazing pipelines. We have created this embedding manager. Now we also have this vector store. Now\n1:19:47 what I will do is that I''ll create another pipeline which will be a rag retriever. Okay, just to get the\n1:19:52 specific context. So let''s go ahead and discuss about that. So guys, now let''s go ahead and create the rag retriever\n1:19:59 pipeline. So first of all, what we are going to do is that I will go ahead and create a class which is called as rag\n1:20:04 retriever. Now this rag retriever class you will be able to see that it handles query based retrieval from the vector\n1:20:11 store. So inside the constructor we will be giving two important parameters.\n1:20:16 One is the vector store and one is the embedding manager. And if you remember\n1:20:21 we have created both this. We have created the embedding manager. We have created the vector store manager. Right\n1:20:27 now after giving this we will be initializing two class variables that is vector store and embedding manager and\n1:20:33 we''ll be assigning with this. Now whenever we create a retriever one thing\n1:20:39 you really need to understand this retriever is actually built on the top of a vector store and retriever is\n1:20:45 nothing but it is a simple interface based on whatever query we get this retriever is just going to give you the\n1:20:50 response back. Okay and this retriever is basically a kind of interface which is connected to the vector store and\n1:20:56 chart. Okay. Now uh the next step that we are going to create is another\n1:21:02 function which will be called as retrieve function. Now this is really important because this retrieve function\n1:21:07 main work is to retrieve based on a specific query. So let me go ahead and\n1:21:13 define the specific function. Now this function again see to write it\n1:21:18 will definitely take a lot of time. So we will try to understand this particular function. Okay. So here a\n1:21:24 retrie function you can see we are giving query we are giving top key results. How many top key results we\n1:21:29 want and there is also a threshold value. By default it is 0.0. zero and this function is basically going to\n1:21:35 return a list of results. Okay, so here you can see retrieve relevant document\n1:21:40 for a query arguments are the search query, top K documents and score threshold and it returns a list of\n1:21:47 dictionaries contain the retriever documents and metadata. At the end of the day this function is actually help\n1:21:53 us to get this specific context. So you''ll be able to see over here we\n1:21:59 are using that same self embedding manager and we are calling this generate embedding function. Now if you remember\n1:22:05 this generate embedding function is already defined in my embedding manager right. So if I go on the top so here is\n1:22:13 my generate embedding function and this is nothing but this is basically uh you''re just using model.enccode and\n1:22:19 you''re giving the text and it is converting into embeddings. Yeah. So that is the reason we are basically\n1:22:25 using this because at the end of the day first of all whenever we get a query\n1:22:31 right so let me go down over here inside this retrieve whenever we give this query first the query needs to be\n1:22:38 converted into an embeddings right so this query that is given we need to apply embedding for this also so that we\n1:22:44 can do a um similarity search in the retriever itself right so the first the query is basically converted into a\n1:22:51 vector by the help of embedding manager dot generate fun embedding functions.\n1:22:57 Then we are going to use the vector store dot collection and we are going to use this dot query and here we are going\n1:23:04 to give our query embedding which is nothing but this embedding in the form of a list and then we are also going to\n1:23:10 give the top results. So by using this this is basically going to hit the vector DB whichever vector vb we have\n1:23:16 initialized and it is going to give you the results. Once you get the results,\n1:23:22 the results internally there will be a key which is called as documents. Okay, you can get document information, the\n1:23:28 mech metadata information, the distance information and some of the ids information. So all the specific\n1:23:34 information we are using it and here you can see very similarly what we are doing\n1:23:40 we are using all these parameters like ID, documents, metadata and distance. We are zipping it. Zipping it basically\n1:23:47 means we are just trying to create a pupil over here and then for every values we are just trying to calculate\n1:23:54 the distance right one minus distance 1 minus distance will basically give you the similarity score like how similar\n1:24:01 those text data is basically coming up outside this vector store. So we are\n1:24:06 creating the similarity score and if the similarity score is greater than the threshold then what we do we basically\n1:24:12 add this inside my text context documents and context documents is basically created in this particular\n1:24:18 variable which is nothing but retrieve docs which we have kept it empty over here. Okay. So all the information we\n1:24:25 are just trying to add it over here so that we''ll be able to see it. Okay. And finally we return that retrieve docs. So\n1:24:31 if you say step by step we''re not doing anything we like not very complex thing we are getting the user query we''re\n1:24:37 converting this into embeddings we are hitting the vector store right then we are getting the response okay once we\n1:24:44 get the specific response that context we are putting it in the form of a list if you just go ahead and see the code\n1:24:50 that is how things are happening okay so this is one of the very important function uh that you''ll be able to see\n1:24:58 now here what I can do is that I can quickly go ahead and create a variable called as rag retriever and I can call\n1:25:06 this same class. So if you see over here I will use this same rag retriever over here\n1:25:14 and let''s give our vector store vector store which I''ve defined it earlier\n1:25:20 which is my vector store manager and then my embedding manager.\n1:25:25 Once I do this I should be able to see this. Okay. uh it should be vector store file right so now you''ll be able to see\n1:25:34 this is my rag retriever rag retriever it is an object of this now if I call this particular function\n1:25:41 with a query right I can call dot retrieve with a query so let''s go ahead and do this okay so here I will write\n1:25:48 rag retriever dot query sorry dot\n1:25:56 retrieve is my function Okay. So here you can see quickly this\n1:26:02 is my function retrieve right and I need to give a query. Now let''s test for a specific query. I''ll say hey what is\n1:26:11 attention is all you need because I know inside my data there is a PDF file which\n1:26:19 is called as attention or I have also created some kind of proposal over here\n1:26:24 embedding some files are there. So we''ll try to execute this. So here you can see\n1:26:30 as soon as I asked what is attention is all you need. Now it is giving me the top K for all it is printing all the\n1:26:36 information and it is generated embedding for one text. Right? And the text shape is 1, 384 because I have used\n1:26:43 the embedding that is called as all mini LMV6 that creates a 384 dimension. Now\n1:26:49 once we go ahead and apply this particular function right this function it is basically getting the results over\n1:26:55 here and we are printing that same thing right and at the end of the day we we we can also go ahead and return this\n1:27:01 retrieve docs okay so in short this is basically this function is going to give me all the retrieve docs so this is the\n1:27:07 retrieve docs you can see content metadata author so these are my context information so here you can see\n1:27:14 attention function can be described as a mapping a query as a set of this one and this entire entire thing is basically\n1:27:19 the context. So from this particular diagram here you can see easily we are able to get the context right and this\n1:27:26 is nothing but this is your context. Now let''s try some more things. Okay I will just go ahead and open some PDF. Okay.\n1:27:34 Um this is some very new research paper embedding technical report. Okay. Uh\n1:27:41 we''ll search for any topic over here. Uh embedding model training. I''ll just go ahead and search for unified multitask\n1:27:48 learning framework. Okay, because this information also we have put it over there. So here I''ll go ahead and create\n1:27:55 one more this one and I will copy this entire code. Okay, quickly\n1:28:02 and this is the query that I''m actually going to give that is nothing but unified\n1:28:10 multi multitask learning framework. So if I go ahead and execute this you can see that I''m able to get this and then\n1:28:16 you can see content benchmark ranking over on both the leaders effective of\n1:28:21 our approach. So we are able to get the response very very much quickly right and this response is basically coming\n1:28:26 from the vector store right in a very similar way very easy way uh we are able\n1:28:33 to get the specific response over here right and let me tell you right this is\n1:28:39 the most easiest way like how things are basically happening over here right now\n1:28:44 uh what we can do is that see if you know if you have created all these things right till here you have created\n1:28:50 now the further step is that you have to just integrate LLM with the uh with this specific context. Okay. Now for this LLM\n1:28:58 with this specific context, what you can do is that you can directly take this particular context and give it to the LLM and that is what we are going to see\n1:29:05 in the next video. But in this particular video, we saw the entire thing the complete rack pipeline from\n1:29:11 data injection to the vector DB pipeline. Right now you can go ahead and write any kind of queries and definitely\n1:29:18 with all these information here you can see similarity score is also coming up right distance is also basically coming\n1:29:23 up all the information you''re putting it over here and we have also used modular coding right now in the next step what\n1:29:29 I''ll do I will take this vector store and uh we will go ahead with the next integration that is llm and output which\n1:29:36 I will say it as a retrieval pipeline but this entire data injection pipeline with this uh query retrieval we have\n1:29:44 actually created. Now the next two steps will this one and after doing this we will try to convert the same code\n1:29:50 whatever same whatever code we have basically written over here in the form of modular coding right we''ll try to see\n1:29:56 that how we can put this inside our source folder so here what I will do we''ll quickly create a source folder and\n1:30:04 inside the source folder I will show you that how we can take this entire pipeline and how we can actually create\n1:30:11 it in such a way that we have a kind of pipeline over here right pipeline\n1:30:17 basically means from data injection to vector embedding how in a sequential way we can actually go ahead and call it.\n1:30:23 Hello guys so we are going to continue the discussion with respect to rag. Uh till now we have already discussed about\n1:30:29 the entire data injection pipeline and with the help of user query you know we are also able to retrieve the context.\n1:30:37 uh we have completely implemented this first pipeline that is called as data injection pipeline where we did the data\n1:30:43 injection. We did the chunking uh then we converted the text into vectors and\n1:30:48 after that you know uh we were able to probably store everything inside a vector DB and we also persisted in the\n1:30:56 local directory so that we can always read whenever we definitely want okay based on a specific query. Now we are\n1:31:02 going to go towards the second pipeline that is the query retrieval pipeline wherein we are also going to use LLM\n1:31:09 with it. Okay. So here we are going to specifically use LLM models and this LLM\n1:31:14 models will actually help us to generate a summarized output. Okay. In the rag.\n1:31:21 So the entire pipeline will look something like this. And uh when we talk about this query retrieval pipeline, we\n1:31:28 are specifically talking about something called as augmented generation. Okay.\n1:31:34 See in retrieval uh rack basically means retrieval augmented generation. And this\n1:31:39 augmented generation how does it specifically work? Okay. So let''s consider that this vector DB is already\n1:31:46 ready and you know that how did I create this particular vector DB? By following this particular pipeline, right?\n1:31:54 Now once we follow this pipeline the data is stored inside the vector DB. Now\n1:32:00 whenever a user gives a new query okay it has a new query related to the\n1:32:06 documents that are already ingested inside the vector DB then what we do we\n1:32:11 take up this query we apply the same embedding and in this particular embedding what we do we convert the\n1:32:18 query to vectors right and then from this particular\n1:32:23 embedding we hit the vector DB we get the context and then whatever context we\n1:32:30 get along with the prompt engineering like basically with a simple prompt we\n1:32:36 give that instruction to the LLM right so prompt is just like an instruction to the LLM like how the LLM should\n1:32:41 basically work now once we are doing this right this this step is basically\n1:32:47 called as augmentation okay this step is basically called as\n1:32:52 augmentation wherein we are giving we are taking the context and along with that we are also combining it with a\n1:32:58 specific prompt And finally you''ll be able to see that we''ll generate the output from the LLM.\n1:33:03 And this step is nothing but generation right this is the retrieval step. So\n1:33:10 here I have my retrieval step wherein we are giving a query we''re converting that into vectors and we hitting the vector\n1:33:16 DB. So you really need to understand the entire concepts with respect to rack. Okay. So let''s go ahead and implement\n1:33:24 this entire retrieval uh query retrieval pipeline along with the LLMs. Okay. Now here we also going to go ahead and set\n1:33:30 up the LLM. So guys, now let''s go ahead and implement this uh with the help of\n1:33:35 practical implementation. So here we are going to integrate vector DB context pipeline with LLM output. U as suggested\n1:33:42 we are going to implement the augmented and generation. Now first first of all what we going to do is that I''m going to\n1:33:48 use the my Gro API key. Okay. Okay, so I have updated the gro API key over here in the ENB file and uh you know here we\n1:33:56 are going to probably go ahead and create a simple rag pipeline. Okay, uh\n1:34:03 with the gro lm okay so first of all what we are going to do is that uh again\n1:34:09 uh if you remember in our requirement.txt we will go ahead and import this two libraries that is called\n1:34:15 as langin-g gro and then you have python.nv PNB okay\n1:34:20 and then after this uh we will go ahead and uh you know quickly initialize from langchain\n1:34:27 grock import chat gro okay along with this I''m also going to go ahead and import os then from env I''m going to use\n1:34:35 load env so that we import or we load the entire environment variables then\n1:34:42 the next thing is that we will go ahead and initialize the gro lm and set your\n1:34:47 environment gro API key inside this. Okay. And in order to do this again here\n1:34:52 you''ll be able to see that I''m using gro API key OS.get env something like this. Okay. If you just go ahead and call this\n1:34:59 sometime uh my suggestion would be that directly don''t call from get env. Initially you can directly test it by\n1:35:06 pasting the environment keys directly over here. Okay. So here I will go ahead\n1:35:11 and paste it. Otherwise you go ahead and replace it. Just for testing purpose I''m actually doing this. Now we''ll go ahead\n1:35:17 and initialize our LLM model chat gro and here I will use my gro API key is\n1:35:23 equal to API sorry gro API key okay and\n1:35:29 then model name is gamma 2 temperature I will select it as 0.1 and maximum number of tokens it will generate is 1024 okay\n1:35:36 so this is my lm we have initialized the gro lm now the second thing is that we will quickly go ahead and create a\n1:35:44 simple rag tag function and this is going to integrate everything from\n1:35:51 retrieve context plus generate response and if you remember guys here is my retriever before class like the previous\n1:35:59 u session we have already seen that how this rag retriever was actually created we created a class for that okay so here\n1:36:05 uh we are going to probably take two different parameters inside this we''ll first of all define a function called as\n1:36:11 rag simple and then here we are going to go ahead and give our query\n1:36:16 Then we are going to go ahead and give our retriever llm\n1:36:24 top k is equal to three. Okay. And then uh over here quickly let''s go\n1:36:31 ahead and first of all retrieve the context. Yeah. So we''ll going to\n1:36:37 retrieve the context. So here I''m going to write results is equal to retrie dot\n1:36:42 retrieve query. So here you have this query and top k is equal to k. Okay. And\n1:36:48 then uh we are just going to get the context or I''ll go ahead and define my context. Inside this context I will say\n1:36:54 that hey whatever information I''m getting from my results right just go\n1:36:59 ahead and combine everything and put it inside this. Right? So here I''m saying\n1:37:05 that hey for doc in results whatever content I''m getting I''m going to join it with a uh double new line over here. If\n1:37:12 results are this empty, we are just going to keep it as empty. So this is my context over here, right? then uh I can\n1:37:19 still go ahead and write one more condition saying that hey if not context\n1:37:25 okay we just going to go ahead and return saying that no relevant context\n1:37:32 form okay to the answer question and then we are going to generate the answer\n1:37:41 using grock lm okay and now I''m just going to go ahead and define prompt\n1:37:48 obviously I required a prompt. If you remember here I can again use a prompt\n1:37:55 template also I can directly use a prompt over here. So here with respect to the prompt I will give a query saying\n1:38:01 that hey this is what you really need to do. You need to go ahead and answer this specific question and you should\n1:38:07 probably get a response for that. Right? So here what I will do I will quickly go ahead and paste it. Use the following\n1:38:13 context. So here you can see use the following context to answer the question uh uh question concisely. Okay. And here\n1:38:21 what we can basically do is that we can just go ahead and um do one thing on\n1:38:26 over here quickly. I''ll say just put tab. Okay. So use the following context\n1:38:32 to answer the question uh precisely or concisely. So here I have given the context. Here I''ve given the query.\n1:38:38 Okay. Now the next thing after this is that we will go ahead and create a response. So response is equal to this\n1:38:44 time we going to use llm dot invoke. Okay. And here uh let''s go ahead and put\n1:38:52 something like prompt dot format. And here we are going to write context\n1:38:59 is equal to context and here you have query is equal to\n1:39:05 query whatever query I have. Okay. And then we go ahead and return the response\n1:39:12 dot content. So once we do this uh then we can\n1:39:18 specifically call this particular function. Okay. So now what we are going to do is that I will just go ahead and\n1:39:23 write answer is equal to rag simple and\n1:39:28 let''s say I go ahead and ask a question. What is attention mechanism?\n1:39:36 Okay. And here I need to give my rag retriever along with the llm and then we\n1:39:41 can go ahead and print the answer. Okay. So here you can see attention\n1:39:47 mechanism is a function that maps a query in this right and we are able to get the answer over here. This is really\n1:39:53 good. See a very simple pipeline where I have initialized my lm model. I''ve\n1:39:58 defined a function and then this function what it is doing first of all it is hitting the rag retriever retrieve\n1:40:04 function. It is getting the context. it is combining the context and along with the prompt we are hitting the llm. So if\n1:40:09 you remember we are we are just following this entire process and generating a proper output right if that\n1:40:15 particular output is available inside the uh vector DB right now guys uh what\n1:40:21 we are going to do is that we are going to enhance the rack pipeline the simple rack pipeline that we have created over\n1:40:26 here okay we''ll enhance in such a way that it will have more amazing features in it okay so now we''re going to go\n1:40:33 ahead and create an amazing enhanced track pipeline and this is the code so now you can see over Here we have a\n1:40:39 function called as rag advanced. I''m giving a query retriever lm topk elements like how many we want minimum\n1:40:46 scores return context is equal to false. So here you can see that um before we\n1:40:51 were simply like we were just combining the context we are putting the information in the prompt and we were\n1:40:56 probably generating the response. In this what we will do is that here we are going to generate this entire pipeline\n1:41:03 with some more additional features like what all additional features we''ll be requiring. See here we are directly\n1:41:09 getting the answers right but we do not have much information about the source about the context over here right. So\n1:41:16 here what we are doing we will return answers sources confidence score optionally fully context full context\n1:41:22 okay so first of all again the code will be similar where we are retrieving the context so this becomes my context when\n1:41:28 we are retrieving it from retriever retrieve and then uh I have written if not results if results are empty we are\n1:41:34 saying that no relevant context found and here we are giving sources is blank confidence is 0.0 zero and context is\n1:41:40 blank. This context is basically coming from the vector DB. Let''s say that if we are getting some kind of results over\n1:41:46 here, we are combining all those results and we are preparing the context over here and then we are adding sources. See\n1:41:52 this sources which is the list here we are adding metadata information source file right and along with that you can\n1:41:58 see metadata page number from which page number you are able to get then what is the similarity score and here what I\n1:42:04 will do is that I''ll just try to go ahead and you know display at least 300\n1:42:10 um length of the content right so up to 300 characters we''ll try to display and then we are going through each and every\n1:42:16 docs that is available inside this results then we are going to calculate the confidence uh we are actually\n1:42:22 getting that information in this doc similarity score. Here is my prompt. In this prompt we are giving context query\n1:42:29 each and everything and we are invoking it and the output will be in this format. So let''s now go ahead and\n1:42:34 execute this rag advanced function. Here I''ve given all the information like I''ve asked what is the attention mechanism?\n1:42:41 What is rag retrieval like rag retrievy I''m given over here llm return context\n1:42:46 is equal to true minimum score all these things is given right. So now I''ll go ahead and execute this. Now as soon as I\n1:42:52 ask what is attention mechanism here you''ll be able to see that I''m getting this particular information right and it\n1:42:57 is also giving me the source information which number page number what is the score and what is the preview\n1:43:02 information along with that here is my final information that you can see right where we are displaying the first 300\n1:43:09 characters let''s say that I go ahead and change my question okay I I ask\n1:43:15 something else I''ll say hey u attention mechanism was one of the thing but if I\n1:43:20 go ahead see my data, my PDFs. Okay, I will go ahead and ask something else.\n1:43:27 Okay, let''s see what I can ask. So, I''ll go to embeddings PDF. I''ll say okay. And\n1:43:33 then let me search something else, right? I will say hard negative. I''ll\n1:43:38 ask this question hard negative mining techniques. Okay, so I will go to my\n1:43:46 question over here. hard\n1:43:51 negative mining techniques. Okay.\n1:44:00 And I''ll go ahead and search this thing from my vector retriever. So here you\n1:44:05 can see that I''m able to get this entire information. The test is several hardcand\n1:44:10 embeddings NV retriever all these information and again you can see that embedding.pdf PDF page 4 I''m able to see\n1:44:17 all the information along with the context right so this is uh really amazing and here we have just created an\n1:44:23 Nstrack pipeline why we say this as an NS rack pipeline because here we are providing information related to answers\n1:44:30 we are providing information related to confidence score and each and everything now let me just show you one more\n1:44:36 amazing way and this is also an advanced rack pipeline but this time I will tell you to probably go through this\n1:44:42 particular code and tell me so here what What we doing? We''re doing streaming, citation, history and summarization. So\n1:44:48 all these things we have included over here and uh you can just go and search for this and you can see the answer.\n1:44:53 Okay, final answer roment context found because that question may not be there. Okay, I will just or let me just change\n1:45:01 this minimum score to 0.1. I think we should be able to get something. Still nothing. Uh let me change the\n1:45:09 question. Let''s say hard negative mining techniques. And here we are just going\n1:45:14 to go ahead and display this particular output. Okay. So now you just go ahead and explore this. Okay. I''ll keep this\n1:45:21 for you at least see some kind of coding. Okay. So here uh we are not able to get anything as such. Uh let''s see\n1:45:28 advanced rack query hard query to top querying summarize equal to true. Uh no\n1:45:34 relevant this one. Let''s see that I go ahead and ask what is\n1:45:39 what is attention is all you need. Okay, I''ll go ahead and\n1:45:47 execute it. So here you can see that I''m able to see all these particular answers over here. Right. Yeah, for some of the\n1:45:54 queries this will not it is not giving there may be some problem with respect\n1:45:59 to the context size but it''s okay. You can try out with different different things. If it if something is not coming then we''ll try to optimize that also as\n1:46:06 we go ahead we''ll try to see this. So here we have seen three amazing rack pipelines. One was a simple rack\n1:46:11 pipeline. Here was an enhanced rack pipeline. And here uh in the last one we have made sure to put streaming citation\n1:46:18 and history and summarization with all this kind of information over here. You just go ahead and check it out all the\n1:46:23 information and just see the code. I think you should be able to understand it. So overall uh if you see I hope you\n1:46:30 were able to understand this particular video and uh yeah this was about rack\n1:46:36 pipeline. Now in the upcoming videos what we will do is that we will try to create some modular coding because see\n1:46:42 here the entire everything is basically created in one IP file. So guys now it''s\n1:46:48 time that we implement the entire rack pipeline in the form of a modular structure. Already in our notebook we\n1:46:55 have seen about PDF loader.pipinb IP and B you know wherein we discussed how to probably go ahead and create the entire\n1:47:01 data injection and how to probably store all the information into the vector DB and finally you''re also able to make the\n1:47:08 query right along with that uh I have also shown you how to work with typesense uh which was an open-source uh\n1:47:13 vector store itself which was also again amazing for searching anything in a\n1:47:19 quicker way right now all the kind of implementation that we have done what we are going to do is that I''ll try to show\n1:47:25 you how in a modular way you can go ahead and integrate this in a form of a pipeline. Okay. So already we have this\n1:47:31 source folder. Now inside this source folder, what I am actually going to do is that I''ll go ahead and create my_init_.py\n1:47:39 file. And after creating this particular file, what is the next step is that I\n1:47:44 will go ahead and create all my components important components that will be required in order to create your\n1:47:51 uh rack pipeline. The first important component is nothing but data\n1:47:56 loader. Right? Data loader. py file. Right? So this will be my first\n1:48:02 component because initially we need to load the document. We need to do the chunking and then we need to probably go\n1:48:07 ahead and store it into the vector store. Right? So inside my data loader you know I I will just try to go ahead\n1:48:13 and read all the documents uh that is actually required. Okay. Then uh after\n1:48:19 this uh the next step should be your vector store. Right? Now the vector store what vector store we are basically\n1:48:25 going to use. Uh so for that I will be creating my another file. So here inside my source I will go ahead and create one\n1:48:32 more file which is called as vector store. py. Okay. So this is my next file\n1:48:38 that is basically created. Okay. uh along with this uh while while actually\n1:48:43 inserting anything into the vector store I also need to probably go ahead and do some kind of embeddings right and uh I\n1:48:49 will try to show you some open source embeddings that we are going to use. So for that I''ll be creating my embedding\n1:48:55 py file and finally uh the last file that I really want to create is something called a search py. Now my\n1:49:01 entire rack pipeline needs to be integrated in such a way that there should be a linkage between all the\n1:49:07 specific files. Now the first case is that I will go ahead and start working on data loader. Now you know data loader\n1:49:14 work is nothing but it should be reading this particular data. Okay, it can be from any source itself. Um we will try\n1:49:21 to read this specific data itself. Right? So for this what I''m actually going to do is that I''ll go ahead and\n1:49:26 import some of the libraries. So quickly I will go ahead and import these all\n1:49:31 libraries like uh pi PDF loader, text loader and all. Okay. So I''ll start working on this because I need to form a\n1:49:38 pipeline itself right. So inside this particular file my main code should be in such a way that I will go ahead and\n1:49:45 read all the documents let it be of a PDF text loader or CSV. Okay here I''m\n1:49:50 also going to give you some of the assignments because uh in this entire series of videos we have discussed about this. Okay. So quickly what I''m actually\n1:49:58 going to do is that I will go ahead and create one function which is basically called as load all documents. Now see\n1:50:05 this. Okay. So here I''m just going to go ahead and write this function. Now please have a look onto this particular\n1:50:11 function. This function function definition is load_all\n1:50:17 documents. I''m given the data directory. This should be in the form of string format and it is returning list right\n1:50:23 list of anything right of any kind of data type. Now the main important thing about this function is that it loads all\n1:50:29 supported files from the data dictionary and convert to langen document data structure because as soon as we read any\n1:50:36 kind of data like PDF, CSV, TXT, right? We need to probably go ahead and convert that into a langen document structure\n1:50:43 then only we''ll be able to apply the chunking. Okay. So here you can actually see that I have used data path uh of the\n1:50:51 data directory itself. the data directory I will be giving in the runtime and obviously by just seeing\n1:50:56 this the data directory is nothing but data itself. Okay. Now this is the code specifically to read all the PDF files.\n1:51:04 Okay. So here I have created a list documents which will be storing all the documents itself. Uh here we have used\n1:51:11 data path globe globe function and here I have used this pattern this kind of\n1:51:17 regular expression to match all the PDF files. So what it will do is that inside this data directory it will start\n1:51:24 looking for all the PDF files. So inside this you know that in the inside my PDF folder there are some PDF files. So it\n1:51:30 is going to go ahead and read all these particular PDF files. Okay. So once it reads the PDF files uh we will be having\n1:51:36 those PDF files over here in the form of a list. Okay. Then what we are doing we are writing for PDF and PDF files. We\n1:51:43 are going through every PDF and then we are using pi PDF loader to read the\n1:51:48 content inside this and we are using loader.load and finally I get all the information over here and we are going\n1:51:54 to extend that documents. Now this is just an example of PDF files right now. Same thing you can also do over here for\n1:52:02 text files. Okay, text files. You can also do it for CSV files. Right? See\n1:52:08 similar kind of code is basically suggested by GitHub copilot. But I really want to give you an assignment.\n1:52:14 Okay. So this will be for CSV file. This can be for SQL files. Any kind of files\n1:52:19 that you really want to work with. you can go ahead and write that particular code and keep on appending inside this\n1:52:26 particular documents. Okay. So as soon as you do that automatically you''ll be able to do this specific stuff and\n1:52:32 you''ll be able to get all the documents. Okay. Now what I will do just to test it\n1:52:37 out whether my PDF files is working fine or not. I will just go ahead and create one app. py file over here. Okay. Now\n1:52:44 inside this app py file let me go ahead and import some of the libraries. So\n1:52:49 first of all I need to read everything over here right. So I have written from source dot data loader import load all\n1:52:56 documents. So this load all documents is nothing but this is the same function that is present inside my data loader.\n1:53:01 py. Okay. And then from source dove vector store files vector store and rack\n1:53:07 search I will create in the later stages. So right now I''ll remove this. Okay. Now let''s try to test the example.\n1:53:13 So example usage I will write if name\n1:53:19 main okay and then here I will go ahead and write documents is equal to load all\n1:53:25 documents and I''ll give my data folder okay data folder then what I can\n1:53:31 actually do is that I can just go ahead and print my docs okay\n1:53:37 if you see inside this data loader what this is returning right now it is not returning anything so what you can\n1:53:43 actually do do is that from here so here what we are going to do is that we are going to return the specific documents\n1:53:48 over here so that we should be able to print that particular documents over here right now what I am quickly going\n1:53:54 to do is that I will just go ahead and write open command prompt okay and here\n1:54:00 I''m going to go ahead and write python app py now let''s see whether it''ll be\n1:54:06 able to read the uh pdf files or not now here you can see it has found four pdf files all the pdf file URL is over here\n1:54:14 and you are able to see that it is also able to see all the content that is available inside that particular\n1:54:19 documents which is good right and this is basically in the form of a document data structure I guess yeah so all the\n1:54:26 information is basically happening so that basically means so clearly I can see something really amazing over here\n1:54:33 is that my entire data the PDF code that we have written is working absolutely\n1:54:39 fine okay now uh comes the next step. Now the next step you should probably\n1:54:44 start thinking whether we should basically go ahead and work with embedding so that to do the chunking and\n1:54:50 all right so here uh I will go ahead and start working on embedding now inside my embedding what we are going to do is\n1:54:57 that I''ll be importing these libraries now these all are same thing repeated but here I''m using classes and function\n1:55:04 definition so here you can see that after reading all the documents after loading all the documents I''m going to\n1:55:10 use sentence transformer recursive character text splitter and here you can see I''ve defined a function uh class\n1:55:16 called as embedding pipeline right the model that I''m going to use is all mini v6 uh lm l6 v2 chunk size is nothing but\n1:55:24 1,000 and chunk overlap is nothing but 2,00 200 then here we are writing self\n1:55:30 dot chunk size chunk self overlap and then we are also initializing the sentence transformer now in the next\n1:55:36 function that we are going to go ahead and do is nothing but uh we are going to go ahead and create a function which is\n1:55:42 called as chunk documents. Now inside this chunk documents we are giving the\n1:55:48 documents which can be a list of any documents. Here we are applying recursive character text splitter based\n1:55:53 on all these values that we have initialized. Along with this we have also used different different separators\n1:55:59 if you''re interested or you can directly use this blank separator. Okay. Then you\n1:56:04 can see that I am also using the splitter.split split documents over here and then you will be able to see the\n1:56:10 remaining chunks over here itself. Okay. Now this is for uh any document that I\n1:56:15 pass inside this particular function right but one thing is very important is\n1:56:20 that because after the chunking is done right you need to also convert that chunking into vectors with the help of\n1:56:26 this particular model. So for that I will be creating one more function which is called as embedding chunks right. So\n1:56:33 here what I will be doing is that I''ll create this particular function called as embed chunks. Here we will take this\n1:56:39 chunks. So what happens is that first the load all documents will be called right after that the chunk documents\n1:56:45 will be called wherein all these documents will be chunked. Then all the chunks will be passed through our model\n1:56:52 to probably convert that into a vector embeddings. Right? So here you''ll be able to see self domodel.enccode.\n1:56:59 So show progress bar is equal to true. Right? So here what we are doing we are reading all the page content and we are\n1:57:05 performing the embeddings and finally we return the embeddings over here right so this is what we are actually doing right\n1:57:11 so two important function one is chunk documents and one is embed chunks inside a class called as embedding pipeline now\n1:57:18 the same thing you can go ahead and test it in your app py right so in the app py\n1:57:23 what you are going to do is that here um I will just go ahead and\n1:57:28 go ahead and just a Okay, let me go ahead and initialize just a second uh the\n1:57:36 embedding pipeline. Okay, so here what I will do, I will go ahead and write from\n1:57:42 from src dot embedding import embedding pipeline.\n1:57:48 Right? And once you do this, I will go ahead and initialize the embedding pipeline. Okay? And then I will just go\n1:57:56 ahead and give this right. So this basically becomes my vectors\n1:58:03 sorry embed chunks it is there right so embed chunks before that I need to chunk\n1:58:08 the documents I also did not call the chunk documents so let''s first of all call the chunk documents over here\n1:58:16 okay and then this will basically be my chunks\n1:58:21 and finally you can also go ahead and write over here as my chunk vectors ve\n1:58:30 chunk vectors is equal to and here uh you can go ahead and use the same\n1:58:37 embedding pipeline dot embed chunks right and finally you can go ahead and\n1:58:42 print the chunk vectors. So once you do this\n1:58:47 that basically means you''ll be able to understand whether the chunking is happening or not. So let''s quickly run\n1:58:52 this particular file again. And now you should be able to see the chunking that may be happening over here. Okay. So\n1:59:00 it''ll take some amount of time because it is going to load all the documents again. Okay. And then the chunk document\n1:59:06 function is going to get applied over here. The chunk documents what it does is that it is just going to apply\n1:59:12 recursive character text splitter on every documents that we specifically give. Right? And once we do that you''ll\n1:59:18 be able to see that it is loading. You can see all the things are happening over here. 21 PDFs, one PDF like 21\n1:59:25 pages PDFs is over here with respect to this proposal load embedding all models\n1:59:31 splitted 64 documents I got into uh 359 chunks you know and then we basically go\n1:59:38 ahead and store this. Now the next step is that after this uh I will try to create a vector store and uh we will try\n1:59:45 to save those embeddings also. Okay. So here you can see all the chunks is uh\n1:59:51 vectors are visible over here right. So this is really really good. So just just\n1:59:56 imagine right in a pipeline it is specifically working one by one right it is it is working over here and that''s\n2:00:02 that''s the best part out here right now the next step is that what I will do is that I will try to create some more\n2:00:10 functions uh which can be for save and load uh like if I want to save this\n2:00:16 entire chunks how do I go ahead and save it you know u what do I save it each and\n2:00:22 every information that you''ll be able to see over here Okay. Now, uh this was about uh the two important\n2:00:30 pipeline which is basically load all documents and uh embedding pipelines with uh two important function. One is\n2:00:37 chunk documents and one is embed chunk. So guys, now the next step is that what we are going to do is that now already\n2:00:42 we have created this embedding pipeline, right? Now let me do one thing because after performing the embedding, we also\n2:00:48 need to store it in some kind of vector store and it should be persistent in any kind of directory or in cloud. Right? So\n2:00:53 for this I will start working on this vector store. py file and here I''m going to use some code. Now you can see what\n2:01:00 all things I''m actually using. So I''m using the sentence transformer and embedding pipeline over here. Fiest\n2:01:06 vector store is the class name that we going to use. Uh I''m going to specifically use fis. Uh here we are\n2:01:13 going to use the same model. All mini l6 v2 chunk size everything is over here. And uh we are also making some kind of\n2:01:20 directories. the persistent directories like fire store should be the name and then here you''ll be able to see I''m\n2:01:25 initializing the embedding model sentence transformer and all now the first step is that build from the documents now see here uh the same code\n2:01:32 we will go ahead and write what we had written in embedding pipeline right so here we are initializing embedding pipeline model dot self embedding model\n2:01:39 chunk size and I''ve given the chunk documents embed document embed chunks I''ve got the metadata and I''m adding all\n2:01:46 these embeddings inside my vector store and once I use selfsave Save. What is this self dots save? Save is a function\n2:01:53 which is going to save all the vector inside this index dotpickle files. Right? So metadata is basically getting\n2:01:59 saved in pickle file and files.index will basically be my vector store which will be in the persistent directory. So\n2:02:05 that is the reason I have written files.right index self.index files path right with open metame this and all\n2:02:13 information is there right. So this same method is basically there add embedding method is over here. Add embedding is\n2:02:18 nothing but it is basically taking it it is adding as a index flat tail two. So these are some basic stuffs when you\n2:02:25 actually work on this. Along with that I''ve also created two more function load and search. Load and search what it does\n2:02:32 is that it will actually allow you to load the files index the vector store. Okay. And will uh load it in the read\n2:02:40 byte mode and then with the help of search and query you should be able to ask any kind of queries that you have.\n2:02:46 Right. You can also use this query method. Uh here you can see we have written self domodel.enccode with\n2:02:51 respect to the query test as type float 32 and with the help of query search\n2:02:56 you''ll be able to get the output. Okay. So this was about my vector store. Now in the app py what I am actually going\n2:03:03 to do I will just go ahead and make some changes. Okay. Now what what are the changes that I will be making? Okay.\n2:03:09 Instead of calling this two, okay, I will just go ahead and write store is\n2:03:14 equal to first of all let me go ahead and initialize this files vector store. So\n2:03:20 source dot embeddings files vector store here okay and here I will go ahead and\n2:03:26 initialize this and let me go ahead and give the path name. The path name is fires h o r e.\n2:03:34 Okay. Now initially if this p path is there then it is fine. Otherwise it''ll go ahead and I''ll just go ahead and\n2:03:41 write store.build from documents of all the docs. That''s it. Now if I do this it\n2:03:47 is just going to go ahead and for the first time it is going to build it. Okay it is going to build it. So let''s see\n2:03:53 whether it''ll be able to build it or not. So here I''m going to clear the\n2:03:58 screen. Python app.p py let''s quickly see this\n2:04:07 now it is going to read first of all it is going to read it then this is fine loading perfect load all the PDF files\n2:04:14 perfect now the chunking will happen automatically and it''ll save it in the vector store inside that particular\n2:04:20 folder that is files let''s see now it is generating 359 chunks\n2:04:26 all the steps are almost same what we have discussed from starting but this is A very super cool way of building\n2:04:32 something. Right? Now you can see save files index metadata to fire store vector store also. So here you can see\n2:04:38 fire store is there fires.index and metadata.pickle right now we need not\n2:04:43 run it each and every time right uh because uh once we have this right from the next time what we can do instead of\n2:04:49 always building unless and until you have a new documents I can also go ahead and write store.load\n2:04:55 okay if I go ahead and write store.load. Okay, I should be able to print anything\n2:05:02 that I want, right? Let''s say I will go ahead and print something like this. I can use the same query method that we\n2:05:09 had. What is attention mechanism? Top K is equal to three. Right? So once I do this, you should be and this time I\n2:05:16 don''t think so we need to also read any kind of documents also over here. Right? So I''ll comment it down over here. This\n2:05:22 also you can uncomment it if you really want to or you can also give another conditions. Now what it''ll do, it''ll\n2:05:28 directly go ahead and read from the vector store. It''ll pick it from the persistent directory and it''ll give you\n2:05:33 the output. Let''s see. So from the fire store, it''ll go ahead and pick it up. And here you go. Here\n2:05:40 you get the answer clearly, right? See loading embedding models. This is there loading fire index and metadata. What is\n2:05:47 attention mechanism? All the information is over here. And this is the output that you are able to get. Right.\n2:05:54 Perfect. This this is what exactly uh I was actually talking about. But the best\n2:06:00 part is that we have created this in the form of a pipeline. You have data loader, you have embedding, you have vector store. Now for search what you\n2:06:06 can do is that you can integrate any LLMs over here. Right? So for this also I have written the code. Again I don''t\n2:06:12 want to discuss it step by step line by line. So that it''ll be again taking a\n2:06:18 lot amount of time to complete this. Right? So here I have my load_.env. You can just go ahead and load all these\n2:06:24 things. Groc API key is given over here. You can use it or you can use your own Gro API key. It''s fine. Okay. And then\n2:06:32 we are doing the search, right? Wherein we are using this vector store do.query getting all the documents getting all\n2:06:37 the metadata and then we''re giving some prompt and we are invoking it along with the LLM. So once we do this, it is\n2:06:44 superbly easy to execute this. Anyhow, you can do the research because I have discussed all these things in my Jupyter\n2:06:51 notebook, right? Uh now what I will do in my app.py py I''ll see what changes\n2:06:56 needed to be added and uh what I will do is that I will first of all import rack\n2:07:02 search again from search dot search import rack search and then I will go\n2:07:07 ahead and initialize like this right and now I don''t even require this okay now\n2:07:14 let''s see whether it''ll be able to give the summary or not it is loading from\n2:07:20 the vector store now I''m asking the question search and summarize This is the function here. What we do? We first\n2:07:26 of all do the query from the vector store that we were usually doing before. Then we give a prompt and then finally\n2:07:32 LLM will be able to give the output. So, so here you can see if my LLM is fine\n2:07:38 then I think I should be able to get an answer. So here you can see all the output is basically over here.\n2:07:44 So this was a complete idea or a kind of crash course that I really wanted to give on the entire uh rag. Rag is one of\n2:07:53 the most important use cases. That is what I always believe. Most of the companies are specifically building rag\n2:07:59 applications. So I think this is really really important and super cool topic. I hope you like this particular video.\n2:08:05 This was it from my side. I''ll see you on the next video. Thank you. Take care.',
  '{"channel": "Krish Naik", "video_id": "o126p1QN_RI", "duration": "2:08:08", "level": "INTERMEDIATE", "application": "LangChain, RAG", "topics": ["RAG", "Retrieval-Augmented Generation", "LangChain", "Vector Database", "Embeddings", "LLM", "Document Retrieval", "Knowledge Base"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

INSERT INTO knowledge_base (url, chunk_number, title, summary, content, metadata)
VALUES (
  'https://www.youtube.com/watch?v=x0AnCE9SE4A',
  1,
  'LangChain GEN AI Tutorial  6 End-to-End Projects using OpenAI, Google Gemini Pro, LLAMA2',
  '0:00 Lang chain is a framework for developing applications powered by large language models like gp4 in this course you will\n0:06 learn Lang Chain by building six inin projects with the open AI API G...',
  '0:00 Lang chain is a framework for developing applications powered by large language models like gp4 in this course you will\n0:06 learn Lang Chain by building six inin projects with the open AI API Google\n0:12 Gemini Pro and llama 2 you''ll also learn how to deploy projects to the hugging\n0:17 face platform Chris neack developed this course considering this here is an amazing video of the entire Lang chain\n0:25 crash course along with this we''ll be discussing six plus endtoend projects\n0:30 llm projects using different different llm models from open AI we''ll be using\n0:35 Lama 2 from meta we''ll also be using Google giny pro not only that we''ll also be using hugging face and we''ll also be\n0:42 seeing how we can deploy this in hugging space uh platform itself which is provided by hugging face itself right so\n0:49 there will be somewhere around six plus end to end projects we''ll develop both the front end back end I hope you enjoy\n0:55 this entire Series this is a long video so please make sure that you devote some some amount of time every day and try to\n1:03 practice as much more you can right so in order to push this video to many people out there guys I''ll keep the like\n1:10 Target to 2,000 please make sure that you hit like share with all the friends share your work in LinkedIn in different\n1:17 platforms tag me over there I''ll be very happy to see what all things you specifically developing so let''s go\n1:22 ahead and enjoy the series and I hope you practice along with me all the materials will be given in the\n1:28 description of this particular video so let''s go ahead guys so finally finally finally here is the langin series and\n1:34 this is probably the first video where I''m probably going to discuss a very good one-hot video about Lang chain all\n1:42 the important components that you specifically need to know to build projects now why I''m stressing on langin\n1:50 is that because many people recently some of my students and some of experienced professional who switched\n1:57 into data science Industry they are getting work that are related to large language models and they''re specifically\n2:03 using Lang chain and the community with respect to langin is also increasing so\n2:08 that is the reason I''ve created this dedicated playlist and I''m going to discuss a lot many things this video we\n2:15 will understand all the important competents of this Lang chain what you specifically need to know with respect to the Practical orientation and from\n2:22 the next video onwards lot of end to endend projects are probably going to come up uh in this video also I''ll\n2:28 discuss about one Q&A project uh chatbot in short you know we''ll try to use streamlet it''s not like we only have to\n2:36 use streamlet you can also use flask you can use anything as you want right but streamlet it provides you a good UI you\n2:42 can also use gradio if you want right it is up to you so what all things we are\n2:47 specifically going to discuss first of all we''ll try to understand the agenda and then we will try to uh do step by\n2:53 step each and every line of code will be done and as I said this is just like a kind of L chain one shot video just to\n3:01 give you an example that how we are probably going to create our end to endend application after we deploy that\n3:06 in a specific cloud like hugging phas right it provides your entire environment to probably host your\n3:12 application uh this is what we are going to create this chatbot see if I probably ask what is the capital of New oh sorry\n3:21 of India then you''ll be able to see that I''m I''ll be able to get the answer what is the capital of Australia many people\n3:28 get confused with the capital Australia please do comment down in the comment section what is the capital of Australia\n3:33 and let''s see whether it is right or not so here you can see the capital of Australia scan so we are going to create this application at the end of the day\n3:40 but before that we really need to understand lot of important components in Lang chain so let''s go ahead and\n3:46 let''s start this particular session before I go ahead please make sure that you subscribe the channel press the Bell\n3:51 notification icon and share with many friend so that it will also be helpful for them and it will also be helpful for\n3:59 me so that you are able to provide this open source content to everyone out there many people require help and you\n4:06 should be the medium to provide this specific help by just sharing it right so what is the agenda we''ll understand\n4:13 as I said this will be completely a practical oriented video it''ll be a long video where I''ll be talking more about\n4:19 practical things how you can Implement Lang chain how you can probably implement various important functionalities in Lang chain and use it\n4:26 probably to build an endtoend application so first of all we''ll go ahead and do the environment setup this is commonly required you also require an\n4:32 open AI key API key so this also I will show you how it is done then we''ll try\n4:37 to build simple applications uh with the help of llms three important things are there llms prompt templates and output\n4:44 parsel okay in llms you specifically have llms and chat models chat models\n4:49 are basically used in chat B to understand the context reference and all right I will also be discussing this\n4:55 practical promt templates can play a very important role and then coming to the third one that is called as output\n5:02 parser uh in short prompt template and llms and output parser gives a very good combination of output like it it''ll give\n5:09 you a good output the output like you specifically want right so that is where output Parcels will also be used before\n5:16 to go ahead with what I will do is that uh first of all just go to the open aai uh website itself and get your API key\n5:23 how you should get it just go to this account and there you''ll be able to see view API key and create the new C secret\n5:30 key right so once you create it give the key name and then copy it and keep it right I''m not going to share mine so\n5:36 that is the reason why I''m showing you this specific step now I will go to my vs code all my coding will be done in\n5:43 this VSS code itself here you can see it is completely blank right I''ve just opened a folder in the form of a project\n5:51 now all you have to do start your project over here now the first thing as usual what we need to do we need to\n5:57 create a new environment right so this is the first step that you specifically\n6:03 need to create with respect to an environment so don''t miss this specific step it is important and probably\n6:10 whatever projects that we are going to discuss in the future we have to do the specific step so I will write pip\n6:17 install okay sorry cond create I''m creating an environment so let me just\n6:24 not open in Powers shelf instead I''ll go and open in command prompt okay so here I will just right K create minus p v\n6:35 EnV V EnV python equal to 3.9 so 3.9 is the\n6:43 version that I''m going to specifically use and also going to give- Y so that it does not ask me for the permission\n6:49 instead start creating the specific environment the reason why I''m using this environment understand one thing\n6:55 that guys for every project that we probably create we have to create a new environment that actually helps us to\n7:02 just understand or just use only those libraries that are actually required for this particular project okay so this is\n7:09 the first step go ahead do it with me you know uh and do it in vs code because vs code is a good idea if you want to do\n7:16 it in pycharm then also you can do it but since you following this tutorial I''m actually going to do it in vs code I\n7:22 feel vs code is good I I''ve used both okay I''ve used different different IDs but I feel vs code is good okay so here\n7:29 it is now here you can probably see my V EnV environment uh the next thing we\n7:35 will go ahead and activate this V EnV environment so I will write cond\n7:40 activate cond activate V EnV Dash Okay so this is\n7:46 my first step done I''m good with it now the next thing what I will go to do is\n7:51 that I will just go ahead and write my requirement. thxt right because we need\n7:57 to install the library since inside this particular venv environment so I will go ahead and write all the list of\n8:04 libraries that I''m specifically going to use now what are libraries I''m going to use over here uh we will just write it\n8:11 down so that uh the first library that I''m going to use is Lang chain then open\n8:16 Ai and I think I''ll be using hugging face also later on it is called as hugging face Hub that I will show you as\n8:24 we go ahead okay so these are the two libraries that I''m going to specifically use okay now the next thing what I will\n8:30 do I will go ahead and write pip install minus r okay so let me just hide my face so\n8:38 that you''ll be able to see pip install minus r requirement. txt so once this\n8:43 installation will take place that basically means my requirement. txt is getting installed that basically means\n8:50 all the libraries that I actually require over here that is getting installed and for this I require Lang\n8:55 chain and open aai okay so once this installation will will be taken place like it will be done now one more\n9:02 Library I''ll be required is called as IPI kernel because if I really want to run any Jupiter notebook over here I\n9:09 have to use that okay so let''s wait and let''s see once the installation is probably done and then we will continue\n9:16 the video so guys the all the libraries that were present in the requirement. txt has been installed now the next step\n9:24 that I''m probably going to do is also install IPI kernel which will be required to run my Jupiter notebook so I\n9:32 will go ahead and write pip install iy kernel Now understand one thing I''m not\n9:38 writing this library in requirement. txt because when we do the deployment in the\n9:43 cloud IPI kernel will not be required okay so that is the reason I''m installing separately pip install IPI\n9:49 kernel in the same VNV environment because VNV environment also we are not going to push it right so you can\n9:56 probably see over here downloading this this will happen and automatically the download will happen itself right now\n10:02 the next thing what I''m going to do I''m just going to write Lang chain dot\n10:10 ipynb okay py NB so this will basically be my jupyter notebook that I will\n10:17 specifically be using okay so right now it is detecting kernel once this installation will probably happen and\n10:23 then we will also be able to see the kernel okay so this is all the steps all the basic steps that you probably\n10:30 require from this you can start creating end to endend project but at least you require this uh along with this I''m also\n10:36 going to add two more steps one is about environment files EnV file okay so here\n10:42 what I will do I will write Dov file okay uh inside this EnV file the reason\n10:48 why I''m uh writing this EnV file because I need to probably use my open API key\n10:54 and probably mention the open API key over here so if I probably write it like this and and whatever API key that I''m\n11:01 probably getting from the website I can upload it over here and using right uh\n11:06 load environment function right we can load this API key uh as a variable so\n11:12 that we can actually call our API API key over there so I will update this later on as we go ahead okay so till\n11:18 here everything is done this is my Lang chin ipynb file I will go ahead and detect the environment this is what\n11:25 3.9.0 so here it is so let''s see whether it is working or not 1+ one it''s working\n11:32 right so this is perfectly all right so everything over here is done with a respect to this and I''m very much Happy\n11:38 this is working uh which is really really good okay so this is done now\n11:43 what I am actually going to do over here is that we need to import some of the important libraries like open Ai and all\n11:51 right so for this I will go ahead and right from Lang\n11:56 chain. llms okay import open AI see there there lot many\n12:05 models like open llms and all first we''ll start with open AI understand one thing guys the reason why I say this is\n12:12 completely practical oriented because you need to have the basic knowledge of machine learning deep learning and all\n12:19 but this specific libraries is used to build application uh fine tune your\n12:24 application with respect to models with respect to your own data set most of the things just with writing proper lines of\n12:31 code will be implemented in an easier way so you really need to focus on things how things are basically done\n12:37 okay now the next thing what I will do I will write import OS and I will say OS\n12:42 do Environ environment okay and here I will give my\n12:51 opencore API underscore key okay this is\n12:56 how you should basically write it down to import the API key now what I will do\n13:02 I will keep this hidden from you so just imagine I cannot show you the API key because I will be using my own personal\n13:09 API key itself so I will go ahead and probably pause my video and update the\n13:14 API key and I''ll remove this specific code okay that is what I''m actually going to do so that none of you\n13:20 basically sees that and it is important you have to use your own API key so let me quickly go ahead and do that and let\n13:27 me come back so guys this is how you have to probably import your API key\n13:33 this will I''ve made some changes so if you also copy it is not going to work I made some internal changes in between\n13:39 changes so uh you just need to write os. environment open API key and this API\n13:45 key that you have specifically got okay so this is the initial step uh I have\n13:50 also imported open AI so that I will be able to call this particular uh open AI itself now this is done this is good\n13:57 everything is working Absol absolutely fine now what I''m actually going to do I''m going to create my llm model and go\n14:03 ahead and write my open AI let''s see open AI open AI function is the called\n14:09 or not let''s see okay open AI from L chin open Ai and inside this open AI\n14:14 what I''m actually going to do I''m going to basically call a variable which is called as temperature and temperature\n14:20 right now you can keep the value between 0 to 1 the more the value towards one the more different kind of creative\n14:27 answers you may get right if the value is towards zero then what kind of output\n14:33 you you are probably getting from the llm model is going to be almost same from the uh anytime you number anytime\n14:39 you probably execute so here I''m just going to keep it as 6 so this is basically my open a llm model okay now\n14:47 this is done my llm model is there so here you can see did not find an open AI key please add an environment variable\n14:54 open AI key now this is the error that you are specifically getting right so\n14:59 why this particular error is probably coming you should definitely understand okay without\n15:05 this understand that this kind of Errors can come to you the reason why I will not edit this particular error because I\n15:11 really want you all to understand it is saying did not find open API key now what you can probably do with respect to\n15:18 this okay there are two different things that you can probably do either you can take this API key save it in a constant\n15:25 variable and try to use that particular variable over here right so for that also you can directly do that uh I have\n15:31 also created this do EnV file what you can do you can load this environment variable and probably directly read it\n15:38 over there right but let me just go ahead with a simple way you know so you\n15:43 will be able to understand with respect to that also so here what I''m going to do here I will go ahead and probably say\n15:50 open API key and here I will going to write OS do\n15:55 environment okay and here I''m going to Define my open API key okay now let''s\n16:01 see whether this will get executed or not I will show you much more better ways when we are probably executing our\n16:07 end to end application so let me go ahead uh open a key is not defined because I use double equal to perfect\n16:15 now you can see that it has got executed perfectly now when I am actually\n16:20 creating an end to end project I will show you a better way the most efficient way that we should specifically use when\n16:26 we are building an end to end project but right now I''ll go to focus like this now understand one thing is that with\n16:32 respect to temperature variable right the temperature that we specifically used I will give a comment over here and\n16:39 you can probably see over here right so temperature value how creative we want\n16:45 our model to be zero means temperature it is it mean model is very safe it is not taking any bets it will risk it\n16:51 might generate wrong output may be creative so more the value towards one\n16:57 the more creative if the model becomes right it is going to take more risk to provide you some more better but again\n17:04 with respect to risk again there may be a problem you may get a wrong output perfect this is the step simple step\n17:10 that we have done at the end in this video only I''m going to probably do\n17:16 create an end to end project understand this will be very important for everyone because as we go ahead in the next\n17:22 videos the project difficulty will keep on increasing okay now this is done now\n17:27 what I will do quickly I will go ahead and write text let''s say the text is\n17:32 what is the capital of India okay so\n17:39 here I will write print llm do predict\n17:45 and here I''m going to basically write my text so here you can see the capital of India is New Delhi so here what I have\n17:52 done is that this is my input and if we use lm. predict I''m going to probably get the text so if you like liking this\n17:58 video till here guys as I''m teaching you step by step I''m explaining you each and everything please make sure that you\n18:05 practice in a better way right if you like it please make sure that you subscribe the channel also okay so this\n18:11 is done lm. predict and we are able to probably get the output okay so\n18:17 understand what what all things we did we created an open AI model right but here in the open AI right you should\n18:24 also understand one important thing here there is a parameter which is called as model now what all model you can\n18:31 probably bring it over here what all model you can probably call so here I will go to my documentation page okay\n18:38 and uh if I probably click on the models now here are the set of models that you can probably use by default it is\n18:46 calling this GPT 3.5 turbo it is the most capable GPT 3.5 model and optimized\n18:52 for chat at the 110th cost of a Dy 003 we''ll be updated with our latest model\n18:58 iteration 2 weeks after it is released it''s not like you only have to use this you can use this you can use this you\n19:03 can use this you can use this whatever models you want you can probably use it if you want to probably go with GPT 4\n19:10 you can use this you can use this you can use this but at the end of the day gp4 is the most uh amazing thing it is\n19:17 more capable than GPT 3.5 so this is what models is by default over there it\n19:23 is probably taking this specific model which you can probably use it now as we go ahead it is not like you can only\n19:29 call this model itself in hugging face you have open source models also right from Google from different companies you\n19:36 have that uh open source llm models you can also call that and I will also show you an example with respect to that okay\n19:43 so till here I hope you have understood it I think you should give a thumbs up if you''re able to understand till here\n19:49 now let''s go ahead and do one more thing I will also show you with respect to hugging face now okay now with respect\n19:55 to hugging face what I will do I will quickly go ahead head and write hugging\n20:01 facehub right I will probably install this library now let me go ahead and\n20:06 open my terminal okay so I will delete this and\n20:11 quickly we will go ahead and write pip\n20:17 install pip install minr requirement.\n20:25 txd done so this is getting executed and then I will probably show you with respect to hugging face also so hugging\n20:32 face this is done okay the installation is done so you have to use hugging face Hub now in case of hugging face also\n20:39 right you will specifically get a you will also be getting a because at\n20:48 the end of the day we''ll also try to deploy it over here if you go to settings right and if you go to access\n20:54 tokens here you can probably see I have some kind of token right and with the help of this particular token only you\n21:01 will be able to call any models that are available over here so in this models if\n21:06 you probably go ahead and see there are lot of llm models that are available right so if you probably go over here\n21:11 natural language token classification question answer like uh let''s say text to text generation this is basically one\n21:17 kind of llm model I will search for one name okay so the name is flan okay flan T5 base okay so this\n21:26 specific model or T5 large this is a text to text generation model right so\n21:31 this is also an llm model if I want I can use this also why I can use this because this is an open source okay if\n21:38 you want to probably see the answer right text to text generat false or not false or false is uh if I want to\n21:44 compute it right it''ll give me some specific output okay false or false or false is the verb the verb right\n21:50 something like this it is getting an output right so I can also use my I can also create my chat models with the help\n21:56 of this kind of models directly by using this API right but how to do it let''s go ahead and see it okay so here what I\n22:03 will do quickly I will first of all import one more important thing again in\n22:09 my environment variable os. environment and here I''m going to basically write in the case of\n22:15 hugging P I have to use something called as hugging oops I have to write in\n22:21 capital letter hugging face hubor API uncore to token okay so\n22:30 here is my token okay hugging face Hub _ API _ token now with respect to this\n22:37 particular token I have to write my own token the token is basically given over here as I have already shown you if I\n22:43 probably click on show you''ll also be able to see it so I''m not going to do the show part over here so what I will\n22:48 do I will just pause the video upload it execute it change the token and then come back to you okay so let''s go ahead\n22:55 till then you can go ahead and create a hugging face account also so guys now I have set up my hugging face Hub API\n23:03 token with this specific token again I''ve made the changes so if you probably use this again it will not work okay but\n23:09 I have actually imported it now let''s go ahead and probably see how we can probably call llm models with the help\n23:16 of hugging face so again and again I''ll be using Lang chain Lang chin is just like a wrapper it can call open llm\n23:22 models it can call hugging face uh llm models anything you can probably call with it okay so that is the reason why\n23:29 I''m saying it is powerful so I will write from Lang chain okay\n23:35 import hugging face Hub okay and then here I''m going to probably Define\n23:43 my let me just execute it and then probably I will call it so here I will\n23:48 go and write hugging face Hub here first of all I need to give my repo ID now repo ID is nothing but uh\n23:54 whenever you search for any model this will be your repo ID Okay Google slash this okay so here I''m going to probably\n24:01 go ahead and Define it okay and this will basically be given in a sentence\n24:08 form okay so this is done now the next thing that I will have is model quarks\n24:14 okay and here I will go ahead and Define my\n24:25 temperature and then here I''ll go ahead and write max\n24:31 length column 64 that basically means I''m giving the maximum string length to\n24:36 64 okay now let''s see whether this will execute or not I know it is going to give us an error let''s see so it is not\n24:43 giving you an error it is probably executing it perfectly it has probably taken that particular key itself\n24:49 inference API is working perfectly everything is working so I will go ahead and Define my variable so here I''m going\n24:55 to basically write my l lmore hugging face okay is equal to this one right so\n25:04 this is done we have probably written this specific thing over here and then I will go ahead and write name or I will\n25:12 see output let''s see the output and here I''m going to basically write llm do hugging\n25:17 face do predict and let''s say I''ll say\n25:23 can you tell me the capital of of Russia\n25:29 okay so let''s see whether we will be able to get the output or not so here I''m going to write print output and\n25:36 let''s see whether we are so here you can see the output is a simple word right\n25:42 the capital of Russia is Moscow right but here the previous output that we saw\n25:48 with respect to the llm models here it shows that capital of India is New Delhi it is giving you entire sentence and\n25:54 probably here it is just giving you a word and this is what the difference is with respect to an open source uh model\n26:00 itself and models like GPD 3.5 or 4 right so here uh you can probably see\n26:05 this okay and I will also execute one more thing over here let''s see uh can you write a\n26:13 poem can you write a poem about\n26:20 AI let''s say I give this what kind of poem it gives okay it probably taking time to\n26:28 probably give the I love the way I look at the world I love the way I feel the way I think I feel I love the\n26:34 way I love see what kind of output you''re specifically getting right now if\n26:39 I probably give the same output and probably write\n26:44 llm okay dot predict llm dot predict and if I\n26:52 probably give the same sentence let''s see what is the output now you will be able to understand why we are\n26:59 specifically using this let''s see it should give you a better output I\n27:06 think I love the way I look at the world oh so mysterious was so curious It''s\n27:12 technology so advanced IT Evolution so enhance it''s a tool that can assist um in making life much less hectic a force\n27:20 that can be used for good or a cause that''s misunderstood it can make\n27:25 decisions so Swift and learn from the sto Swift tool that can be better see so how what an amazing poem it has\n27:31 basically written and by this you''ll be able to understand the differences why\n27:37 I''m specifically using this open AI model and with respect to hugging face yes there are also some models which you\n27:42 can probably take paid one in hugging face that will give you better output but this is what happens with respect to\n27:48 open source uh models I think we should uh we have lot of Open Source models that are probably coming up Mr 7B and\n27:55 all that also we''ll be seeing in this playlist as we go ahead but as I said this is just a basic thing to probably\n28:00 understand we will be focusing on understanding these things right so here uh till here I hope we have discussed so\n28:06 many things how to probably call open AI models with respect to open AI uh Library API itself and then hugging face\n28:13 API also llm models right and using Lang chain so here are what all things we have done now the next thing that we are\n28:19 probably going to discuss is about prompt templates prompt template is super amazing uh it will be very very\n28:25 handy when we are talking about about things with respect to prom template and all so that also we are going to discuss\n28:31 as we go ahead so guys now let''s go ahead and discuss about prompt templates which is again a very handy component in\n28:38 the Lang chain even in open AI with the help of prompt templates you will be able to get efficient answers from the\n28:46 llm models itself right I''m not talking about prompt engineering that can get you three CR package okay I''m just\n28:52 talking about simple prompt templates and how prompt templates can be probably used okay now now let''s go ahead and\n28:58 first of all import from Lang chain from Lang chain. prompts import\n29:07 prompt template okay uh what we are going to do is that whenever we call a specific llm model you know the llm\n29:15 model should know what kind of input it is probably expecting from the client or\n29:20 from the end user and what kind of output it should probably give it okay so what we do if you want to really\n29:26 Define how our input should be and how our output should be we can specifically use prompt template because understand\n29:32 if we directly use GPD 3.5 it can be used for various purposes right but here\n29:37 I want to restrict all those things within something right so with respect to the input and the output so here I''m\n29:43 going to probably Define my promt template so let me go ahead and Define my promt template now with respect to\n29:50 the promt template here first thing that we need to Define is our input variables\n29:55 so here I will go ahead and write write my input variables okay now in input variables we\n30:03 need to first of all say that what input we are specifically giving there will be a fixed template in that template I\n30:10 really need to give my input okay so let''s say here I Define my input and here I''m just using one input let''s say\n30:17 the capital or the country I''m just going to write it as country okay so this is my first parameter that I''m\n30:23 going to probably give in my template itself and this I will also store it in my variable which is called as promt\n30:30 template okay great now here I''m given country as\n30:35 my input variable okay here I will Define my template now inside this template what\n30:42 I''m going to say is that tell me the capital\n30:48 capital of this whatever template that I''m\n30:55 specifically giving that is country so this is just like my variable\n31:00 whenever I give the input to this variable it is going to replace it over here right so what will happen by this\n31:07 is that the open AI will be able to understand okay this is the question that is asked but this value is dynamic\n31:15 that I''m probably giving it during the run time something like that okay now here if I want to execute this again I\n31:22 can also write prompt uncore template\n31:28 okay prompt undor template uh dot format right so there is\n31:36 a function called as do format and here now instead of how should I give my input so here I will say\n31:43 country is equal to right whatever the variable name is country is equal to let''s say\n31:49 India okay now here you can see that my entire prompt is generated in this way\n31:55 saying that tell me the capital of India okay so here you can probably see tell me the capital of India right now if I\n32:03 want to probably predict I will write llm do predict okay and here I will just\n32:08 say prompt template is equal to whatever my prompt template is defined so here\n32:14 I''m getting an error predict missing one required positional argument text now\n32:20 inside this I''ve given this prom template but it is expecting one text right what is that particular text that\n32:27 that is particular this particular value right so here what I will do I will go ahead and Define my second variable text\n32:34 and here I will write it as India let''s see whether it will get executed still it is giving me an error so guys now you\n32:40 can see when I''m using this llm do predict and I''ve given my prompt template I''ve given my text also that\n32:45 was the error that was coming but still it is giving me an error saying that the prompt is expected to be a string\n32:51 instead found class list this this if you want to run the llm multiple prompts use gener instead okay something like\n32:59 this I will show you a way because that is the reason the reason I''m keeping this error over here there is a simple\n33:05 way of understanding things right because this is not the right way to call promt template along with the\n33:11 inputs itself so what I''m going to do quickly I will go ahead and import one\n33:16 important thing that is called as chains so I will say from Lang chain dot\n33:24 chains import llm chain understand one thing guys chain basically means combine\n33:32 multiple things and then probably execute I have my llm model I have my prompt I know I have to give my input\n33:38 based on that input I need to probably get the output so instead of executing directly by using llm do predict I''m\n33:46 going to use llm chain and inside that I''m going to give my llm model I''m going to give the prom template and I''m also\n33:52 going to give the input so this is what we are specifically going to do now this will get gots executed from Lang chain.\n33:59 chain import llm chain now I''m going to create my chain let''s say the chain is equal to\n34:05 llm chain and here I''m going to basically write llm is equal to llm okay whatever\n34:12 is my llm model and my prompt is equal to my prompt template okay now this is\n34:19 there this is my chain not chain what it is doing it is combining the llm model it knows what is the prompt template\n34:25 right I''m going to use both of them and now in order to run it so I will write\n34:30 chain. run and here I will say India let''s say I''m going to probably say\n34:36 India and I know what is the output it is going to get tell me the capital of\n34:42 India so if I write chain. run it is definitely going to give me the output the capital of India is New Delhi this\n34:49 is perfect right so I can also probably print it right see guys I''m not going to\n34:55 delete any of the errors I want you all to see the errors and then try to understand that is the best way of\n35:02 learning things okay other than this there is no other way right you need to\n35:07 find a way to learn things you don''t worry about any errors that are probably coming up you just worry about okay fine\n35:14 you have got that error how you can probably fix that what is the alternate way of probably fixing it right so you\n35:21 can probably use all these techniques as we go ahead right so this is with respect to prompt template and here I''m\n35:27 going to also talk about and llm chain right so these are some important things\n35:32 because all these things we will probably be using in creating our end to end application now let''s go ahead and\n35:39 probably discuss some more examples with respect to llm so guys now we are\n35:44 understanding one more important topic which is called as combining multiple chains using simple sequential chain\n35:50 till here we have understood about llm chain LM chain was able to combine an llm model along with the prompt template\n35:57 through which we can also give our input and get our specific output now if I have multiple chain let''s say if I''m\n36:03 giving one input I want to use those input in both the chain or three chains four chains how can I specifically do it\n36:09 and for that I''m going to probably use Simple sequential chain okay so let us\n36:14 go ahead and let us probably see how this can probably be done so first of all I will probably say Capital template\n36:22 prompt Capital prompt okay so first like what is the capital of this specific country right so this will basically be\n36:28 my prompt so here I''m going to use my prompt template and here I''m going to\n36:33 basically use input variables is equal to and here I''m going to basically say\n36:40 country let''s go to the next input the next input will basically\n36:46 be template and here I will say I want uh\n36:51 please tell me the capital of of the whatever input I''m\n36:59 specifically giving away as country so this becomes my first template right and\n37:05 what I will do I will create a chain the chain name will be Capital chain okay\n37:10 and here I''m going to probably use my llm chain and my llm model will be llm\n37:16 okay and then I will also be using my prom template is equal to as capital\n37:25 template capital template okay so this is done let''s see\n37:30 Capital prompt what is capital prompt oh sorry Capital\n37:36 prompt Capital prompt is not defined why uh please tell me the capital of\n37:44 this uh template oh double equal to Let''s it no\n37:50 worries uh two validation error for LM chain so first I''ve used an LM chain\n37:57 where prompt template is equal to this uh where it is capital prompt so guys\n38:02 after just checking the documentation this should be prompt itself okay because in llm chain we have used prompt\n38:09 and here is capital template here also I''m going to probably use Capital tempate now if I execute this this works\n38:14 absolutely fine uh one thing you can probably see over here that I''ve given my template name and then I''ve also\n38:20 given the capital chain right so if I want to probably execute it I can just give my chain. run and that particular\n38:26 parameter okay but now what I want is that I also want to create one more prompt template I want to give the same\n38:33 input to that chain also so here uh let''s say I will write famous _ template\n38:40 and I will just say promp template and here again my input variable what is my\n38:46 input variable so my input variable will be whatever specific things that I''m\n38:53 trying to give right let''s say please tell me the capital of uh India if I say right the capital whatever Capital I''m\n38:59 going to get that variable only I''m going to pass it over here so my input variable will basically be my capital\n39:06 okay and this will be my second one and here I''m going to probably sayest template and I''m going to probably ask a\n39:13 question suggest me some amazing\n39:19 things amazing some amazing places places to visit\n39:27 in that specific capital okay so this is what I''m probably telling right please\n39:32 tell me the capital of the country so I will have that capital information that will be my input variable to from this\n39:39 particular template uh in that specific chain okay so I will get two answers first of all I''ll get the capital of\n39:45 that particular country and then what are the some amazing places to visit in that specific Capital place okay so\n39:51 these are all the information that I have put up okay so I hope this also works fine now now what I''m going to\n39:58 create I''m going to create an another chain which will be for this particular famous chain right so here I''ll write\n40:04 famous chain okay is equal to and I''m going to probably use my llm chain oh\n40:11 llm chain and here I''m going to give my llm models but the second one that is my\n40:17 prompt is equal to uh whatever template that I''m going probably going to give the famous template okay so this is what\n40:24 I''m probably going to do uh and I''ve probably given this prompt also over there and this will basically be my\n40:30 chain so once I probably execute it both the chains are ready now I need to give one input it should go to one chain get\n40:37 the output from that particular chain and pass that output to the next chain okay so this is what I specifically want\n40:42 to do how can I do it so again from Lang chain do chains I''m going to import\n40:51 simple sequential chain I know guys uh here you may be thinking why I have to\n40:57 use this see you''re passing one input to the get the other output from the one chain and pass that particular output to\n41:03 the other chain to get the just output itself right so this is quite amazing when you see an end to end application\n41:08 there you''ll be able to understand these are some of the important components you should definitely know and try to\n41:13 understand okay so here finally what I''m going to do is that I''m going to probably create my chain is equal to and\n41:19 this will be my final chain and here I will probably say I''ll import this okay so I get that so chain is equal to\n41:28 simple sequential chain capital letter simple sequential\n41:33 chain and inside the simple sequen chain I just have to name all my chains what\n41:39 all chains are specifically there in the form of list so the first chain uh that I have over here is nothing but Capital\n41:47 chain the second chain that I have is something called as famous chain okay so\n41:52 both the chains are ready now in order to run it all I have to do will write chain. run and here I''ll specifically\n41:59 give India okay done let''s see what kind of output I will probably\n42:09 get so it is running it is a bustling Metro Police and a great place to visit\n42:16 for historical site cultural this this this red Fort see most popular city the\n42:22 iconic monal is a multivisit who fought in World War one the 16th century mugal\n42:28 era Tom in UNESCO world heritage site everything it is probably giving it right so it did not give us the first\n42:35 answer with respect to the chain because it only provides the last input information okay uh if you want to\n42:40 probably display the entire chain I will show you a way how to do that for that we have to use buffer memory uh there\n42:46 will be something called as buffer memory but one amazing thing I gave one input I got the output and then probably\n42:53 I pass that particular output to my next chain and I able to to get one amazing out answer over here so definitely try\n42:58 it out from your side by using different different examples also now what I''m going to do is that I''m going to\n43:04 probably discuss one very important component about chat model open AI so that is also super important uh that is\n43:10 something related to chat models whenever you probably want to create a chart models you can have to use that okay so let''s have a look onto that so\n43:16 guys one more additional thing that let''s say I want to probably see the entire chain so here we will specifically use something called as\n43:23 sequential chain and let me just show you one example of that also uh it is not much to do with respect to that but\n43:29 you should definitely know this important video as said again I don''t\n43:35 want to probably take more time with respect to this but it is good to know this okay sometime when you are\n43:40 developing things and that you''ll probably be understanding once I start end to end project right today one end\n43:46 to end project will be done okay don''t worry about this uh in this particular video it will be done uh but definitely\n43:53 I want to show this example also as we go ahead now now uh let''s quickly go ahead and do the same thing I will copy\n44:00 this entirely okay I will paste it over here\n44:05 okay now along with llm promt template I will give my output key also so where I\n44:11 specifically want my output key so the output key will be nothing but it will be something called as capital okay so\n44:18 this is my Capital chain with this specific output okay so here I have created this now let''s go ahead and\n44:24 probably create the next template uh that is this famous template\n44:31 okay so here also you can probably see the famous template uh suggest me some names of the capital and here I''ve\n44:38 probably created my template name uh and my chain is over here right so this will\n44:44 basically be my chain okay so the same name whatever output Keys over here I''ve given this as my input key and here uh I\n44:52 can also derive one output key like this\n44:57 output key places something like this okay so\n45:02 done this is done see two simple templates that I actually created uh s me some amazing places to visit in this\n45:08 particular Capital uh the capital is probably given from here so now the chain will probably be able to\n45:15 understand each and everything as we go ahead you know where the output is and all right so here now what I''m going to\n45:20 do I''m going to probably import from Lang chain do chain\n45:26 chains import sequential chain okay and then finally you can see I''ll write chain is\n45:33 equal to simple okay I will let me just execute\n45:40 this because it is not giving me any suggestion so I will write chain is equal to\n45:47 sequential chain and now I''ll Give All My Chains name so the first chain name\n45:53 is um to Capital chain\n45:59 D famous chain dang okay and uh after\n46:06 this you will basically be able to understand the input variables now the input variables that\n46:13 we specifically have input variables is nothing but\n46:21 whatever is my variable name what is the variable name in this case it is nothing but country\n46:27 okay and then my output variable I''ll also create my output uncore\n46:35 variables so these are the two parameters see guys this this parameters\n46:41 is nothing but whatever parameters I''m specifically giving one is the capital and one is the\n46:52 places done so this is my entire chain now if I want to run any chain what I\n46:59 will do I''ll basically write something like this and give what is my country\n47:05 name right so it should be given in the form of key value pairs so here is my country\n47:12 colon India right something like this now if I execute it I will now be able to see my entire chain right it''ll take\n47:20 some time so what I have done over here I have in every llm chain that I''m probably creating I''m creating an output\n47:27 key uh two chains so two output key and here you can see chain country India country was India Capital the capital of\n47:34 India is New Delhi here are some amazing places to visit in New Delhi and all the\n47:39 information I have probably over here now let''s go ahead and discuss chat models uh specifically in Lang chain and\n47:47 we also going to use one Library which is called as chat openi uh this is also\n47:53 very good if you want to probably create a conversational uh chat bot itself so\n47:59 in chat models with chat open AI first of all what we will do is that we will go ahead and import Lang chain uh do\n48:06 chatore models and I''m going to probably import chat open aai so we will quickly go\n48:14 ahead and import it now after I specifically import this in chat open AI\n48:20 there are three schemas that you really need to understand okay whenever a conversation basically happens if a\n48:28 human is probably giving some kind of input and expecting some response that basically becomes a human message right\n48:34 if by default when the when your chat bot is probably opening a default\n48:39 message will probably come right and that is something related to domain like\n48:44 what that specific chatbot does right so it can probably come up with a new schema which is called as system message\n48:52 and then there is also one more message which is called as AI message which is again a schema which probably gives the\n48:58 output right whatever the chatbot is giving an output uh the AI whatever models is specifically giving the output\n49:05 that is nothing but that is related to the schema that is called as AI message okay now from this here we what we are\n49:12 going to do we are going to import from langin do\n49:20 schema I''m going to import human message\n49:26 system message right as I said system message is also required and AI message\n49:32 here everything you''ll be able to get this as an example because uh probably in the upcoming videos we''ll create\n49:37 conversational U chat bot right at that point of time we''ll be seeing all these\n49:43 things what we''ll be using and how we will be using okay so here quickly we''ll import this now uh obviously my llm\n49:50 model is there right by uh and while creating before the llm models how did we use it we basically use something\n49:57 called as uh open a right we used open AI now in this case I will probably copy the same thing\n50:04 okay and I will just past it over here and write chat llm okay and instead of\n50:11 writing open AI I''ll use chat open AI right so this is what I''m specifically\n50:17 going to use chat open AI with some temperature in this and here I''m also going to give one model so let me go\n50:23 ahead and write my model name uh the model name that we are going to basically write from here is GPT\n50:30 3.5 Das turbo right I showed you like what models we can specifically use so\n50:37 this is my chat llm model so here if I probably go ahead and write my chat llm so here you''ll be able to see that it is\n50:44 a chat open AI uh and with all the information with so temperature what is the uh this and all open AI key I cannot\n50:51 show you so it shows you open AI key also over here so going to remove this okay so that you don''t find the opening\n50:58 I key now let''s use this three schema and then probably see how my output will\n51:04 look like let''s say first of all I''ll create the schema in the form of list okay first of all the system message\n51:12 right let''s say system message I will go ahead and initialize and I''ll write a content content one variable is there I\n51:18 say you are an you are a\n51:24 comedian AI assistant okay so this is the this is\n51:30 what I''m telling the chatbot to behave like right it is basically acting like a comedian AI assistant okay then in the\n51:38 next one I will say human message and here again I will go ahead and write the\n51:44 content and I will say please and this is\n51:49 what I will probably write as a human this is what the input that I am probably giving right\n51:56 so I''ll say please make a\n52:01 comedy about or please provide some punch lines some\n52:08 comedy punch lines punch\n52:13 lines on okay AI okay so here you can probably see these\n52:21 are my two things these are the two information that I''m going to give to my chat llm models right and then let''s see\n52:28 what is the output okay with respect to that now in order to give this input to my chat llm so I will write chat llm and\n52:35 here only I will open my brackets so it has two information first\n52:40 by default it knows the system is a comedian AI assistant and here as a\n52:45 human input what we are saying is that we are saying please provide some comedy punch lines on AI okay so if I execute\n52:52 this you will be able to see I will be able to get an output now this is how we are going to design later on in the end to endend project we\n52:59 are not going to give this as a hardcoded it''ll be quite Dynamic so here you can see AI message see this is the\n53:05 output if I''m getting the output that basically becomes an AI message so this schema that we are able to see from the\n53:11 output of this particular chatbot the system message is basically telling that\n53:17 okay beforehand you have to act something like that we instructing the chatbot to probably act in that way\n53:24 right the human message is basically our input and AI message is what is the output so AI may be smart but it can\n53:29 tell me if your output makes look like a potato AI is a virtual therapist except\n53:35 it never judges you for eating an entire Pizza by yourself something like this so this is what comedy messages you can\n53:41 probably see right and I think this is quite amazing and you can probably set this up any number of time right you can\n53:49 probably say you can add this AI message over here and you can still build more conversational AI right so as AI also\n53:57 give the message you can probably store it inside this s let''s say if I probably consider a list and I append this\n54:02 particular list with all this information it can act as a chat model as our as we go ahead right now guys we\n54:09 are also going to discuss about one topic and after this we are going to implement our project okay so over here\n54:15 we going to discuss about prom template plus llm plus output parcel now first of all we''ll try to understand what exactly\n54:22 is output parcel now in order to make you understand about output plaster and how we can probably implement it I will\n54:29 use Lang chain again for this um as said guys langin is a powerful Library it has\n54:35 everything as a wrapper right so I will say from Lang chain okay from Lang\n54:43 chain. chatore models first of all I''m going to\n54:50 import chat open AI okay chat open AI I\n54:55 see there so many things Chad vertex AI chap open AI very powerful very powerful\n55:00 and the way it is getting developed right quite amazing right so from Lang\n55:07 chain dot prompts I''m also going to use some prompts and like how we have a\n55:13 prompt template when we use open AI right similarly in chat open AI we use uh prompts which is basically called as\n55:19 chat promate chat prompt template okay so I''m going to basically import\n55:25 chat prompt value no template chat prompt template so I''m\n55:32 also going to import this along with this as I said output parser right output parser is that if I want to\n55:39 modify any output of an llm model beforehand right so I can specifically\n55:44 use output parsers so for Lang chain I will also import this from Lang chain do\n55:52 schema import base output parel right so\n55:58 these are the three things I''m specifically importing and here I''ll basically go ahead and write class let''s\n56:04 say I am defining one output parser and I''ll Define this in the form of class\n56:10 it''ll inherit the base output class so let''s say uh I will say comma separated\n56:17 output okay that basically means it is basically called as a comma separated output this is the class that I''m going\n56:23 to Define and uh even even in the documentation it is\n56:28 given in an amazing way okay so comma separated output and this will basically be inheriting the base output parcel\n56:36 okay now inheriting when I probably inherit right that basically means we are inheriting this base output par and\n56:43 we can call this along with an llm models here I will Define a parse method\n56:48 and inside this par I will take self as one keyword and whatever text the output\n56:54 that we have specifically getting which will be in the form of string format all we''ll do we''ll just write return text.\n57:02 strip dot dot split right and this will be a\n57:10 comma separated split understand one thing now this is what is the class that I''ve defined and\n57:15 this is just like an output parser by default the output parser is what you can probably see whenever I''m\n57:22 specifically using the chat models I''m I''m getting some kind of output right AI may be smart something it is giving in\n57:27 the form of sentence and it is adding a new line at the end but what I''m saying is that whatever output I''m getting I\n57:33 will take that output and divide all the words in comma separated okay something like that so for this again I will\n57:39 Define my template I will\n57:45 say you are a helpful assistant okay so this is my\n57:52 first message that is probably going as a template right so this becomes a system template the schema that we\n57:59 probably discussed right I will also give some information um let''s\n58:05 say when the user gives any\n58:12 input okay you should generate five words okay in a\n58:21 comma separated list so this is what is my entire message okay the template I''m\n58:27 saying that whenever the user give any input you have to probably generate five words which should be comma separated\n58:35 okay so this is what I have specifically done okay now what will be the input what will be the text Will Define all\n58:41 those things okay and here I will say this will be my human template so what is the word that I''m going to probably\n58:46 give over here uh that will specifically defined over here right so here I will say Okay test uh you should generate\n58:54 five words synonym let''s say synonym I''ll just go ahead and write synonyms okay\n59:01 synonyms in comma separated so here will basically be my text whatever text I''m\n59:06 specifically given now I will go ahead and create my chat prompt now again from this chat prompt what I have to use I''ve\n59:11 already used chat prompt Pro template okay and inside this I will say do from\n59:18 message let''s see that chat prompt template\n59:23 from from underscore messages okay now inside this from underscore messages I\n59:30 have to give two information okay whatever is the template right so first template is nothing but the system one\n59:37 so system information that I really want to give uh that system one is nothing\n59:42 but this normal template that I''ve defined and the second one will basically be my human template right\n59:48 whatever human message that I''m actually giving right and this will basically be defined as human uncore temp temp\n59:55 plates template right so once I execute it here you''ll be able to see this is my chat prompt okay now in order to execute\n1:00:04 this obviously I have to use chain right because I have a prom template over here\n1:00:09 I have a human text I have this specific template also so how do I probably combine all these things that is what\n1:00:16 I''m actually going to show you over here so quickly first of all I will use this chat llm okay chat llm now see this is\n1:00:25 quite amazing and this is the best way of running chains so I will say chain is equal to whatever is my chat prompt so\n1:00:33 this is my chat prompt to this chat prompt I will give my\n1:00:39 API whatever API I''ll write over here control V so I have to just give a or\n1:00:45 sign kind of thing right so this is getting chained up this symbol basically says that it is getting chained up and\n1:00:51 remember the order also okay or you can also initialize chat open AI over here\n1:00:57 now along with this I will also give my output parser the output parser will be the last one right so this will\n1:01:03 basically be my output parser comma separated output okay now see I''ve given\n1:01:09 each and everything over here one by one list by list right so here it is so once I probably execute it it will get\n1:01:15 executed so here what it is saying I''m giving this chat prompt the chat llm model is there and the output should be\n1:01:22 comma separated output which is getting derived from this particular class okay now here finally what I will do I will\n1:01:28 write chain do invoke and inside this I will again whenever I use chain I have to probably\n1:01:34 give it in a key value pair right colon something whatever the value is now in\n1:01:40 this case I will say the word is um intelligent let''s say now I have to\n1:01:48 probably give in the form of text right so that is what I really have to give it right so this text is equal to in now\n1:01:54 let''s see what is the output uh it is coming as okay there is some syntax\n1:02:00 issue that I have probably made because I have to close my dictionary over\n1:02:05 here now if I write chain. inor you can see that five words smart clever\n1:02:11 brilliant shop aute I don''t know this specific word but here you''ll be able to see\n1:02:17 whatever output that I''m probably getting right so if I probably remove this let''s see okay this is how the\n1:02:25 output will look like okay AI message content this this this right but just by\n1:02:30 adding this output parsel you can see what an amazing message you''re able to get and you''re getting able to get the\n1:02:35 right thing that you specifically want this is what powerful a prompt template\n1:02:42 is all about right now this is done right and this is more than sufficient to know because more new things about\n1:02:49 PDF how to read PDF how to what is text iding and all we will discuss as we go ahead\n1:02:54 but now let us go ahead and try to create a simple chatbot okay simple\n1:03:00 chatbot I''ll create an app.py and by using the simple chat bot we''ll try to\n1:03:05 understand how things actually work and what all things we can basically do okay\n1:03:10 again here I''m going to probably use streamlet and I''ll be writing the code line by line so let''s go ahead and have a\n1:03:16 look so guys finally now we are going to develop our Q&A chatbot uh with all the\n1:03:23 concepts that we have probably learned I''m just not going to use all the concepts right now in this specific video itself because we should have\n1:03:30 probably uh 10 to 12 projects that are going to come up in the future in this series of playlist so there we are going\n1:03:37 to discuss about more projects as we go ahead right but now in this video I''m going to probably create a simple Q&A\n1:03:45 chatbot just with the help of open aai Lang chin and obviously use openi apis\n1:03:51 and llm models specifically to do that that here I''m also going to use streamlet okay so let''s go ahead and\n1:03:58 let''s see initially what all settings I need to do see this was Lang chain. iynb because I will be giving you this entire\n1:04:05 file uh again in the reference with respect to GitHub also so uh first of\n1:04:10 all in the requirement. txt I will be importing one more library because I need to install this Library this is the\n1:04:16 important Library itself which is python. EnV right so python d.v actually\n1:04:23 helps us to create or upload all our uh environment variables that we have\n1:04:28 created with respect to our application so here this is the library that I have to do it and just go ahead and install\n1:04:34 the requirement. txt I''ve already done that uh and this will be a specific task to you now we are starting over here so\n1:04:42 from lin. llms I have imported open aai then from EnV load load. EnV so as soon\n1:04:50 as I probably call this it will take all the environment variables from EMV file so here I''ve already created the\n1:04:56 environment variable I''m not going to show you again the environment variable because in short the environment variable will be something like this see\n1:05:03 I I I may have written like something like this open AP opencore API _ key is\n1:05:09 equal to this particular environment variable right so this is basically my open uh API key itself right so I''m\n1:05:14 going to probably use this uh in my application so here uh these are the basic steps that we will probably go\n1:05:20 ahead with now along with this what I''m actually going to do I''m also going to to import one more Library which is\n1:05:26 called as streamlet because we are going to use streamlet itself so let me go ahead and open my terminal and quickly\n1:05:34 let''s go ahead and write pip install minus r minus r requirement. txt and then the\n1:05:44 installation will start taking place and the streamlet uh Library will also get installed Streamlight we are\n1:05:50 specifically using for a front-end application uh see it''s it''s not light only you have to use streamlet it''ll be\n1:05:55 very much easy for me to probably create it and do the deployment because I''m also going to show you the deployment in the hugging face uh space itself right\n1:06:03 what is exactly hugging space uh space I will also discuss about all those things so quickly uh let''s do this uh it''ll\n1:06:09 probably take some time and then I will go to my app. Pui uh let me do one thing\n1:06:15 quickly uh let me go ahead and import streamlet also so I''ll import streamlet as St okay so this will\n1:06:23 basically my streamlet itself okay so it''ll probably take some time to download it let''s see how much time it\n1:06:29 is going to take but again it depends on your internet speed and how fast your system is right my system is really\n1:06:35 really fast till it is taking time so for you it may probably take more time okay so let this installation\n1:06:44 take place till then I will go ahead and start creating our application now I\n1:06:49 will first of all create a function to load open AI model and get\n1:06:56 response okay get response so I will call this function\n1:07:01 something like definition um getor open AI response okay something like this and\n1:07:09 here I''m probably going to give since it is a Q&A chatbot so here I will have my\n1:07:14 question as my parameter which will be of a string type okay it can be a string type it can also be a numerical type so\n1:07:21 I will just keep it like this okay so this is done and here you can also see the installation is done so I will just\n1:07:26 close this now here as soon as I probably call this function what I really need to do I need to call my llm\n1:07:33 model so llm model I will say open AI okay open Ai and here I will go ahead\n1:07:39 and Define my model uh have I imported open AI yes I\n1:07:45 have imported model uh uh open itself so I will go ahead and write model uncore name is equal to and I will will Define\n1:07:54 my model I''ll be using text Davin C uh this is one of the model that we\n1:08:00 have you can probably refer it so text Davy 003 and here I''m also going to Define my\n1:08:09 temperature temperature is equal to let''s say5 Okay uh along with this uh I''ll\n1:08:15 just go ahead and copy one more thing I will just set up my open API key also so\n1:08:21 I will set it up like by using this OS environment so here will be my first parameter okay so all this is done uh I\n1:08:28 think I need to also import OS okay so this is done in short what I''m doing is\n1:08:34 that I''m initializing my llm model OKAY in this code now the next thing is that\n1:08:39 I need to probably get my response so response will be nothing but llm\n1:08:45 directly how do I give a question over here I can probably give the question over here okay see I''m just creating a\n1:08:53 basic one then whatever things you really want to do from here you can probably do it try\n1:08:58 to create a own prom template try to use chain if you want try to do multiple things but just to start with I''m going\n1:09:04 to use a simple application where it is just taking an input and giving some kind of output it has no AI message set\n1:09:11 it has no human system message set no system message set also we have not given any prompt template also over here\n1:09:17 this just to give you an idea how things starts okay so now we will initialize in initialize our streamlet\n1:09:28 app okay now with respect to streamlet I will write STD do\n1:09:34 set underscore page underscore config so this is one of\n1:09:40 the function in streamlet which will actually help you to set the page title so I will just go ahead and write title\n1:09:48 is equal to I will say q& a demo okay Q a demo and this is done with respect to\n1:09:56 my uh and here I will set my another header the header will be something like\n1:10:03 Lang chain application something like this okay so I''ve given my header also with\n1:10:10 respect to this okay now I need to find out a way to get a user input okay uh if\n1:10:18 I get a user input then I should be able to submit the button and I should be able to get the text itself so first of\n1:10:25 all I will go ahead and create my submit button I will say St do\n1:10:30 button and here I will go ahead and write generate generate or ask the\n1:10:38 question something like this okay\n1:10:44 if ask button is clicked right if it is clicked that\n1:10:51 basically means if I write if submit okay if it is clicked this usually becomes true okay so if this is true it\n1:10:58 is probably going inside this particular function and here you''ll be able to see s I''ll just put a header and uh I''ll say\n1:11:08 the response is okay and then I will probably write St dot right with respect\n1:11:17 to the response okay so this is what I am probably doing it okay I''m getting\n1:11:23 the response over here and with respect to this specific response I will probably this response is probably\n1:11:28 coming from here but still whatever is the input that input we are not able to\n1:11:33 capture it yet right because if we capture those input then only we''ll be able to send that particular input somewhere right and for that also I may\n1:11:41 have to probably create another function so let''s go ahead and handle the input part\n1:11:48 now so guys now what I am actually going to do over here is that first of all we''ll go ahead and and capture our input\n1:11:54 text so let me go ahead and write over here input is equal to St do textor\n1:12:01 input because I''m going to use a text field over there and here I will\n1:12:06 probably be waiting for the response itself right so sorry from the request\n1:12:12 right so so here I will write input colon okay and I''ll keep a space over here and I will just write key is equal\n1:12:19 to key is equal to input something like this so this will basically be my input\n1:12:24 itself okay now once I''ve done this okay once I''ve have done this I''m going to\n1:12:30 take this particular input and now call I hope you should know what we should call we should basically call this\n1:12:36 function right so this here I''m going to probably write uh not here itself uh so\n1:12:42 let me just write it over here and this input I''m actually giving it over\n1:12:47 here okay so this will basically be my input over here uh whatever input I''m probably getting it it''ll just go ahead\n1:12:54 with respect to this particular question and I will probably get the response here I will just go ahead and write return\n1:13:00 response okay and then I will store this particular variable inside my response\n1:13:06 okay done see the way I probably got the input over here I sent this input to my\n1:13:12 get open AI response my open AI model has got probably loaded and then it is\n1:13:17 basically calling with respect to this llm you can either call predict message or predict functionality also uh you can\n1:13:24 also use chain you can use multiple things you can assign promt template in this particular function and all right\n1:13:29 and then finally you have S do Button as the button and if submit this is there okay now quickly let''s go ahead and\n1:13:36 probably run it okay uh let me see whether if I directly\n1:13:44 call python app. Pui it will give us a error why because it is a stream L library right if it was flask I would\n1:13:51 have probably said okay it would have work ke now it says key error open API\n1:13:56 key okay so os. environment open API key load.\n1:14:02 EnV so guys one mistake that I have specifically done whenever I really want to call all the environment variables\n1:14:08 from EnV with the help of load uh this the specific library that is called as\n1:14:13 this this functionality which is load. EnV at that point of time I''ll be using get EnV function and here I will just\n1:14:21 remove all the things brackets and probably call this function now I hope so it should work and I think we should\n1:14:27 not get any problem so Streamlight run app.py and here we have our entire\n1:14:34 application quickly it''s running let''s see um this is getting\n1:14:40 loaded and here we have right now probably I''ll ask the question what is\n1:14:46 the capital of India right so I''ll just\n1:14:53 ask the question over here the response is the capital of India is New Delhi um let''s see what is generative AI\n1:15:04 right so I''ll ask the question you''ll be able to see that generative AI is a type\n1:15:09 of artificial intelligence that focuses on creating new data from existing data this this this is there still I''m I''m\n1:15:15 getting some kind of weird responses over here so that is the reason we''ll also be using output parsers we''ll make\n1:15:21 sure that we''ll use conversation buffer memory we''ll also Implement schemas like human message system uh human human\n1:15:28 system AI system um system messages all those things were there right all the schemas that part we probably discussed\n1:15:35 but this is a simple application that we are probably going to discuss with respect to this it is going to be quite\n1:15:41 amazing and uh you know this is just a basic Q&A chat bot uh wherein whatever\n1:15:47 questions you specifically ask like what is the please right write a\n1:15:54 poem on on please write a romantic poem I''ll\n1:16:00 just give it as romantic poem on generative AI something like\n1:16:06 this because many people are now using this ask the\n1:16:13 question so here you can see gener way I knew love in my life your data driven hurt is perfect fit your algorithm so\n1:16:21 precise your knowledge is so wise so everything is over here now what I''m actually going to do is that I will go\n1:16:26 and show you the deployment part everything is working fine I will first of all login into the hugging face go to\n1:16:32 the spaces and create a new space because I''m going to probably do the deployment over here let''s say I will\n1:16:40 say Lang chain Q&A chatbot okay I don''t\n1:16:45 have to use license this will be a streamlet now in space Hardware like you have paid Hardwares also but I''m\n1:16:52 probably going to use a simple one CPU Basics 2v CPU 16GB and I will create\n1:16:58 this as public so that you can also refer it um let''s see okay I will just\n1:17:04 remove this spaces please match okay this underscore is not there QA a\n1:17:11 chatbot I will go ahead and create the space now after creating the space uh there''s couple of things that I''m\n1:17:17 probably going to do over here is that uh this is where this is just like a GitHub repository you know you if you\n1:17:23 probably go to the files you''ll be able to see it now here I''m probably going to upload the file that I have okay uh but\n1:17:31 before that what I''m actually going to do I''ll go to my settings okay and if I go down right so there will be something\n1:17:37 called as secret keys because this secret key I have to put it with respect to open AI so here no secrets are there\n1:17:45 so I will go and clear or click on uh new secret and you know that with respect to the new secret what I have to\n1:17:52 probably use I have to use open API key I will put it over here okay and now\n1:17:58 oops just a second open API key let me open\n1:18:04 this and I will put it over here okay and what I''m going to do I''m also going\n1:18:10 to upload the value okay so I''ll not show you the value uh let me update this and let me come back and quickly and\n1:18:17 show you the next steps that we are probably going to do after adding the open AI API key uh\n1:18:23 you can see it over here in the secrets you''ll be able to see this specific key now what I will do after updating that I\n1:18:30 will go to my app now again my entire application will start getting buil up now here you can see as soon as I add\n1:18:37 the open AI API you''ll be able to see my application will start running now here I can probably ask question what is the\n1:18:45 capital of India okay and uh you can see that I will be able to get the response\n1:18:52 now clearly you''ll be able to see uh I''ve been able to do the deployment in hugging pH spaces uh it was very much\n1:18:59 simple you can see the files over here itself on the app.py requirement. txt I\n1:19:05 had commented out all the codes with respect to EnV and all because soon as we add the secret variable as soon as\n1:19:11 this open a model is called it is going to take the open a API key from there and it is going to use it over here so\n1:19:18 yes this was with respect to the deployment and quickly we were able to also create a simple Q&A chatbot along\n1:19:25 with deployment so guys in this video we''re going to create one Amazing llm Project which is nothing but PDF query\n1:19:31 with langen and cassendra DB uh cassendra DB will be probably creating in a platform which is called as data\n1:19:37 Stacks so if you have probably heard about this particular platform which is called as data Stacks which will\n1:19:43 actually help you to create cassendra DB in the cloud itself and why this platform is quite amazing because from\n1:19:49 this you will be able to perform Vector search and whenever we talk about this kind of documents or if you want to\n1:19:56 really create an Q&A applications from huge PDFs itself Vector search is the\n1:20:01 thing that you really need to implement now before I go ahead let''s first of all understand the entire architecture we\n1:20:08 will be solving this entirely step by step what are the steps specifically\n1:20:13 that will be taken to probably complete this specific project that we really need to understand so let''s begin with\n1:20:19 the architecture initially let''s say you have a specific PDF this PDF can be of\n1:20:24 any size and any number of pages first of all we will read the documents and\n1:20:30 understand here we are going to use langin as I said because langin has some amazing amazing functionalities which\n1:20:37 will actually help you to perform all the necessary tasks to create this specific application now first we will\n1:20:43 go ahead and read the documents that is specifically the PDF and the first step\n1:20:49 usually when we whenever we work with this kind of data set is with respect to some kind of transformation we really\n1:20:54 need to do so after reading this documents we will convert this into various test chunks that basically means\n1:21:01 we''ll split the data set into some kind of packets right so this text Chunk will\n1:21:07 be of some specific size based on the tokens that we are probably going to use\n1:21:12 so over here you can see some example reading the document and then we have divided this into some chunks then we\n1:21:18 will convert all this chunk into text embeddings now from here we will be\n1:21:24 specifically using open AI embeddings okay openai embeddings actually helps\n1:21:29 you to convert text into vectors now why you specifically require these vectors I\n1:21:35 hope you have heard about text embedding techniques in machine learning right there we have specifically used bag of\n1:21:42 words tfidf and many more things that is already present in my YouTube channel we have also used word to work average word\n1:21:48 to what are the main aim of all these techniques to convert text into vectors\n1:21:54 because once we probably convert into vectors we can perform various tasks like classification algorithms like\n1:22:00 similarity search algorithm and many more so that is the reason we will specifically be using openi embeddings\n1:22:06 which will be responsible in converting a text into vectors itself now once we\n1:22:11 convert every text into vectors we will also see this as text embeddings once we get this embeddings what we are\n1:22:18 specifically going to do now this will be quite amazing because understand if we have a huge PDF document right so\n1:22:24 definitely the vector size will keep on increasing so it is better we store entirely all this vectors into some kind\n1:22:31 of database and for this we are going to use Centra DB so in short what we are\n1:22:37 basically going to do is that we will take all the specific vectors and save it in a vector database here currently\n1:22:44 we are going to use cassendra DB now what exactly is cassendra DB so in order to understand about cassendra DB I have\n1:22:51 opened the entire documentation page over here cassendra aperture Apachi cassendra is an open source no SQL\n1:22:59 database and it can definitely be used for saving massive amount of data so it\n1:23:04 manages massive amount of data fast without losing sleep right so again understand this is a nosql database and\n1:23:11 for vectors kind of thing we definitely have to save it in this kind of database itself many bigger companies are\n1:23:17 basically using this cenda DB for this specific purpose so if you really want to read more about Apache cendra you can\n1:23:24 probably see over here Apache cendra is an open- Source nosql distributed database trusted by thousands of\n1:23:29 companies for scalability and high availability this is the most important point for scalability and high\n1:23:35 availability without compromising performance linear scalability and proven fall Tolerance on commodity\n1:23:41 Hardware or Cloud infrastructure make it as a making it as a perfect platform for\n1:23:46 Mission critical data now how we are going to probably create this specific database for that we will be using this\n1:23:52 data Stacks platform wherein it will actually help you to create this Vector\n1:23:58 cassendra DB so that you can store entirely all these vectors into this specific DB and at any point of time if\n1:24:05 a person is trying to query from this particular DB you will be able to get that specific response from that right\n1:24:11 and the most similar response that you''ll be able to get it now that is the next step what we are basically going to\n1:24:16 do all these vectors we are going to save it in some kind of vector database as I said we going to use casser DB or\n1:24:23 we can also say astrab and this will be created in this data STS\n1:24:28 platform wherein you can actually perform Vector search now the next thing is that after you probably save entirely\n1:24:35 all your vectors in in the database itself then a human whenever a human tries to query anything that is related\n1:24:43 to that particular PDF document it is going to probably apply similarity search along with text iddings and is\n1:24:49 going to get that specific response so this this is the entire architecture that we are specifically\n1:24:56 going to perform in this specific project all the steps will be shown step by step everything will be explained in\n1:25:03 an amazing way along with the code and along with the explanation now let''s go ahead and start our specific project for\n1:25:10 this PDF query with Lang chain and cassendra DB so guys now let''s go ahead and implement this specific project I\n1:25:17 will be going step by step I will also be showing you how you can create the cassendra DB specifically in the data\n1:25:24 Stacks uh platform itself uh we''ll be seeing step by step all the comment regarding this code and all is given\n1:25:30 over here I will also be providing you the code in the description of this particular video so first of all uh what\n1:25:37 exactly we are doing we are going to query PDF with astb and Lang chain uh it\n1:25:43 is basically powered and uh understand it is powered by Vector search so first of all you need to understand what\n1:25:48 exactly is Vector search so there is an amazing documentation that is given in the data stack documentation itself so\n1:25:54 Vector search enhances machine learning models by allowing similarity comparison of the embeddings embeddings basically\n1:26:00 means whatever text is basically converted into the vectors that is basically embedding right and over there\n1:26:06 you can definitely apply multiple algorithms right machine learning algorithms on the fly right as a\n1:26:11 capability of astrab vector search supports various large language models so large language models can be is very\n1:26:18 is supported in an amazing way in this the integration is very much smooth and easy right since this llm are stateless\n1:26:24 they rely on Vector database like Astro DB to store the embeddings see understand because uh when we say\n1:26:30 stateless that basically means what suppose if we have embeddings once we lose it we cannot again query it right\n1:26:36 so it is definitely require a database to probably store all these things and what you can do after that you can query\n1:26:42 any number of time so let us go step by step and let us see okay so first of all we need to create a database on astb so\n1:26:50 I will probably click this specific link everything is basically given over here for this we will be going to\n1:26:57 astra.com right so first of all it will probably ask you to sign in right and\n1:27:04 here you can probably sign it with your GitHub or with your Google account so here I''m going to go ahead and sign it\n1:27:10 with GitHub and probably once I probably sign in over here you will be able to\n1:27:16 see that uh I''ll be providing you the link along with everything in the uh code itself right so it''ll be very much\n1:27:23 easy for you so once you go to Astra data.com the next step is basically to\n1:27:29 create a database right so this database uh what kind of database we are going to probably create it will be serverless\n1:27:35 vector and this is specifically a cassendra DB okay so here I will probably give my database name let''s say\n1:27:42 I want to do PDF query right so this will basically be my PDF query DB okay\n1:27:47 this will basically be my database name you can give anything as you want and here I''ll be basically be giving Lang\n1:27:53 chain _ DB a key space name it should be unique the provider that you can specifically use you have multiple like\n1:28:00 Amazon web services Microsoft Azure but here I''m going to probably use Google Cloud which is the default that is\n1:28:05 selected in the next step we will go ahead and select the country region which is by default Us East one so as\n1:28:12 soon as you probably fill all this details and as you know that we are specifically going to use this Vector\n1:28:18 database itself because at the end of the day the algorithms that we probably going to apply it will be easy with\n1:28:24 respect to this kind of database right so finally we will go ahead and create the database now once how we create the\n1:28:31 database you will be able to see that my database is basically created over here right so this is what is my database\n1:28:37 that looks like right PDF query diving now if I probably go to my dashboard I''ve already created this kind of\n1:28:44 database a lot so let me consider one database which I have already created and over here some important information\n1:28:50 that you really need to take first of all I will go and click on connect okay so when I probably click on connect one\n1:28:58 some information you will definitely require one is generate token right and the other one is the DB ID so DB ID is\n1:29:05 basically present over here right the token is basically present over here now I''ll talk about where this specific\n1:29:11 information will be required okay so here I will go with respect to my code now let''s start our coding initially we\n1:29:18 will be requiring some of the important libraries like Casio data set langin open Tik toen so here I will go ahead\n1:29:26 and execute it and I will go ahead and install all the specific libraries so it\n1:29:31 will probably take some time right I have already done that installation so for me it has happened very much quickly\n1:29:37 now the next thing is that as we know that we are specifically going to use cass. DB so in Lang chain you have all\n1:29:44 these libraries which will actually help you to connect with cassendra DB and perform all the necessary tasks like\n1:29:49 text T meetings uh creating V vors and probably storing it in the database itself so here I''m going to probably\n1:29:55 import all these libraries from lin. Vector stores. cassendra I''m going to import cassendra along with this I''m\n1:30:02 also going to use this Vector store index wrapper it is going to wrap all those particular vectors in one specific\n1:30:08 package so that it can be used quickly then I''m also going to import open AI because open AI is the thing that we\n1:30:14 really need to use along with this we are also going to use open a embeddings which will be responsible for converting\n1:30:20 your text into vectors along with this if you want some kind of data set from hugging face you can also\n1:30:25 use this and one more important library that we are going to use is cashio now Casio actually helps you to uh probably\n1:30:33 integrate with the Astra DB right in Lang chain and it''ll also help you to initialize the DB connection so all\n1:30:39 these libraries we are going to use I''m going to execute this step by step we going to probably see and this is the first step installing the libraries and\n1:30:46 initializing all the libraries that we are specifically going to use along with this what we are going to also import is\n1:30:52 one PDF which is called as Pi PDF 2 this will actually help you to read any PDF\n1:30:58 uh read the uh text inside the PDF itself so this is one amazing library to probably use okay so here I have\n1:31:05 basically used pip install Pi PDF 2 so let me just go ahead and execute it and inside this you will be able to see it\n1:31:11 shows requirement already satisfied because I''ve already installed over here then from PI PDF you''re going to use PDF\n1:31:17 reader because this will be the functionality that will be used in order to to read the document here is the\n1:31:24 document that I have specifically taken so this is one budget speech PDF so this is the Indian budget that is probably of\n1:31:31 2023 it''s a big document with somewhere around 461 KB file it has around 30\n1:31:37 pages so I''m going to specifically read this PDF and then convert into vectors store it in the database itself and then\n1:31:44 query from the database anything that you have any information about that particular PDF now let''s go ahead with\n1:31:50 the setup okay now with respect to the setup you require three important information one is the astrab\n1:31:57 application token one is the Astra DB ID okay so where you can probably get this\n1:32:03 two information so go to your vector database uh Vector database in the data\n1:32:08 Stacks so here uh is what you have specifically logged in okay as I said\n1:32:14 inside your DB just go and click on connect here you need to click on generate token as soon as you probably\n1:32:20 click on generate token then you will be getting some code which looks like this this token you will specifically go\n1:32:26 getting so this will be probably found in your token Json file so it''ll probably show you a Json file which will\n1:32:32 have this information okay the first information that you have over here is the Astra DB application token so here\n1:32:39 you can probably see it starts with Astra CS so what you need to do just click on the generate token and you''ll\n1:32:45 be able to see it this is the first information you just need to copy and paste it over here the second information is Astra DB ID right Astra\n1:32:53 DB ID is nothing but this specific information that is your database ID where do you get it you just need to\n1:32:59 copy it from here so this is the information with respect to your astrab ID so this two information once you do\n1:33:05 it you paste it over here I''ve already pasted it and then you can also see that I''ve also used some open API key and\n1:33:11 this specific API key don''t use this only because Ive made some changes I''ve already executed the code also okay so\n1:33:18 I''m going to take this three information this two information is basically used to connect to your Astra DB right which\n1:33:24 has a cassendra DB hosted over there in the cloud right and the other information is basically to use the open\n1:33:31 AI API features right so all this information is basically there I''m going to probably execute this and then we\n1:33:37 will go ahead and read our budget speech PDF so this is the first step according to this we are reading the specific\n1:33:44 document before that we have initialized everything with respect to this okay so once we specifically do this I will\n1:33:50 probably be reading this now after reading as I said we are going to divide all our content into some kind\n1:33:57 of chunks right so here is what chunks we are basically going to do now first of all I will read all the raw text so\n1:34:03 for this I''m going to use from type extension using concatenate I''m going to read from each and every pages I will\n1:34:10 extract all the text so here you can probably see for I comma page in\n1:34:15 enumerate PDF reader. Pages page. extract text Will basically take out all\n1:34:21 the text from those pages and it will concatenate in this particular variable that is rawcore text so once I probably\n1:34:27 execute this what will happen is that you will be able to get all the text\n1:34:33 inside this particular variable so you can probably see over here rawcore text has all the entire text so this is the\n1:34:40 entire text from that specific PDF right slash in basically means new line so this step is basically done just imagine\n1:34:46 before if we did not had this specific Library it was very difficult to read a PDF right and we have actually done this\n1:34:52 just with writing four to five lines of code now the next step is that we will\n1:34:57 initialize the connection to your database I have all my database information right like uh token ID and\n1:35:03 the database ID I''m going to use that cashio cashio is basically used as a library over there for initializing of\n1:35:09 this particular database so cashier. init here I''ll be giving one parameter which is called as token which will be\n1:35:15 nothing but astb application token and then your database ID which is nothing but astb ID right so I''ve taken this two\n1:35:21 information I will execute this you''ll get some kind of warnings so don''t worry about the warnings it is just like it is\n1:35:27 showing you some kind of warnings okay with respect to some drivers issues and all but this will basically get executed\n1:35:33 and now I have uh basically initialized my DB itself right now we are going to\n1:35:38 create the Lang chain embeddings L LM objects for letter use so for that I''m going to use I''m going to initialize\n1:35:44 open AI with my open AI key and embeddings also open AI embeddings with my open a key so I have my llm I have I\n1:35:51 have my embeddings okay now is the main step I need to create my lch Vector\n1:35:57 store so over here this is what we are basically going to create now right and for that you know we have initialized\n1:36:04 cendra right we have we have imported cendra now what will do is that in this cassendra we will provide three\n1:36:10 important information what is the kind of embeddings we are going to use what is the table name inside this particular\n1:36:16 database session none keyp space none so this is the default parameters we have specifically used QA mini demo is my\n1:36:23 table name okay just like a question answer table name and what kind of embeddings we are going to specifically\n1:36:29 use that basically means whenever we store any whenever we push any data in my cassendra DB in my Astra DB itself\n1:36:35 what it is going to do it is going to probably convert all the text using this embeddings into vectors right and this\n1:36:41 is the embeddings that we have initialized over here so here is the next step we will go ahead and execute this so this is my Astra Vector store\n1:36:48 but still I have not probably converted my text into vectors only when when I''m pushing my data inside my DB that time\n1:36:56 this entire embeddings will probably convert that particular data into vectors then what we are specifically\n1:37:03 going to do is that we will take this data and we will try to uh we''ll take\n1:37:08 this entire data we''ll convert into checks uh chunks and we''ll also do the text embedding right text embedding\n1:37:14 while inserting right so here you first of all we are dividing the data or the entire data entire document into text\n1:37:20 Chunk so for this we are using character text splitter which is basically present in Lang chain. text splitter we need to\n1:37:27 split the text using character text splitter it should not increase the token size so here I''ve given character\n1:37:33 text splitter I''m saying use the separator slash in use chunk size some chunk size of 800 characters chunk\n1:37:39 overlap can be 200 and how much is the length with respect to that specific length you can probably provide it over\n1:37:44 here right and once I probably do this you can see text splitter. split text\n1:37:49 here you will be able to get all the text and if I see the top 50 text you\n1:37:54 can probably see that I''ll be able to see all the top 50 text over here right all the data itself this is amazing\n1:38:01 right and this is basically from the PDF right all the data all the data right it\n1:38:06 is basically taking the top 50 right and understand the token size is basically over here as the chunk size is somewhere\n1:38:13 around 800 okay now this is done we have the text I''m going to just use top 50\n1:38:18 and probably store it in the vector database to see if everything is working fine now how to add this specific text\n1:38:25 now what will happen when I add this text inside my Cassandra DBC axtra Vector store what is this this is\n1:38:31 basically initialized with respect to the cendal library right so here you''ll be able to see that I have used\n1:38:37 embeddings so now when I''m inserting inside the cassendra DB what it is going to do it is going to apply this specific\n1:38:43 embeddings also so that is the reason you''ll be able to see that when we write extraor Vector uncore store. addore text\n1:38:51 and I''m taking the top 50 top 50 texts over there this will also perform\n1:38:56 embeddings so that basically means if I see over here it is going to perform this task and it is going to insert in\n1:39:02 the Astra DB which is having that cassendra over there right so it is going to do this both the steps with\n1:39:07 respect to this particular code so we are going to add this text and then we also going to wrap wrap this entire\n1:39:14 inside a wrapper okay so these are the information this is the index that we''ll be getting with respect to those text so\n1:39:20 once I proba executed you''ll be seeing that in the same database it is going to insert all this headlines okay now\n1:39:28 finally let''s go ahead and tex it that basically mean I have my vectors inside my database now it''s time that we just\n1:39:34 query and we ask some kind of questions now I have read this entire PDF guys I could find out some of the question like\n1:39:40 what is the current gbd how much agriculture Target will be increased and all so I will take this particular\n1:39:45 example and let''s say I''m writing first question is equal to True while true if first question I''m just say that input\n1:39:51 okay it will just ask like what kind of question you want to type else uh it is\n1:39:56 just asking you to uh put more questions if I write quit it is going to break otherwise it is going to continue now\n1:40:03 see this is the most important as soon as I give my first question it will go ahead with v Astra Vector index and\n1:40:09 it''ll query whatever query text we are specifically using and the llm models that we are specifically initialized and\n1:40:15 after that we will be getting the answer along with this we''ll also be providing some information\n1:40:21 right like for Doc score or similarity Source score like some other information also right so let''s go ahead and execute\n1:40:28 it and as I said I''m going to use this question okay how much is the agriculture Target to be increased and\n1:40:36 what focus it will be okay so I''m going to paste it over here I''m going to press enter so as soon as I press enter you\n1:40:42 can see that it is now taking the information see this um you can probably\n1:40:47 see over here we are quering this particular DB right and it''s going to give me the top four results okay so\n1:40:52 here you can see that agriculture credit Target will be increased to 20 lakh CR with the focus on animal husbandry da\n1:40:59 and Fisheries right why it is giving only this much data because I''ve told that take the 84 characters or 84 words\n1:41:06 84 characters text still there and probably give the results right if I increase this it''ll give you more result\n1:41:12 along with this you can probably see that it is giving me stop K queries that is the four query Hyderabad will be\n1:41:18 supported as Center of Excellence some more information but the most suitable answer that you have specifically got is\n1:41:24 this one right and this is what probably if you go ahead and search in the PDF if you give the same question you will be\n1:41:31 able to see the same answer right along with this probably if I want to probably see what is the current GDP if this\n1:41:37 information is present over there it''ll also be giving you that specific answer it''ll just do the similarity search\n1:41:43 right so here you can current gbd is estimated to be 7% isn''t it amazing now you can probably take any huge data\n1:41:50 because at the end of the day you specifically using DB right and finally if you want to quit it I will just go\n1:41:55 ahead and write quit and this is basically quit right so in short we have performed each and every step now this\n1:42:01 is what which is happening whenever a human is giving an text query text emings will happen and based on that similarity search and then you''ll be\n1:42:07 probably get the output right and this is the entire steps We have basically done step by step so guys yet another\n1:42:15 amazing video on generative AI where I will be specifically discussing about llama 2\n1:42:21 uh Lama 2 is an open-source model uh again it has been created by Facebook or\n1:42:27 meta and you can use this specific model even for commercial purpose uh so this\n1:42:32 is quite amazing this is an open-source llm model altogether I will try to show you how we can use this\n1:42:39 create an end to endend project also in this specific video so there are many things that are going to happen and\n1:42:45 probably whatever topics that I teach going forward that is related to generative AI I will definitely follow\n1:42:51 this kind of approach so that you also get a brief idea about all these kind of models so what is the agenda of this\n1:42:58 particular video the agenda is that we will get to know about Lama 2 then we\n1:43:03 will go ahead and see the research paper where I will be talking about the key points uh about the Lama 2 model again\n1:43:09 since this is an open source and uh soon Lama 3 is also going to come up so that is a reason I''m going to create this\n1:43:15 particular video I really want to be in sync with all the open source llm models that are coming up right\n1:43:21 then we''ll go and apply and download the Llama 2 model so we''ll be seeing like how we can actually use this particular\n1:43:27 model in our project also so for that purpose I will be downloading this model you have to also apply this in the meta\n1:43:33 website itself and there is also one way how uh we can also use it directly from\n1:43:39 hugging face so I will also show you that and after that we will try to create an end to end llm project and\n1:43:45 this will be a Blog generation llm app uh all these topics I will be covering\n1:43:50 it I know it''ll be a little longer video but every week one kind of this kind of video is necessary for you all and since\n1:43:58 2024 I have the target I really need to teach you gener in a way that you can\n1:44:04 understand it and use it in your industries also so I will keep a Target so every video I''ll keep a Target like\n1:44:10 this target for this particular video is, L likes not thousand lcks but\n1:44:16 thousand likes and comments please make sure that you write some comments and I''ll keep the target to 100 okay\n1:44:24 so this will actually motivate me this will probably help this particular video to reach to many people through which\n1:44:31 they can actually use this and entirely this is completely for free which will also be beneficial for you and I my aim\n1:44:37 is to basically democratize the entire AI education okay so let''s go ahead and\n1:44:42 let''s first of all start with the first one that is introducing Lama 2 what exactly is Lama 2 Lama 2 is an again\n1:44:49 open source a large language model it can be it is used and it is uh available\n1:44:54 for free for research and commercial purpose you can actually use this in your companies in a startup wherever you\n1:45:01 want to use it okay now let''s go ahead and read more about it so inside this\n1:45:06 model uh it has till now Lama 2 has released three different model size uh one is with 7 billion parameters the\n1:45:13 other one is 13 billion parameters and the the best one is somewhere around 70 billion parameters uh pre-training\n1:45:19 tokens is taken somewhere around 2 trillion context length is 4096 uh again when I say that if I\n1:45:27 probably compare most of the open source models I think Lama 2 is probably very good we''ll be seeing all those metrics\n1:45:34 also so here you can see Lama 2 pre-trained models are trained on two trillion tokens and have double the\n1:45:39 context length than Lama one it''s fine tune models have been trained on over 1\n1:45:45 million human annotation okay and now let''s go ahead and see The Benchmark and this is with respect to the the\n1:45:50 benchmarking with all the open source models so it is not comparing with chat GPT sorry GPT 3.5 GPT 4.0 or Palm 2 okay\n1:45:58 so all the open source models uh here you can probably see this is the three version 7 billion 13 billion 65 billion\n1:46:04 70 billion right all Lama 2 right llama 1 was 65 billion one uh one model it had\n1:46:10 over there so if you see Lama 2 with respect to all the metrix is very good MML that is with respect to human level\n1:46:18 understanding Q&A all all the performance metrix is superb natural language processing gsmk human evalve in\n1:46:25 human evalve it is probably having a less when compared to the other other open source models so here you can see\n1:46:31 in human uh human Val human eval human eval basically means with respect to writing code code generation there it\n1:46:38 has a lot of problems so here you can see 12.8 18.3 it is less it is less when compared to all the other open source\n1:46:46 models over here and there are also some other parameters you can probably see over here with with respect to different\n1:46:51 different tasks you can see the performance metrics okay so this was more about the model now let''s go ahead\n1:46:56 and probably and this is one very important statement that they have come up with we\n1:47:01 support an open Innovation approach to AI responsible and open Innovation give us all a stake in the AI development\n1:47:08 process so uh yes Facebook is again doing a very good work and then soon\n1:47:13 they also going to come up with the Lama 3 Model now let''s go ahead and see the research paper so here is the research\n1:47:18 paper the entire research paper now see uh what you should really focus on a research paper you know in research\n1:47:24 paper they''ll be talking about how they have actually trained the model what kind of data points they have they\n1:47:30 actually taken in order to train the model and all right so over here you can see that um in this work we developed\n1:47:36 and release Lama 2 a collection of pre-trained and fine-tune Lun language models ranging in scale from 7 billion\n1:47:42 to 70 billion parameters so if you talk about parameters it is somewhere around 7 billion to 70 billion our fine tune\n1:47:48 llms called Lama 2 chart are optimized for dialog use cases just like a chat bot and all right uh more information\n1:47:54 you can probably see over here what is the pre-training data see so they have told that our pre-training data includes\n1:48:00 a new mix of data from publicly available sources which does not include data from meta products or Services\n1:48:06 we''ve made an effort to remove data from certain sites known to contain a high volume of personal information about\n1:48:12 private individuals now this is where ethics comes into picture they really want to use this AI in a responsible way\n1:48:18 right so we trained on two trillion tokens uh and obviously for all these things you have to use Nvidia GPU okay I\n1:48:25 know guys it is boring to read the research paper but it is good to have all this specific knowledge so please\n1:48:31 keep your energy up watch this video till the end then only you''ll be able to understand things right not only here\n1:48:38 because later on you''ll be having other models like Mistral I''ll probably create a video on Mistral also in the upcoming\n1:48:44 video right so everywhere with an end to end project everything I will take this format let me know know whether you''re\n1:48:50 liking this format or not so training data we adopt most of the pre-training settings and model architecture from\n1:48:56 Lama one we use the standard Transformer architecture now you can understand how important Transformer is right most of\n1:49:02 the open source model are based on Transformer architectures itself right we trained using adamw Optimizer okay\n1:49:09 with so and so parameters we use consign learning rate schedule with so and so and here you could probably see with\n1:49:15 respect to the performance like how well it was training BPL process tokens how many tokens was actually done with\n1:49:22 respect to all the different varieties of llama model now this is basically the training loss you can probably see\n1:49:27 training loss for Lama 2 okay this is also important training hardware and carbon footprint it is basically saying\n1:49:33 that how much it is using they used Nvidia a100 I''ve seen this GPU it''s quite amazing it''s very huge okay and it\n1:49:41 is very fast also but again with such a huge amount of data it is also going to take time right so all these things are\n1:49:46 there you can also see time how much time it has basically taken how how many hours 70 billion this many number of\n1:49:52 hours power consumption this this all information is there right this is good to have right all all you should know\n1:49:59 like we just taking more energy and all right and here um with respect to the uh\n1:50:05 llama 2 you can probably see with respect to Common reasoning it is very good when compared to all the other models open source model World Knowledge\n1:50:12 reading comprehension math mlu math it is little bit less you can see over here\n1:50:19 when compared to the other model I think it is still 35 itself but remaining all it has basically come I think this 35 is\n1:50:25 also greater than all these things right MML is very much good it is able to achieve till 68.9 Google gini has said\n1:50:32 that it is reach to 90% okay but again this is the thing that you really need to know uh some more information fine\n1:50:39 tuning fine tuning also okay this is very much important guys it has it has used this uh reinforcement learning okay\n1:50:47 where uh and with human feedback so our R lhf basically means reinforcement learning with human feedback and this is\n1:50:54 what chat GPT is also trained with right so uh definitely I think as we go ahead\n1:50:59 as we go ahead and see Lama 3 and all it is going to give us very good accuracy I guess okay so superv fine tuning uh if\n1:51:06 you go ahead and just check how generative AI how llm models are trained you''ll be able to get a video on this I\n1:51:12 created a dedicated video where I explained about supervised fine tuning how does supervised fine tuning happen\n1:51:19 what how does uh rhlf happens right reinforcement sorry R lhf human feedback happens all\n1:51:26 those things I''ve actually explained so here you can see some of the prompts right a poem to help me remember the first 10 elements on the periodic table\n1:51:32 hydrogen come first as the element one helium is second for balloons this this I want you to roast me now see this\n1:51:38 statement is also very important right so uh I want you to roast me I want you to make it particular brutal swearing at\n1:51:46 me so it is saying I''m sorry but I cannot comply with that request using language or intentional hurting someone\n1:51:51 feelings is never expectable so some kind of feelings they''re trying to bring inside all these kind of models okay uh\n1:51:58 sft annotation is basically there you can probably read all these things this is good to have good to learn how this\n1:52:03 reinforcement learning with human feedback was done and all everything is given over here so uh this was all about\n1:52:09 the research paper still there are many papers to go ahead you can probably go ahead and check it out uh there is a\n1:52:14 concept of reward modeling also reward is also given right the parameters they have used two separate parameters over\n1:52:20 here and various kind of test is basically done so this was all the information about this now the next\n1:52:26 thing is that how you can go ahead and apply or download this specific model just click on download the model over\n1:52:31 here so the third part provide all the information over here and what all things you specifically required like\n1:52:37 Lama 2 and Lama chart code Lama Lam guard so go ahead and just put all this information and click on submit after\n1:52:44 submitting probably it''ll take 30 minutes and you will start getting this mail okay\n1:52:50 you all start to set building with code llama you will also be getting the access from Lama 2 see you''ll be getting\n1:52:56 this entirely right model weights available all the models weight will be given to you in this specific link you\n1:53:02 can click and download it also if you want so that you can use it in your local or you can deploy it wherever you\n1:53:07 want okay so this kind of mail you''ll be getting uh Lama 2 commercial license all\n1:53:12 the information with all the info over here and these all models it is specifically giving again I told you 70b\n1:53:19 70b chat why these two models are there this is specifically for Q&A kind of\n1:53:24 application dialog flow application I can basically say uh remaining one can be used for any kind of task uh in a\n1:53:30 complex scenarios and all okay so once you do this the next thing is that you can also go to hugging face in hugging\n1:53:36 face you have this Lama 270b chat FF and there is the entire information that is\n1:53:42 probably given about the entire model itself you can probably read it from here with respect to this Lama 2 is a\n1:53:48 collection of pre-trained this this information is basically there you can also directly use it if you want the code with respect to Transformer you\n1:53:54 just click on using Transformer you''ll be able to get this entire code where you can directly use this also okay what\n1:54:00 we are basically going to do I''m not going to use 70 billion parameters since I''m just doing it in my local machine with the CPU itself okay so what I will\n1:54:08 do I will be using a model which is basically uh it is basically a quantized model right with respect to this same\n1:54:14 llama model it is called as Lama to 7B chat gml so if you go ahead and see see\n1:54:19 this uh you''ll be able to see that this particular model you''ll be able to download it and you''ll be able to use it\n1:54:26 it is just like a good version but uh less parameter versions right so when we\n1:54:31 say contage that basically means uh this model has been compressed and probably provided you in the form of weight so\n1:54:37 what you can do any of these models the recent model what you can do over here which is of 7.16 GB you will first off\n1:54:43 all download it so I''ve already downloaded it so I''m just going to cancel it over here okay because I''ve\n1:54:50 already downloaded it over here okay so I will do that specific download uh over here and then you can probably go ahead\n1:54:57 and start working on this and start uh using this and now how you can probably use it I Will Show You by creating an\n1:55:04 endtoend project so for creating an NN project what are the steps uh again the project name that I''ve already told is\n1:55:11 basically a b blog generation llm app here I''m going to specifically use this open-source llama Lama 2 model again I''m\n1:55:19 going to use the hugging face API also for that uh and let''s see how the specific uh step by step how we''ll be\n1:55:25 doing this specific project so let''s go ahead and let''s start this particular project okay guys now let''s start our\n1:55:31 blog generation llm platform uh application so the model that I had actually generated over here you can\n1:55:37 probably see the model over here in the bin size and this is the size of the model is over here I''m going to\n1:55:43 specifically use in my local machine for my local inferencing and all so over here what I will do I will go quick\n1:55:49 quickly go ahead and open my VSS code so my vs code is ready over here okay now\n1:55:57 let''s go ahead and do step by step things that we really need to do first of all I''m just going to create my requirement. txt file requirement. txt\n1:56:05 file and now I will go ahead and open my terminal so I will go ahead and open my\n1:56:12 command prompt and start my project okay so quickly I will clear the screen I\n1:56:17 will deactivate the default environment cond deactivate okay and\n1:56:23 we''ll do it step by step so first step as usual go ahead and create my environment cond create minus P VNV\n1:56:32 environment I hope I''ve repeated this specific step lot many times so here I''m going to create cond create minus pvnv\n1:56:39 with python wal to 3.9 y okay so just to give you an idea\n1:56:47 what how exactly it is going to run run how things are basically going to happen\n1:56:52 uh step by step we''ll understand so first of all we are creating the environment and then we will go ahead\n1:56:59 and fill our requirement. txt now in requirement. txt I''m going to specifically use some of the libraries\n1:57:06 like sentence Transformers C Transformer fast API if you want to specifically use\n1:57:12 fast API I I''ll remove this fast API I think uh I will not require this IPI\n1:57:17 kernel so that I can play with Jupiter notebook if I want I can also remove this I don''t want it langon I will\n1:57:24 specifically using and streamlet I''ll be using okay so first of all I will go ahead and create cond activate Okay cond\n1:57:33 activate uh venv so we have activated the environment and the next thing is that I\n1:57:39 will go ahead and install all the requirement. txt okay and in this you don''t require\n1:57:48 uh okay so okay I''ve not saved it so requirement. txt is not saved now in\n1:57:54 this you don''t require any open AI key because I''m just going to use hugging face and from hugging face I''m going to\n1:57:59 probably call my model which is basically present in my local so here is the model that I am going to\n1:58:04 specifically call okay so once this installation will take place then we will go ahead and create my app.py and\n1:58:13 just give you an idea like uh I''m going to basically create the entire application in this specific\n1:58:20 file itself so quickly uh let''s go ahead and import our streamlet so till the\n1:58:28 installation is basically happening I will go ahead and install streamlet Okay as\n1:58:35 St and then along with this I will also be installing Lang chain. prompts\n1:58:40 because I''m also going to use prompts over here just to give you an idea how things are going to happen it''s going to\n1:58:46 be very much fun guys because open source right it it''s going to be really amazing with respect to open source you\n1:58:51 don''t require anything as such and then I''m going to basically write prompt template because we need to use this\n1:58:57 from Lang chain then I''ll be also using from Lang chain Lang chain do llm I''m going to\n1:59:06 import C Transformer okay why this is used I will\n1:59:12 just let you know once I probably write the code for this okay so three three Transformers also I''m going to basically\n1:59:18 use over here so this is going to be from okay so C Transformers prom\n1:59:23 template and St for the streamlet I''m going to specifically use the first thing is that I will go ahead and write\n1:59:30 function to get response from my um llm uh llama model\n1:59:38 right Lama 2 model I''m going to basically use this okay still the\n1:59:44 installation is taking place guys it is going to take time because there are so many libraries I''ve been installing okay\n1:59:49 so I''ll create a function over here let''s create this particular function later on okay now after this what I''m\n1:59:56 actually going to do is that we''ll go ahead and set our streamlet right setor\n2:00:02 pageor config see now many people will say streamlet or flask it does not\n2:00:07 matter guys anything you can specifically use streamlet why I''m specifically using is that it''ll be very much easy for me to probably create all\n2:00:14 the things right the UI that I want so in a set page config I''m going to\n2:00:21 basically use page title generate blogs page icon I''ve taken this robot icon from the streamlet documentation layout\n2:00:27 will be Central and uh initial sidebar will be collapsed okay so I''m not going\n2:00:33 to open the sidebar in that specific page now I will keep my ht.\n2:00:39 header so ht. header in here I''m going to basically generate my blogs right so\n2:00:48 generate the blogs and I''ll use the same logo if I want so it looks good okay so\n2:00:54 this is the next thing I will probably this will be my head over here first of all I will create my input text okay so\n2:01:02 input text field right and this will basically be my input text field and let\n2:01:08 me keep it as a um a text area or a text box whatever things is required so I\n2:01:15 will write go ahead and write St do st. input textor input okay so this will\n2:01:24 basically be my St so let''s see everything is working fine why this is not coming in the color okay still the\n2:01:30 installation may be happening so over here I''ll go ahead and write this I will\n2:01:35 say enter the blog topic right so if you just write the blog topic it will should\n2:01:41 be able to give you the entire blog itself with respect to anything that you want okay so done the installation is\n2:01:47 basically done over here you can probably see this good I will close this up now I''ll continue my writing the code\n2:01:54 so I''ve created a input box now the other thing that I really want to create is that I''ll try to create two more\n2:02:00 columns or two more Fields below this box okay one field I will say that how many words you specifically want for\n2:02:07 that blog okay so over here creating two more\n2:02:13 columns for additional two Fields additional two field okay so here\n2:02:20 first of all will be my column 1 let''s say column 1 and column 2 I will just\n2:02:25 write it like this and here I will say St do\n2:02:31 columns and uh here I''ll be using I''ll be giving right what should probably be\n2:02:38 the width like let''s say 5 comma 5 if I''m giving you''ll be able to see that the width of the text box of width of\n2:02:43 the column that I specifically have I''ll be able to see it okay I''m I''m just creating that width for that columns\n2:02:49 okay now I''ll say with column one whenever I probably write anything in\n2:02:55 the column one or select in anything in the column one this will basically be my number of words okay number of words and\n2:03:01 for here I will be creating my St do text input and this text input will\n2:03:07 probably retrieve the details of number of words okay so here I have\n2:03:14 specifically number of words great now the next column that I specifically want\n2:03:20 the detail so for whom I am actually creating this particular blog I want to probably put that field also so with\n2:03:27 column 3 I will probably create something like this I will say okay fine um what blog style I will I''ll create a\n2:03:35 field which is basically called as blog style okay now inside this blog style\n2:03:41 what I am actually going to do sorry not column 3 column two because I''ve created those variable over there okay so the\n2:03:47 blog style will be basically be a drop down so I will say St do select box okay\n2:03:54 and I will say what box this is specifically for so that first message I\n2:04:00 will say select write writing the blog\n2:04:07 for for okay so this I''m basically going to say that okay for whom I''m going to\n2:04:12 write this particular blog okay and with respect to this I can give all the options that I really want to give okay\n2:04:19 so for giving the options I will also be using this field so let''s say the first option will be for researchers whether\n2:04:25 I''m writing that particular block for researchers or for data\n2:04:30 scientist okay data scientist or I am basically writing this block\n2:04:38 for for common people okay common people so this three information I really want\n2:04:45 over here and this will basically help me to put some Style filing in my blog okay that is the reason why I''m\n2:04:51 basically giving over here okay and by default since we need to select it in the first option so I will keep it as\n2:04:58 index as zero okay so here is all my stylings that I''ve have specifically\n2:05:03 used so if you want to probably make it in this way so you''ll be able to understand this so this will be my\n2:05:08 column one and this is basically be my column two okay and then finally we will go ahead and write submit button submit\n2:05:17 will be St dot button and this will basically be my\n2:05:23 generate okay so I''m going to basically generate this entirely uh generate is\n2:05:30 just like a button which will basically Click by taking all this particular information so from here I''ll be getting\n2:05:35 my input from here I''ll be getting number of words from here I''ll be getting my blog style okay all this\n2:05:41 three information now this will actually help me to get the final response here okay\n2:05:48 so I will say say if submit okay if submit I have to call one function right\n2:05:54 and what will be that specific function that function will return me some output okay and that output will be displayed\n2:06:01 over here now that function I really need to create it over here itself let''s say I will say\n2:06:06 get llama response okay so this is basically my function and this I will create in my\n2:06:14 definition and what all parameters I specifically require over here right this three parameters right and uh if I\n2:06:20 probably call this function over here what are the parameters that I''m going to write over here is all these three\n2:06:26 parameters so first parameter is specifically my text input input\n2:06:31 text the second parameter that I''m actually going to give over here is number of\n2:06:37 words the third parameter that I really want to give is my block style so like what block style I really want okay so\n2:06:44 all this three information is over here so this will basically be my input text\n2:06:49 okay uh I''ll write the same name no worries number of\n2:06:54 words and third parameter is basically my block style so all these materials\n2:07:00 will be given in the description if you''re liking this video please make sure that you hit subscribe press the Bell notification icon hit like again\n2:07:07 just to motivate me okay if you motivate me a lot I will create multiple contents amazing content for you okay now here is\n2:07:14 what I will be calling my llama model right l Lama model Lama 2 model which I\n2:07:21 have actually downloaded in my local and for that only I will be specifically using this C\n2:07:28 Transformers right now if I probably go ahead and search in Lang chain Lang\n2:07:34 chain see whenever you have any problems related to anything as such\n2:07:41 right C Transformer C Transformer go and search in the documentation everything\n2:07:46 will be given to you so C Transformer what exactly it is it is it is over here it is given over here or not here let''s\n2:07:54 see the documentation perfect so here you can see C Transformers the C trans Library\n2:08:00 provides python binding for ggm models so gml models the blog gml models\n2:08:06 whichever model is basically created you can directly call it from here let''s say in the next class I want to call mistal\n2:08:11 so I can go ahead and write my model name over here as mist and it''ll be able to call directly from the hugging phas\n2:08:16 okay um not only hugging face but at least in the local uh if you have the local if you want to call it from the\n2:08:22 hugging face then you have to probably use the hugging face API key but right now I don''t want to use all those things\n2:08:28 so I want to make it quite simple so CC Transformers and here I''m going to basically write my\n2:08:33 model model is equal to and this should be my model path right which model path\n2:08:39 this one model slash this one right so here you can probably see this specific\n2:08:44 name V3 Q8 Z bin okay so I''m going to to probably copy this entire\n2:08:50 path and paste it over here okay so this will basically be my model and inside\n2:08:56 this what kind of model type I want there is also a parameter which is basically called as model type and in\n2:09:02 model type I''m going to basically say it is my llama model okay and you can also\n2:09:08 provide some config parameter if you want otherwise it will take the be\n2:09:13 default one so I''ll say Max newcore tokens\n2:09:18 is equal to 256 and then the next one will basically\n2:09:25 be my temperature colon 0.01 let me keep the temperature\n2:09:33 point less only so I want to see different different answers okay so this is done uh this is my llm model that I''m\n2:09:39 basically going to call from here and it is going to load it okay now after my llm model is created I will go ahead and\n2:09:46 write my prompt template because I''ve Tak taken three three different information so template here I will go\n2:09:52 ahead and create this will be in three codes if you want to write it down because it is a multi-line statement and\n2:09:58 I will say write a blog write a blog for which style right\n2:10:07 blog style for whom for this specific blog style for researchers for freshers\n2:10:12 for anyone you can write right or I''ll say job profiles I can for researcher job profile for fresher job profile for\n2:10:19 normal people job profile right so something like this job profile for a\n2:10:25 topic which topic I''m going to basically say this will be my number of words sorry not number of words this will be\n2:10:32 my input text so this is how we basically write prompts\n2:10:39 within how many words the number of words okay this many number of words I''m\n2:10:45 going to basically write this okay so this actually becomes my prompt template entirely okay this is my entire prompt\n2:10:52 template write a blog for so and so for block style this this this to make it\n2:10:58 look better what I will do I''ll just press tab so that it''ll look over here\n2:11:05 okay so this is my template that I''m probably going ahead with I''ve given the three information blog style input text\n2:11:12 number of words everything is given over here now finally I need to probably create the prompt template okay so for\n2:11:18 creating the prompt template I''m going to use prompt is equal to prompt template and here I''m going to basically\n2:11:25 give my input variables so input uncore variables and inside this I''m\n2:11:33 going to basically write first information that I want what kind of inputs I specifically want right uh\n2:11:39 whether I want um this block style so for block style I can just write style second one I can probably\n2:11:46 say text third one I can basically say ncore word so this will basically be\n2:11:53 my three information that I''m going to provide it when I''m giving in my prompt\n2:11:58 template okay and finally uh this is my input variable this next parameter that I can also go ahead with I can provide\n2:12:04 my template itself what template I want to give so this will be my template over\n2:12:10 here now finally we will generate the response from the Lama model OKAY Lama 2\n2:12:19 model which is from gml okay so here what I''m actually going to do I''m going\n2:12:25 to basically write llm and whatever things we have learned in Lang chain till now prompt\n2:12:30 dot prompt. format and here I''m going to basically use email sorry email what are the\n2:12:38 information that I really want to give over here prompt. format so the first thing is with respect to style the style\n2:12:44 will be given as block style so I''m going to basically write blog undor\n2:12:51 style okay the next information that I''m probably going to give is my input text\n2:12:56 input text is equal to not input text text is equal to input text I have to give text is equal to input undor text\n2:13:05 and the third parameter that I''m going to give is my ncore words which will basically be by number of words done so\n2:13:15 this is what I''m specifically giving with respect to my prompt uh and what llm will do it will try to give you the\n2:13:21 response for this and then we will go ahead and print this response and we\n2:13:28 will return this response also okay\n2:13:34 response response okay and what we''ll do we will go ahead and return this\n2:13:41 response so step by step everything is done now I''m going to call this get Lama\n2:13:47 response over here here already is done now let''s see if everything runs fine or not uh hope so at least one error will\n2:13:54 at least come let''s see so I will delete this and let''s go\n2:14:00 ahead and write over here to run the streamlet app all you have to do is just just write streamlet Run\n2:14:07 app.py Okay so once I probably execute this you''ll be able to see this is what\n2:14:14 is my model but still I''m getting a model streamlet has attribute no head\n2:14:20 okay so let''s see where I have specifically done the mistake because I think it should not be head it should be\n2:14:27 header okay I could see the error header okay fine no worries let''s run it\n2:14:37 baby let''s run this again stream L run app.py no I think it should\n2:14:43 run this looks good uh enter the block toping number of words researchers writing the researcher blog data\n2:14:49 scientist common people so let''s go ahead and write about large language model so 300 words so\n2:14:57 number of words I will go ahead and write 300 I want to basically write it for common people and we will go ahead\n2:15:04 and generate it now see as soon as we click on generate it is going to take some time the reason it is probably\n2:15:10 going to take some time because uh we are using this particular in my local\n2:15:15 CPU but we got an error let''s see key error block style it seems so I will go\n2:15:20 to my code block style block style block style so one minor mistake that I have\n2:15:26 specifically done over here so what I will do is that I''ll give the same key name so that it does not give us any\n2:15:33 issue okay so this will be my input text and number of words the thing is that\n2:15:39 whatever things I give in that prompt template the input variables should be of that same name okay so that is a mistake I had done it''s okay no worries\n2:15:46 so let''s go ahead and and execute it now everything looks fine have assigned the\n2:15:51 same value over there number of words number of words so here also I''ll go ahead and write number of words block\n2:15:58 style input text and this also should be block style\n2:16:04 the name I''m giving same right for both prom template and this okay so I think\n2:16:09 now it should work let''s see so go ahead and write this and now my page is open\n2:16:17 opened now I''ll go ahead and write large language models and it will probably\n2:16:24 create my words so this will be 300 I want to\n2:16:29 create it for common people let''s generate it as I said that the output\n2:16:35 that I''m probably going to get is going to take some time because I''m running this in local CPU um let''s say if you\n2:16:41 deploy this in the cloud uh with respect to if there are GPU features then you will get the response very much quickly\n2:16:47 so so just let''s wait uh till then uh we get the output hardly but I think it is\n2:16:52 5 to 10 seconds Max and since I''ve told 300 wordss it is again going to take time so let''s see the output once it\n2:16:58 comes so guys it hardly tookes 15 seconds to display the result so here you can see that large language models\n2:17:04 have become increasingly popular in recent year due to the in due to the ability to process and generate humanlike languages it looks like a good\n2:17:11 blog you can also create any number of words blog itself Now understand that I have a good amount of ram my CPU has lot\n2:17:18 of cores so I was able to get it in 15 seconds for some of the people it may take 20 seconds it may take 30 seconds\n2:17:25 now you may be asking Kish how can you specifically reduce this time is very much simple guys we will probably do the\n2:17:31 deployment in AWS or any other Cloud Server itself which I will be probably showing you in the upcoming videos and\n2:17:37 there you''ll be able to see that how with the help of gpus the inferencing also becomes very much easy not only\n2:17:43 that we''ll also see how we can probably fine-tune all this data set with the OWN custom data itself guys yet another\n2:17:50 amazing llm project for you all now this llm project will be quite amazing\n2:17:55 because from this like this is just like a base you can probably create any kind of app on top of it you can create text\n2:18:01 summarizer you can create a quiz app or you can create any other app itself that is probably something related to text\n2:18:07 right so what is the main aim of this particular project is that from this project you will get all the guidance\n2:18:14 that is probably required to create that production grade application whenever you specifically work in the companies\n2:18:21 why because we are going to also include Vector search database and this is where you''ll understand the power of the\n2:18:27 vector search database whenever you work in any NLP project something that is related to\n2:18:33 text you try to convert those text into embeddings or vectors if you have a huge\n2:18:38 vectors you you cannot just store it in your local machine you probably are requiring some kind of database and\n2:18:44 specifically with respect to vectors or embeddings V Vector DB is very super beneficial why because you can probably\n2:18:51 apply some of the important algorithms like similarity search or you can also uh apply any other algorithms that is\n2:18:58 related to text it can be text classification very much quickly by just squaring it from the vector database and\n2:19:04 getting the right kind of output so all these things we are basically going to cover it will probably be a longer video\n2:19:10 because every step by step I''ll probably show you I will also write the code in code in front of you wherever any\n2:19:16 documentation is probably required I will also show you all those things so yes without wasting any time let''s go\n2:19:23 ahead and probably see this project my main aim is basically to teach you in such a way that you get the right kind\n2:19:29 of knowledge and this you apply it in your company and nowadays many companies are specifically asking interviews\n2:19:35 regarding Vector DB they asking related to open a llm models and many more so let me go ahead and share my screen and\n2:19:42 as I said we will be doing completely from Basics right so here is my vs code\n2:19:48 I have a document over here budget speech. PF I am probably going to take this particular document upload it in my\n2:19:55 Vector DB right and then ask any kind of queries from it convert that into a quiz\n2:20:01 app right let''s say this is a general knowledge book I can probably convert this into a quiz app with four options\n2:20:08 and get the right kind of answer from it right so this is what I''m planning to do other than this any idea you have you\n2:20:14 can probably do it on top of it right so first first thing first what is the first thing that we really need to do\n2:20:20 over here is that create a environment right and this is in every project I at\n2:20:26 least make you do this because it is super beneficial because at the end of the day with respect to every project\n2:20:32 you need to probably create a separate environment so in order to create an environment I will go ahead and write\n2:20:37 cond create cond create minus p v andv will\n2:20:44 be my uh environment name and then here I''ll basically going to use Python 3.10\n2:20:50 right so once I execute it it''ll ask me for an option whether I need to install or not I will just say why and go ahead\n2:20:57 with the installation so this is the first step that we should specifically do right and uh it is important step\n2:21:03 because at the end of the day you should definitely create a different environment don''t always work in the same environment whenever you are\n2:21:10 actually working in this kind of projects the second thing that I''m probably going to do is that create my\n2:21:15 requirement. txt requirement. txt the reason because\n2:21:22 whenever I''m using an lmm model or anything as such I have to probably install a lot of packages so over here\n2:21:29 first of all I will go ahead and cond activate activate this specific environment V NV slash okay so this is\n2:21:37 done the environment is activated and we are ready to go right now from my\n2:21:43 requirement. txt what are things I''m basically going to use I''m going to to probably note down all the requirements\n2:21:49 over here the libraries that is unstructured Tik toen pine cone client P\n2:21:54 PDF open aai Lang chain pandas numai python. EnV and at the end of the day\n2:22:01 guys uh I would always suggest you to please understand about Lang chain Lang chain is an amazing Library it has lot\n2:22:07 of power lot of functionalities which you can specifically do the community is huge and many many companies are\n2:22:13 specifically using it okay so requirement. txt has been saved so I will quickly go ahead and install this\n2:22:21 requirement. txt so probably it may take some time and before that uh since it is\n2:22:26 probably installing what I will do I will also go ahead and create my EnV file right so in this ENB file what I''m\n2:22:34 actually going to do I''m going to put my open API key so quickly I''ll create my\n2:22:40 open AP API key save it and start using it okay so this will basically be my\n2:22:46 open API key in the EnV so that I can basically load it so along with this python. EnV is also there so let''s wait\n2:22:53 till the all the libraries has been installed okay so this is the initial steps that we should specifically do\n2:23:00 right our environment is ready we have installed all the libraries that is probably required we have also kept our\n2:23:06 open AI key because at the end of the day I''m specifically going to use Lang chin you can also do it with hugging face if you want but I will try it with\n2:23:13 open AI because the accuracy is pretty much better in this okay now the next step uh what I''m quickly\n2:23:20 going to do is that quickly create one\n2:23:25 file and here I will just show you test ipynb and this will basically be my\n2:23:33 ipynb file itself and here I''ll be showing you the entire code later on you can probably convert this into an end\n2:23:39 to-end project you can probably create a streamlet app but here is the main thing that I''m probably going to show you by\n2:23:45 executing step by step and and what all things are specifically required to create this app again understand what I\n2:23:51 am planning to do right so I will just show you over here first of all let me just clean this screen and what is my\n2:23:59 entire agenda like what I really want to create as an llm application over here\n2:24:04 so I have a PDF okay I have a PDF so this is basically my PDF you can also\n2:24:10 say this is a data source it can be a GK book it can be a maths book it can be anything right I will will first of all\n2:24:18 load this document right once I load this document or read this\n2:24:24 document what I am going to do is that I''m going to convert this into chunks\n2:24:29 right because we cannot open AI hugging face models they have some restriction with respect to the Token size so I''m\n2:24:36 just probably going to create chunks and this is what we say it as text\n2:24:42 chunks right after this I''m going to use open AI embedding\n2:24:48 okay and this embeddings will be responsible in converting all this text CHS into\n2:24:55 vectors right so this will basically be my vectors I hope you know what exactly\n2:25:00 is vectors a numerical format for different different text right so this will specifically be my vectors and this\n2:25:08 I''m going to basically do with open embeddings further this vectors needs to be stored in some Vector search DB so\n2:25:15 here I will put all these vectors in some kind of vector surge DB now why\n2:25:21 this DB is required because at the end of the day whenever a human being queries any\n2:25:26 inputs right because of this Vector search DB here we can apply similarity\n2:25:35 search right and probably get any kind of info that I specifically want so this\n2:25:41 is my what my entire architecture of the specific project looks like right and here this Vector search DB I''m probably\n2:25:48 going to use something called as pine cone and they have lot of DBS I will talk about the advantage and disadvantage there is some amazing DBS\n2:25:55 called as data Stacks where they specifically use cassendra DB Pine clone is one right we''ll see all the\n2:26:02 documentation page with respect to this okay so step by step I''m going to basically do this and you can actually\n2:26:07 do it for any number of pages one more thing that I''m probably going to install is IPI kernel since I''m going to work in\n2:26:15 my Jupiter notebook okay so this all steps are basically happening it is very much good and we\n2:26:21 are able to see this okay so let this installation happen and then I will set\n2:26:26 up my kernel okay so these are the initial steps that we really need to focus on and uh understand this project\n2:26:34 will be the base to create any kind of chatbot application mcqs quiz apps okay\n2:26:40 question answering chat Bots not only question answering chat Bots text summarizer anything that you probably\n2:26:46 want right or you can also basically say it as a chat B that gives you specific\n2:26:51 answer with respect to specific domain right with respect to the data that we have so all these things are done now\n2:26:57 I''m going to probably select the kernel V EnV python 3.0 right I''m going to save\n2:27:03 this perfect now the first step as usual I will go ahead and import start\n2:27:09 importing libraries and now we will do it completely step by step okay so what\n2:27:15 all things we basically require I''m going toire open a I''m going to import\n2:27:21 Lang chain uh apart from open and Lang chain what I''m going to also going to do go\n2:27:27 ahead with pine con I will talk about Pine con more when I probably show you the documentation apart from Pine con\n2:27:34 I''m going to basically go to Lang chain okay and I''m going to basically use\n2:27:39 something called as document loader document loaders will basically\n2:27:45 be responsible for for loading any kind of documents it can be a PDF file and all so for PDF we specifically use\n2:27:52 something called as P PDF I can also use directly Pi PDF loader but since my PDF\n2:27:57 is inside the directory I''m going to use Pi PDF directory loader okay so this is the next thing now from the next thing\n2:28:05 what we need to do is that as soon as we load any PDF we will get all the documents we have to basically do uh\n2:28:11 text splitting right because we really need to convert those into chunks we cannot take the entire token right there\n2:28:17 will be a restricted token size right like uh recently open has come up with\n2:28:22 the open 4. 4. o turbo right so I think it is GPD sorry GPD 4.0 turbo there 188\n2:28:31 128k is the token size right so for that I''m going to basically use from Lang\n2:28:36 chain dot text spitter I''m going to probably import recursive character text\n2:28:43 spitter you can also use any other based on this right my always suggestion would\n2:28:48 be that go ahead and check out the documentation with respect to all the libraries that I''m probably uh uploading\n2:28:54 right now the next thing is that whenever I probably convert this into chunks right the next thing that I need\n2:29:00 to probably convert that into vectors and for that I''ll be using some kind of embedding techniques so over here we are\n2:29:06 going to basically use do embeddings\n2:29:11 doop so it is embeddings do openai and we are going to import open AI embedding\n2:29:18 so open AI embeddings is a technique wherein it will probably convert any chunk into vectors right so this is the\n2:29:25 next step now the next step is basically also to uh import a library that will be\n2:29:31 responsible in creating a vector DB with respect to Pine call or we also say it as Vector store so here I''m going to\n2:29:38 basically use from Lang chain\n2:29:43 dot vector dot I think Lang chain dot it should be\n2:29:51 dot I''m writing comma I don''t know why Vector oh spelling is mistaken do Vector\n2:29:58 stores okay and here I''m basically going to import I think it will be there pine\n2:30:05 cone right so I''m basically going to also use this pine cone pine cone will be our Vector store uh later on we can\n2:30:12 integrate this with our Vector DB that is present in Pines conone so the next thing is that I will also\n2:30:18 import our llm model because we will be requiring llm\n2:30:23 import open AI right so all these libraries I''m going to specifically use\n2:30:29 it I will quickly go ahead and execute it let''s see if everything works fine you may get some kind of warning but\n2:30:34 it''s okay right so this is your initial load that you are specifically\n2:30:41 doing now you know that I have an environment variables that is with respect to hugging pH so what I''m actually going to to do I''ll go ahead\n2:30:48 and write from EnV import import load. EnV so this will\n2:30:56 specifically load all your environment variables okay so whatever environment\n2:31:01 variables that you have with respect to open API key or anything that is required you can basically do this right\n2:31:07 now I will also be importing OS over here right OS we can specifically use later on okay quickly now the first step\n2:31:15 as I said we need we have a PDF file we need to read it right so now I will write let''s\n2:31:24 read the document okay now first step while reading the document I''ll create a\n2:31:30 function so that I can reuse it and I''ll write read Doc and here I will give my\n2:31:35 directory I can also give my file for that what library will be used Pi PDF\n2:31:41 loader right over here I''m using directory loader since I have to probably give my directory name and then\n2:31:47 I will basically write file _ loader and I will initialize my P PDF directory\n2:31:52 loader and basically give my directory path over here okay so directory path\n2:31:59 over here right so as soon as I give my directory path it will go to this specific directory and it will see\n2:32:04 whichever PDF is there it will start loading it okay and then I will go ahead and write file uncore loader dot load\n2:32:14 right now see why I''m showing you this step by step because everybody should understand what steps we are be doing it\n2:32:20 later on to convert this into a modular code it will be very much easy that is the reason I''m writing it in the form of\n2:32:25 functions all these functions will go into your utils.py file okay and finally\n2:32:31 you can see over here I''ll get file loader _ load that basically means it is going to load all the documents and here\n2:32:37 I will basically be getting my documents right and finally we will return this\n2:32:43 documents done right now let''s check check if everything is working fine so\n2:32:48 here I''m going to basically write read uncore Doc and this will basically be\n2:32:53 returning my document and here I will give my document folder so let me just\n2:32:59 go ahead and write documents in string right so this will basically be my directory\n2:33:06 path okay and now if I execute what is this doc it will probably read all the\n2:33:12 docs that are probably there now see every page by Page content this this is my first page second page third page\n2:33:18 fourth page fifth page like this I have 54 pages in my PDF right 54 Pages now if\n2:33:24 you also write length of Doc here also you''ll be able to see it right so length\n2:33:30 of Doc I''m going to get 58 so that basically means we have done this first step right we have loaded we have read\n2:33:37 this particular PDF right now the next step is basically with respect to dividing these documents into text\n2:33:43 chunks okay so this is what we are probably going to do in our step two but till here everything is working\n2:33:50 fine so guys now we have finished reading the document uh now what we are basically going to do is that we are\n2:33:56 going to convert this into chunks right now for converting this into chunks what\n2:34:02 we are specifically going to do let''s see so here I''m going to write the code here I''m going to basically say divide\n2:34:08 the docs into chunks and again because of the model restriction of the token size\n2:34:14 we really need to do this okay so here I''m going to basically use defin I''ll create a function which is called as\n2:34:20 definition chunk data here first thing I will basically give my docs then I''m\n2:34:27 going to basically mention my chunk size right so let me go ahead and mention my\n2:34:32 chunk size my chunk size I''m going to mention it as 800 you can also mention it as 1,000 right don''t keep a very huge\n2:34:40 value and then I can also say what about the chunk overlap Okay so so the chunk\n2:34:46 overlap like 50 characters can overlap with from one sentence to the other sentence right so here the next thing\n2:34:53 I''m going to basically create a text splitter and that is where we going to basically use this recursive character\n2:35:00 text splitter so first first thing first I going to mention my chunk size which will basically be my chunk size itself\n2:35:08 and along with this my chunk overlap which will be nothing but the\n2:35:15 chunk overlap that I have basically mentioned great uh so here I get my text\n2:35:21 splitter and now I''m going to basically up take this text splitter and split all\n2:35:26 the documents based on the kind of splitting that I''ve actually mentioned so here basically I''ll provide my docs\n2:35:33 as my parameter and then I will convert this into and I''ll return this docs\n2:35:39 perfect if you want to know more about this chunk uh recursive chunk splitter uh sorry recursive character splitter\n2:35:46 you can probably check this out documentation also I''m going to specify\n2:35:51 over here uh this documentation is good to understand what exactly it does and all right so perfect this is my chunk\n2:35:59 data function now what I''m actually going to do is that quickly use chunk\n2:36:04 uncore data and try it on my docs file right so here I''m going to basically\n2:36:10 mention my docs docs is this I''ve actually got this docs is equal to\n2:36:18 Doc okay and let''s see so this will basically be my documents it is just\n2:36:24 going to apply all this right it is going to convert that entire document into a chunk size of 800 with the\n2:36:29 overlap of 50 okay and probably if I go and see my documents you''ll be able to see it now see every document has now\n2:36:37 been properly mentioned right and the document that we are specifically reading is the Indian budget document\n2:36:44 right so any question that I proba asked related to Indian budget I''ll be able to get the answer done this is good if I\n2:36:52 probably want to see the length of the documents also I can also see it okay just to give it get an idea like 58 is\n2:36:58 the length okay great so this is done uh now the next step what I''m actually\n2:37:04 going to do is that I''m going to also initialize my embedding technique so embedding technique of open AI right so\n2:37:14 we are going to initialize this so here I''m going to mention embeddings is equal to open Ai embeddings and here I''m going\n2:37:22 to use my API key OS do environment I can directly use\n2:37:29 os. environment and I can mention what is my API key so here I can say\n2:37:35 opencore open AI uncore API\n2:37:41 underscore key right so this is what is my embedding so let let me just\n2:37:47 quickly see what exactly is my embeddings so here is my embeddings that I''m going to probably use and this is\n2:37:54 what is basically used to convert that text into vectors right so quickly we\n2:37:59 have done this uh then I''m going to probably create my vectors let''s say Let''s test any vectors with this\n2:38:05 embeddings okay uh and there are various other embedding techniques like one h word to uh average word to and all but\n2:38:12 uh in open AI embeddings this provides you a much more advanced technique okay and over here also it will provide you\n2:38:18 some kind of vectors like there will be some Vector size also with respect to this okay like every every sentence will\n2:38:25 be provided with with respect to a vector size so here if I want to probably check and write embed\n2:38:31 underscore query and just test it with respect to anything like how are you\n2:38:37 right and let''s see what kind of vectors we will probably get so this is my vectors that I''m actually getting see\n2:38:44 the entire text and if I want to probably check the length of these vectors it will also give you the length\n2:38:49 of this vectors okay so it is some something around 15 36 and this length\n2:38:55 will be super important because at the end of the day where I probably create my Vector database I have to specify\n2:39:00 this length okay for my problem statement now great now let''s create our\n2:39:06 Vector search DB and pine code okay and now this step is very much important uh\n2:39:13 because after this particular step we will be able to see what kind of vector database we probably get okay Vector\n2:39:19 search DB in Pine con so let''s go ahead and let''s quickly create this Vector DB\n2:39:24 okay so guys here is the pine cone documentation you can probably check it out uh get started using pine con\n2:39:30 explore our examples this this is there what is the main important information\n2:39:35 about this Pine C is that it definitely helps you in semantic search in chat Bots also it helps you right where it\n2:39:42 probably helps you to store the entire Vector right and it provides you generative QA with open integration Lang\n2:39:49 CH leral argumentation uh rag also we basically say open integration and it has multiple\n2:39:56 uses okay so if I probably show you one of the document or guide right so if I\n2:40:01 probably go ahead and click on the guide right and this is where we really need to create the vector database I''ll show\n2:40:06 you the steps of creating it right so Pine con makes it easy to provide long-term memory for high performance AI\n2:40:12 application it is managed Cloud native Vector database with a simple API no infrastructure has less pine cone serves\n2:40:19 fresh filtered query results with low hency at a scale of billions of vectors so if you have a huge data set you want\n2:40:25 to probably work with the vectors you can probably store it over here in the form of vector database um what is the\n2:40:31 importance Vector embedding provides long-term memory for AI Vector database stores and queries embedding quickly at\n2:40:36 a scale you know so anytime it is probably saved if you''re quering it you will be able to get the response very much quickly now first thing first how\n2:40:44 you have to probably create this okay so if you once you log in once you log in over here or sign up you''ll be able to\n2:40:51 see this okay so and here I''ve already created one index okay but this index will not work because see uh in the free\n2:40:57 tire right it allows you to just create one index so I will just delete this and show you how to probably create it\n2:41:03 completely from scratch so this is the name so I''m I''m going to delete this index because at the end of the day\n2:41:10 whatever vectors you are probably storing it will start indexing it okay so this is terminated now we will go\n2:41:15 ahead and create a new index so for creating a new index uh what I have to probably do is give a name so let''s say\n2:41:21 I''m giving langin Vector now this is super important configure your index the dimensional metrix depends on a Model\n2:41:26 you select right now based on a Model if I probably uh see to it right so here\n2:41:32 you''ll be able to see what is the length that I was able to get from my embedding 1536 so this is what is the dimension\n2:41:39 that I''m also going to give it over here and since I have basically doing cosine similarity kind of Stu or you can also\n2:41:45 do Dot product ukan but I''ll stay to cosine similarity because at the end of the day the similarity search that is\n2:41:50 probably going to happen is with the help of cosine similarity and then finally we go ahead and create the index\n2:41:56 this is the main thing that you probably need to do this is basically getting terminated and this was the data that I\n2:42:02 had actually inserted but again we will do it okay so if I go to back to uh um\n2:42:08 back to probably a index let''s see why it is not created or still it is showing\n2:42:14 terminating it should not not show terminating but at the end of the day because one I have already deleted it or\n2:42:20 I can just change the name if I want so but it is created over here okay Lang chin vector and it is a free Tire in the\n2:42:26 case of free tire they will provide you this thing right now from this there are some important information that you\n2:42:32 really need to retrieve one is the environment one is the database name\n2:42:37 okay so what sorry index name so I''ll go back to my code here you''ll be able to\n2:42:43 see Vector search DB and pine code let''s let''s initialize it I I''ve imported P\n2:42:48 cone I will say do in it okay so do in it basically does the initialization\n2:42:53 here two information are required API key okay API key with some information\n2:43:00 comma the next thing that I probably require is environment okay so environment something is required so\n2:43:06 let''s retrieve this two information along with this what I can also do I have to also provide my index uncore\n2:43:12 name so here I will specifically say my IND Indore name index name I''ve already\n2:43:17 copied it from there so it is nothing but langin Vector let''s go and see where is the API and environment so here I''m\n2:43:24 going to go back if I go and see there''s an option of API keys so this is basically my API key I will copy it and\n2:43:31 I will paste it over here okay so Ive pasted my API key over here now the\n2:43:36 environment thing where will I get my environment so if I go to indexes and if I click this this is the environment\n2:43:43 that I''ll be able to get it right so this two information is specifically required I will paste it over here right\n2:43:50 so this two information is done by executing it my Vector search DB will get initialized over there but at the\n2:43:56 end of the day I need to put all these embeddings specifically all these embeddings uh in my Vector DB right so\n2:44:03 over there I will again use pine cone pine cone which I have actually initialized and I''ll say from\n2:44:11 documents and here I will give all my Docs so from documents I''m going to\n2:44:16 first of all give my doc parameter over here the dock which where which I need to probably store it in my Vector DB and\n2:44:22 then I will go ahead and write embeddings embeddings what kind of embeddings that I''m specifically giving\n2:44:27 is the same embeddings that we probably created and then I have my index name is equal to whatever index name I have\n2:44:34 basically initialized so as soon as I probably execute this you will be able to see that I will be able to get one\n2:44:40 index over here okay so let me just go ahead and execute it it''ll probably take some amount of time because I have a\n2:44:46 huge data over there but you''ll be able to see the changes once I probably go over here okay so if I go ahead and\n2:44:54 click it and if I refresh it let''s see okay whether we''ll be able\n2:45:02 to see everything or not whether the data part and all we will be able to see so here you can probably see query by\n2:45:09 vector and all all the data is there if I want to probably see the vector count it is 58 because the document size that\n2:45:15 you could probably see is 58 right and these are all the information you can see over here all the data has been\n2:45:23 basically stored and based on the metadata you can probably see it it has already done the indexing now any query\n2:45:29 that you probably hit it over here in the form of vectors you''ll be able to get those specific response okay now to\n2:45:35 do the query part what I will do I will apply a cosine similarity so I will say cosine\n2:45:42 similarity retrieve results okay so I will try to retrieve the results over\n2:45:47 here so here let me just go ahead and write definition retrieve uncope\n2:45:55 query and here I will say query K is equal to 1 let''s say k is equal to 2 I I\n2:46:00 will probably see the top two query now the second thing is that if I want to probably query I will get the matching\n2:46:07 results whatever matching results I''ll use the same index dot whatever index is over here dot similarity\n2:46:15 let''s see index dot\n2:46:21 similarity unor search right the similarity _ search what I can do is\n2:46:28 that let''s see what function I''ve actually made for similarity _ search\n2:46:35 because I need to probably get those documents also right so basically uh I\n2:46:41 need to also create one more function over here right and that function will basically be my similarity underscore\n2:46:46 search that basically means what is my result that I''m probably going to get\n2:46:51 right inside this so index. similarity unders search is a function that is\n2:46:57 probably present inside this index itself right so similarity unders search\n2:47:02 and here I will probably give my query comma K is equal to some K value okay so what is the k k is equal to 2 whatever\n2:47:08 results I''m specifically getting and here I''m going to return my math\n2:47:15 matching results okay matching results so once I execute it so this is\n2:47:21 basically my retrieve query uh example so any query that I specifically give\n2:47:27 with respect to that particular PDF I''ll be able to get the answer now this is fine this is the function to get the\n2:47:34 data from the database itself right so cosine similarity retrieve results from Vector DB I''ll write it over here so\n2:47:40 that you will be able to understand okay so done this is the function that is basic Bally required now two important\n2:47:46 libraries that I''m going to probably use one is from langen Lang chin. change. question answering I''m using load\n2:47:53 question answering chain and along with this I''m also going to use open AI open AI will specifically be used to create\n2:48:00 my model and here my llm model will be created with the help of this so here I have written open a model name will be\n2:48:06 tax uh text Dy uh 003 and I''ve taken the temperature value as uh this one and\n2:48:12 then I''m also going to create my chain where I specific ially use this load QA chain and load Q chain actually helps\n2:48:18 you to create a question answer application and then here I will go ahead and write chain type is equal to\n2:48:25 stuff okay so my chain is ready my llm is ready everything is ready now all I\n2:48:31 need to do is that retrieve my queries right for retrieving the queries I will specifically be using this retrieve\n2:48:37 query function also so let''s go ahead and write my definition I will go search\n2:48:43 answers from uh Vector database Vector DB okay and\n2:48:50 here I will basically write definition retrieve answers and\n2:48:55 whatever query I specifically give over here based on that I will should be able to get it right so if I probably see doc\n2:49:02 search then I will write doc search is equal to I''ll call that same function retrieve query with my query that I''m\n2:49:09 actually going to give and then I will also print my doc search okay whatever\n2:49:16 print I''m basically getting now whenever I get that specific doc search I should\n2:49:21 also run this chain chain that I''ve actually created right to load QA chain so here I''m going to basically say\n2:49:27 chain. run and inside this what will basically be my input documents so I\n2:49:32 will say input input documents is equal to I will\n2:49:41 give my doc search right the document that I''m probably going to search and then the next will be my question which\n2:49:47 will be in the form of query right the query that I''m actually getting so once I do this my chain will basically be\n2:49:53 running and it will provide you some kind of response that it probably gets gets from the vector DB okay if it\n2:50:01 matches right and then we are going to return this specific response done now see the magic okay\n2:50:10 once I probably call this function what will happen so here I will write our query okay so the query will be I I saw\n2:50:18 something right I I basically so this will basically be my retrieve answer okay now see this I read the PDF I could\n2:50:26 find one question over there how much the agriculture Target will be increased by how many cores right how much the\n2:50:33 agriculture Target will be increased by how many cores I have just written this kind of statement now this retrieve\n2:50:38 query as soon as we call it will basically sorry retrieve answer as soon as we call okay so here I''ll just make\n2:50:44 make this function change also the spelling should be right right retrieve and here also I''m going to\n2:50:52 probably make this to retrieve answers okay now as soon as I give my query over\n2:50:57 here it''ll go to retrieve query it''ll see from the index right it''ll probably do the similarity search it''ll give you\n2:51:03 the two results itself so let me just go ahead and see the answer over here now\n2:51:09 retrieve answer is not defined why okay I have to execute this sorry now it should be able to get the answer see so\n2:51:17 agriculture credit Target will be increased to 20 lakh CR with an investment of so and so information I\n2:51:22 I''ve just put right uh I can also write any other question\n2:51:29 how is the agriculture doing\n2:51:34 right so I may get some kind of question answer if it is not able to get something then it will say I don''t know\n2:51:40 okay the government is promoting corporate based this this this this I''m getting that information from the entire\n2:51:46 PDF itself right so this is how beautifully you can probably get this and now it is probably asking being\n2:51:51 acting as a question answer application right now see this is what is the base\n2:51:58 you have a vector DB you''re asking any question and you''re getting some kind of response now on top of that you can also\n2:52:04 do a lot of prompt templating you can probably convert this into a quiz app you can convert it into a question\n2:52:10 answer uh chatbot you can convert this into a text summarizer you can do what whatever things you want right and this\n2:52:17 is what is the specific power over here right and this is really really good and\n2:52:22 that is what I''m probably going to show you in the next video on top of it like how can I probably do a custom prompt\n2:52:29 templating on my llm application this is what I''m probably going to show you in the next video but here i'' shown you\n2:52:36 about what is Vector database and why it is very much important what exactly how\n2:52:42 the vectors are basically stored and here you can probably see see this is how your vector is stored if I probably search for any Vector over here I''ll be\n2:52:49 able to get those kind of response over here right based on that uh search itself so start using this many\n2:52:55 companies are basically using this at the end of the day just for your practice sake it is completely for free\n2:53:00 right but if you have if you want more than one indexes two indexes then at that point of time you can probably take\n2:53:05 a paid tool paid version that specifically requires in a company itself so I hope you are able to\n2:53:11 understand this Amazing Project hello all my name is rush nyak and welcome to my YouTube channel so\n2:53:17 guys just a few days to end this specific year this month was amazing\n2:53:23 because I was able to upload many many videos related to generative AI many\n2:53:28 people had actually requested it uh the reason is very simple because all the students who have made successful career\n2:53:34 transition into the data science Industry they''re working in different different llm projects so I hope you''re\n2:53:40 liking all those videos that I''m specifically uploading um again as requested if you like this\n2:53:47 particular videos and all please do make sure that you subscribe the channel press the Bell notification icon hit\n2:53:52 like this will motivate me to upload more videos as we go ahead so it is a\n2:53:58 sincere request please do that and share with many people as uh many people as\n2:54:03 possible by you the reason is that all these videos is completely for free uh my main aim is basically to teach you in\n2:54:10 such a way that where you understand each and everything in depth Let It Be theoretical intuition practical\n2:54:15 intuition and many more right so what are we discussing in this specific video so I hope everybody has heard about um\n2:54:24 gini right gini API that was probably gini model that was launched from Google\n2:54:29 obviously the demo that they had actually shown on the website it was not true demo they had made made up that\n2:54:35 particular demo but now the gini pro model is available for you and you can also use it for your own demo purpose uh\n2:54:43 we''ll understand we''ll see a complete demo like how these models are these large language models are and how it is\n2:54:49 probably performing we''ll be seeing various codes I''ll be showing you that how you can probably create an API for\n2:54:55 this and the best thing is that with respect to this they have come up with this two different plan you know one is\n2:55:01 free for everyone the thing the model the gini pro model which is free for everyone it\n2:55:08 has a rate limit of 60 queries per minute that basically means within a minute you can actually just hit 60\n2:55:14 queries price is free Price output is also free input output is free and you\n2:55:19 can start using this uh to just get to know like how powerful the gini pro is\n2:55:25 and I probably used it it looks pretty much good you know um at least uh not\n2:55:31 like that fancy thing what it showed in the demo not like that but yes we can compare with chat GPT or we can compare\n2:55:38 with open AI apis which perform various tasks like test summarization uh text\n2:55:44 classification or let''s say other tasks like Q&A and many more right so the next\n2:55:50 uh plan that is probably go uh that is going to come which can be specifically used in industries that is pay as you go\n2:55:56 and that basically starts for more than 60 queries per minute okay so here is is\n2:56:01 the entire information I will be providing the link with respect to the description in the description of this particular video now let''s go ahead and\n2:56:07 check the documentation and one by one I will show you how you can probably create the API the API Keys how you can\n2:56:13 probably load it and start using it okay so first of all just go to the python section over here and I''m just going to\n2:56:20 click on this Google collapse so as soon as I probably clicked on this Google collap you can probably see over here\n2:56:25 I''ve got this entirely okay so we will go ahead step by step we''ll understand each and every line of code we''ll see\n2:56:31 multiple examples and then let''s see how good the Gemini API is okay so as\n2:56:38 suggested as I told you already this is like Gemini has come up in three different version and gini Pro is right\n2:56:44 now available for you okay where you can probably try and uh just by seeing this things right I I definitely I''ve already\n2:56:51 tried it out I feel that yes it is good enough right um if I compare with any\n2:56:57 openai apis yes it is doing a fabulous task okay I know the demo was bad but\n2:57:03 here the things that I''m going to execute is very much good so to get this particular notebook file all you have to\n2:57:09 do is that click on python over here and just click on this okay so this is the entire information\n2:57:14 how you can probably create the API key and all I will be showing you step by step okay so let''s start this video\n2:57:20 before that please do hit like as you know that I uh I I always thought that\n2:57:25 within this year right I may reach 10 lakh subscribers but I could not but it''s okay for the next upcoming 3 to\n2:57:31 four months I would definitely like to reach 10 lakh subscribers okay 10 lakh\n2:57:37 subscribers 10 lakh is a very good number right and just imagine how much beneficial it is for so many people out\n2:57:43 there so so let''s go ahead and start this okay so we will go step by step I\n2:57:48 know please try to understand this things because I will be explaining you the code what exactly gini API is all\n2:57:56 about what this gini model why it is so good because this is a multi model right multimodel that basically means it is\n2:58:02 trained on both text and images so I''ll show you if I probably give a image and it will be able to read that image and\n2:58:08 it''ll be able to give you the information about that image that powerful the specific model is okay so\n2:58:14 uh we''ll set up our development environment and get the API access to gini so this is the entire agenda we\n2:58:19 will generate text response from text inputs we will generate text responsive for multimodel inputs and then we''ll use\n2:58:26 gmin for multi- turn conversation and we''ll also see how different embeddings can be applied for\n2:58:32 large language model okay so first of all this generative AI entire there is a\n2:58:39 library which is called as Google generative AI with the help of API you can use various features that is available in this gini model right gini\n2:58:46 pro model so first of all I will go ahead and install this entire library that is Google generative AI okay with\n2:58:52 the help of the specific code it may take some time again it depends on your internet speed how good it is uh and\n2:58:58 then after we probably go ahead and install this so import the specific library now here are some of the\n2:59:05 important things that we are going to do okay so first of all you''ll be able to see we are importing path lib text WP\n2:59:11 Google generative AI as geni so this is the allias that we are going to specifically use and this is the library\n2:59:18 that has almost all the features over there from google. collab you import user data inside user data you can store\n2:59:25 your API key okay I will show you how to how you can probably do it along with this I''m probably importing display and\n2:59:31 markdown to display the information and there is another method which is called as definition to underscore markdown\n2:59:38 that basically means whenever I get any kind of response I''m going to replace this dot by star because I think this\n2:59:45 generative a is gini pro gives some kind of response where it will be having this kind of output okay and then we are\n2:59:52 going to convert that entire data with respect to markdown okay so all these things we are specifically importing now\n2:59:58 let''s go to our next step setting up your API key so for do doing that just click on this particular link okay so it\n3:00:05 will go to this makers suit google.com app API key you can also create Palm apis from here and all right so after\n3:00:12 going over here what you can do you can actually go ahead I''ve already created one particular API key over here which\n3:00:19 you can actually see if you want to create a new API key just go ahead and click on this as soon as you click on\n3:00:25 this you will be getting the API key just copy from there and here what I have done I have written the API key\n3:00:30 over here okay so don''t worry that uh this API key is not right uh I''ve\n3:00:35 already executed or I''ve already stored that particular API key itself right so probably when I share this particular\n3:00:42 notebook with you you''ll not be able to see this API key over there until then I may have also deleted it from here okay\n3:00:49 so the reason don''t use this do it with your own because it is completely for free right 60 queries you can actually\n3:00:55 hit it now let''s go ahead and do one thing first of all I''m creating an environment\n3:01:01 variable called as Google API key okay so this will basically be my API key\n3:01:08 okay and I will remove this I will not require this and then what I''m doing over here I''m I''m saying gen. configure\n3:01:16 API key and I''m using the environment variable and I''m setting that API key to this okay so once I do this done that\n3:01:23 basically mean my API is configured this is the simple steps that you specifically need to do now what all\n3:01:30 models you specifically get from Gemini API so over here there are two models\n3:01:36 one is Gemini Pro this is optimized for text only prompts so text summarization Q&A chat anything that you specific Al\n3:01:43 want to do you have to use this specific model that is Gemini Pro and then you\n3:01:50 have one more model which is called as gini provision this is optimized for text and image prompts okay so for text\n3:01:58 and image prompts specifically you can use gini pro version if you want to check how many different models are\n3:02:04 there just write for M gen. list uncore model list underscore models will give you all the models that it is basically\n3:02:12 having and it is saying that if generated content in M supported generation methods then it will show you\n3:02:17 all the both the model name so here you can probably see that as soon as you execute this you''ll be getting that you\n3:02:24 have two models one is gini pro and then the other one is Gemini Pro Vision now\n3:02:29 let''s go ahead and let''s try something okay so first of all I''m going to use\n3:02:35 the gemin pro model this is specifically for text related task okay so here I''m\n3:02:41 going to use generative AI gen AI dot there is a method which is called as\n3:02:46 generative model and there I''ll be giving my model name so once I execute this this basically becomes my model so\n3:02:52 if I probably go ahead and execute over here so here you''ll be able to see that\n3:02:58 I am getting a model trust me guys this is good enough because if you don''t have open API key also you can use this and\n3:03:06 when you use this you''ll be able to understand more about the llm models okay so now let''s go ahead and use one\n3:03:14 method which is called as generate content this generate content method can handle a variety of use cases including\n3:03:20 multi multi-t chat multimodel input depending on what the under underlying\n3:03:26 model supports so here I''ve used gin Pro so in this particular case I am going to\n3:03:32 specifically use it for text related task okay so now here we use model dot\n3:03:37 generate content and here I''m giving the message what is the meaning of life okay\n3:03:43 so this is the default message once I execute this you see the response time the response time with respect to this\n3:03:49 particular time it will go ahead and capture so uh and also remember guys uh\n3:03:55 when I was executing some some some time before you know this was really really fast right now it is a little bit slow\n3:04:03 is it a problem internet connection I don''t know okay so but here you can see user time it tooks 120 millisecond\n3:04:09 system time was 10.5 millisecond total time was 131 seconds which is very good\n3:04:15 like open AI API speed I think it''ll take more than this okay that I specifically know now in order to see\n3:04:22 the response I will use this 2core markdown and we will write response. text as soon as I write this you will\n3:04:28 now be able to see the output of this right what is the meaning of life so here you can see that the meaning of\n3:04:35 life is a philosophical question that has been posed by human for centuries so and so blah blah blah blah all the\n3:04:43 information is very much easily given and it looks good you did not have to do\n3:04:48 any other prompt template techniques to probably put it in this dotted point it\n3:04:53 has considered in a better way yes you can again use prom template also for this purpose okay so if the API and now\n3:05:01 this is the most important thing which I like about gini Pro okay if the API failed to return a result use generate\n3:05:08 content response. prompt feedback now you may be thinking why the API May Fail\n3:05:14 it may be because of some of the other reason to see it was blocked due to safety concerns okay because of safety\n3:05:19 concerns also it may not give you some response now what may be that safety\n3:05:25 concern okay so first of all we will go and see response. prompt feedback when I\n3:05:31 probably execute this I also have these all features that are probably coming up this information in the response safety\n3:05:38 rating category harm category sexually explicit probability neg eligible so you\n3:05:44 can see it is categorizing based on this four important categories whether it is\n3:05:49 a hate speech whether it is a harassment whether it is a dangerous content right\n3:05:56 and here you can probably see that it is also giving that particular feedback if any of the feedback is positive I think\n3:06:02 you may not get a response so let''s go ahead and execute this particular code\n3:06:07 and let''s see whether this works true or not okay I definitely want to check it out so I will go ahead and execute and I\n3:06:15 will say um I will say Okay I I did not I do not mean anything to write anything\n3:06:22 bad over here but uh I would just like to see how to insult someone let''s\n3:06:30 see okay I''m just trying I''m not I I''m I don''t mean to but let''s see so here\n3:06:38 you''ll be able to see that whether I''ll be able to get a response or not and again over here now now it has becomes\n3:06:44 fast right so now let''s go ahead and do this see the response part quicker ex\n3:06:51 only works for a single C but none were returned now I will go ahead and check this prompt feedback now this is where\n3:06:59 block reason safety see over here harm category harassment medium so this is\n3:07:05 what is so good in this ethics is definitely there right so it is not\n3:07:11 giving you any kind of resp resp right so I hope you have understood the importance of this okay now similarly G\n3:07:20 gini can also generate multiple possible responses for a single prompt okay it can also generate right so for that you\n3:07:27 just need to use this responses. candidates now what I will do I will not execute this but instead we will go\n3:07:34 ahead and execute this what is the meaning of life okay okay so let''s\n3:07:40 execute this it''s okay um or I I''ll just go ahead and ask\n3:07:46 a different question can you let me know about\n3:07:53 the future of future\n3:08:02 of generative AI okay so I''m asking this specific question now let''s see I''m not\n3:08:10 executing this now it should definitely give me some kind of response and this also depends on the\n3:08:17 kind of like amount of tokens that is probably coming from the llm model so here it has taken 122 milliseconds now\n3:08:24 let''s see the prompt feedback I think this is absolutely fine okay and here I\n3:08:30 will also go ahead and see my text it''ll give so here you can see the future of generative AI holds immense potential\n3:08:36 and Promises to revolutionize various style Gan gpt3 this this is there now if\n3:08:42 I go ahead and right response. candidates okay now here you can\n3:08:47 probably see that I''m getting the entire info this is so nice see so this is my\n3:08:53 first text right role model find this this this all the information is there with respect to all the information that\n3:08:59 you will be able to see that right all whatever thing we executed step by step we getting everything okay so this is\n3:09:07 very very good okay now by default the model returns a response after completing the entire generation process\n3:09:13 you can also stream the response as it is being generated and the model will return chunks of the response as soon as\n3:09:18 they are generated which is absolutely amazing again right so what we can also\n3:09:24 do is that you can stream the response okay no need to wait for the entire response so let''s see okay so here I\n3:09:30 will use the same question let me see so I will so can you let me know the future\n3:09:36 of generative now I think for this it will not take much time when compared to\n3:09:42 the previous one one that we had done right so here I will go ahead and write all you have to do is that just write\n3:09:48 over here let me just show okay you just need to add a\n3:09:54 parameter which is called as stream is equal to True by that you''ll be able to get uh the response as it is being\n3:10:01 generated so let me just go ahead and execute this okay now I''m not going to\n3:10:07 directly print response. text but I''m saying that for every Chunk in response\n3:10:13 print chunk. test text okay and then it will show 80 like dotted lines okay so we can also\n3:10:20 display this in this way so here you can probably see how fast it was able to chunk by chunk obviously when chunk by\n3:10:27 chunk is getting displayed it will be very very good right so here you can probably see all the information ethical\n3:10:34 this needed for skill Force long-term impact on society and all now one more\n3:10:39 thing is that when streaming some response attributes are not available until you have iterated through all the\n3:10:45 response CH this is demonstrated below so here if I say what is the meaning of\n3:10:50 life with streaming is equal to true now you''ll be able to see\n3:10:56 that let''s see let''s see okay so once this gets executed now I have this response. prompt feedback in prompt\n3:11:03 feedback you''ll be understanding whether it has some problems or not with respect to that thing now if I directly go ahead\n3:11:09 and write response. text okay it will automatically tell me please let\n3:11:15 the response complete iteration before accessing the final cumulated attribute okay so this is a very good problem\n3:11:22 statement and now based on your requirements what I feel when compared to open AI still Gemini uh API that we\n3:11:30 have right it may have more functionalities as we go ahead but till now it really looks promising it looks\n3:11:36 really good okay now we will try to generate and text from image and text\n3:11:42 text inputs okay so this is also good now let''s download this image so by using a curl operation I am probably\n3:11:49 downloading the image if you see the image the image looks something like this okay or I may also use some more\n3:11:56 image right I will write cat playing image okay so cat playing image so here\n3:12:03 you will be able to see some images let''s see let''s see let''s see let''s see\n3:12:09 this looks like a complicated image I guess it should find some problem okay\n3:12:14 uh downloads okay downloads downloads here I will\n3:12:20 say upload that specific image\n3:12:25 okay I will save it let''s rename this image I will say cat. jpj now if I\n3:12:33 execute this over here I will write cat. jpj\n3:12:39 okay now this is the image it looks something like this now what I''m going\n3:12:44 to do I''m going to give this image to my Gemini Pro Vision model and then we will\n3:12:50 see what it can understand from this image like if it is if I say that the\n3:12:55 Kay is playing with some cotton fur or Hol right this cotton holes now let''s\n3:13:01 see what answer gini Pro Vision gives right so first of all we will go ahead and load gini provision okay and then we\n3:13:10 will go ahead and generate model. generate content image and then we''ll say to markdown response. text okay so\n3:13:18 let''s go and see the answer so it is going to take that image here directly we are giving the image\n3:13:24 now it is going to understand the specific image it is going to read and it''s is going to understand it is going\n3:13:29 to give me a response in text okay now let''s\n3:13:35 see so this looks amazing till here but let''s see the response I really want to\n3:13:40 see the response of this and it is taking some amount of time I think when we go with respect to the paid API it\n3:13:47 will be little bit less but again it depends on the image size how we are specifically calling it and all right\n3:13:54 let''s see I think as we go ahead the correct answer is play right so here you\n3:14:00 can probably see if I go ahead and see this image right it shows play now it\n3:14:06 was not that well but let''s see the previous one okay so let''s see the\n3:14:11 previous one over here okay I think with just play it is showing I think I''m not\n3:14:17 satisfied with that specific answer but we can also try with the previous one so let''s say this is the image and then we\n3:14:25 will go ahead and execute this let''s see what kind of response it gives us so here you have some tiffen box you have\n3:14:32 broccolis you have some food item right so let''s see till then I will download\n3:14:37 some more images Okay um\n3:14:45 let''s see let''s take this image also images. jpj I will go ahead and\n3:14:54 upload it over here okay I just want to see the answer\n3:15:01 like what all different kind of answers we specifically get yes uh right now for\n3:15:07 this image to text I think we are taking some time but I don''t know like in the future future it may have a good\n3:15:15 response time when compared to right now but here definitely some amount of time is basically happening right\n3:15:23 or these are meal prepared containers with chicken BR brown rice broccoli and\n3:15:29 Bell papers this also looks good so it is able to give minute details over here\n3:15:35 this is really good okay now I will go ahead and write play images. jpg\n3:15:43 let''s see so this is the image I think this image is small so it should be able to\n3:15:50 give it should be able to check the image and generate some kind of text let''s\n3:15:56 see it was an error post intern please try a report okay some some kind of\n3:16:02 error is basically coming up okay let''s see I I think it was a\n3:16:07 timeout issue or I don''t know but I think now we should be able to see the\n3:16:13 answer okay it is giving in some Chinese\n3:16:18 text okay so this I think works with some\n3:16:23 other other kind of images itself or where you have a detailed image but\n3:16:29 definitely some of the use cases is failing over here okay so I feel that it\n3:16:35 is failing over here okay now let''s try this one okay so there is one more option I don''t know like with respect to\n3:16:41 image do we have to give only this kind of images if I''m giving cat images playing cat playing\n3:16:47 images I don''t know what kind of response that we are getting or do we have some options to probably change it\n3:16:52 we''ll have a look okay now the next thing is that to provide both text and images in a prompt I''m saying that right\n3:16:58 a shot engaging blog post based on this picture it should include description this this I think for this uh it should\n3:17:05 be able to give right it should be able to give I think that is the reason it was not giving that well so now let''s\n3:17:12 see I will go ahead and write cat. jpg I hope it was cat.jpg only okay so yes\n3:17:18 this was CAD jpg uh can you include the description\n3:17:25 of the photo of the\n3:17:32 of okay uh let''s see can you write a short blog based on this picture okay it\n3:17:40 should includ a description of the photo okay now let''s see how what kind\n3:17:45 of response we specifically get and here the second parameter I''m giving it as image and that is how we are going to\n3:17:51 probably get it and uh to provide both text and image pass a list containing\n3:17:57 the strings and the image so this is my string the first thing that I''m giving this along with this image so I''m saying\n3:18:02 that write a short engaging blog post based on this picture okay so now let''s see what kind of response we will\n3:18:09 specifically get okay and quickly let''s see but I did not find a good output\n3:18:16 something like this so here it says like that only right is designed to handle\n3:18:22 multimodel prompts and return a text output okay perfect so yes uh this is\n3:18:27 there now let''s convert this into text the adorable Ginger kitten is\n3:18:34 having a blast playing with a colorful cat toy the kitten is batting at the toy with a spa this looks good right I''m\n3:18:42 really impressed by seeing the image it is providing you all the information okay\n3:18:49 now let''s see one more okay sparton image I don''t know let''s see\n3:18:55 sparton group image I will put this image let''s see it\n3:19:02 looks good I like this image spans jpg okay so I put this image let''s\n3:19:11 upload this and let''s see whether it''ll be able to provide the description or\n3:19:17 not first is observing what all things are there in the image I think that is\n3:19:22 what gini provision actually gives you okay so here I will say\n3:19:29 Spartans do jpg not directory why spotten it is\n3:19:35 spotten spotten Spartan okay Spartan so this is my image which\n3:19:41 looks oops oops oops oops what is the error now\n3:19:46 Spartan okay it is jpeg okay so uh I''m getting this\n3:19:54 specific image of Spartan right now and let''s see how it looks like so this is the entire image right now let me just\n3:20:03 go ahead and give this image over here okay I''m saying it to generate the\n3:20:09 content model. generate content let''s see and I''m saying write a short engaging blog post based on this picture\n3:20:15 it should include a description of the photo okay so it is what I feel is that\n3:20:21 that Vision it was not able to just recognize everything but the text\n3:20:27 information that we getting over here it''s good in this epic scene from the movie 300 Leon is the king of spot okay\n3:20:33 it is able to understand from the movies also completely right which is absolutely very very good okay so I hope\n3:20:41 you like this this particular video guys please go ahead and try it out and there is one more assignment that I really want to give in this go ahead and try\n3:20:49 there is something called as chat conversation so there are a couple of very simple by using Gemini Pro you can\n3:20:56 actually do it just go ahead and try this you just need to execute by appending this in the history all the\n3:21:01 information is given over here okay but yes I feel Gemini Gemini Pro looks\n3:21:09 promising and uh yeah it looks good in some computer vision right the images\n3:21:14 that I gave like cat and all it was it was just saying play so at least it was able to understand someone is playing\n3:21:20 over there okay so guys yet another amazing end to endend project for you and in this particular project we are\n3:21:27 probably going to create a multilanguage invoice extractor and we are going to use gini\n3:21:34 Pro API for this gini pro has been quite amazing you can definitely do a lot many\n3:21:40 things I have preped prepared more 10 to 15 different kind of projects that are related to Real World Industries and\n3:21:47 trust me all the specific projects are performing exceptionally well with respect to accuracy so over here we are\n3:21:54 going to focus on creating a multil language invoice extractor app we''ll be using gemin Pro we will be writing all\n3:22:01 the codes step by step so please make sure you practice along with me and once\n3:22:08 we practice things right one then you get multiple ideas like what different kind of projects we can basically do\n3:22:16 okay so let''s go ahead and first of all let me show you the agenda what all\n3:22:21 things we are basically going to focus on so here is the entire agenda so in\n3:22:28 this agenda what we are going to focus on first of all I will go ahead and show you the multil language in wased\n3:22:35 extractor app demo okay how the demo looks like later on we will start the\n3:22:40 process of creating the project project by creating the environment first of all then we will go ahead with the\n3:22:45 requirement. txt what all libraries we are specifically required and then we will start writing our code this will be\n3:22:52 an endtoend project code step by step we''ll try to build this app again it\n3:22:57 will take some time let''s say this project will probably take somewhere around 25 to 30 minutes and then in\n3:23:03 fifth point we''ll also discuss about what more additional improvements you can specifically do so that you can also\n3:23:09 try it from your side and as usual guys I''m actually keeping Target likes for\n3:23:14 every video so let''s target uh th000 likes for this specific video because all these videos will be super\n3:23:21 beneficial for you in the companies so let me first of all complete the first one that is the demo\n3:23:29 okay so here you can probably see this is my entire app okay and here I have\n3:23:36 actually uploaded one of the uh invoice okay so this is the GST invoice and it\n3:23:42 is completely in Hindi okay the best thing is that if I ask any question related to this using gini Pro so here I\n3:23:50 have asked what is the address in the invoice so address basically means over here 1 2 3 uh SBC building DF state so\n3:23:59 this is just a common invoice I''ve taken from the internet itself so here you can probably see we are able to get the\n3:24:06 entire response so this is quite amazing not only this I''ve asked for different different questions what is the date\n3:24:12 let''s say if I go ahead and say what is the date what is the date in the invoice and\n3:24:21 here you can see date basically means the knock so that usually this Google gini pro is\n3:24:27 able to understand those things okay so I think we will be getting the response so let me just go ahead and see it so\n3:24:34 it''s running uh so here you can probably see 12 27 21 so all the information in this\n3:24:41 specific invoice you are able to extract just by putting a prompt over here now\n3:24:46 the best thing about this particular project is that it is very difficult to automate it because I''ll tell you uh we\n3:24:53 have tried the specific project with the help of testra OCR and all right just\n3:24:58 imagine that Google Germany is able to perform exceptionally when well when compared to all those kind of tools okay\n3:25:05 so this was the demo now we will go ahead and probably develop this project completely end to end and we''ll start\n3:25:11 from completely from scratch itself so uh let me go ahead and let me start this specific\n3:25:17 project so guys here is one of the project that I''ve started in my vs code\n3:25:22 itself so first of all just go to the terminal so I will show you all the\n3:25:28 steps what you should basically do as I suggested the first step is basically to create my virtual environment so for\n3:25:36 creating the virtual environment I''ve already created it so that it does not take much time because for creating the\n3:25:41 virtual environment also it takes some time so in order to create it just go ahead and write cond create minus P okay\n3:25:50 V andv your environment name okay and then you can also give python version and remember to give python version\n3:25:57 greater than 3.9 in this case because Gemini Pro is suitable for python\n3:26:02 version greater than 3.9 so here I''m going to basically use 3.10 and then you\n3:26:07 just give- y so that it does not ask you for any permission while doing the installation as soon as you probably\n3:26:14 press enter so this kind of V EnV environment will get created in the same project folder okay so I''m not going to\n3:26:21 repeat this thing and probably execute it because I''ve already done this okay\n3:26:26 so just do it from your site with the help of this specific command the second thing is that I will go into theb file\n3:26:33 and I''ll create an API key which will be available from the Google okay so Google API key this is basically for gin Pro\n3:26:41 and and if you don''t know gini pro gini pro is again an amazing model that is provided by Google which actually\n3:26:48 provides you in a free way so you can actually hit 60 queries per minute okay so here is the API key that I have got\n3:26:55 if you want to also create your API key go to this website okay maker suit.\n3:27:01 google.com/ API key and you can just click on this create API key new project\n3:27:06 okay so I have already created it so I don''t want to create it again okay so these are the first two steps you really\n3:27:13 require the API key and you require the environment now after that you just\n3:27:19 activate like you just write cond activate venv Okay and just activate this\n3:27:26 specific environment okay once you activate it you will be able to see that you''ll be in that same environment\n3:27:32 location now let''s go to the next step in requirement. txt what all libraries\n3:27:37 we specifically require so here is streamlet then you you have Google generative AI then you have python. EnV\n3:27:44 then you have Lang chain you have P PDF P PDF is basically to load any PDF as suchar or read any PDF uh then you have\n3:27:52 chroma DB chroma DB is specifically for Vector embeddings so we will try to also do Vector stores or vector embedding\n3:27:58 will try to create it so uh these are the basic steps that we specifically\n3:28:03 require now let''s go ahead and start my coding or creating this specific\n3:28:10 application uh I will start writing the code completely from end so first of all what I will do I will go ahead and load\n3:28:17 my environment variable so I will say load do from EnV\n3:28:23 import load uncore do EnV so the reason why I''m\n3:28:29 doing this is that so that I can upload my all I I can load all my environment\n3:28:35 Keys okay so if you remember we have also installed python. EnV so python. EnV is basically for all my environment\n3:28:42 variables now I will go ahead and write load doore EnV so this will what it''ll\n3:28:48 do it will take it''ll load all the environment variables all the\n3:28:55 environment variables fromb file okay so this is what it is\n3:29:01 specifically going to do we''ll do step by step now you''ll be able to understand and please make sure that you write the\n3:29:07 code along with me so that you''ll be able to understand it now the next thing is that I will go ahead and use\n3:29:12 streamlet as streamlet is a better framework to quickly you know create an app and\n3:29:19 definitely I use chat GPT for taking out the code and all right so that is the reason you''re able to see that I''m able\n3:29:25 to upload daily videos see not the entire project is created by chat GPT but how to use stream late how to create\n3:29:32 this uh website kind of app you know all those things I can usually use streamlet uh use chart GPD so then uh so this is\n3:29:41 is my streamlet the next thing is that I will go ahead and import OS uh OS will basically be useful by for\n3:29:49 picking up the environment variable assigning the environment variable from somewhere else okay now this is done uh\n3:29:55 the next thing that I want is from P I also import image okay I don''t know\n3:30:01 whether I''ll be using this but let''s see the next thing I will also go ahead and\n3:30:07 import from Google Dot generative AI as gen AI okay so I''m\n3:30:14 also going to import this specific because gen AI will be my entire\n3:30:20 libraries that I''m going to access it okay so done this is done these are some\n3:30:25 of the basic uh things that we are specifically going to load it okay now usually when we start our any\n3:30:32 application using gin API so what we need to also do is that we need to configure uh the API key so here I''m\n3:30:39 going to write gen a configure API key is equal to\n3:30:45 os. getet OS dot get EnV okay and here I''m going to\n3:30:54 get my environment variable that is nothing but Google API key so here whatever environment variable is\n3:31:00 basically present over here we are going to take this okay so configure okay so\n3:31:06 gen. configure uh the API key with this okay now\n3:31:11 it''s time that we will create our function to\n3:31:18 load gini gini provision since the invoice\n3:31:24 instructor is on top of an image so we have to use this gin provision okay so\n3:31:30 function to load or first of all I''ll load the model so I''ll say model dot gen\n3:31:38 dot generative generative\n3:31:45 model so I''m going to specifically use gen. generative model and here I''m going to basically give my model name so it\n3:31:53 will be Germany Pro Vision okay so once I do this that basically means we are\n3:31:58 going to use this specific model now I will go ahead and write definition get\n3:32:04 giny response input image\n3:32:12 prompt okay so let me go ahead and write model\n3:32:18 equal to gen AI sorry I''ll not initialize the model\n3:32:23 again so what I will do I will go ahead and write see the thing is that here I''m\n3:32:28 going to give three parameters one is this specific input input basically means uh uh whatever input I really want\n3:32:36 okay with respect to all the images that I''m giving and I''ll also talk about this specific thing okay okay the three\n3:32:41 important information this input is basically I''m telling what what I want\n3:32:48 the assistant to do okay if I say hey you need to act as an invoice extractor\n3:32:53 you need to act like an expertise who is very good at uh taking out details from\n3:32:58 the invoice right so that basically becomes my input okay this prompt is\n3:33:04 what message I want like what is the address I actually return this is basically the image that we are going to\n3:33:10 pass okay so all those information this three information what we can basically do I''ll write response\n3:33:16 dot model dot generate content and here we are going\n3:33:24 to use this three information first of all is input then you have image of zero the\n3:33:31 second one and then you have the prompt okay so this three information basically\n3:33:38 when you''re generating this content you can give this three information in this same way okay input image of zero and\n3:33:43 prompt okay so in gini Pro they take they take all the parameters in the in\n3:33:49 the form of a list okay and remember the first parameter is basically the kind of\n3:33:55 prompt that you''re giving where your model needs to behave in that specific way so I will talk more about it as we\n3:34:01 go ahead and finally we are going to just return the response. text so this easy it is with the help of Gemini Pro\n3:34:08 okay and that is the reason I''m loving it when I probably compare with open Ai and the best thing is that I can also\n3:34:14 use uh this along with my Lang chain you can probably use it with different different things I will show you I''ve\n3:34:20 also created a project where you can chat with multiple documents okay so that will also be we''ll be using Lang\n3:34:26 chain and all so this is the function that is specifically done now understand one thing guys um we will do our\n3:34:33 streamlit setup okay so streamlet setup what I will do so here I will go ahead and copy and paste like this so here I''m\n3:34:40 using st. set page config let''s say that I''ll go ahead and say over here\n3:34:45 multi- language invoice\n3:34:50 extractor okay multi- language invoice extractor now in this multi- language\n3:34:55 invoice extractor I will probably also give this information let''s say okay now\n3:35:02 here I''ve given one input box this one input box is my input prompt okay and\n3:35:08 this is basically my upload file file upload so I''m saying that choose an image the image can be jpg jpg PNG this\n3:35:15 is the image of the invoice okay so let me go ahead and write this message of\n3:35:20 the invoice so once I specifically upload this specific file then we can do\n3:35:25 anything that we want okay now the next thing is that I will create an image\n3:35:31 variable I''ll keep it blank initially and let me go ahead and write if uploaded\n3:35:36 file is not none so that basically means when I''ve when when I have uploaded some\n3:35:42 file then I will go ahead and write image and again I''ll be using image.\n3:35:48 open and we will upload uh open this uploaded file okay now once we upload\n3:35:54 this so what we can also basically do is that we can uh write some kind of image\n3:35:59 and all I want to display the image also as soon as I upload it I probably want\n3:36:05 to display it so I can just try use this st. image functionality and I''ll say caption uploaded image\n3:36:11 and we can use this properties Now understand that this this code right I have directly searched from uh uh chat\n3:36:19 GPT okay and uh I''ve just written okay just create me an image where to upload\n3:36:25 files and all right uh so very simple it is not like I am learning from somewhere\n3:36:31 I even not seeing the documentation chat GPT actually provides everything a Google Power provides everything that is\n3:36:36 basically required now this is my uploaded file now I will also go ahead and create my submit button so here I\n3:36:43 will go ahead and write St do button and I will talk about it saying that tell me\n3:36:49 about the image okay tell me about the invoice something so this is my message\n3:36:57 that I''m actually going to give in my submit button and finally I have to also\n3:37:02 design my input prompt now see this input that I''m actually going to give right so this basically becomes my input\n3:37:08 prompt I what how I want the germini pro llm model to behave so here I will go\n3:37:14 ahead and create my input prompt just see this okay this is important and this\n3:37:19 will also give you an idea like how improv prompt works okay how we can actually work with any kind of improv\n3:37:26 prompt I will say you are an expert\n3:37:31 okay in understanding invoices okay\n3:37:39 um we will\n3:37:45 upload we will upload a image image as invoices okay I''m just\n3:37:53 writing some messages and you will have to\n3:38:01 answer any questions based on\n3:38:07 the uploaded invoice image so this is just a basic\n3:38:13 prompt that I''m specifically using over here I''m telling this to do something\n3:38:19 related to this okay so this is my input prompt and all I''ve written it over here then let me go ahead and write if submit\n3:38:27 button is clicked if submit button is clicked so this is my default input prompt now what\n3:38:34 I will do is that I''ll also create my prompt template itself and probably go ahead right if submit button okay okay\n3:38:41 is clicked now what will happen if I click the submit button so first of all I will\n3:38:47 go ahead and write if submit first I need to get my image data\n3:38:52 okay now understand over here as soon as we load the image but still we have to\n3:38:57 do some kind of image processing and convert those images into some bytes okay so for that again how do we do it\n3:39:05 so I will write definition input uncore image set up so for this I have just\n3:39:12 written in chat G saying that and here will be my uploaded file okay uh\n3:39:19 uploaded file uh okay uploaded file okay see now you may be thinking what I''m\n3:39:24 doing in this function in this function what we are writing is that it will take that uploaded file it will convert that\n3:39:31 into bytes and it will REM it will give all the image format all the image information in the bytes now I did not\n3:39:37 write this code I just went and searched in the chat GPT and this is the code that I specifically got okay and this\n3:39:44 code is quite amazing same way nothing I did not do anything see here if the\n3:39:49 uploaded file is not done so we first of all we are getting all the values then the image part what all things we\n3:39:54 basically require the type the data and byes data right and then we are returning the image Parts in these two\n3:40:00 format okay the M type and data and if the file is not uploaded this is that so\n3:40:06 this is completely I got it from chat GPT I''m not bragging anything about myself and all um again charit is already trained in\n3:40:13 internet data so it''s just like writing an input prompt and I''m saying that okay I require this two specific information\n3:40:18 please give me that information now in this image data what we will basically do is that we will get all the image\n3:40:23 information so here I will go and write input image\n3:40:29 setup so let me do one thing input image\n3:40:35 details okay so I will call this give a good name okay and here I will give my\n3:40:42 uploaded file okay uploaded file so uploaded file\n3:40:48 whatever uploaded file I''m specifically getting I''m going to give that specific thing over here now by this I will be\n3:40:55 getting my image data now image data once I get it okay then I will go ahead\n3:41:00 and write my response and go ahead and call my get Gemini response so here I''m\n3:41:06 going to basically write my input input prompt first p parameter is this second\n3:41:11 parameter that I''m going to give is my image data as usual remember all the information will be coming in the form\n3:41:17 of list okay so image underscore data and this will basically be my input\n3:41:23 input and this input is nothing but whatever information I''m putting it over here all this information will go over\n3:41:29 here and you have all this information in this format right now after this I\n3:41:34 will get the response and now I will go ahead and write st. subheader\n3:41:40 I''m giving some kind of subheader and I will write\n3:41:46 the response is st.\n3:41:53 WR and I will just display the response okay whatever response we are specifically getting okay that response\n3:42:00 we going to get over here so all this information is done and\n3:42:06 this is really good now it''s time that we can just run the code so guys now let''s go ahead and run this uh we have\n3:42:13 completed almost everything that we really want to do now is the most amazing thing whether the project will\n3:42:18 run or not okay so if the project runs it is absolutely good because at the first time we have written the code and\n3:42:24 everything should work fine so here I''m going to write streamlit run\n3:42:30 app.py and let''s go and execute this so it has opened let''s see so I''ve\n3:42:36 have downloaded two invoices let''s see what all things will be there first we will try with the normal invoice okay so\n3:42:43 here you can see all the information who is this\n3:42:50 invoice build to okay so I''m going to put this information over here and I''m\n3:42:56 going to click it tell me about the image tell me about\n3:43:02 the invoice on the all the information will be provided over here this is\n3:43:07 good so your client so all the information is over here your client this this this this even the number has\n3:43:13 been extracted which is quite amazing it is really a daunting process guys okay uh let''s see I will just take a small\n3:43:20 one what is the deposit requested okay so I will just go ahead and write it what is the deposit requested this is\n3:43:29 good this giving an amazing response so I will go ahead and click tell me about the invoice over here and here you go\n3:43:37 let''s see what it is going to get so tell me\n3:43:43 about the deposit requested it is just saying your company uh who is the deposit okay my\n3:43:48 prompt is wrong tell me so it is not able to understand the context obviously\n3:43:54 if you don''t give the proper uh tell me how much was the\n3:44:00 deposit requested I''ll give a good\n3:44:08 response Okay so I will go ahead and now click on it should give uh the proper answer I think now it is somewhere\n3:44:15 169.99 5 it''ll pick up that exact info and provide you all those\n3:44:22 information this is good so let''s see 169.99 this is good guys this is trust\n3:44:28 me this is very very close uh what was the Consulting fees let me go ahead and write what was\n3:44:34 the Consulting fees now I think it should get confused\n3:44:40 with those two values what was the amount of the Consulting fees I think it\n3:44:47 should be able to give it let''s see then we''ll try with some other language invoice like Hindi and all\n3:44:54 okay let''s see let''s see let''s see I think it should work fine but this is a\n3:45:00 good thing guys you can automate this entire process just imagine it is such a daunting process for with respect to\n3:45:06 invoice just see that whether you get an invoice all so the amount of the Consulting fees was\n3:45:13 $550 okay it is taking this information um okay there are some minor\n3:45:18 mistake but other than that I think let''s see what is the total what was the\n3:45:24 total\n3:45:30 discount let''s see discount is somewhere around 17.8 but if you give proper prompt I\n3:45:38 think you''ll be able to get a good response response okay 179.1 4 okay so\n3:45:43 let''s go ahead and try some other invoice this also looks good and uh let me go ahead and write this so here I\n3:45:50 will go ahead and write what is the HSN of Lenovo 51251 Lenovo in Hindi it is\n3:45:56 written in Lenovo so what is\n3:46:01 the HSN number\n3:46:06 of of of\n3:46:12 Lenovo okay I''m writing it in English 51251\n3:46:20 5125 5125 I let''s see whether it''ll be able to\n3:46:25 give or not see this small information also it\n3:46:33 will be able to take now over here the date is denak okay in Hindi we basically say it as dinak so here you can see\n3:46:39 84713 01 0 amazing amazing just amazing okay so what is the date in the\n3:46:47 invoice and you can try anything you can try different different invoices if you\n3:46:53 want I downloaded this invoices from internet you can also do it okay so yes\n3:47:00 here let me see whether you''re able to get it yes perfect so guys this was it for my side I hope you like this\n3:47:06 particular video if you like it please make sure that you subscribe the channel and all the information regarding this will be given in the description of this\n3:47:12 particular video this video we are going to see how we can actually build a conversational Q&A chat bot with the\n3:47:18 help of Gemini Pro API not only that we''ll also try to save all this chat in\n3:47:24 the form of a history and will will also display all the results that we all had a conversation about that is the reason\n3:47:30 we are discussing about conversational Q&A chatbot so before I go ahead guys we will keep the light Target of this\n3:47:37 specific video to th000 so that I will definitely get motivated and I''ll try to bring more similar kind of projects for\n3:47:43 every one of you out there now before I go ahead please let me go and go ahead\n3:47:48 and show you the demo so this is how the demo demo will look like so if I ask hi\n3:47:54 you can probably see I will be getting the answer the response how can I assist you today the chat history is Ive asked\n3:48:01 hi bot says hi hello how can I assist you so let''s say if I go ahead and ask\n3:48:06 what is generative AI okay all all the previous information I really need to\n3:48:11 record it somewhere right so let''s see after this the chat history right now is this much right so generative AI also\n3:48:17 known as so and so all the information is coming up and after that your chat history will also get updated and the\n3:48:23 best thing is that I''m streaming all the specific data okay so how streaming actually works and all we''ll also be\n3:48:30 discussing about that so here you can see in the history I''ve asked what is generative Ai and this is specifically\n3:48:35 all the information now this is what we are going to implement step by step I''ll show you each and everything again all\n3:48:41 the files regarding this all the code regarding this will be given in the description of this particular video so\n3:48:46 let me go ahead and let me solve this specific project so guys I have opened\n3:48:52 my VSS code over here and right now uh if you remember in my previous video\n3:48:58 I''ve discussed about all these things like vision. py how you can actually play with images with the help of gini\n3:49:04 API gini Pro API then we also discussed about some kind of simple Q&A chat B now\n3:49:10 in this video I am going to show you this entire code with respect to this\n3:49:15 qpy so here is what is my entire code I''m going to specifically write step by step I''ll try to show you what all\n3:49:21 things is basically required as usual first of all we need to have an environment file with respect to Google\n3:49:27 API key this you can actually get it from makes. google.com so from there\n3:49:32 which will basically provide you all the features to create the API key for gini pro the first thing is EnV file once we\n3:49:39 create the EnV file then we will go ahead and start implementing our code if you have not seen all the videos in My\n3:49:46 Gin playist I would suggest go ahead and have a look so let me quickly write over there all the code so first of all I\n3:49:52 will go ahead and uh load all the environment variables so for this I will use\n3:49:58 forv um import loore EnV okay and then we will go ahead\n3:50:05 and initialize load. EnV and finally I''m going to import\n3:50:12 streamlet as STD so I''m going to use streamlit over\n3:50:18 here and remember one thing guys there are some important libraries that you need to install that is present in the\n3:50:23 requirement. txt everything will be provided in the description of this particular video the GitHub code right\n3:50:29 so I''m going to use streamlet Google generative a and python. EnV all this Library needs to be installed before\n3:50:35 ahead right so I''m importing stream. asst then I will go ahead and import OS\n3:50:40 because I will be requiring it and then I will say import Google do generative\n3:50:47 AI as gen okay so I''m going to use this Library only for doing all my task now\n3:50:54 initially whenever we load any environment key first of all we need to set this in my gen AI so for doing that\n3:51:01 I will write gen ai. configure configure is a method where it will be asking for the API key that I have and since I''ve\n3:51:07 already loaded that from my environment so you can see that os. getv I''m using this Google API key okay now the next\n3:51:14 thing over here I will try to create a function function to load gini pro model\n3:51:22 okay so here you can probably see gini\n3:51:27 pro gini pro model and get response okay\n3:51:34 so I''m doing this and here you have model. geni Dot\n3:51:41 generative model I''m going to initialize my generative model itself and in this case also I''m going to use gini Pro okay\n3:51:49 so gini pro and gini pro vision for conversational Q&A we will specifically\n3:51:54 be using this one that is called as Gemini Pro okay and then we will go\n3:51:59 ahead and execute it and write chat model dot startor chat and here I''m\n3:52:06 going to specifically use history okay so this history will also maintain\n3:52:12 all the things that we are probably going to uh have in our conversation but\n3:52:18 I will show you another way where I''ll use the power of streamlit to store all the history in a form of a session later\n3:52:24 on you can also put that inside your DB or you can also use it from your session\n3:52:29 itself right uh so first of all let me go ahead and Define my definition so here I will basically write definition\n3:52:37 get gini under underscore response response and inside this\n3:52:44 response I will specifically give my question whatever question we have asked\n3:52:49 so this function should basically uh you know give me the response that I''m getting from the generative model itself\n3:52:56 like from the Gman Pro right so this question I will send it to my llm model and then I will specifically get the\n3:53:01 response so here I''m going to write response chat. sendor\n3:53:07 message and here I''m going to basically write question stream is equal to True\n3:53:13 okay so stream is equal to true because as the llm model is giving you the output we are going to stream it and we\n3:53:19 are going to display it right and then we will go ahead and return the response so once this response is basically\n3:53:26 coming this response is nothing but it is the output right when we specifically get response. text now we are going to\n3:53:33 initialize our stream late app for initializing it what we are going to do is that I''m going to use this three\n3:53:39 Fields one is the Q&A demo J Min L application page config and header it is\n3:53:45 very much common the most important thing is that if I really want to record\n3:53:51 the History part right the history of the conversation we will initialize session state so in stream late it\n3:53:58 provides you session States for chat history if it does not exist I''m saying if chatore history not in session State\n3:54:06 then we will go ahead and create a sess State and we''ll right chatore history over here so right now it is blank as\n3:54:12 soon as we have any conversation later on we will try to record all those we''ll\n3:54:18 try to put all those conversation inside this particular session state that is what we are going to look at then in the\n3:54:25 next step we are going to basically say input is equal to\n3:54:31 st. textor input and here I''m going to basically\n3:54:36 use input\n3:54:41 input and here we are basically going to write key is equal to input okay so\n3:54:48 input will be my variable name in short whatever text box I''m specifically using along with this I''m going to also use a\n3:54:54 submit button St do button and here I''m going to basically\n3:55:00 write ask the question okay so what I''ve actually done\n3:55:06 I''ve initialized my session okay session state so here that the name of the session state is chat history okay\n3:55:14 now if I click on submit or input right\n3:55:19 basically both the fields input should also be filled okay so if both this\n3:55:25 satisfies both this conditions satisfies then what I will do I will write I will go ahead and call my get Gemini response\n3:55:31 and here I''m going to basically give my input whatever input I am probably giving it as soon as it calls this\n3:55:38 function it is going going to get that message and it goes to get that response okay\n3:55:43 now the next thing is that add user query and response to session chat\n3:55:52 history now see as soon as I probably have created this input this is what is my input that user has given and this is\n3:55:59 what is the output that we have got right now what I''m actually going to do\n3:56:05 here for all this entire history right we going to save this in our chatore\n3:56:11 history session state so for that we will go ahead and write St do session\n3:56:17 State okay st. session State and here we going to basically use chatore history\n3:56:26 okay chatore history and I''m going to do append with this\n3:56:33 specific let''s say I''m going to write you basically you who''s sending the message\n3:56:40 and then I''m going to basically use input okay so I am storing all this session\n3:56:47 inside this particular U variable okay and finally I will get my response also\n3:56:52 so let me write over here St Dot subheader and here I will write a\n3:56:58 statement saying that the response is the\n3:57:03 responses and in short I have to probably display the response Now\n3:57:09 understand one thing we used something called as stream is equal to true that basically means this entire response can\n3:57:16 without getting the entire content can also be populated in whatever Pages we want to probably display right what what\n3:57:22 do I mean is that now my process did not have to wait for the entire content to\n3:57:27 come from llm so as llm is sending text Data whatever response it is basically\n3:57:33 sending it is going to display it in the front end screen okay so that is what we are basically looking at so now I''m\n3:57:39 going to write from Chunk in response see that is the reason when we write stream is equal to True okay that\n3:57:46 basically means we get the exess of all the streaming data and then we can write a for Loop and I''m saying from Chunk in\n3:57:53 response and here I write st. write and we will go ahead and write chunk.\n3:58:00 text so we are displaying the text part by part and along with this I will go\n3:58:06 ahead and append this entire response okay and here instead of\n3:58:12 writing you I will write bot okay and inside this instead of writing input I\n3:58:18 will go ahead and write my entire chunk. text so what I''m actually doing is that\n3:58:25 as soon as I get the response it is coming in the streaming manner we are displaying it and accordingly we are\n3:58:31 also appending in this chatore history okay now this is perfect this is done and finally what I will do is that I\n3:58:38 will just go ahead and create my history because I need to display all the history right so here I will go ahead\n3:58:44 and write the chat history is okay so this is what I am going to basically\n3:58:50 Implement and and here I will say for RO comma I\n3:58:56 will create two common variables in St dot session State and here I''m going to\n3:59:04 use my chatore history okay chatore history and whatever is there it\n3:59:12 is either in the ski value pairs U colon bot colon some kind of answers here I''m\n3:59:18 going to basically write st. write I''ll use a f one over here and I\n3:59:25 will say roll colon and then here I will say text so\n3:59:31 in this specific format okay roll colum text now let''s go ahead and see if\n3:59:39 everything works fine or not okay and I will go ahead and open my terminal I''ll\n3:59:44 delete this let''s see so we are now displaying perfectly all our details uh\n3:59:52 this VNV has got activated I''ve already shown you in my previous video how to activate V EnV environment also and here\n3:59:58 we will go ahead and write stream late run uh QA chat q a chat. py and let''s\n4:00:05 see if everything works fine or not so it has got executed so here is my\n4:00:13 things I will say Hi how are you and then we will go ahead and ask\n4:00:20 the question let''s see whether we''ll get the response the response is I''m a conversational AI chat B this the chat\n4:00:26 history is having all the details okay and it is giving chunk by chunk you can see over here right uh let''s ask some\n4:00:33 other question my name is krishn okay something\n4:00:41 krishn what is your name okay something like this and I will go ahead and ask\n4:00:46 the question let''s see whether this is getting saved or not okay so here you can see how are you\n4:00:53 this my name is Krishna what is your name I''m a chat bot assistant I do not have a name bot designed to help users\n4:00:59 so and so so all the information that I probably type it over here along with the response it is getting recorded in\n4:01:05 the chat history now this is what I really wanted to show it to you and it was so much easy many people had asked\n4:01:11 this specific question in my upcoming video what I''m going to specifically do is that I''m going to create a PDF\n4:01:19 document with the help of gini API I''ll show you different embedding techniques how we can convert a word into vectors\n4:01:25 and then how we can utilize for a normal document Q&A kind of thing with the help of Gemini API right so I hope you like\n4:01:33 this particular video this was it for my side all the information regarding this will be given in the description of this particular video thank you take care\n4:01:39 have a great day bye-bye',
  '{"channel": "freeCodeCamp.org", "video_id": "x0AnCE9SE4A", "duration": "4:01:40", "level": "INTERMEDIATE", "application": "LangChain", "topics": ["LangChain", "OpenAI", "Google Gemini Pro", "LLAMA2", "Astradb", "PDF Chat", "Blog Generation", "Pinecone VectorDB", "Invoice Extractor", "Q&A Chatbot"], "source": "Playwright Browser Extraction", "ingested_date": "2025-12-21", "content_type": "youtube_tutorial", "category": "LangChain"}'::jsonb
);

