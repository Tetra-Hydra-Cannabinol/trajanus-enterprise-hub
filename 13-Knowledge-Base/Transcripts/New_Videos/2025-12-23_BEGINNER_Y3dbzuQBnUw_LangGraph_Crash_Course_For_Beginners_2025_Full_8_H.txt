Title: LangGraph Crash Course For Beginners 2025 | Full 8 Hour Course | LangGraph 0.4V LATEST!
Channel: Harish Neel | AI
Video ID: Y3dbzuQBnUw
URL: https://www.youtube.com/watch?v=Y3dbzuQBnUw
Duration: 525:10
Level: BEGINNER
Ingested: 2025-12-23
Source: yt-dlp Auto-Generated Captions
==============================================================================

WEBVTT
Kind: captions
Language: en
Hello guys, welcome to a complete
end-to-end Land graph crash course for
beginners. If you're looking to build
powerful AI agents for your product, for
your business, or even if you're looking
to make your next big job switch, then
this course is for you. By the end of
this course, you will learn how to build
powerful AI agents that can think for
its own. It can make decisions for
itself. If there is a human approval
needed, it can pause its work and come
to you for approval. And even this is
the tip of the iceberg. And if this
sounds too much for you, don't worry.
This course is going to be very very
beginner friendly and we'll add on more
difficulty and more concepts one step at
a time. And by the end of this course,
we are going to be building an AI agent
that remembers your past conversations,
streams information to the front end,
answers questions if it knows the
answer, else it smartly makes an
internet search, pulls data from various
URLs in the internet as you can see
right here, and gives you an accurate
answer exactly like what perplexity a
billiondoll valued company does. So
let's actually go ahead and look at the
course overview. So first we will look
at the different levels of autonomy in
LLM applications. So autonomy meaning
freedom. So starting with code which has
very little freedom to think on its own
right it does exactly what we tell it to
and in the end we have agents which have
very high autonomy. It can think on its
own and that is exactly where LAN graph
is going to come into the picture. But
to get here we will have to understand
how we progressed here. Okay. So now
understanding agents and tools and this
is where we'll be deep diving not just
on what agents are but actually look at
how agents are built under the hood.
Okay. So basically this would set a very
strong foundation for us for the rest of
our course. So yes we'll learn them how
to build them from scratch. Also we will
also be learning how to build agents
using predefined classes that lang chain
provides out of the box and then we will
understand the graph data structure. the
differences between DAGs and cyclical
graphs. So now that we've understood the
prerequisites then we will move on to
what is a lang graph, why is it
required, why not just stick with lang
chain, right? What are the limitations
of lang chain? So then with lang graph
we will be building different agentic
architecture patterns like the
reflection agent, reflexion agent,
multi- aent workflows, right? So we will
understand all of the key terminologies
that come with learning LAN graph like
what is a graph what is a state what is
a node right what is visualization what
are break points etc etc okay so at that
point we would have learned enough to
start building our very own land graph
chatbot the chatbot we'll build will
have the ability to answer questions by
searching the web it will be able to
route complex queries to humans for
review it can go back in the chain and
then explore alternative paths and this
is where these agentic patterns like
human and loop react patterns come into
the picture. Okay. So we will also build
multi- aent systems like very similar to
what crew AI does but I believe that
langraph is much more powerful because
it's much more low-level and that gives
us a lot more flexibility on how we can
actually build the flows. All right. So
multi-agent systems meaning we basically
create multiple agents and those agents
can sort of communicate with each other
to get a particular task done and then
we will integrate rags into LAN graph.
So we will explore what C rags are or
it's basically called corrective rags.
We will also see what a rags are.
Basically it means adaptive rag and also
self rags and most importantly we'll
also be looking at how persistence works
in LAN graph. also learn some of the
other tools that Lang graph provides to
help build production grade agents right
so like the LN graph studio is something
that it provides it provides LN graph
cloud API etc etc we'll also be looking
at that and finally we will look at
agents in production so basically
explore enough real life use cases so
that you get a complete absolute
complete picture for what is to come in
the future of technology so if this
excites you make sure to subscribe to
the channel and hit the bell icon as
well so that you'll be notified every
time I release new videos. Now let's
actually go ahead and look at the
prerequisites for this course. So first
thing obvious Python because we will be
working with Python and secondly
understanding of lang chain can be
essential and I'll tell you why lang
graph which we are going to be looking
at in this course uses lang chains
classes under the hood for example chat
models prompt templates and these are
things that lang chain provides and
learning that is going to be very
important to continue with this course
so if you don't know lang chain already
I've already made a 2.5hour long
tutorial that teaches you in depth the
concepts like you know chat models
prompt templates right rags agents and
tools right so these are things that are
very important these are prerequisites
so if you're not familiar with these
concepts I would highly recommend that
you watch lang chain video first and
then come back to this particular video
so with that said let's get
started in this section let us explore
the levels of autonomy in LLM
applications and this list is going to
be starting from the least zero autonomy
to maximum autonomy. So the first thing
that we have is code. Code has zero
autonomy and is 100%
deterministic. We all know that
everything is hardcoded and it is not
even really a cognitive
architecture. What are the disadvantages
though? The problem is you would need to
write rules for every possible scenario
making it impossible to handle real life
complexity. Let us now look at the next
one. LLM call. A single LLM call means
your app basically does one main thing.
You give it an input, it processes it,
and it gives you back an
output. Think of chat bots that just
take your message and respond or apps
that translate
text. This was a huge leap from
hard-coded rules, even though it is
still pretty simple and it is only in
the second stage of autonomy.
So here is a very simple diagram that
I've prepared. So we have the user input
right here. The user is inputting an
input to the LLM which is right in the
middle and then it gives out an output.
So an example user input could be
something like you are an expert
LinkedIn postwriter. Write me a post on
AI agents taking over content creation.
So the user is giving just one prompt,
one particular task to the LLM and then
the LLM is pretty good at doing that one
particular thing. Okay. But there is one
disadvantage as well. So as you can see
right here, trying to get everything
done in one shot often leads to confused
or mixed up responses. Just like how one
single person cannot be an expert at
everything. So you can imagine that if I
ask it to you know write a Twitter post
and a LinkedIn post and I don't know a
blog post as well in all in one single
prompt, it is not going to do it really
well. If you're going to tell the LLM to
do one particular thing, give it enough
instructions and just give it one task,
it is capable of doing it really well.
But multiple things it cannot do really
well. And this brings us to the next
level of autonomy called
chains. Think of chains as having
multiple specialists instead of one
generalist. Instead of asking one AI to
do everything, we break it down into
steps where each AI is really good at
one thing. Imagine a customer service
chatbot. The first AI reads your
complaint and figures out exactly what
product you're talking about. And the
second AI finds the right solution from
the company's help docs. And then the
third AI turns that solution into a
friendly
response. Each step is simple, but
together they create a much smarter
system than a single LLM call.
And this is where we first started
seeing AI applications that could handle
more complex tasks. Not just by being
smarter, but by breaking big problems
into smaller, manageable
pieces. But what are the disadvantages?
The downside is that these fixed
sequences are like a rigid assembly
line. They always follow the same steps
defined by the human. So let's look at
an example of chains. So right here you
have a user input. Okay. So the user
input is going to be just a title for a
particular post. So for example, the
user is saying uh AI agents taking over
content creation. That's it. The user
wants to wants this particular
application to go off and create a
LinkedIn post based on this particular
topic as well as a Twitter post based on
this particular topic as well as a blog
post. Okay. So what you see right here,
this is a chain for LinkedIn. This is a
chain for Twitter. This is a chain for
blog post. So basically we are we are
having these parallel chains and just by
initiating this particular node all
these chains start executing parallelly
at the same time. All right. And what
are the upsides of this? What are the
advantages of using this particular
chain? It is not a single LLM call.
These are three different LLM calls.
Right? So these are three different
specialists doing uh you know one
specialist is doing LinkedIn. The second
one is doing Twitter. The third one is
doing blog post. So the the advantage of
using chains is that we have three
specialists instead of one generalist.
But what are the downsides? What did we
see right here? The downside is that
these fixed sequences are like a rigid
assembly line. Right? They always follow
the same steps defined by the human. So
basically what I'm trying to say here is
that the level of autonomy is very less.
Right? The level of autonomy is very
less. It is not making any intelligent
decisions or anything right here. Okay.
So let's now look at the next level of
autonomy in LLM applications. And we
come to router. This is where it gets
really interesting. Routers are like
smart traffic ops for your AI. Instead
of having a fixed path like in chains,
the AI itself decides what steps to take
next. Imagine a personal assistant bot.
When you ask it something, it first
figures out if you need help with
scheduling, research, or calculations
and then routes your request to the
right tool or chain for the
job. Okay, so I know this looks a little
bit complicated, but let's take it one
step at a time. Okay, so the example
user input is going to be write me a
LinkedIn post on AI agents taking over
content creation. Okay, so the user is
actually being very specific about
LinkedIn. All right, so let's see what
happens. This is the user input and then
we have another LLM right here. Okay, so
we have an actual LLM where we would
have written the prompt. Okay, so
basically look at this particular user
input and then figure out which social
media channel that the user is asking
this topic to be written on. All right,
so this router LLM what is it going to
do? It is just going to return you know
is it LinkedIn or is it Twitter or is it
X whatever uh I mean um threads or
whatever right? So that keyword is being
returned by this particular LLM node and
that is being returned to a simple input
classifier. So what is this classifier?
It could just be a simple Python
function. It just takes in an input a
keyword. that keyword depending on
whether it is LinkedIn or you know uh
Twitter or a blog post or whatever
depending on that particular keyword it
is then you know either it is directing
it is either directing the control flow
to the first chain or the second chain
or the third
chain. Okay. So it if based on this
particular input since the user is
asking for LinkedIn it is then going to
direct the control flow to this LinkedIn
chain. All right. So that is the only
difference. Basically in the previous
chains you saw that you know there is no
real intelligence happening. So whatever
we decide it is just going to go you
know there is no real uh you know
decision making happening there. If we
basically chains are predefined we
define okay this is the direction that
it needs to go into. But in a router
this router LLM it is actually making a
decision based on the user input. It is
looking at okay LinkedIn is what we
want. So it is directing the control
flow to that particular
chain. Okay. But there is another
disadvantage with router as well. So
while it can choose different parts, it
still cannot remember previous
conversations or learn from mistakes and
and it is exactly to fix this particular
disadvantage that we have the next level
which is going to be called state
machine or in other words agent. And
this is exactly where LAN graph is going
to come into the picture. Let's dive
into it. So this is combining the
previous level router but with
loops. And then why do we call state
machine as an agent? Basically whenever
the control flow is controlled by an
LLM, it is then called an agent. Very
simple. Uh I just wanted to give you a
reason why we are calling state machine
as an agent. And this involves features
like the ability to have human in loop
ask for approval before moving on.
multi- aent systems, advanced memory
management, going back in history and
exploring better alternative paths,
adaptive learning, meaning it gets
better even if it makes mistakes. So it
won't make the same mistake again and
many more and this is where LAN graph
comes into the picture. So let's
actually look at what is possible with
this particular type which is the state
machine. All right. So, so we have the
user input right here and then this user
only has to deal with the head of
content agent. Okay, so we're taking the
same example which is the user input is
like you know write me a LinkedIn post
on this particular topic, write me a
Twitter post, write me this thing, write
me both a Twitter as well as a LinkedIn
post, right? The user can basically say
anything to the head of agent I mean the
head of content agent. So normally this
is this is how you know it works in
business, right? Basically you would
have one highle you know uh subordinate
that you'll give all your instructions
to and that subordinate will have
multiple subordinates under it and then
you know that will take care of it right
and that is exactly what is happening
here the user is inputting this to the
head of agent and this headoff agent you
know looks at this particular prompt and
you know decides okay does it need to
talk to the LinkedIn script writer agent
does it need to talk to the blog post
writer agent does it need to talk to the
social media publisher agent. Okay, so
it looks at this thing. Okay, it sees
the LinkedIn thing. It sees the topic as
well. So now it instructs the LinkedIn
script writer agent to write this
particular uh post. All right. And we
can also you know add a human in loop
here as well. Okay. So we have time
travel abilities when it comes to state
machine basically land graph. So once
the LinkedIn script writer agent is done
with writing the LinkedIn post, we can
also have an approval step right here.
So basically an approval step is it
sends the you know draft back to the
head of agent I mean the head of content
agent and then asks basically okay is
this good? Do you want me to make any
more corrections? So this head of agent
content agent is going to send it back
to the user because we want this to be
the point of contact. We don't want to
deal with these agents right? So this
head of content agent is going to come
back to the user say okay this is going
to be the first draft do you want me to
make any few any more suggestions I mean
improvements right so if the user says
okay make it shorter make it more you
know punchier add a better hook to the
post or something like that all of that
feedback is going to be again sent back
to the LinkedIn script writer agent this
agent already has what it generated
previously and then it has the feedback
as well so it has the full context right
so and then once it is then it again
does it. So you can see that there is a
sort of sort of a loop happening right.
It can go back in time and then you know
it can sort of iteratively make a
particular thing really good. All right
that is where a state machine really
shines and this is exactly where LAN
graph you know is used basically. So
what would happen once the human
actually approves a particular draft?
Okay. So the control flow comes back to
the head of content agent and then it
looks for all the other subordinates and
it looks at the social media publisher
agent. So this agent actually can go
ahead and post that particular LinkedIn
post to LinkedIn. So it's coming right
here. It already has tools. If if you
know if you don't know what tools are,
they're basically just functions. Python
functions, right? They're special Python
functions. And you know first tool could
be having the Twitter uh endpoint to
post a particular tweet and the second
tool could have you know um the endpoint
to post to LinkedIn right. So it is this
agent has these tools and depending on
what the instruction is coming from the
head of content it can use the LinkedIn
tool and then post it as well. So you
can actually imagine how disruptive this
technology could be. the the the user
just has to speak to this one agent and
the agent is going to you know speak to
other agents under it like a
hierarchical structure and then get a
particular task done right all right so
let us move on so this is a diagram that
is present in the lang chain
documentation as well the levels of
autonomy in LLM applications and this is
what we saw just now so we have the code
right here we have to decide the output
of each step we have to decide you know
which steps to take we have to decide
what steps are available to take. All
right. So in the LLM call the output is
decided by the AI but the rest of the
two things we have to say right the
chains are also very similar but in a
router's case it can actually make
decisions on its own right it can
actually we we still have to specify
what are the available outputs like we
still have to specify the Twitter chain
and the LinkedIn chain and the blog post
chain but which chain it needs to go to
can be decided by the AI. So that that
is why we have the AI right here. But
then we also saw that there was a
drawback. We saw that we had no cycles.
There was no way to come back and then
refine a particular thing. Right? So
that is why it says no cycles right
here. And the fifth one is what is
called agent executed. The AI takes care
of you know it decides the output of uh
the step. The AI decides which steps to
take as well. All that we have to do is
decide what steps are available to take.
Okay. And also it can actually loop
through uh and sort of refine it and you
know there are cycles available in state
machine as well. And then we have the
sixth one which is completely autonomous
agents. Uh all these three columns you
can see that the AI is doing it and we
are not really there yet. You know
there's a lot of startups out there that
are already trying to build this. you
know you can check out baby AGI you know
um um I think autog is doing that as
well so you can actually check it out
but we are not really there yet the
technology is not really there yet but
there is one more thing that I want you
to notice in this particular diagram
here you can see that the first four
points we have something called
human-driven whereas in five and six it
says agent executed so what actually is
the difference between
human-driven why is this coming under
agent executed so let's actually dive
deeper into it. So we'll see what is the
difference between chain or a router
versus an agent. So a very simple
definition a chain or a router is just
one directional. Hence it is not an
agent. That's it. Very simple. Whereas
in a state machine we can actually go
back in the chain have cycles and the
flow is controlled by the LLM. Hence it
is called an agent. So in the previous
few diagrams for router and chains we
saw that you know it starts from the
left side and then it just keeps on
going to the right side until the end
node is reached right so there is no
real intelligence happening and that is
why chains and router are not considered
as agents okay but when when we talk
about state machines you can see that
you know there is loops happening there
is time travel happening there is human
review happening you know there is
refinement of a particular you know the
user can critique a particular ular post
and then the post is again critic and
generated again and then it comes back
and then the loop can go on several
times. So there is actual intelligence
happening very similar to how you would
do things in a real world as well. Let's
say if you have a content writer under
you, you know the same thing happens.
There are multiple iterations happening.
The same thing for UIUX designers as
well, right? So that is why the state
machine side is going to be called
agentdriven whereas chains and router is
going to be sort of dumb, right? Even
though there is an LLM involved, it is
still considered to be dumb because the
LLM is not actually deciding, you know,
the control flow. All right, so that is
it for this section. In the next
section, we will actually deep dive into
what are AI agents. So, I'll see you
there. Quick pause. I wanted to let you
know that I offer a premium version of
this Langraph course that takes your
learning experience to the next level.
While this free tutorial covers the
essentials, the paid version includes
structured learning path with
comprehensive modules, hands-on
assignments to reinforce your skills,
quizzes that you can test yourself on,
detailed notes and slides for every
single module, and finally regular
updates whenever Langraph introduces new
features or breaking changes. Unlike
this YouTube video which remains static,
the premium course evolves with the
technology. I've priced it to be
accessible for everybody serious about
mastering land graph. If you're enjoying
this content and you want to deepen your
expertise, check out the link in the
description. Your support helps me
create a lot more content like this. So,
let us get back to the
tutorial. All right. So, in this
section, let us understand AI agents.
So, you can think of agents as the
problem solvers of the AI world. Agents
are capable of thinking on their own. So
in other words, it's AI that can make
autonomous decisions. In the case of
chains and routers, they follow our
specific instruction. But with agents,
they actually take it a step further.
They can decide for themselves what
steps to take on their own. But what are
tools then? Tools are specific functions
that agents can use to complete tasks.
Just like a chef's kitchen tool, knife
for cutting, oven for baking, blender
for mixing, tools are the special
abilities we give to AI like giving a
calculator tool or a search engine tool
or a calendar tool. So how can we
actually create an agent? So let's
actually look at you know a very popular
pattern that we use to create AI agents
and the pattern is called react agent
pattern. Okay, so this is one of the
best known patterns in AI today to build
agents. It stands for reasoning plus
acting. This is basically a concept that
mimics how human beings think. So let us
actually look at what this pattern is
all about. So you can see here that we
first have a think and then an action
and then an action input and then we
have something else and we have observe.
Okay. So what is this all about? Let's
get some clarity on what is this react
pattern because this is very very
important to understanding a lot of
things. Okay. So it it basically react
pattern is sort of mimicking how human
beings think. Okay. So for example if we
look at a particular problem first we
think about the problem. Okay. So how do
we solve this? Okay. What exactly is the
problem? Okay. So we identify this is
the problem and then we then identify
what is the action that we have to do to
solve that particular problem. Right? So
first thinking happens and that is
exactly why we have think right here and
then we have to take action to solve
that particular thing. Right? So that is
why we have action. So what then
happens? We take some action and then we
observe the result of that particular
thing. Okay. Is it solving it? Is it
giving some other answer? You know we
actually do that observation. Right? So
that is why we have the observe right
here. So we basically humans we observe
the result of it and then we tweak it.
If basically if if we get the answer
that's it it's over the cycle is over
but if we don't get the answer we again
you know think okay what happened right
so that is why we have think action
observation think action observation in
loop until we get the final answer. So
let's actually take it step by step. So
first we have think the LLM first thinks
about the user's prompt or the problem.
Okay. So next thing is action. Okay. LLM
decides if it can answer by itself or if
it should use a particular tool. Okay.
And then if there is a tool available to
that particular agent to solve that
particular problem. Basically you can
imagine it is a Python function right?
Any function requires some arguments
right? Some inputs. So that is going to
be the action input. The LLM provides
the input arguments for that particular
tool. Okay. Now the LLM is just going to
decide okay this is a proper argument
that can be passed into that particular
tool. So now the control flow comes back
to langchain to our system. Okay. So
here langchain executes the tool and
then returns the output back to the
LLM. Okay. So once so it's basically
like LLM is thinking it is executing the
tool but actually it is just suggesting
the the input the control flow is coming
back to our system our system function
is executing that particular tool with
the arguments that the LLM suggested and
then the output of that function
execution is again sent back to the LLM.
So the LLM has complete context of what
happened before as well. all the think
action observation think action
observation everything all the context
is available to the LLM so it is
actually looking at the output of this
tool right so that is what this observe
is LLM observes the output of the tool
so if the output is like uh it's enough
okay I have actually found out the
answer to the particular problem if it
is if it has found it out then it is
going to end it right here if there is
some other additional information right
if this is a complex problem it is a
multi-step problem in that case you know
there's going to be another cycle
happening so this cycle is going to keep
on repeating okay so that is in a
nutshell what react pattern is all about
so here is another diagram that I've
prepared basically to help you visualize
you know in a very simple way what makes
an agent right so whenever there is an
LLM basically the reasoning ability of
an LLM which is a brains this is the
brain right whenever you you know equip
a brain with some tools right here
Right? If you equip the brain with the
ability to, you know, make API calls or
Google make a Google search or run a
Python function, combining both of it
together gives rise to an agent. Okay,
so that is how you can think about it in
a very simple way. So enough theory. I
know it's been a little dense. So in the
next few sections, we'll actually be
coding it out and you know um so you'll
be able to better understand how things
work. All right. So first off what we'll
do is we'll build a very very basic
react agent using lang chain and then we
will then look at what are the drawbacks
of doing it that way and then we will
then you know sort of segue into what
does langraph do and how does it come
into the picture what is the problem
that it solves all right so I'll see you
there hello guys welcome to the section
in this section we are going to be
learning how to build a react agent
using lang chain from scratch And this
is particularly important because this
will form the foundation for the rest of
the course because later on we'll be
building the same thing using LAN graph
and you'll be able to see the
differences very easily. So let us go
ahead and set up the development
environment. So the first thing that I'm
going to do is I'm just going to create
a folder called LAN graph let's say and
I'm going to open it in my Visual Studio
Code. Okay. Let's make sure that it is
all zoomed in properly. All right.
Okay. So to start off with I'm going to
create a virtual environment. If you're
used to doing it with poetry, you can do
it. But I like to do it with a very
simple virtual environment. So I'm just
going to say python
3-m basically stands for module. So we
are using a particular virtual
environment module inside of Python. So
we can say benv and we can give the name
of the folder that will be getting
created here as well. These are
prerequisites. A virtual environment is
basically something where you can have
all of your packages contained and you
don't want to have it available
globally, right? Okay. So now that the
virtual environment is created right
here, we have to go ahead and activate
this virtual environment next. So
basically we have to go inside of this
bin and then activate this particular
file. So I'm just going to say source um
uh venv bin and then activate.
Perfect. So now you can see that in the
terminal you can see that we have this
benb right here. So the next thing that
I'm going to do is I'm going to create a
folder called
introduction. So let's create a file
inside of this and this could be a
simple react
basic react agent
basic. Okay. So let's talk about the
dependencies that are required to run a
react agent. So obviously the first one
would be installing the lang chain
classes right so I'm just going to say
pip install langchain and the second
thing since we are building a react
agent basically if you remember what
agents are they're basically the
reasoning ability of an LLM plus tools
right so we might also have to install
lchain community because community in
the community package there are
pre-built tools that we can easily use
so we'll be using one of that and in
this particular section We'll also be
building our own custom tool as well. So
you're able to see the full extent of
what is possible. So let's actually go
ahead and install
langchain community. And the next thing
that I want to install is a chat model
and more specifically Google's chat
model. If you do not know what chat
models are, they are classes that lchain
provides out of the box which lets you
you know interface or rather communicate
with LLMs very easily. So I'm going to
be using the Google's chat model because
it is free and I want to make it make
this particular course accessible to as
many people as possible. So you can just
come to Google if you if you want to
know what is exactly the package that
you want to install. You can just say
langchain and Google chat model and that
is going to give you the exact document
right here. If you want to switch to,
you know, open AI for some reason, you
can just say open AI right here. And
this is going to give you another chat
model. But right now, we are interested
in the Google generative AI chat model.
So let's click on that. And you can see
that this is the package that we want to
install. So I'm going to add this here
as well. And let me think what else do
we need. So we might also, you know, for
this particular chat model, we might
also have to have environment variables,
right? because and then we have to make
that environment variables available in
our python file and for that we might
need the python sorry
python python dash.env env. All
right. So, since there's quite a lot of
packages, it might take a while. All
right. You can see that everything is
installed right now. Great. So, let's
actually go ahead and start importing
our first chat model. So, I can just say
from langchain Google
genai
import chat Google generative AI. Okay.
And uh I just want to test if everything
is working fine. So I just I just want
to make a simple llm call you know
something very silly and make sure that
everything is working fine. So I can
just say llm and I can just initialize
this particular class. So we have the
model right here and I can say invoke on
it. If you do not know what we're doing
right now um I've made a separate again
guys I've made a separate langchain
crash course where I'm going in depth on
chat models but we're not going to be
deep diving into that. So we have the
model right here and we are going to be
invoking on it and we can just say
something like you know give me a fact
about cats I don't know right something
very simple so we are going to be
getting the result right here we can
just go ahead and print it and before I
run it uh you can imagine that it's not
going to work because we haven't really
provided any API key right so let's
actually go ahead and create av file
right here and I'm not really sure you
know what property to put here. So I'm
just going to go back to the
documentation. Let's click on this. And
you can see that this is the key that
the class requires. So I'm just going to
put this here. And as for the key
itself, uh I'm going to go to a
studio.google.com. And uh basically just
login. And you can see that we have the
get API key right
here. And I've already created a couple
of these things. I'm just going to
create another
one. So I'm going to choose this
particular project. Create API key. All
right. So the key has been generated.
I'm going to copy it. Put it right
here. And now the second step is to
actually have this environment variable
be available inside of this particular
file. Right? We need to load the
environment variables. And for that we
already installed a package. So I'm
going to say env. And then I'm going to
import the load
env. Um, so I have to call this
particular method right here. And now
basically everything that we have inside
of here is going to be available in this
file. All right. So now everything
should work fine. So let me actually go
ahead and run this
file. Okay. So the reason why this is
not working is because we have to
provide a model right here. Okay. So I'm
going to say Gemini dot uh
Gemini-1.5 Pro. Okay, so this is the
model that I want to use. All right, so
let's go ahead and run this
again. Perfect. You can see that it uh
returns the content which is cats can
jump up to six times they hide. All
right, great. So now that we know that
everything is working fine, now I want
to go ahead and give a slightly more
complicated prompt that the LLM itself
would not be able to answer. So for
example, we know that LLM is just the
brains, right? it does not have any
capability to access real world
information like it cannot go and make a
Google search. it cannot actually you
know hit APIs or anything like that
right so I want to force the LLM to
answer something that it cannot answer
by itself so you know I'm going to think
something funny um I'm going to
say uh give me a tweet about today's
weather in
Bangalore you can imagine that the LLM
couldn't possibly give an accurate
answer because it cannot access today's
information because it cannot access you
know the um the Google search API or
anything like that right so let's
actually go ahead and let's see what
happens all right so you can see that it
says Bangalow weather today pleasant
with a mix of sun and clouds expect a
high uh you know you can already see
that it has placeholders right here
right and a low of might feel a bit
humid you can see that this is not
really accurate the what is essentially
happening is the LLM is sort of
hallucinating an answer an accurate
answer it is thinking thinking that it
is giving an accurate answer but it is
actually not. Okay. So this is exactly
why we need to provide tools to the LLM
so that it can actually use those tools
and then uh you know call those
particular functions and get accurate
answers. So let us see how we can
actually go ahead and create an agent.
So I'm going to comment both of these
lines out and I'm going to use a method
that lang chain provides out of the box
which is called initialize agent. So I'm
just going to say langchain dot aagents
I'm going to use the agents module and
from that I'm going to use the
initialize agent method and this is a
very convenient method that lang chain
provides that lets you create an agent
very quickly. So I'm going to say agent
and then I'm going to use this
initialize agent and this takes in a
couple of properties. So the first one
is it takes in the list of tools. We
don't really have any tools right now
but we will build it later on. And
secondly, we have to provide in the llm
here as well. So we can say llm equals
llm. And the next thing that I'm going
to provide is the agent type. So you can
see that if I were to hover over this,
we can see that we have to provide the
agent type here. Okay. So I'm going to
provide the
zeros react
description. So I know that we haven't
actually looked at the exact prompt, the
react prompt yet, but we will look at it
in a minute. But we already know that
you know it follows the thought action
observation thought action observation
loop right so we will look at the actual
prompt later on it is baked in inside of
this particular method okay so if you're
wondering also what is this zero shot it
basically means that you're prompting
the agent the agent is doing something
without any prior knowledge you're not
actually giving it examples or anything
it is doing it based on its own
reasoning ability from scratch without
any prior knowledge that we are giving
All right. And finally, I'm also going
to provide verbose equals true. So that
you know, every time some information,
something is happening inside of the
agent, we'll be able to see exactly what
it is thinking, what it is observing,
everything in the console right here.
All right, cool. Now, let's actually go
ahead and invoke the agent. And now we
can provide in our prompt right here. So
I'm going to copy the same prompt that
we put in earlier. And let's see what
happens. We don't really have to return
a particular result. and then print it
out because the verbose is true already.
Okay, so let's actually go ahead and run
this file. Let's clear the terminal and
run this
file. Okay, so it says that got no tools
for zeroot agent. At least one tool must
be provided. Okay, so if you get this
error, just make sure that there should
be at least one tool provided in this
particular list right here. So right now
you must think about what is that tool
or tools that we have to provide to this
react agent in order for it to use it
and then solve this particular problem
of giving a tweet about today's weather
in pack. So you can imagine that we need
the Google search tool right. So for
this there is a pre-built tool. We are
going to be using a pre-built tool first
and then we'll also see a case where we
can write it by ourselves as well. So
I'm just going to go ahead and say
lchain community dot
tools and from this tools I am going to
use the tabi search tool. So if you're
wondering what this tabili search is, it
is basically a tool that is uh
specifically written for you know what
we're building which is the LLM
applications and under the hood it
basically makes a Google search. Okay,
so I'm going to go ahead and say uh
search tool. Okay, let's say search tool
and let's go ahead and initialize this
particular class. And also regarding the
arguments that we have to send inside of
this class, you can see that we already
have plenty right here. We can leave
most of it as default. But the one thing
that I want to change is the search
depth. Right here you can see that the
search depth is set to advanced. We
don't really want advanced for this
case. Advanced basically means that it
is going to collect a lot more
information, a lot more data from the
search results. But we the the query
that we have right here is going to be a
very simple query. So we don't really,
you know, need that much information. So
I'm going to set it to the search depth.
I'm going to set it to basic. All right.
Now I can say tools right here. I can
basically just move this outside and I
can provide this down
here. All right. So let's actually give
this a world
again. Let's see what
happens. Okay. So you can see that it
says the Tavili API key is not provided
because you can imagine that making a
Google search is also something that you
know costs somebody else money. So I
mean but Tavili you know it provides a
free tire as well. So I'm just going to
go ahead and say Tavili API
key. Let's go in here and let's create
an API key
quickly. So if you haven't logged in
quickly go ahead and login.
All right, you can see that I've already
created one thing. Let's create another
one right here. So, let's call it
langraph crash
course. So, you can see that this is
also free. It is rate limited to 100
requests per minute. So, let's go ahead
and create it. So, I'm just going to
copy this key come down to the
environment and what is the name that we
have to provide? You can see that it
requires the tabul API key. So, let's
put it before this. Let's save the file
and also before I run it, let's make it
a funny tweet. All right, so let's go
ahead and run this file. Let's see what
happens. All right, you can see that it
is entering the new agent exeutor chain.
This is a class that lives inside of
this particular initialize agent class.
All right, so let's see what happens.
You remember what would happen. First it
would think about uh the question and
then action and then observation. Think
action observation. All right. You can
see that everything is done right now.
Let's actually go through the steps that
the LLM took in order to arrive at this
final answer right here. So this is
going to be very interesting. Let's take
it one step at a time. All right. So you
remember that the React pattern is all
about think observation. I mean uh think
action observation. think action
observation right so that is exactly
what is happening right here as well so
initially we have the question this is
the question that I gave it this is the
prompt that I gave it give me a funny
tweet about today's weather in Bangalore
now the LLM is thinking what do I have
to do first in order to solve this
particular problem so it is saying I
need to find out what the weather is
like in Bangalore today then I can try
to write a funny tweet about it okay so
this is the first problem that it
identifies
Also the next thing is taking action
right. So it is actually looking at all
the tools that are available to it. If
you are curious how the LLM has access
to you know what are the tools that we
provided here. So this initialize agent
method is doing a lot of magic under the
hood. It is providing the description of
this particular tool as well as the you
know the basically the contents of that
particular function to the LLM. Okay. So
that is why the LLM is able to make a
guess that this is the tool that I want
to use. Okay. So it is saying I want to
take action on this tabi search results
JSON tool right here. So right after it
identifies that you know this is the
tool that I want to use. It is
suggesting an input to that particular
tool. So basically tools are like
functions and functions require inputs.
Right? So it is actually suggesting this
search term right here weather in
Bangalore today. So as soon as the LLM
suggests this particular thing and as
soon as this observation basically it's
streaming right the LLM is streaming
information as soon as this information
keyword is encountered by this
particular tool this this initialize
agent method the control flow is stopped
and the control flow basically comes
back to our langchain environment. This
is very very important. Okay. So up
until this point up until this point is
where the LLM is in control. So right
after this observation keyword is hit
this initialize agent is going to sever
the connection with the LLM. Okay. And
it is going to come back and at the same
time this initialize agent also has
access to okay this is a tool that the
LLM suggested me to execute. This is the
input. So this particular tool is going
to call this particular tabi search
results
tool. Okay, I really hope that you
understood this particular part. This is
very important. If you don't understand
it, you can rewatch it. Okay, so our
lang chain it is going to execute this
tool with the input that the LLM
suggested and this is going to be the
output of the tool. And now as soon as
the output of the tool is is received by
the lang chain, it is going to send the
result again to the LLM. Okay. So the
LLM is thinking that okay this is the
observation. This is the output of the
tool. It is basically looking at the
entire this thing. So you can see that
it is using the weather API u you know
the tool basically this this is the
output of the tool right. So it's it's
got a lot of information. It's got a lot
of information about today's weather in
Bangalore. And then the LLM is basically
looking at all of that information and
then it's thinking to itself, it seems
the weather in Bangalore is sunny and
warm around
27Â°C. That's good information for a
tweet. Okay, so it now has the answer to
the problem. It used a tool uh it
function called and got the output. It
is real world information and now it has
that information. So the final thought
is going to be I can now write a funny
tweet. Okay. So right after the thought
action observation is done it is again
thinking it is thinking to itself ha
okay now I have every information that I
need. So the final answer is going to be
Bangalore weather is so perfect today. I
almost forgot I have responsibilities
almost Bangalore weather sunny side up.
So this tweet is actually very accurate
and is grounded in reality. So this is a
very simple example of an agent using
tools and using the brain power of an
LLM to achieve a particular output. So
now you might have a small question how
exactly is this thought action
observation thought action observation
is happening. So to do that we that is
that is basically just a prompting
technique. React prompting technique is
just a prompting technique. Let's
actually take a look at you know what it
is made of. So to do that I'm just going
to say react
um hub. Okay. So just go to this
particular URL. This is where this is
the particular prompt that the
initialize agent method is using inside
under the hood. Okay. So this is the
prompt. Answer the following questions
as best you can. You have access to the
following tools. So the list of all the
tools are provided to the agent in a
different format that it can understand.
So we are also instructing the LLM use
the following format question the input
question you must answer the thought you
should always think about what to do all
of this we've already seen right action
the action to take should be one of the
tool names and that is exactly what we
saw in the terminal as well and then
action input the input to the action
right the input to the tool and then
finally our observation observing the
result of the action and then this thing
goes on and on and n times until all of
the problems are solved in order to give
the final answer. Okay. So finally we
saying thought I now know the final
answer and then finally we have the
final answer. The final answer to the
original input question. So this the
question the actual user question is
provided right here and this agent
scratch pad is very important. So I told
you in the past few sections where you
know the LLM has complete context about
everything that happened before. Thought
action observation thought action
observation all of that history is
maintained by that initialize agent
method that I told you about. And every
time there is a new LLM call being made
for the next iteration all of that is
provided in this particular placeholder.
The context the history context is
provided right here. I hope that makes
sense. If it does not make sense,
re-watch it, try, you know, reading some
different blogs, attack it with
different directions and I promise that
it will make sense. So, now that we have
a React agent set up right now, I want
to push it a little bit further. I want
to challenge the
agent and I want it to be able to use a
few more tools in order to solve a
particular problem. So, right now, we
know that we've provided it a Google
search tool. So, I want to think of a
problem that would make it ask for
another tool. Okay. So, let's say a
couple of days ago, I I noticed that
SpaceX launched a rocket. Okay. So, I'm
going to, you know, um say a tweet like,
let's say, you know, when
was
SpaceX's last
launch and how many days
ago was that
from this
instant?
Okay. So basically what I'm trying to do
here is that you know just knowing you
know how many days ago or when the
SpaceX launch was that is not enough.
The agent also needs context about what
is the current time of my particular
system right here. I'm currently in
India right? If somebody else were to
you know run the agent it's going to
show a different time. So our agent not
just needs the Google search facilities
but at the same time it also requires
the current system time which we have
not provided as a tool just yet. Okay.
So we will be providing that later on
but for now I just want to run it and
let's see what we actually get. So I'm
going to run this. Let's clear this
out. Let's see what happens. So again
you can see that entering new executor
agent exeutor chain.
Okay, so the question when was SpaceX's
last launch? This is what I provided and
you can see that it is thinking. I need
to find the date of the most recent.
Okay, let's actually scroll up. It is
going to think for a while. But let's
actually make sure that you know we
understand what is happening right here.
So it is thinking to itself, I need to
find the date of the most recent SpaceX
launch and then calculate the difference
between that date and today. I can use
the Tavi search results tool to find
this information. Okay, so this much you
know it is first figuring out first part
of the problem which is when did the
SpaceX's launch happen. So it is
actually providing the input to the tool
which is SpaceX most recent launch date
and this is going to be the result of
the tool. It is again being sent back to
the LLM and then fine let's see what
happens next. So it is thinking to
itself. The search results indicate the
most recent launch was a Starlink
mission on January 1. Uh I think it's
18th, 2025. And now I need to calculate
how many days ago was that. Okay. All
right. So uh you can see that it it this
is the second problem that it needs to
solve. And now it is taking an action
calculate input I mean calculate days
since. Right? So the action input is
going to be uh this thing. Uh you can
see that this is not even a valid tool,
right? We haven't actually provided a
tool like this. It is actually starting
to hallucinate a tool, you know, sort of
trying to fit uh its own flow, right?
But it's not going to work. It is
actually saying the observation
calculate day sins is not a valid tool.
Try one of these particular tools. And
it's saying that thought I can't
calculate the days since um since the
date using the available tools. I'll try
searching for the number of days between
this and this. I'll need today's date
first. And then it is doing another.
It's actually trying to figure it out
using the one tool that it has. Guys,
this is amazing. Um so you can see what
is what is today's date, right? And then
it is making another API call. Today's
date is February 7th or February 8th
depending on the source and the time
zone. Right? Right now it's it's
actually neither right right now it's
neither currently it's ninth where I am
right now but it is not accurate because
we are not really providing the current
system
time. So you can see that there's
another observation and then you know
it's it's not working. This is a an
endless infinite loop problem right if
you don't provide the right tools there
is an infinite loop happening and we get
problems like this which is not going to
work. Okay. So, which is why this
actually brings us to another problem
which we'll look at later. This is one
of the issues that we can run into when
you know we try to work with React
agents if we do not provide the right
tools. So let's deal with that later but
for now I'm going to provide a current
system time tool so that the agent can
actually make use of it get my current
system time using a Python library
datetime library and that should uh you
know it should be able to solve it very
easily. So to do that I'm going to paste
a tool that I've just written. Okay. So
this is a this uses the datetime module.
I'm just going to quickly import the
date time module right here. Okay. So
let's walk through what is exactly this
particular function. Okay. So this
function basically just returns the
current date and time in this particular
specified format. Okay. So this just
takes so this just makes use of the
datetime module and then it formats it
based on this particular formatting
right here. So if the LLM in the future
were you know needs only the time or the
minutes or the years or something it can
actually modify this particular input
argument. But for now we're just having
a default argument right here which
we're providing and this is just going
to return the current system time. So
now let's actually make this tool
available to the agent. So to do that we
we can just put it in this particular
array and that is it. But if you run it,
it's not going to work because there is
one more thing that we have to do, which
is we have to explicitly let lang chain
know that this function is a tool. And
to do that, we can just go right here.
We can just use this tool, this thing.
And then we can use this particular
notation. And this is going to make it
apparent to lang chain that this is
going to be a tool. So accordingly, it
is going to format it and send it to
LLM. Right. All right. So now it should
uh so now to to solve this particular
problem the agent has all the tools
required. So let's actually give it a
whirl again and we should be able to get
the final answer. Let's try
it. And also guys you can see that it
says land graph you know it's already
suggesting that you know this is
deprecated use land graph right because
that is the direction that we are also
going in this particular course. But for
now let's actually learn how react
agents work first.
Entering new agent executor chain. Let's
wait for a while. All right. So the
question, this is my question. Thought I
need to Okay, let's scroll up. So
thought I need to find the date of the
this thing. This we've already seen the
last time. Uh it's searching this thing.
It's sending this particular keyword to
the search tool. The search tool is uh
you know calling and it's outputting
this particular result. Coming down, it
is again thinking it seems the last
confirmed launch is January 15th. I need
to get today's date to calculate the
difference. Right? So it is saying this
and now it is properly going and looking
at the tool that we've provided which is
the get system time tool. It is also
providing the input that the LLM wants
the tool to execute with. Okay. So
instead of instead of this hours,
minutes, seconds, it is only providing
years, minute uh months and date, right?
It is actually making an intelligent
decision right here. And the again the
control flow comes back to lang chain
and then this tool is partic this
particular tool is executed by langchain
and then this is the observation which
is again sent to the LLM and then
thought today is February 9 2025 the
last launch was around January 15th
that's roughly 25 days ago. So the last
SpaceX launch was around January 15th
which is approximately 25 days ago. So
this is an accurate answer that is
grounded in reality and that the agent
is actually making use of real world
APIs, real world functions, real system
time in order to calculate this thing.
And that is the power of React agents.
But a couple of minutes ago we saw that
we ran into the infinite you know uh rec
iteration problem right because if we do
not pass on the right tools then you
know the it can actually misbehave. So
that is one of the disadvantages but
there's also a few other disadvantages.
So I hope that this was clear. In the
next section we will actually look at
what are the disadvantages when it comes
to using react agents and how does lang
graph actually solve those problems. So
I'll see you
there. Hello guys. So in the previous
section we went ahead and built a react
agent using lang. In this section, we'll
actually go ahead and look at what are
the drawbacks of using react agents
using lang chain and where does lang
graph actually come into the picture. So
let's look at an advantage actually of
react agents. So we saw that the react
agents are flexible, right? Any state is
possible right here. Okay. So what we
mean by that is we start the execution
and if we have the first tool, it
executes the first tool and comes to an
end. If there is a tool two, it executes
a particular tool two to figure out the
problem and comes to an end. And there
is also situations where the tool one
could be executed first and the tool two
could be executed first. And it could
also be the opposite. Okay. So we in the
previous section we saw it executed two
tools, right? First it used the tabi
search tool to make a Google search and
then figure out you know when was the
SpaceX launch was and then it figured
out the current time. It could have also
done it the opposite way. It could have
also figured out the current system time
and then went ahead and did the tabi
search as well. So that is what we
seeing right here. Okay. So react agents
are very very very flexible. But high
flexibility can also mean less
reliability. Okay. So this is an example
that we saw as well wherein you know the
tool one keeps on getting called again
and again and again and it goes on till
infinity. Thankfully uh Google Gemini it
stopped it right there. But you know
infinite loops are a pretty big problem
with React agents. That could be a
couple of reasons why this happens. We
see that you know we did not define the
tools correctly. The LLM is not capable
enough or the prompting doesn't define a
clear end condition. Okay. Now so
basically we have the chains on one hand
we have the react agent on the other
hand. So this is what we saw just now.
The react agent is very very flexible
but it is less reliable because it is
not really in our control completely.
All right. And on the other hand we have
chains right here. We we saw that chains
are like a fixed assembly line. Right.
It is not really flexible but it is more
reliable. Right? So we need something
that has the best of both worlds. We
need something that is going to be
flexible but at the same time reliable.
And that is exactly where LAN graph
comes into the picture. Okay. So it is
going to be flexible as well as
reliable. So this state machine that we
see right here, this is where LAN graph
comes into the picture. All right. So
let's what is LAN graph? Let's look at
the textbook definition of what is a LAN
graph. It is a framework for building
controllable persistent agent workflows
with built-in support for human
interaction, streaming and state
management. It uses graph data structure
to achieve this thing. So this is a very
simple preview of how the graph data
structure looks like and how we can
actually build agents. So we have the
start node right here and we have the
agent right here. There is an action
that agent takes and then it can
continue this particular thing and then
once it has reached the end point it
comes to an end. Okay. So I don't want
you to think too much about what this is
but just know for now that this is how
the structure looks like. So let's look
at the some of the key features of LN
graph. So the first one is looping and
branching abilities. We've already
touched upon it in a a few sections ago,
but let's look at it again. It supports
conditional statements and loop
structures allowing dynamic execution
paths based on state. State
persistence automatic saves and manages
state supporting pause and resume for
longunning
conversations. Human machine interaction
support. It allows for inserting human
review during execution, supporting
state editing and modification with
flexible interaction control
mechanisms. Let's look at the fourth
one. Streaming processing supports
streaming output and real-time feedback
on execution status to enhance user
experience. Seamless integration with
lang. Reuses existing langchain
components. supports the lang chain
expression language and offers rich tool
and model
support. And now you might have a
question why do we use the graph data
structure for building land graphs? Why
not use some other easier data structure
to handle it? So the answer is that you
can see that a lot of the papers a lot
of the research papers that have been
published to solve difficult problems
they all use the graph data structure
because it is flexible but at the same
time it is controllable. It gives you a
lot of flexibility but at the same time
we can actually have a lot of autonomy a
lot of autonomous decision- making baked
in into those particular into those data
structures and that is it for the
section in the next section we will see
what are the core components of LAN
graph so I'll see you there hello guys
in this section we will look at what are
the core components of LAN graph so you
can see that the core components there
are four core components we have nodes
edges conditional edges and state so
I'll give you a very simple example to
understand these different components
and that example is the reflection agent
pattern. This is something that we will
be looking at sort of deep diving into
the later part of the course. But for
now, I'll give you a very small overview
of what this is capable of. So whenever
we talk about a reflection agent
pattern, we're always going to have
something that is going to generate
something and we're al always going to
have something that is going to critique
that particular thing. Basically, you
know, this is not good. this is not
right. We have to make this better. So
this is the criticizing component and we
have the generation component. So as a
whole what this is doing is one agent is
going to generate a tweet. Okay. So this
agent's only primary job is to generate
a tweet and anytime you know something
else criticizes that particular you know
tweet it is going to make the tweet
better. So this only focuses on
generation and this this LLM's only job
is to criticize a particular tweet. It
is going to say okay to make this viral
you have to make this short. You have to
make this punchier. You have to you know
add a call to action. Okay so that is
what this does. So basically whenever a
graph is going to start we are always
going to have the start node right here
and then right after the start node is
executed the control flow is coming to
the generation tweet LLM right after the
tweet is generated there is going to be
two different options okay so there's
going to be two different options it can
either go to this particular criticize
tweet LLM or it can end the process
right here okay so it is let's say it is
going to come to this particular node in
uh in that case this LLM is going to
look at the tweet that this LLM
generated and it is going to critique it
and that is why we have an arrow
pointing right here. Okay. So it this
can loop through several times to make
the tweet really good to polish it to
iteratively make it better and better
and better and after a certain number of
iterations that we specify as developers
it can actually instead of going to this
particular branch it can end this
particular process. And the reason why I
wanted to rush through it is because we
are just learning the core components of
LAN graph right now. So let's look at
what are the core components. So these
things that you see right here this
start block this generate tweet LLM
block this end block. So these are what
is basically called nodes. Okay. So if I
come back here this nodes that you see
right here these are the nodes. Okay.
And let's look at the second one edges.
So what are these edges? So you see
these straight these lines these are
dotted these are not edges these lines
are the edges basically whatever
connects two nodes are what is called
edges all right let's look at the third
one conditional edges what does this
mean so I have actually made it very
clear that this dotted line is what is
going to be called the conditional edge
because right after the execution of
this node it can either go here or it
can either go here based on a condition
all right so that is We have a
conditional edge right here. And
finally, we have state right here. So
state uh it should be pretty intuitive
to to understand state. Even in this
particular simple example, you can you
can sort of imagine that this LLM is
generating something and then this LLM,
this red LLM is going to give a feedback
and then this green LLM is going to
generate something for that again. So
the entire context needs to be
maintained, right? So that is why the
state is also going to be a core
component of LAN graph. So I hope that
makes sense. In the next section we are
going to be deep diving into what this
reflection agent pattern is, how to
build this, what are the why do we even
have to build it, what are the real
world applications and we'll actually be
building it from scratch using code as
well. So I'll see you
there. Hello guys. Uh welcome to the
section where we are going to be deep
diving into reflection agents in land
graph. Uh this is particularly important
because this is sort of like the entry
point for you to start learning land
graph and you know how powerful uh you
know um certain uh agents can be. Okay.
So in this section we are going to be
starting to uh look at you know what is
a reflection agent system you know what
are the three types of reflection agent
systems. uh we'll do the setup and
installations and finally we will
implement a reflection agent system
using our code. So by the end of this uh
particular section the reflection agent
it could go on for a few more sections
as well but by the end of it you should
have a pretty good idea you know how
powerful this can be and where you can
actually sort of apply this type of an
agent this reflection agent in your you
know
workflows. So uh before we jump into you
know all the nitty-g gritties of you
know implementing it using code etc etc
before that let's actually just
understand the very English word called
reflection okay so just like how you're
looking at your reflection in the mirror
reflection means looking at yourself or
your actions right so that's what
self-reflection is right so for example
after giving a presentation you're
thinking about how it went uh after
writing an email you're reading it again
just so it's clear Right? And also uh
you know after making a decision
considering if it was the right choice
right so this is what reflection is in a
nutshell. Right? So it's basically like
having a mirror in front of you and
looking at you know basically could have
done something better. You know thinking
to yourself about you know what you've
done in the past right so now let us now
that we know what reflection is let's
dive into what is a reflection agent. So
what is a reflection agent pattern? A
reflection agent pattern is an AI system
pattern that can look at its own outputs
and think about them or make it better
just like how we look at ourselves in a
mirror and self-reflect making ourselves
better. A basic reflection agent system
typically consists of a generator agent
and a reflector agent.
So here's a diagram that I've prepared
about you know this is a very simple
application that we are going to be
building which is uh a very good example
of a basic reflection agent pattern. So
in the previous slide we saw that every
reflection agent pattern is going to
have a generation agent and a reflector
agent. So we are going to be using that
dynamic to help me create the best tweet
possible, the best viral tweet possible.
Okay. So this is a very simple graph. Uh
so we have the start node right here and
then we have the tweet generation agent.
Okay, so this is going to be the first
agent which is going to be the
generation agent. And then as soon as
the tweet generation agent is done
generating a particular tweet based on
my input, based on my topic that I'm
providing it, what is what ends up
happening is that this is a conditional
um edge, right? So it is coming to this
tweet critiquing agent. Okay. So what
this tweet critiquing agent's role or
responsibility is is that it looks at a
particular tweet. Okay. Basically the
prompt of the critiquing agent is
something like this. So you are uh you
know you are a critiqueer of tweets and
you critique such that the tweet can be
made viral something like that. So
basically just looks at a tweet you know
give suggestions okay this is too long
you have to make it short you have to
add a CTA you have to add a punchy you
know intro uh a hook right so add these
hashtags. So this this agent just like
critiques it, critiques it, critiques
it. So as soon as generation is done, it
critiques it and the critique of that
thing is going to be passed to the
generation agent and then the tweet
generation agent makes it a little bit
better and then sends it to the critique
agent again and then the loop continues
on and on and on for a few times like
four or five or six times that we set
it. But you can imagine that with every
single iteration the tweet gets better
and better and better and it comes close
to a very viral tweet, right? You can
actually think of an application or a
product where you can actually add this
as a feature and your clients your
customers would love it. Right? Uh there
this is another example that a lang
chain website the documentation provides
where you know it sort of explains what
a reflection agent pattern is. Right? So
the first system that you see right here
this is going this is not the reflection
pattern. Okay. So system two is the
reflection pattern. So let's look at
what are the differences between the
two. So system one is basically how you
would normally use you know chatd or
claude right you just put in a prompt
let's say hey uh agent your your
responsibility is to make a viral tweet
uh you know uh write me a tweet um and
this is going to be the topic right so
it's a very simple prompt it's a it's a
it's going to do it in one shot right so
that's going to be system one it is
going to be very fast it is subconscious
meaning it's not really thinking like
actively thinking about it. It's just
passively with its own reasoning
ability. It is going to generate
something right. Uh it is also
automatic. It is everyday decisions.
It's really good for everyday decisions
and errorprone. It is a little bit
errorprone as well because it's not
really deeply thinking about something.
I hope that makes sense. This is what is
called system one, right? But this is
what we are interested in which is the
reflection agent pattern where it is
going to be a little slow. If you think
about it, since we have two agents right
here, one is generating, one is
reflecting and then they are sort of
like, you know, working together to make
the tweet better like in a loop, right?
So in that case, you can imagine that uh
it is going to be a little slow. It's
not going to be as fast as a oneshot
prompt. And you can also imagine that
the system two, the system two which has
two different agents, it's going to be a
little bit more conscious as well
collectively together, right? And it is
effortful, right? Complex decisions,
right? There could be certain situations
where the decisions have to be really
precise and not just like uh you know
just subconscious but rather it needs to
be actively thought out right so in that
case you know if you want it to be
reliable you can go for this reflection
agent pattern right here. So this is
another simple reflection loop diagram
that the documentation provides. So
first we are whatever it is we're first
generating you you don't really have to
think about tweet you can think about
pretty much anything out there right. So
it is generating something this is going
to be the initial response and then
another brain another LLM or another
agent is going to reflect on it uh you
know reflecting as in like it's going to
give some critiques you know merits okay
this is what went right this is what
went wrong right so it is going to
provide all of that and again it's going
to the first brain the generate brain
and that is going to incorporate all of
these you know uh uh critiques and then
it's going to uh you know make the post
better it's going to revise the existing
post better based on these things and
And then this loop is going to go you
know repeat n times which is put right
here. And then after a certain you know
iteration loop is reached it is then
going to go to the user. So we'll also
look at the types of reflection agents
in LAN graph. So we have three types. We
have the basic reflection agents which
we saw in the previous slides wherein
you know there is this reflector chain I
mean the reflector agent and the
generator agent working together and
that is the basic type. We also have the
reflexion agents. uh that is something
that builds on top of the basic
reflection agents. We'll look at that uh
after the section and then we also have
the language agent tree search or lats.
So basically I want us to first get
really comfortable with this first basic
reflection agents so that you know
learning the other types is going to be
very simple for us. So let's actually go
ahead and implement a basic reflection
agent using LAN graph. So I'll see you
in the next
section. Hello guys. In this section,
we'll be going ahead and building a very
basic reflection agent using LAN graph.
And this is going to be the first
section where we are actually going to
be touching on LAN graph. So I'm pretty
excited. We also have a condition right
here. Okay, should continue or not. So
basically in this should continue node,
we have like an iteration, you know,
only after four iterations you can go to
end. So until that four iterations is
reached, always go to the reflect node
and then the reflect node is going to
give some critique. it's going to be
passed on to the generate node and then
the generate node is going to generate
something else and then it's going to
come back here. Now the should count is
going to be two. So you can imagine that
it allows for let's say four different
four more iteration cycles. So the first
thing that I want to do is I want to
build out the generation chain and the
reflection chain alone. And once we
built both of those chains then we will
in the in the next section we will go
ahead and create the graph and put it
all together. So let us do that. We will
go back to our Visual Studio Code. So I
am in my VS code. What I've basically
done is I've created a folder right here
called the basic reflection system. And
then inside of which I've created a file
called chains. py. So this is basically
the file where I want to sort of put all
of my uh simple I mean the chains alone
together. This is not the file where I
want to create the graph. I'll export
the chains into the graph and then I'll
start using it. So right now we just
want to create two chains. We need one
chain for the reflector chain and then
another chain for the generator chain.
Right? So in this file I've imported two
things. Uh chat prompt template messages
placeholder. If you're familiar with
lang chain then you should probably know
what these things mean. If you do not
know uh basically you know chat prom
template is like basically like you
create a template and then dynamically
you sort of input different values. So
for example you know you are a helpful
AI pot your name is this thing. So in
the future if you want to use this
particular prompt you can just pass in
the name to be Bob and the user input to
be what is your name and then this is
going to dynamically replace these
placeholder values. Okay so below this
I'm just going to copy paste the
generation prompt. So this is going to
have a simple system message. Okay. So
basically what it says is that you are a
Twitter techie influencer assistant
tasked with writing excellent tweet uh
Twitter posts generate the best Twitter
post possible for the user's request. If
the user provides critique, respond with
a revised version of your previous
attempts. Okay, so you can imagine that
the first system, the generation
system's only job is to generate a
particular tweet. And if at all the
other system, the other agent provides a
critique. It needs to respond with a
revised version of your previous
attempts. And that is exactly what we're
specifying right here. And what is this
message placeholder? Basically in the
future you know we are not going this is
not this alone is not going to be
helpful right there needs to be a lot
more messages like you know uh this
would generate something and then there
there would be a reflection there would
be a generation there'd be a reflection
so you can imagine that there is going
to be a good amount of you know history
getting generated and whatever history
is getting generated at that point we
will input all of this inside of this
particular area. So that is what this
placeholder is for and we have the name
for that as well which is called
messages. All right. So let's also go
ahead and create the reflect prompt. So
this is going to be the reflector
prompt. Uh so you can imagine that
basically the reflection agent's only
work is to look at a generated tweet and
then make it better. Right? Okay.
Critique it. Right. So that is exactly
the prompt that we provided here. So the
system message is going to be you are a
viral Twitter influencer grading a
tweet. Generate critique and
recommendations for the user's tweet.
Always provide detailed recommendations
including requests for length, virality,
style, etc. Okay, so this reflection
prompt's job is to make the tweet better
by criticizing it. Like really like this
is bad. Okay, do it better. Do it
better, you know. So now let's go ahead
and create chains out of these prompts.
Okay, we have to create chains out of
these promps so that we can export it
and use it inside of our graph later on.
So to do that, I'm just going to use the
same chat Google generative AI model and
then I'm going to use the lang chain
expression language to actually go ahead
and generate a chain. So I'm going to
call it generation chain equals and let
me just put it right here. I'm going to
pipe it with this llm. That is that is
it. Very simple. Okay, I'm and then I'm
going to do the same thing for the
reflection prompt as well. So let's make
this let's call this chain reflection
prompt and then llm and that is it.
Okay, so now we've got two different
chains right here and that is what we
are going to be importing into our graph
and then connecting it all together in
the graph. So I'll see you in the next
section.
Hello guys, in this section we are going
to be building our own very first graph
and we are going to be building the
reflection agent using some of the
chains that we built in the previous
section. So you can see that I've
already gone ahead and created a simple
file called basic py alongside with the
chains py that we created in the last
section and also you can see that I've
imported a few things. before you start
importing it in your own code. Uh just
go ahead and make sure that you've
installed this particular package
langraph. Okay, so this is going to be
the command. Very simple. Just do that
and you know you should be able to
import some of these things. All right.
So let me walk you through what are the
things that I've imported. So we have
the from typing import list sequence.
This you can think of it like a very
simple type checking class like you know
very similar to TypeScript. um not
really important for the actual
execution of the graph but it helps us
make sure that there aren't any errors
and then secondly we have the
env basically just like loads up all the
environment variables and then we have
the messages classes the base message
and the human
message this I think if you've worked
with LAN chain you you should be pretty
comfortable with it already and then
this is what we are going to be doing a
little bit new so we are importing from
langraphgraph we We are importing the
end class and the message graph class.
Okay, so this is pretty important. We
will come to this in a minute. And then
finally we have from chains and we're
importing these two chains that we have
written in this particular file right
here. Okay, very very simple. So let us
quickly take a detour and really
understand what this message graph is
all about so that we can confidently
proceed forward with the rest of the
code. So let us see what is a message
graph. So it is a class that land graph
provides that we can use to orchestrate
the flow of messages between different
nodes. So this graph that you see right
here, this is going to be a message
graph that we are going to be using. So
the example use cases are simple routing
decisions, simple chatbot conversation
flow. And what we are doing is something
that is very basic. So we are going to
be going with the message graph. So if
you just want to pass messages along
between nodes, then go for message
graph. Okay. So we will look at what
this means in a few minutes. And there
is an other type of graph which is a
state graph. And you know if your app
requires complex state management then
we can go for state graph. We'll look at
more of this in the future. But for now
for this particular use case we uh we
can actually use the message graph.
Okay. So uh let's actually dive deeper
into it. So to put it simply, message
graph maintains a list of messages and
decides the flow of those messages
between
nodes. So every node in message graph
receives the full list of previous
messages as
input. Each node can append new messages
to the list and return
it. The updated message list is then
passed to the next node. So basically
each of these nodes receive as the input
all the previous exchanges that happened
and then each of these nodes can
actually receive the entire list and
then append another output a response
and then send it to the next node and
then the next node can do the same
thing. It receives the entire input and
then it can use that and generate a new
response and then append it to the list
and then pass it back to generate and
then reflect and then generate. So that
is a very simple workflow and that is
exactly why we are going for message
graph. So let us actually now go ahead
and start building our own graph. So to
do that I'm just going to come down here
and then I'm going to invoke the message
graph class. So I'm going to call it
graph message graph and then I'm going
to invoke it. Perfect. So we have the
graph ready. Now there's a couple of
things that we have to do. So the graph
is currently empty, right? So I'm just
going to first go ahead and create the
generate node and then I'm going to go
ahead and create the reflect node. So
once I have all the nodes ready, I'm
going to add it to the graph and then
I'm going to connect those nodes
together. Okay, so it's very much like
you know drawing a diagram and then
controlling the flow of you know the
control flow between nodes. So first off
I'm just going to you know name the
nodes properly. So we have the generate
node and the reflect node. So to make it
easy, make our lives easier, I like to
usually go ahead and create a constant
like this. So
reflect and then generate, right?
Generate. All right, perfect. So now
let's go ahead and create the generate
node and that is nothing but a simple
function. All right. So I'm just going
to say generate node and depend if you
remember from the previous slides we
know that every node receives the entire
state right and then we are invoking on
that state using the chain that we've
created in the previous section and then
we are appending the response to the
history. Okay so that is what we are
going to be doing. So this is going to
be the state. You can basically think of
it as a list of messages that happened
in the past. Okay. So this is going to
be the list of messages and then this we
can actually make use of the generation
chain and then we can invoke on it. So
basically whatever state has been
generated or acred all of that we need
it to sit right here. Okay. So we have
the messages placeholder. So let's
actually go ahead and say messages and
then we pass in the list of messages and
that is it. So this what this is going
to do is it is going to generate the
tweet based on whatever is existing in
the past. So later on when we actually
invoke this graph we will provide the
prompt the main user prompt and that is
going to be what is appended and it's
going to be provided right here. Oh and
also don't forget to return this
particular thing. So whatever is going
to be the returned object of the LLM, it
is just going to extract the content
property alone and then it is going to
append that message to the existing
state right here. Okay, I hope that
makes sense. Let's now go ahead and
create our next node in our system which
is going to be the reflect node. So I'm
going to say reflect node state. So here
we are going to be doing something very
similar. So here uh we are going to be
using the reflection chain and then we
are going to
invoke the
messages and then finally the same
state.
Okay, perfect. So uh I hope that makes
sense. There is also one more thing that
I want to do. Um so we have a generate
node. Obviously the generation is being
taken care of by the AI and whatever is
returned is considered to be the AI
message. But usually in real life the
reflection is being done by a human.
Right? So we can actually trick the
system into thinking that the the the
message that is being returned is by a
human. Right? So we can also do that. So
I'll tell you how. So we can actually
call this to be the response and let me
just paste something here.
Return
response. Okay. So we can also do
something like this. So the previous
version that is perfectly fine as well.
first AI is generating the second AI is
critiquing the first AI is generating
the second AI is critiquing that works
perfectly fine but uh for readability
for tracing using langid etc etc
sometimes it might make sense if the
reflection is actually if we're tricking
the system into thinking that the
reflection is being done by a human so
in that case we can actually return a
list and we can just pass in the content
right here and this content is going to
be appended to the existing list of
messages okay so this is also a
possibility I'm just giving it to you.
But if you don't want to complicate your
system too much, we can just leave it as
it is as well. Okay. All right. So,
we've already gone ahead and created
both these nodes. Let's now go ahead and
add these nodes to the uh graph. So, to
do that, it's very simple. I can just
say graph add node. And in this node,
the first one is going to be the name of
the node and then the actual function,
the generate node. And then secondly we
also have let's also add the next node.
So we can just say reflect and then
reflect
node. All right. Perfect. And also we
can uh you know specify the entry point.
Okay. So there is always going to be an
entry point. So in this graph if you
look at the entry point it is going to
be this generator. You might ask okay
why is it not start right? So the answer
to that is start is something that is
like you know inherently sort of assumed
to be the entry point. So it's not
really considered explicitly considered.
It's more like implicitly considered. So
let's actually use the generate node
which is going to be the first node
where the entry is going to happen. So
I'm going to say the entry point is
going to be this particular generate.
Okay. So let's come back to the diagram.
Let's see what else is there. So we now
need to create this particular should
continue function. Okay. So the should
continue function is basically keeping
track of how many iterations have
happened and if you know we can actually
set okay after four iterations you stop
the cycle go to the end give the user
the end output which is the more refined
tweet uh so until then you know just
like uh you know after the generate come
here go send it to the reflect instead
of the end so it just does this four
times and then it is going to go to the
end right so let's actually go ahead and
write this should continue function so
to do that I and just say should
continue and this again is going to get
the list of messages and here I can say
if the length of the state if that were
to exceed four okay in that case I want
the uh the the graph to end right so I
can just say end right here and you can
see that I've already imported this
thing from the graph right it is a
pre-built uh uh uh constant and uh then
we if that is not the case I can just
return the reflector Okay, I can say go
to reflect. Okay, so so far we've
created the generate node, the reflect
node, the should continue node, right?
We've also specified that this generate
node is going to be the entry point as
well. But we haven't really done any
sort of connecting yet. So the first
connection that I this this is going to
be the connection, right? So these edges
are going to be the connection. So the
first thing that I'm going to do is I'm
going to say add conditional edge. Okay,
so the conditional edge is going to be
this thing right here. Okay, so right
after this should continue, it can
either go to the reflect or it can
either go to the end, right? So this is
going to be a conditional edge. This is
not going to be a normal edge. So what
you see right here is a normal edge.
What you see right here is a normal
edge, but this is going to be a
conditional edge. So what I'm going to
do is I I'm going to say right after the
generation is done, right? Right? After
the generation is done, this should
continue node is going to actually, you
know, branch off to two different
things. Okay? So that is why we are
putting this add conditional edge right
here. Okay. I hope that makes sense. So
so far we've done this particular
connection. The should continue should
go to reflect or should continue to go
to end. But if it were to go to reflect
then there is only one option which is
to go back to the generator. Right? So
let's connect let's actually connect an
edge between the reflect and the
generate. So to do that I'm just going
to add a very simple edge. So this is
called add edge. Okay, this is not going
to be the add conditional edge. So the
first one is going to be reflect. So
this is going to be the from node and
then we are going to have the two
node. So you can imagine that this is
very much like just drawing a diagram.
Right? So we've done the we've connected
these two lines. Now the reflect is
going back to the generate. So what else
is left? All right guys, looks like
everything is done. So let's just go
ahead and compile the graph. So I'm just
going to give it a constant named app.
And then I can just say graph dot
compile and this should compile the
graph. So before I actually, you know,
provide a topic for the tweet or provide
an existing tweet and ask it to make it
better. Before I do that, I just make
sure that you know the graph is looking
exactly how I expect it to just to get
ahead of any problems that may happen.
Right? So to do that, I'm just going to
copy paste a few lines of code. So
basically what this is doing is that you
know this is going to give you a mermaid
diagram and an ASKI diagram basically
just it's like a a very simple
visualization of the graph. So right
here you know everything is like neat
and tidy but we just need to make sure
that it is the same thing that is being
replicated right here as well. And just
one more thing guys, if at all you get
stuck anywhere, uh you've got two
options, uh I'm putting this repository
in the in the description below. So you
can always go pull the code and then run
it yourself and that is sure to work.
And uh if at all despite that you're
getting some clarifications, I'm more
than happy to answer questions in the
comment section. So just go ahead and
shoot your questions. So yeah, so as I
said, this is going to give you the
mermaid diagram and this is going to
give you the ASKI diagram as well. So
let's go ahead and run it. And if you go
ahead and run it, there is going to be
one additional package that it might
require uh you to you know install. And
that is going to be this pip install
grandal. Okay. So just go ahead and
install it and you should be able to run
it uh without any problems. So let's go
ahead and run it and let's see what
happens. All right. So perfect. So this
is going to be the
uh this first one is going to be the
mermaid diagram. So as you can see we've
got the start start is going to the
generate right the start is going to the
generate and here there's going to be a
simple edge whereas the reflect uh going
to the generate is also going to be a
simple edge right here right so these
are the two simple edges whereas this
generate going to the reflect is going
to be a conditional edge and that is
exactly why we have the dot in the
middle of this particular arrow the same
thing for the generate going to the end
as well so it is looking pretty well and
also You can see that we've got this
other diagram as well where the start is
going to the generate and the generate
is going to the reflect and the end. So
everything is looking fine. So now what
I'm going to do is I'm going to provide
it a tweet, right? I'm just going to I
basically this is a system that really
helps me churn out really viral tweets
really quickly. And I just need to
provide it a topic or some context,
right? So what I'm going to do is so we
have the app right here and uh what I'm
going to do is I'm just going to say app
dot invoke and inside of this I I'm just
going to pass in a human message. Okay.
So basically whatever we pass in here it
is going to be added or rather appended
to the existing list the message history
list that I told you about. Right. So
this is going to
basically so let me just type it out and
let me explain it on the way. Okay. So
the content is going to be I want there
to be a tweet written on AI agents
taking over content creation. So I'm
just going to say AI agents taking over
content
creation and I'm also going to you know
assign it to this response and then
print it out as well. All right. So
let's let's hope that everything works
fine. Um so essentially if I were to hit
play what would happen is that this
human message is going to be uh you know
that the entry is going to be the
generate node right? So this state which
is going to be a list of messages right
now it is only going to have one message
right which is the human message that we
are providing and that is going to be
put in the place of this variable
message in the generation chain. So each
of those chains would have their own
individual uh system messages but the
history is going to be shared it's going
to be shared between both of those
chains. Okay. So there is this is a
confusion that a lot of people get.
There is only going to be one message
history. Okay, there's not not going to
be two message histories just one
message history. The system message is
going to be separate for each of the
chains and both of those chains are
going to share the increasingly getting
uh message histories. I really hope that
I'm not confusing it but if at all you
have any questions ask me or just go
ahead and ask Chachi Pitty. So yeah,
let's actually go ahead and run it.
Let's make sure that everything is fine
in the chat as I mean the chains as
well. Okay, so let's go ahead and run
it. Okay, so the diagrams are getting
generated. Okay, it's taking a while um
because you you can imagine that you
know the oh okay so it's actually here
guys. So you can see that um great. So
you can see that the first human message
that I've provided is right here and
then there's going to be a lot of other
information like you know oh okay so the
AI message is also here right so yeah
this is going to be the first creation
of the generation chain right so we have
the AI agents are revolutionizing
content creation from automatic tedious
tasks to generating creative text
formats they're empowering creators to
focus on what matters most right and
then uh Yeah.
Uh what matters most telling compelling
queries uh I mean stories and we've got
several hashtags it has provided. So the
generation chain has did its work right?
So I'm providing it a title and the
generation chain is doing its work
properly. So now what would happen the
reflect chain is going to critique that
tweet that the generation chain has
generated. So let's see where that is
happening. I know this is a little not
really that readable but don't worry we
are going to be uh deep diving into what
is exactly exactly happening in the next
section where we are going to be tracing
everything using lang but for now you
can see that
uh great so this human message is right
here so you if you remember we tried to
mimic a human message getting basically
the critiquing voice is going to be a
human message because that's how we're
doing it right this makes it very
readable for us we can put put it as a
AI message we can just leave it as it is
as default but when we because we put it
as human message this is going to be
very readable for us. So here this is
going to be the critique right. So your
original tweet was concise but lacked
context and excitement. It presented a
statement without engaging the audience
or offering any further information.
While brevity is good, it needs to be
paired with impact. Okay. And it is also
providing recommendations. So the
length, the revised tweet is slightly
longer providing more context while
remaining. You can see that it's
providing a lot of different things,
right? That's this is amazing. So if you
if you actually you know you can just
pause the screen and then give it a read
you you would be able to see that you
know human message is critiquing it and
then the AI message is again like you
know making it better and the human
message is crit critiquing it and let's
see how many iterations would it go for.
Okay so right now we've just set it to
two iterations guys. So you can actually
increase it to let's say four and let's
see what happens. I'm just going to
increase it to you know what let's
increase it to six. I want there to be,
you know, three different iterations
back and forth, back and forth, back and
forth. Okay, so if I were to do that,
let's see what
happens. Usually, it's not going to be
enough. Uh there needs to be at least
like a couple of iterations for the
tweet to really, you know, get become a
viral tweet, right? So, let's give it a
uh so let's give it a
while. Okay, so it looks like it gave an
error. it is saying that you know the
the iterations uh this thing has been
exhausted and usually this tends to
happen when you're not really paying for
the API. So usually since we're not
paying for Google Gemini uh they
basically rate limit us. So we can only
make these many you know subsequent
calls at this particular interval right
something like that. So that becomes
kind of an issue sometimes. So what I'm
going to do is I'm just going to quickly
switch to OpenAI because that is what
I'm paying. So uh what I've basically
gone ahead and done is that in the
uh so basically what I'm going to do is
I'm going to go to chains and I'm going
to use the lang chain uh open AI uh chat
model. Okay. So now I I want to
communicate with the open AI chat model
and uh let's also go ahead and say chat
open AAI and I'm just going to quickly
come down here and then replace the
model to
GPT40
gpt40. All right. And let's save this
and I'm also going to provide the
environment variable. Okay. So just go
in here and then you can see that this
is the environment variable that you
need to provide. Uh if you do not know
how to pay for uh this thing, just go to
platform.openai.com. I think go to the
billing section and then you'd be able
to pay a minimum of $5 and that is going
to give you access. You can also pay for
Gemini, but uh yeah, this is what I've
already paid it. So I'm just going to go
go ahead and do it with uh
OpenAI. So all right, so let's
now try to run this file again now that
I've changed it and it should hopefully
work fine. So I' I've set GPD40 which is
correct. Perfect. All right. Now let's
try to run this
again. So this time there's going to be,
you know, uh three different exchanges,
right? So there's going to be six. The
length of the message list is going to
be six. So there's going to be three
different iterations. So let's wait and
see. Okay, perfect. So looks like we've
received the entire thing. Uh okay, you
can see that this is a pretty long
message, right? Because there's quite a
lot of data that is packed in here. So
first off, there's a human message and
then you know this is going to be our
initial message and then there's going
to be somewhere in in here, you know,
you should be able to find the AI
message generated. So where is it? Oh,
right here. Okay, so we have the AI
message generated and there should be
another simulation of the human message
that is going to critique it. So yeah,
so we have you can see that there's a
human message right here. Your tweet
touches an intriguing and current topic,
but it could benefit from more context
and engagement tactics. So again,
there's going to be another uh AI
message. Okay, so I'm not really able to
find it here, but it this will all make
sense, you know, in the next section
where we are actually going to be
tracing the entire thing using
langu. Uh all right, guys. So in this
section we will actually trace the the
reflection agent system that we built
just so we can understand exactly what
is happening where so that we'll
understand how both of these systems are
working together to deliver our final
refined viral tweet. All right. So to do
that uh I'm just going to go ahead uh to
this particular website
smith.lankchain.com. If you don't have
an account already just go ahead and
create an account and I'm going to come
to this tracing projects right here. I'm
going to click on this project and yeah,
I'm going to choose with LAN chain and
there's a couple of things that we have
to do. So, we have to generate the API
key, right? So, I'm just going to copy
this API key and you can see that it's
already being reflected right here. So,
instead of copying this, I can just copy
this entire thing right here. Right? I
can just copy this entire thing and then
come back to my environment file and I
can put it down here. So I don't really
have to provide the a the open AI key
because I've already provided it. But
yeah, the rest of it should be there.
All right, perfect. So now that is
pretty much it guys. If you do not know
what Langmith is, uh you know it lang
chain has very strong support for lang
because they all work under the same
umbrella, right? So you just need to
have the environment variables available
in your file and that's that is it. So
uh what I mean to say is that so inside
of this so we have all the graphs and we
have the message graph and we have the
different nodes and everything. So
within all of these classes it will have
support for lang. So once a particular
operation is done if the langsmith
environment variables everything is like
perfect it will make and stream it to
lang so that it will capture it on its
side. Okay. So we can actually trace it
very easily. All right. So now that we
have put all of this in the environment
variable, let's now try to run it again.
And once we run it again, we should be
able to see another project right here.
Okay. So let's try it. Let's give it a
few seconds. So as it is actually going
through each and every single step, you
know, the first iteration, the second
iteration, the third iteration, after
every single operation, there is going
to be a call that is being made to
LSmith and it is actually recording
everything right now. So let's just give
it a second and you know once we once it
finishes we can actually go to LSmith.
So you can see that everything is
available right here. Now if we come
back to langsmith and refresh this you
should be able to see another project.
Perfect. So you can you can see that we
have the new one that is generated today
right this is 13 which is today. Let's
go in here. All right. So here we have
runs threads monitor setup. We are only
interested in runs because a run is
basically running an entire application
from start to finish. So if we were to
click on this particular run, we'd be
able to see traces. Traces is different
from runs. Traces is like this
particular small component in the
system, you know, did this particular
thing, right? So it gives you a lot more
descriptive uh you know information
about what exactly happened. So at the
very top you can see that this entire
thing took 46 seconds. This is going to
be the complete compilation of what
happened. But if you're interested, we
can also go through each of these
different things just to see what
component did what at what point. Right?
So right now we have the generation
agent and the reflect agent. So we can
actually see what the generated generate
agent or we can't really call it as a an
agent but what are the generate workflow
did first and then what is the reflect
workflow did second. So we can actually
see all of that. So this is going to be
as I said a highle overview of what
happened human me I said this thing
first and then the AI which is the
generate agent generated this particular
thing and then this is going to be the
human message but actually it is not the
human message if you remember right so
this is going to be uh another uh AI
generated this thing which is going to
critique the first generated tweet it's
going it's saying your tweet your tweet
touches on an intriguing and highly
relevant topic however to maximize its
impact and virality
Consider doing this. Expand with
context. Engage with a question. Use
emojis, right? Hashtags. Use a use
better hashtags. Create a thread. Add
media virality and all of that. Remember
the goal is to be thoughtprovoking and
informative. So this reflect agent is
going to generate this thing and then it
is going to now the the control is going
to go back to the generation chain. So
the generation chain is going to
generate a completely new revised tweet
based on all of the feedbacks that it
received. And then once it is done again
uh the critiquing agent is going to look
at the tweet and it's going to suggest a
lot more you know to to how to you know
make it better and taking into account
all of that again the generation agent
is going to make it even more better and
then this thing goes on for um you know
a total of six six different exchanges
right so that is what we've done and
finally we have finally we have a very
viral tweet tweet that is packed with a
lot of, you know, you can see there are
emojis and there are hashtags and, you
know, it's really a pretty
wellthoughtout tweet, right? It's not
like a oneshot tweet, but rather there's
going to be a lot of thinking and lot of
refining and then finally, you know, it
gives you this final tweet and that is
exactly how in the real world it works
as well, right? There's going to be
multip multiple iterations. So that's
what we have right here, guys. So,
initially there is going to be the
generation node. Uh, I'm just going to
provide this. The generation node is
going to you know generate the first
tweet and then what ends up happening is
that you know um there's going to be
reflect node and that is going to look
at the generated tweet and then it's
going to critique it and then finally
again the generate node is going to be
there and it's going to again it has the
context about every single thing right
it has context about every single thing
and then finally the it gives you uh the
output of that and the same thing
happens again and again and again. And
finally, we have the the final tweet
right here. Okay. So, I hope that you're
starting to understand how reflection
agents work and how powerful they are
because they can actually think deeply
through a particular thing and then they
can like reflect and generate and
reflect and generate. And what I've done
is basically a very simple example which
is to generate a tweet. But you can
imagine that we can do it for pretty
much anything out there even for complex
tasks. And if at all you have any
questions, just go ahead and ask it to
me in the comments and I'll be more than
happy to help you out. So in the next
section, we are going to be looking at
the next type of reflection agent which
is called reflexion agent and that I
believe is going to be even more
interesting. So I'll see you in the next
section. Hello guys. So before we
proceed forward with the reflexion agent
system, there is one more thing that we
have to learn about which is how can we
get structured outputs from LLMs. This
is something that I did not get a chance
to cover in my lang chain course. So let
us actually take a look at it and let's
look at some slides right now. So it is
often useful to have a model return
output that matches a specific schema
that we define. So if you can imagine
this is what software engineering is all
about, right? So we're always going to
be dealing with structured data. So
there's going to be some object or in a
JSON format. There are some properties
that we extract and we do some
manipulation. And we the way we put it
in the database is going to be in some
structured format. But so far we've only
been dealing with LLMs that just give
out some random string that we can't
really do anything solid with. Right? So
that is something that we can change. We
can actually tell the LLM give it in a
structured format. Give it in a JSON
with these exact properties. So we can
actually do that. Okay? So I'll give you
an example. So let's say you know um I'm
saying okay tell me a joke about cats.
Okay. So the LLM normally without any
structured outputs what it's going to do
it's just going to give you know a
string saying that okay the here's
here's a joke right but what if I tell
the llm also give it to me in this
particular output okay so I want a JSON
in in that JSON I want a setup punchline
rating and I want these values not
exactly these values but this is what
the output of the LLM is going to be
okay so why was the cat sitting on the
computer this is going to the setup of
the joke. The punch line is going to be
because it wanted to keep an eye on the
mouse. Right? Okay, this is funny. And
also I'm also telling it give a rating
as well and give it to me in this exact
format so that I can take that object
that you give the LLM is going to give
and let's say put it in the database or
you know do something with it or show it
to the user. Right? So we have uh it's
it's not just this. We also have options
to get outputs in formats such as JSON,
dictionary, string, YAML, HTML, right?
Okay, so there's bunch of different
formats that we can actually force the
LLM to provide value, provide the
content in. Okay, so that is what we are
going to be uh learning how to do that
in this particular section because this
is an important thing that we have to
understand so that we can easily learn
the replexion agent architecture
pattern. All right. So the there's a
couple of types
uh there's a couple of ways in which we
can actually you know force the LLM to
give structured outputs and the first
one is going to be using pyantic models.
Okay. So if you're somebody who's new to
Python let's quickly I'll give you
quickly go through you know what pyantic
models are and how it can be actually
used in the context of lang chain and
lang graph. So a pyantic is a python
library that helps define data
structures. It acts like a blueprint for
data. It uses Python's type hints like
string, integer to enforce correct data
types. So how does it work in lang
chain? So we basically define a class
with the fields we need. So let's say we
need name, property, capital and
language. We also specify, you know,
give some description for each of those
properties and we can use this
particular method. This is going to be
the magic method. So we can use this
with structured output to tell the LLM
to follow this particular format. So let
me give you a quick example as well. So
yeah, so you can see that initially I
have imported you know the base model
and the field from pyantic. Okay. And
we've also initialized a model. You
don't have to use open AI. We can always
use other chart models as well and it is
going to work perfectly fine. Later on
in the course we are going to be
switching between a bunch of different
models. We are going to be using you
know chat uh grog. We are going to be
using uh we are going to be using llama
models and a bunch of different things
but no difference. All that you have to
do is swap this chat model with another
chat model. That is it. All right. So
now that we have a model initialized we
are going to be defining a pidantic
model. Okay. So let us say you know the
question that I want to ask LLM is tell
me about France. Okay. Something very
dumb. But the point that I want you to
take away from this is let's say this is
the question that I want to ask the LLM
and I don't want the LLM to just give a
string response. I want the LLM to give
a response in a structured format. So I
want the response to have a name, a
language and a capital. Okay. So in this
case the name is going to be I'm going
to give a description as well using this
field class. So I'm saying name of the
country. I'm giving additional
information to the LLM. So uh language
is going to be the language of the
country. Capital the capital of the
country. Okay. And also we are going to
be giving additional information at the
top right here. Okay. Saying information
about a country. So these are all very
important. I'll tell you exactly how
these actually play out in the API call
that we make to the LLM in just a
minute. But this is how we actually
define the pideantic model. Okay. So we
have the name, we are importing the base
model. uh the description we are also
giving descriptions for each of the
properties and also the data types for
each of those properties. All of these
are very important. So now that we have
the pyantic model, we are going to make
use of the with structured output method
that is going to be available in the LLM
and we are passing in this thing and
that is it. So now we have a slightly
supercharged LLM that we can invoke on.
Okay, it's not going to be a normal LLM.
This LLM now has internally what is
happening is we are this is going to be
considered as a tool. Okay. So this is
going to be considered as a tool for the
LLM. Now this tool is basically made
available to this particular LLM the
structured LLM and we are also forcing
the LLM to only use this particular
tool. Okay. So if this does not make
sense don't worry. We are going to be
deep diving. We're going to be learning
all sorts of different things about
tools and tool calling and all these
different things. But for now just know
that this is a tool that we are
providing to the LLM using this
particular method and we're also forcing
the LLM to only use this particular
tool. Okay. So now we have the
structured LLM and if we now were to
invoke and pass in this string we are
going to get back in a very structured
way. Okay. The response is going to be
very structured in this exact thing. So
let me go ahead and run this block. So
right here you can see that we are
getting a runnable binding and uh in
this runnable binding you should be able
to see this is not really necessary for
you to know right now we don't want to
complicate it but we can see that we
have this output format we have the
schema right so if I were to uh invoke
this run this now this is going to be
the response from the LLM so now you can
see this is going to be a pedantic model
and inside of which we have the name
language and capital Okay, so next up,
let me also show you how the request
payload looks like when we make the call
to the LLM and also the response
payload. All right guys, so so this is
going to be a very uh simple sample
request payload. Okay, so this is what
lang chain sends to the OpenAI API. So
you can see that we have the messages.
This is what we provided. No difference.
But in addition to this, there are two
other things, okay, that are going to be
sent. Okay. So in this tools we are
providing okay. So you can see that this
is a single object. So if you remember
this pantic model that we wrote that is
going to be available as a tool to the
LLM for the LLM to call. Okay. So this
tool is going to have the type function
and the name is going to be country.
Okay. So it is basically going to take
the this is going to be considered as
the name of the tool and whatever we
provided here is going to be considered
as the description of the tool. That is
how the LLM knows we have to use this
particular tool. Okay. And now we have
this important uh property called
parameters inside of which we have these
properties. This is very important.
Okay. So it's basically saying we
absolutely need this uh name. Okay.
Okay. So the name is going to be type
string. Description is going to be this.
Same thing for language and capital. And
we're also specifying that these three
things are required as well. Okay. So
this is this is how so basically
whatever we write here this is how it is
going to get
converted. Okay. When we actually make
the API call to the LLM. All right. So
perfect. And we also finally Okay. So
let me close this. Let me close this. So
now we saw that we are making this
particular pedantic model as a tool and
we're making it available to the LLM.
That is one thing. Secondly, we are also
you know we are also saying the tool
choice absolutely have to be. So when we
say tool choice, what it means is you
don't have a choice LLM. You have to use
this particular tool. You have to use
this particular schema and whatever
response you're getting you have to
absolutely use it. So that is what this
means and we are just you know giving
the same name um so that we can the can
refer back to it. That is it. Okay. So
perfect. Uh so now that we know how the
sample request payload looks like. This
is how the response is going to look
like. Don't don't get confused about all
of this. But this is how you are going
to get the response. Okay. So you can
see that the response from the LLM is
going to be a JSON as well. But it is
going to have the name, language and
capital. And what ends up happening is
after land chain, land graph our
application. Right? After this, you
know, uh this information is received by
our application, it is going to pass it.
Okay. So all of that is going to be
taken care of by this particular method.
Okay. So it is going to pass it. It's
going to convert it into a pyantic
model, right? It's going to validate as
well. Okay. So that is one thing that
you have to remember when we're dealing
with a pyantic model. So you know during
the conversion process it is also going
to validate if each property is actually
present and all the data types are all
matching and all of that. It's just
going to make sure that you know name is
present language is present capital is
present and also you know is this a
string this a string is this a string as
well. So it's going to do all of the
validations. If at all the validations
fail, it is all automatically going to
throw an error that we can easily catch
and then do something do a fall back
with it. Okay, so this is the first way
of doing it. Okay, so I hope that was
clear to you. If at all you have any
questions, uh let me know in the
comments or you can just go to Google,
type structured outputs chain and you
can go through the documentation as well
and it would make complete sense to you.
Okay, so this is the first way of doing
it. So what if you know you don't really
want the validations part, right? Okay,
so you just want to tell the LLM give
the response to me in this particular
schema. So in this case, if you don't
want the additional validations, you can
always go for coming down, you can
always go for type dict. Okay, so before
we use the base model from the pedantic,
but this time we can always use the type
dict. So basically type is uh you know a
python class uh you you're basically
creating a schema for a dictionary and
you're saying okay this is the type data
type of this property this is the data
type of this property etc etc. So right
here you can see that we are defining
the schema this time using the type
ticked. It's going to have okay this is
going to be a joke to tell the user. We
have a setup punch line and rating. And
this is how we you know um give
additional information. So we can
actually use the annotated class from
typing and we can use this to provide
additional information about this
particular property. The first this
thing that we are providing is going to
be the data type. The second argument is
going to be uh the default value that we
assign to this particular property in
case the LLM does not generate a
property. Okay. So in this case we are
not providing a default value. The third
one is going to be the description. We
are giving more information about this
property so that the LLM knows exactly
what this property should hold. Okay,
what value it should hold. I hope that
makes sense. So this is one way of doing
it. Okay. So we can also you know um for
the punch line we are doing the same
thing for the rating we are basically
just saying that you know this is not
absolutely important. Rating can be like
omitted. So in this case we are making
this in as optional. We're giving a
default value of none. And we're also
saying how funny the joke is from 1 to
10. Okay. And finally, now that we have
the uh the class ready, we are providing
the same thing here. And the structured
LLM is going to invoke it. And now when
we run
it, let's give it a few
seconds. Okay, great. So you can see
that we are getting the exact same
thing. Okay, so we are getting setup,
punch line, rating. Perfect. Okay, so
this is the second way of doing it.
Okay, great. So let us look at the third
way of doing it and that is basically
just by providing a JSON schema. So we
have to give a title and a description.
So these are the three properties that
we need in the final dict that it
provides. So here we have the type
string, description, punch line to the
joke. So this is actually basically the
equivalent to what we're doing here but
without the class and we're also saying
the these are the two things that are
required. The rating is not required. So
if we pass it the same thing here, if I
run this, I should get the same thing.
Okay. So why was the cat sitting on the
computer? Punch line is going to be okay
because it wanted to keep an eye on the
mouse. Okay, reading. Perfect. Okay,
great. So now I hope that you understand
how we can actually use the pedantic
model, how we can use the JSON schema or
how we can use the type class to
actually tell the LLM to provide its
output in a structured way. So this is
particularly important because in the
next uh you know section that we are
going to be dealing with where we will
be building the reflexion agent system
there we will actually make use of the
first type which is going to be the
pedantic model. Okay. So there I might
not be using this particular method. Uh
I might be you know explicitly providing
the tool. Okay. This I'm going to
consider this as a tool. I'm going to be
providing it to the LLM you know
directly like this. But the underlying
mechanism is the absolute same thing. So
if you recall the the request sample
that we looked at, it is going to be the
same exact thing, no different. Okay, so
I hope this made sense to you. I'll see
you in the next
section. Hello guys, welcome to the
section. In this section, we are going
to be looking at a brand new system
called the Reflexion agent system. I
hope that that is how it is pronounced.
But this system basically sort of
addresses some of the drawbacks that the
reflection system that we saw in the
previous section tends to have. So we'll
do a quick recap of what we saw
previously. So we saw that the
reflection agent system right the
previous system that we saw consists of
the generator and a reflector component.
Although iteratively making a post
better is significantly better than just
prompting chachi, the content generated
by the reflection system is still not
grounded in live data. It could be a
hallucination or outdated content and we
have absolutely no way of knowing. I'll
give you a very simple example. Let's
say I am a marketer and I want to write
a blog post on you know uh how is AI
helping small businesses grow, right? So
that could be a blog post, right? If I
were to feed it to a reflection
basically the the basic reflection
system that we saw in the previous
section, what it might do is it might
actually write something but it is based
on whatever the model has been trained
you know last year or something like
that. it tends to have a cut off date,
right? But if in order to write a really
enriched blog post, it's very important
that you know we need uh we need to
provide it a lot more recent news that
happened or we need to provide the
system the ability to make API calls
Google search if that is what it's
needed right so that is how you know
having citations inside of the blog post
like uh you know this is the URL that I
referred you know this particular data
from so all of that is what makes a blog
post really enriched right and that is
something that this reflection ction
system is not really capable of because
we are not really making any tools like
tab search tool that we saw right we're
not making it available to the
reflection agent system okay so that is
a drawback of reflection agent system so
that is exactly why this reflexion agent
system it addresses this particular
drawback so let us look at what is the
reflexion agent system the reflexion
agent similar to reflection agent not
only critics its responses but also fact
checks it with external data by making
API calls. In this case, internet
search. In the reflection agent pattern,
we had to rely on the training data of
LLMs. But in this case, we are not
really limited to
that. So the main component of a
reflexion agent system is the actor. The
actor is the main agent that drives
everything. It reflects on its responses
and re-executes it. It can do this with
or without tools to improve based on
self-critique that is grounded in
external data. So let us look at some of
the main subcomponents that are involved
in this particular system. So we've
already seen that this actor agent is
going to be at the very top right. It is
what is going to be driving everything.
Underneath it we have all the
subcomponents. So the first one
obviously is going to be tools or tool
execution. Right? In order to ground a
response with live data, we need tools
like Tabi search tool which is going to
make API calls and understand you know
what is exactly happening. And then the
first sub agent you can think of it as a
sub agent and that is going to be the
initial responder okay or the responder
agent. So basically the responder
agent's job is to generate an initial
response and it is also going to do some
self-reflection. Okay later on we look
at the diagram and it would make sense.
So we've got tools, we've got the
responder agent, and we've got the
revisor agent finally. Okay. So the
revisor agent agents job is to
re-respond and reflect based on the
previous reflections. And also the
reflexion agents are thought to have
episodic memory. In the context of
reflexion agents, episodic memory refers
to an agents ability to recall specific
past interactions, events or experiences
rather than just generalized
knowledge. This is crucial for making
agents feel more context aware,
personalized and humanlike over
time. So here is a very simple diagram
that you know the documentation provides
about what is a reflexion agent system.
Okay. So uh for now you can actually see
that we have the actor right here right.
So this is going to be the high level
this entire thing can be thought of as
the actor inside of which we have the
responder agent we have the revisor
agent and we have the execute tools
right we have the tools right here. So
let's actually take things one step at a
time. Let's understand what this
responder agent is actually going to do.
Okay. So let's say this is the user. The
user wants uh a
250worded blog post on the same prompt,
right? On how can businesses utilize AI
or rather small businesses utilize AI to
increase growth or something like that,
right? So in that case that user request
uh is going to the responder agent and
the responder agent will basically
output something like this. Okay, so
this is going to be the initial
response. So this you can think of it as
a JSON structure and the JSON structure
is going to have three different
properties and the first property is
going to be the response, right? So the
response is basically just a 250worded
blog post that the LLM thinks that you
know this is what is the writing and
along with the response it is also going
to attach another property which is
going to have critique. So essentially
it's like writing its own blog post and
then it is actually looking at the blog
post that it has written and then it is
actually critiquing it. Okay. So the LM
is doing all of that in one shot. This
responder agent is doing all of that in
just one shot. Okay. So it is thinking
okay this could have been better. This
this could have been shorter. This could
have been you know I'm you know there's
a lot of redundant excessive information
that nobody really wants to know. So
it's actually critiquing itself and
depending on uh you know whatever it has
written it is also going to suggest some
search
keywords. Okay. So it's also going to
suggest some search keywords. So
basically let's say you the LLM is going
to write the blog post and then it wants
it would it feels like you know it would
write it better if it could have latest
information about certain things. So it
is actually suggesting those list of
search keywords right here. Okay. in
this third property. I really hope that
this makes sense. So what is next? So
the next thing you can see that the
control flow is coming to this tools
component right here. So essentially
whatever search uh keywords the list of
search keywords that the responder agent
generated this tools component is
actually going to you know the tabi
search tool. Uh if you remember in the
first few sections we looked at what the
Tavili search tool does right it's
basically just a tool that goes into the
internet searches for these particular
keywords that the LLM suggested and then
get the responses and then send it back
to the next component. That is exactly
what the execute tools component does.
So what do we have so far? So far we
have the initial response with the
response critique and the search terms.
We also have the results the internet
search results of the search terms as
well ready. So we have both the initial
response as well as the new data the
live data ready. Now all of both of
those information is now fed into the
second subcomponent which is going to be
the revisor agent. Okay. So this revisor
agent is going to get both those
information and then it is going to look
at you know these are the existing
things these are the new information and
now it is going to generate a revised
response. Okay so it is going to
generate a revised response with a
revised response a revised critique a
revised search uh term list right and it
is also going to add something else
called citations. You can see that it is
the first three properties are very
similar right it's got response property
it's got the critique property it's got
the search property right except since
this is the revised response the content
is going to be the revised content the
critique is going to be the revised
critique and then the search is going to
be the revised search and additionally
there is going to be another property
that is going to be added which is
called citations. So basically uh any
research paper that you look at you know
uh it the content itself would have like
1 2 3 4 and then clicking on it it is
going to take you to the citations link
where that actual fact has been taken
from right so that is what is called
citations and we are also instructing
the revisor agent to add the citations
as well. So all of this content is going
to be revised. All right. Now you can
imagine that you know um after after the
revised thing is done the LLM might
think you know what okay now I have
extra information but I want even more
information based on whatever it is that
I've written. So in that case the search
the search list is going to have new
keywords right one or two or three new
keywords and again the execute tools is
going to go fetch that information and
then it's going to come back to the
reviser and this loop is going to keep
uh it's going to go on for a couple of
times.
I hope that makes sense. You can
actually imagine how powerful this could
be because uh with every single
iteration there's new information
getting fetched from the internet and it
enriches the content a little bit more
every single iteration right it's going
to ground all the content with live data
right and also don't worry that you know
the the the length of the content is not
going to keep on increasing or anything
because we will also tell it okay only
keep it to 250 words. So you can imagine
that that content is going to get more
and more enriched with live recent
information and it would make for a
really good blog post right. So after it
sort of repeats for a couple of times it
is then obviously it's going to go back
to the uh user. So this is what is
called a reflexion agent system. Uh if
this is if this sounds too much for you
don't worry. We are going to be taking
it one step at a time. We are first
going to be building out this partic
this first component right here and then
we are going to be building out the
revisor component and then we are going
to be building out the execute tools and
then connecting it all together using
LAN graph. So by the end of this
particular series this section you
should have a pretty good idea. So in
the next section we are actually going
to be implementing this particular
system using code. So I'll see you
there.
Uh hello guys in this section let's
actually go ahead and build out the
first component right here the
subcomponent of the actor agent right
here which is going to be the responder
agent. Okay. So the reason why we call
both of these agents as subcomponents of
the actor is because the base chart
prompt template the prompt template is
going to be of the actors and both of
these subcomponents are going to use the
same basically reuse the same prompt as
that of the actor. If that does not make
sense don't worry. Uh let me just
quickly copy paste some code right here.
You can see that I've already created a
folder called reflexion agent system.
I've created another file called chains.
So this file is what is going to house
all of the different chains that we are
going to be building. All right. So let
me copy paste something right here and
let me walk you through it. All right.
So all that I've done here is that I've
just called it the actor agent prompt
and then I've imported uh you know chat
prompt template and the messages
placeholder. No different to what we've
done in the reflection agent in the
previous section. All right. So let me
let's actually go back to the diagram.
Let's see what is the uh you know goal
of this responder agent. So we know that
the goal of the responder agent is to
finally you know after so many
processing finally we need a response
like this right let's keep it very
simple let's keep it very high level for
now the responder needs to output a JSON
and the JSON then needs to be converted
into a proper Python dictionary that is
going to have this particular structure
right so the initial response should
have the response the content of the
blog post the critique of the written
blog post the search keyword terms all
of that needs to be available right so
this is exactly the goal and that is how
we are going to be writing the prompt as
well so let's actually go look at the
prompt of the actor all right so uh so
you can see that we have the system
which is going to be you are an expert
AI researcher and then we are also
providing the current time right here
why do we need the current time it's not
really super important to understand
this but to give you a very high level
let's Say you're giving a task to the
agent to talk about something that
happened recently, you know, current
affairs or something like that. It
having access context of what time it is
can be important because accordingly it
can make Google searches and then figure
out if it is actually if it actually
needs to do something or not. If it
actually needs to look look at Google or
not. Right? So we have the first
instruction right here. this first
instruction um basically we've got 1 2 3
the these 1 2 3 corresponds to these
things right here. Okay, so we've got
the response, critic and search. This is
going to be for response. This is going
to be for critique. This is going to be
for searching. Okay, so we will
dynamically import this later. Uh you
know this alone can change depending on
whe whether you are building the
responder agent or the revisor agent.
But for now since we are only focused on
building the responder agent using the
actor prompt template. So we are going
to have it as a placeholder right here.
I hope that makes sense. This will later
on you know tell you know write a
250word blog post right here. Okay that
would be the instruction. Let's look at
the second one. Reflect and critique
your answer. Be severe to maximize
improvement. Right? We wanted to be
really severe about what it has just
written. It is all going to happen in
one single you know two and fro right
after the reflection list one to three
search queries separately for
researching improvements do not include
them inside the reflection okay so we're
also providing you know instructing it
to provide the search uh list of search
queries as well and then finally we also
have a messages placeholder that could
be you know um this could be the place
where the actual human message would
come and sit. So the human message in
the future when we actually run it, it's
going to be something like I want a blog
post on you know this particular topic.
So that human message is going to come
sit inside of here. And then finally we
have another system message which is
just to you know we don't really need it
right here. But the reason why we
provide you know this answer the user's
question above using the required
format. The reason why we provide it at
the bottom as well as the top is because
LLMs tend to you know pay a bit more
attention to what happened at the start
and what happened at the end. So we just
like you know just to make sure we are
providing this. So u this is going to be
the prompt template. This is not really
something that I wrote. This is a prompt
template that is very standard for the
reflexion agent system. I pulled it from
the internet. So yeah um that is what
I've done here. So let's actually go
back to the diagram. So you can imagine
that you know if I were to you know use
this prompt template and if I were to
you know invoke that particular chain
you know send it to the LLM and invoke
that chain it is going to give you you
know uh a response with this particular
response it's going to give you critique
it's going to search all of it is good
but at the end of the day we want
something concrete like this we want a
Python dictionary we want a proper
dictionary or like a model that we can
actually access the values from so that
we can extract those things and then
send it to the execute tools and do a
lot of other things right but you can
imagine that LLMs are a little messy
they sometimes don't provide the right
properties they sometimes you know tend
to mess up as well so it's a tricky
thing but we actually have a solution to
basically force the LLM to ground the
LLM to provide a response in a
particular
schema okay so that is going to be the
main takeaway way in this particular
section. So let's actually look at how
the exact processing is done. What are
the different level? What are the
different steps that we have to take to
ground the response of an LLM to provide
a JSON structure in a very you know we
are forcing the LLM to provide a schema.
We're forcing the LLM to provide the
response in a particular schema. Okay.
So let's actually look at how that is
being done. So I am calling it the LLM
response parser system. So let's
actually understand the theory behind it
and then we'll jump into the practical
stuff. This system converts unstructured
LLM outputs into well- definfined Python
objects through a series of structured
parsing steps ensuring data validation
and consistent
formatting. So what are the key
components? We have already written the
actor prompt template. That part is
done. Let's look at the next one.
Function calling with a pyantic schema.
And then the third one is going to be
something called a pyantic parser. So
since we we already understand
chatprompt template, let's actually move
on to the second one which is going to
be the function calling with a pyantic
schema. So we already know what tool
calling or function calling is. But just
to give a refresher, similar to how we
make tools available to the LLM, we can
also send a schema to the LLM and force
it to structure its JSON output
according to the schema.
But what is paidantic right here? So we
we already understand the we already
understand the function calling. We
understand schema. But what is this
pyantic? A python library that defines
data structures using classes. It
provides automatic validation of JSON
data against these class definitions.
So to quickly recap, we just define the
schema and then we add it as a tool
during our function call to the LLM and
the LLM is going to call that particular
tool, you know, structure its output
accordingly. Very simple. Let's look at
the last one, the pidantic parser. It
takes the JSON output from the LLM's
function call. It validates it against
the defined pyantic schema or the class
definition. It creates instances of
pyantic classes with the validated data.
If the LLM's output does not match with
the defined schema, it will throw an
error. So it might this pyantic parser
you might have some difficulty imagining
what it could look like. So basically
you know the previous pyantic model that
we that is a tool that we make it
available to the LLM that is basically
just going to ground the output of the
LLM. Okay, it's just going to force the
LLM to structure its output in a certain
way. But we also need another line of
you know defense wherein we also have to
validate if the output of the LLM is
actually you know adhering to the schema
to the model pyantic model. Okay. So
that is what this pyantic parser does.
It takes the JSON output, it validates
it and then it is going to create uh you
know an instance a a class or a model
and uh you know it makes it easier for
you know for us to you know use Python
and then extract the stuff and then
start working with it. Okay, so these
are the steps. So so far what we've done
is we've written the actor prompt
template right here. The next step is we
need to actually go ahead and before we
start invoking it, we actually need to
go ahead and create a schema file right
here. So that is exactly what I'm going
to do. So to do this, I'm just going to
copy paste some code right here and let
me walk you through it. All right. So
this could be a lot. So let's take it
one step at a time. So what I've done is
I've imported from the pedantic module.
I've imported the base model as well as
the field. Basically don't let's not
over complicate it. The base model is
just giving you the tools to you know
define a schema. Very simple. All right.
So let's go back to the diagram. So what
are the three things that we need? We
want the the main response. We want the
critique of the generated response. And
then we want the search. And that is
exactly what we are providing right
here. Okay. So we defined a class called
answer question. We are providing in the
based model right here. And first off we
need the answer, right? So the answer is
going to be in a string format. and the
field is going to have a description.
Right? So here we are actually
suggesting a 250word detailed answer to
the question. Okay. So not only are we
providing the 250word prompt in the chat
prompt template, we're also doing it
here. So there is like a double you know
uh defense where we are grounding the
response of the LLM properly. Okay. So
we have the response right here. Okay.
So the first one the response is done.
The next thing is we can probably
provide in the search and the critique,
right? And that is exactly what we've
done. We want the search queries to be a
list of strings and the description is
going to be one to three search queries
for researching improvements to address
the critique of your current answer. And
then finally, we have the reflection
here as well, which is going to be the
critique. We can name it whatever we
want, but the reflection is going to be
your reflection on the initial answer
that is going to be right here. Right?
So that is what it is and also note that
this reflection is also going to have
two different subpropies right. So you
can see that we have missing as well as
something called superfluous. Okay so
we'll get to what that is in a minute
but missing is it is a critique of what
is missing. So whenever we are you know
reflecting on or rather critiquing on a
particular passage or something like
that we either think of it in terms of
you know this could be shortened or this
could be made better right so that is
what we're talking about missing is like
you know what is missing what could be
made better and then superfluous is what
is excessive what is redundant what is
not needed right so a critique of what
is
superfluous all right so this is going
to be the base schema uh that we are now
going to use to ground the response of
the LLM properly. So let's actually go
ahead and do that. So let me come back
to the chains. Let's see if everything
is done. Okay, there's one more thing
that I want to do. I want to do a
partial and I want to prepopulate the
time. Okay, I don't really want to do it
during invoking. So so let me just paste
this right here. So basically this
partial method is something that you can
use to pre-populate placeholders you
know before even invoking a particular
chain right so that is what we've done
right here we are making use of the
datetime module of pythons all right so
now let's go ahead and write the chain
for the responder uh agent so what I can
do is I can call this
first
responder prompt
template so here I can make use of the
actor prompt template right here and I
just want to fill up the first
instruction. Okay, so you can imagine
the first instruction is going to be a
very simple, you know, write me a
250word blog post, right? So that is
exactly what I'm going to fill it up
right here. So, so I'm just going to say
first instruction provide me a detailed
250word answer. Okay, so the first
responder prompt uh template is now
done. Now I'm going to create the chain
for the first uh uh responder. So I'm
going to call it first responder chain
and I can use this particular prompt
template and then I can pipe it with the
LLM. Right? So when I pipe it with the
LLM remember we also have to provide an
that answer question tool that we have
written right here to the LLM so that it
can call that tool and then use that
tool schema to ground its response. So
I'm just going to say llm dot bind tool.
Okay. So we haven't actually provided
the llm yet, right? So let's actually
quickly go ahead and do that. So to do
that, I'm just going to go to, you know,
uh
from lang chain open ai import
u chat openai and I can come down here
say llm is going to be
equal chat open ai. Okay, I can send in
the model to be
GPT4 O. Okay, perfect. All right. So now
the bind tools method is going to be
available. So inside of here I can
provide in the answer question schema
that we've just written in the schema
this thing. So let's just quickly go
ahead and import that as well. So I'm
just going to say schema import.
uh the name of this is going to be
answer question. So let's import that
and let's just go ahead and provide that
and that is it. So we provided this
particular tool and since you know we
are actually confident that this is the
only tool that the LLM has to use right
we we don't want it to use multiple
tools we are sure we we are mandating
the LLM to only use this particular
tool. So there is another property that
we can actually provide which is called
tool choice and inside of this in a
string we can just provide the same
thing right here. So this is going to
force the LLM to call that one
particular tool structure its output in
that particular schema properly.
Perfect. So if you remember we also need
to pipe it with something else. So not
only is it enough that we get the JSON
response, we also have to convert the
JSON response into a pyantic class so
that we can easily sort of interact with
it, extract data out of it. So to do
that, I'm going to be using the pyantic
parser. So that is going to be openai's
pyantic parser. So I'm just going to
come up here and I'm going to paste this
line. So here basically from the core
package output parsers open AAI tools
and I'm going to be using the pyantic
tools parser right here and I can just
come down here and I can instantiate it.
So I can just say pantic
parser and I can just basically call
this and provide in the tools as
well. So it will know exactly how to
extract the information and now I can
provide it right here. All right. So now
all that is left is to basically just
invoke this chain with a human message.
Basically whatever I as a human I as a
marketer want to prompt this particular
system. Okay. So I'm just going to say
first responder chain dot invoke and
right here if I come up here you can see
that in the messages placeholder we have
the messages variable right? So I'm just
going to say messages and this is again
going to be a list right? So inside of
this list I can just put the human
message. Okay, it looks like we have not
imported it up here. So I can just say
from lang chain
core messages import human message.
Let's copy this, put it here and then so
for the content I can just say something
like you know write me a blog
post post on how AI can or rather how
small businesses can
leverage AI to grow. Okay, very simple.
Now this is going to give us the
response. Let's go ahead and print out
the response as well. Perfect. So let me
make sure that everything is looking
fine. All right. So let's go ahead and
run it. Let's hope that everything works
fine. Perfect. So you can see that we've
pro we've gotten the response in the
exact same structure that we wanted
which is in the format of the pyantic
class itself. So we have the answer
question inside of which we have the
answer right here. And then let's
actually go back to the schema. Let's
see you know. So yeah. So we can see
that the answer is right
here. Okay, there's not really enough
space for us to look at everything.
Okay, so we've got the answer right
here. And then we have the search
queries as well. So let me actually copy
paste the search queries here. You can
see that the search queries is also
going to be a list of three things. So
how small businesses use AI, AI tools
for small business growth, AI benefits
for small businesses, right? And finally
we have the reflection here as well. And
you can see that inside of the
reflection class we have the missing as
well as superfluous. So in the missing
it is saying the current answer uh lacks
specific examples of AI tools or
services that small businesses can use.
Uh and then we have the superfluous as
well. Okay this is basically saying this
is overly detailed right make it shorter
right so you can see that we are getting
all the data in the exact format that we
needed it. Uh I know that this output in
the terminal can be a little difficult
to read. So what I'm going to do is I'm
just going to go ahead and create a new
project. Uh let's copy let's actually
generate a new API key and then copy
this thing and then come back to our
environment variable. Let's let's paste
it right here. We don't really need the
API the open AI API key because we
already have this here. Let's come back
and let's clear this out. Let's try to
run it again. So let's actually like
delete off all the terminals and then
let's run it
again. So let's give it a few seconds.
All right, perfect. So let's actually go
back to our Langsmith and let's see.
Perfect. So we actually have something
that is generated right here. So let's
actually go inside look at the traces
and right here you can see that this
runnable sequence is going to be the
highlevel you know overview of what
happened. So here we have the human
message provided and here we have the
output right. So you can see that it's a
neat list of just one uh you know object
and that is going to have the uh the the
class the pyantic class or the p pantic
model and you can see that it has the
answer right okay it is it can be a
little bit difficult to read in the
terminal so I tend to come here so you
can see that this is the answer and then
finally the reflection is going to be
you know these are the things that are
missing okay we're not talking about
costs we're not talking about the uh
technical expertise so you can see that
you know it is generating this thing and
it is also you know critiquing itself.
It is saying these are the things that
are missing. Superflour is like these
are the things that are redundant or
excessive right this uh you know mention
of AIdriven customer service it's
considered generic you know be more
specific is what it's saying and then it
is also suggesting some search queries
for the next component okay we'll get to
that in a minute but these are the
search queries AI for small business
growth AI tools for small businesses and
then finally small businesses AI
solutions all right so this is going to
be the highle thing. If you further want
to look at what were the outputs of each
and every single component, we have it
right here. So this was the original
prompt template, right? So you can see
that this is already populated. This was
the first instruction that we provided
human system and then this was the
output. Okay, so this you can see that
this is a very simple JSON output. uh
the LLM is actually grounding its
response uh properly based on the
parentic schema that we've provided. So
this is going to be the tool call ID. If
you do not know why this is for, we'll
cover that in a few minutes. But yeah,
this is going to be the output of the u
the LLM which is which is a proper JSON
object. Perfect. And if you want to know
what this pyantic tools parser did, it
basically just took this entire JSON uh
object and then it just basically
formatted it in a pedantic model sort of
a way. Uh and this can also be helpful
in certain situations. Perfect. So in
this section we wrote the prompt
template for the actor agent and then we
you used that prompt template to write
the responder prompt template and then
we created a chain out of it and then we
also learned exactly how to parse and
ground the response of the LLM into the
structure um into the structured format
that we wanted and we've learned so so
much in the section so that even in your
own projects you know exactly how you
can actually ground the response of a
particular LLM's output, right? All
right. So, great. So, we are done with
these two sections right now. In the
next section, what we're going to be
doing is we are going to be doing the
same thing for the revisor agent. And
once we're done with that, then we'll
come back to building the execute tools.
So, I hope you learned a lot and I'll
see you in the next
section. Hello guys, welcome to another
section where you know this section is
going to be relatively very simple. So
in the previous section we basically
looked at uh the responder chain right
uh and the actor chat prompt template.
Uh in this section we are going to be
building out the revisor uh chain. Okay
so this is going to be relatively very
simple. We already know how we uh wrote
the responder chain right. So we reused
some of the reflection uh I mean the
actor prompt template. We just changed
the first instruction right here. Right.
So we have the actor prom template and
to build out the responder chain we just
changed the first instruction alone and
then we created a chain out of it. We
provided a tool as well to ground its
response. So we are going to be doing
the exact same thing for the reviser as
well. So this should be pretty easier
for you to understand. So what I'm going
to do is I'm going to call this the
revisor
uh chain and I'm going to use the same
actor prompt template that we've written
right here. So, and then I'm going to
say partial and I'm going to
pre-populate the first instruction
alone. So, the same thing that we did
right
here. So, I'm just going to, you know,
pre-populate with some data right here,
some content. So, let me just copy paste
the content right here. Okay. So, this
is going to be the revise instruction.
So, essentially what we're saying is
revise your previous answer using the
new information. You should use the
previous critique to add important
information to your answer. You must
include numerical citations in your
revised answer to ensure it can be
verified. We also ask you to add a
references section at the bottom of your
answer which does not count towards the
word limit. So at the bottom we we
suggested to add these links. You should
also use the previous critique to remove
superfluous information from your answer
and make sure it is not more than 250
words. Right? So I hope that makes
sense. Whatever the responder is
providing, the reviser has to look at
all of that based on the critique that
the responder provided and based on the
search results, it needs to generate a
revised response. Okay, so that is what
we mean right here. All right. Now, let
me just copy this and put it over here.
Perfect. So, the chain is not yet ready
yet because we still have to ground the
response of the LLM in this particular
revised response structure, right? So
it's basically very similar to what we
did in the responder chain. We had this
uh you know let's go back to the schema.
So we had the answer question uh model
right the pedantic model. So this had
the answer search queries and
reflection. If we come back to the
diagram this also has the same answer I
mean the response critique and search no
different but in addition to that we
just have the citations right here.
Right. So I'm just going to go ahead and
create a separate class for this thing
called revise answer. You can name it
pretty much anything that you want. And
this revise answer is going to inherit
all of the base properties as the answer
question. So if you remember there is a
lot of overlap, right? So I'm just going
to put it right here and then let me
just copy paste some code. All right. So
all that this is doing is it's going to
inherit all of the answer question you
know uh these fields and then we are
saying revise your original answer to
your question and then in addition to
all of that we are just providing the
references. So this is going to be a
list of strings and the description is
going to be citations motivating your
updated answer. All right I hope that
makes sense. Now let's come back to the
chain and very very similar to what we
did in the responder chain. So let me
scroll up right here so that you can
see. So this is the responder chain. All
that we're doing is we are piping this
template with the bind tools and then we
are forcing it to only use the answer
question tool responder right. So we are
going to do the same thing here as well.
So this is going to be the prompt
template. Let's pipe it. So let's go
ahead and pipe it with the llm. So we
are going to say bind tools and then
let's provide in the tools which is
going to be an array of this particular
revise answer schema. So let's provide
that here. Okay, so it is not imported.
Let me go up here and let's quickly
import it.
Okay, perfect. And we're also forcing it
to only use this particular revised
answer which is now going to be a string
as well. And that is pretty much it. So
let's come back to the diagram. So, so
far we've built out the responder chain.
We've built out the revisor chain. In
the next section, we are also going to
be building out the third component
which is going to be the execute tools
uh method. Right? So, once we have all
of the building blocks done, connecting
it all together using LAN graph becomes
a piece of cake. Okay? So, I'll see you
in the next
section. Uh hello guys. So in this
section let us now go ahead and build
out this particular component the
execute tools component because this is
what is next in line right. So so far in
the state you can imagine that we have
the initial human message right if you
remember the initial human users message
and then the second uh item in the list
is going to be the AI message that has
this information right so the AI message
is going to have the response critique
and search terms. The search term is
going to have three different search
terms right. So the next component which
is this execute tools. So all the
execute tools method has to do is take
out the last message which is going to
be the AI message right it just needs to
take that out and then it needs to look
at the search terms that are present. So
we are going to now have three different
search queries right so all that it has
to do is loop through each of those and
then with each iteration call the tably
search get the response compile it all
together and formulate a final one tool
message. So we have different types
right we have human message AI message.
So the initial human message that is the
first item the second responder chain is
going to give out one AI message and
then this execute tools is going to give
out one tool message. Okay. And in that
tool message in that content we are
going to have all the information. I
hope that makes sense. Let us now see
how we can implement it. So let's jump
back to the VS code and you can see that
I've I've gone ahead and I've written
out the code. I've created a file called
execute tools inside of reflexion agent
system. So this method is what we need.
Right? So this is going to be our
execute tools node. Right? So this state
you can imagine is going to get a list
of messages the human message and AI
message at the initial point when this
is executed. Right? So we have human AI
and then when it hits it we are going to
have human and AI message in the state.
So we are just going to get the last AI
message and we are going to do a bunch
of different things. So before we go
through this, let me show you. You know,
I've prepared a dummy state that we can
actually pass in here just to test out
the method to make sure everything is
working fine. So as I told you, we have
the first item of the list which is
going to be a human message which has
some prompt. And next we have the AI
message, right? So this is going to be
the barebones structure that this
particular responder chain is going to
output. Okay, so it's just going to have
uh the content is going to be empty
because this time it is actually making
a tool call, right? So in the tool calls
the first object it is going to contain
which tool it called and uh you know the
tool called id. So the name and ID is
going to be very important. We we we've
just mocked it right now. And then we
also have the args. So inside of the
args uh this is the schema that we had
defined, right? So this is exactly how
we are going to get. So if you're
curious, you can just like go print out,
you know, the output from the responder
chain and this is exactly how you're
going to get the AI message. Maybe
there's going to be a lot more
information, but this is the information
that we are interested in to test out
this method. Okay, so the answer we're
not actually interested in right now.
I've just left it as empty string. What
we are interested in is the search
queries. You know these three things.
Okay, so yeah, that is what I've written
here. the missing and superfluous also
we're not really interested right now to
test out this method. So yeah, these are
the three let's say our imaginary search
queries uh that we need and this method
basically just needs to go through each
of those things call it and then create
a final one tool message. Okay, so in
addition to human message and AI
message, we are now going to add another
tool message inside of this state. Okay,
perfect.
All right. So let's look at what we are
doing right now. We are just going to
extract the last AI message. And all
that we're doing is testing checking if
the tool calls are present or not. Okay.
So if there are no tools call present,
it does not make sense to execute those
tools, right? Because there are no tool
calls to begin with. So that is why
we're just checking it. If there are no
tool calls, we're just returning an
empty array. We don't really need to do
anything here. No tools to execute. But
in case we do have tool calls, we are
going to loop through all of the tool
calls that are available. In this case,
we only have one, right? So we only have
one object right here, which is this one
tool called the answer question.
Perfect. Okay, so we are just going to
get the tool call ID and the search
queries as well. So we are going to get
the uh the tool call ID and all of the
search queries right here. So we have
both of the information here. And now
all that we're doing is we're looping
through those three different items and
then we're invoking the Tavly search
tool that we have right here. Okay. So I
hope that makes sense. So if we invoke
basically okay we're invoking it and
sending each and every single query and
that is going to give us five different
results right and then the with the
results we are getting we're just
putting it in a dictionary right uh with
the query. So the way that this is going
to be is like uh let me just like uh you
know quick and dirty let me just show
you exactly how it's going to look like.
So we are going to have search term one
right. So we are going to have search
term two and three. Okay. So for each of
the search terms we need five different.
Okay. So we need five different results.
Right. So in the first object we are
going to have the URL and the URL is
going to have exactly which URL it
pulled the data from. So ABC and then um
we are going to have content here. Okay.
So yeah, this is the content and the
same thing is going to continue five
different times for each and every
single search term. Okay, so there's
going to be five and similarly this is
going to have five different objects and
this is going to have five different
objects. So this is the structure that
we are going to be creating using these
three lines. Okay, very simple. So
finally once we have the entire query
results right here in this dictionary
all that we are going to do is we are
going to create a tool message and then
we are going to dump all of that. We're
converting all of that into a JSON and
then we're dumping it along with the
tool call ID. So the tool message is
going to be slightly different human
message if we are going to create a
human message we just pass in the
content AI message we already know in
the tool message along with the content.
Okay, along with the content, we also
need to provide the tool call ID. Okay,
so tool call ID is very important. So
the LLM wants to call a tool, you know,
it is going to generate a tool call ID,
right? So the same tool call ID, we are
going to get it from the last AI
message. All that we're doing is
providing it here. Okay, and that is it.
So uh and then once we have the tool
message, we are going to append it in
the tool messages. So we don't really
need this. Okay, we can just like get
rid of this and we can just return
uh okay, we can just return it like this
also. Okay, so we just going to return
one tool message and whatever we return
it needs to be in a list format, right?
Whatever we return, it needs to be in a
list format, right? Because only list
and list can be merged in the state that
we have, right? In the message graph
that we have. So we can do it like this.
This should work perfectly well, but I'm
just going to do it a little bit more
neater. Okay. So I'm just going to you
know do it this way and that is it. All
right guys, so I hope that makes sense.
That is it for the execute tools
component. So we are finally done with
you know building out the last component
in our application. All right. So please
if you have any questions at all feel
free to ask me uh in the comments and uh
um go ahead pull the code run it try to
break it try to understand what we're
trying to do here and then I'm sure that
it would start making sense all right so
now that we have built out all the
components that we need for the system
all that all that's left is to just put
it all together build the graph and then
run the system so I'll do that in the
next section so I'll see you
there hello guys I hope you're excited
about this last section in this
particular topic the reflexion on agent
system where we are going to be piecing
together all these different components
that we've built into a graph using land
graph and then we are going to see it in
action. So to give you a quick recap of
what we've done so far we've built out
the responder chain. We've built out the
revisor chain. We again built out the
execute tools method as well. Right? So
here's how the message conversation
history is going to extend. Okay, I'll
give you it is going to be very very
similar to what we saw in the reflection
system that we saw in the previous uh
this thing right. So initially as usual
we are going to have the human message
as the first item in the conversation
history. Okay. So the chains and the
system message they are separate but the
conversation history itself that thing
is going to have human message at the
very top which is going to have my topic
you know I can say something like you
know write me something like this write
me
uh a blog post on this particular topic
right so the first human message that
human message is going to be sent to the
responder that chain is going to be
invoked by passing in that particular uh
history and then it is going to spit out
You can imagine it is going to spit out
an AI message. That AI message inside of
the args you're going to have all the
properties uh based on that particular
schema, right? So that is going to be
the second step. So the control flow is
going to go from the human message to
the responder. So right now we have the
human message at the very top and then
we have the AI
message. Now that AI message is going to
be sent to the method, right? the
execute tools method that we we've
written, right? So you can imagine that
we've mocked the input and you know the
method is like perfect to accept that AI
message right so that is going to be the
uh you know the third step which is to
send all of that the last AI message to
the execute tools right so once that is
done the tools is going to output
another tool message right so uh that
tool message is going to have the
content and that one tool message inside
of the content is going to have uh it's
basically going to be an object of three
different phrases. And each of those
properties, each of those search uh you
know uh terms are going to have you know
five five of you know objects five
objects inside of each property and each
of those objects is going to have the
URL and the content. So it is only going
to be one tool message. Okay. So very
similar to what we saw in the reflection
system, every node is going to return an
array of just one AI message of one
human message of not not necessarily
human message but sometimes yeah we can
do that sometimes but yeah it's going to
return an array usually and that array
is going to be appended to the existing
conversation uh list. I hope that makes
sense. And right after the tools that is
going to return an array of just one
tool message that is again going to be
this entire thing this entire you know
history is again going to be sent to the
revisor chain and the entire thing is
going to be inserted in the message
placeholder of the revisor chain right
and now we have an option right so we
can either go back to the execute tools
right the revisor chain is going to
output another AI message with its own
search results and with with its own
critique. So we can either go back to
the execute tools or we can go uh you
know end the graph cycle as well. So we
can actually set a limit here as well.
Very similar to what we did before we
can set the limit to two this time right
because there's a lot of computation
happening. There's a lot of there's
quite a lot of you know tokens that are
going to be not a lot but a decent
amount of tokens are going to be
consumed. So I'm just going to set the
limit to two. All right. So I hope that
that makes sense. So in this uh section,
let's go ahead and create our graph
file. And this is going to be relatively
very very simple. It's going to be fun.
So let's actually go ahead and get
started. So I'm just going to go to our
you know this particular folder. And I'm
going to create this file called let's
call it u you know
reflexion graph. All right. Perfect. So
up here I'm going to import a few
imports. So basically what I've done is
I've imported the list and I've imported
the base message tool message. These are
some things that you know basically to
strongly type our nodes. I'm just going
to you know use some of these things. We
you we are using the message graph and
you uh you I hope you remember what a
message graph does right and then we are
using the revisor chain and the first
responder chain from the chains uh
module and we are also making use of the
execute tools that we've written in the
module. Perfect. All right. So the next
thing that I'm going to do is I'm going
to go ahead and initialize our graph. So
to do that I can just say graph is equal
to message graph and then I can call
this right that's pretty much it. If you
remember this is exactly what we did
previously. All right. So let's actually
go back to the diagram. So let's go
ahead and first add these three nodes.
So we have the responder node, we have
the executor node, we have the revisor
node. If you remember, we just need to
say graph dot add node and the first
argument uh is going to be the name of
the node and then the second is going to
be the actual node, the function or the
chain, right? So I'm just going to say
graph dot add
node and I'm going to call this
responder node as the draft. So you can
imagine that it's going to basically
this uh component is just going to
generate the first draft and this is
what is going to actually like make it
better and revise it. So I'm going to
call it draft but you can call it
anything that you want. You can call it
you know some random name and doesn't
matter. Don't uh you know just follow
what I'm doing. But yeah so uh we have
the draft right here and I can just go
ahead and put the first responder chain
right here. Perfect. So let's actually
do the next one as well. So um the next
one is going to be the um this execute
tools right here. So for this I can just
uh use the same name right here and I
can use this method as well. And finally
we also need to
add we also need to add the uh the
revisor right. Okay so this revisor
chain right here. So I'm going to call
it the revisor and I can put in this
thing right here as well. All right
perfect. So uh we've added three of
these nodes. Let's now go ahead and
connect these nodes together. So
obviously the first one is going to be
you know there's got to be an edge
between the responder node as well as
the execute tools node right so to do
that this is going to be another simple
edge right there is no like you know
conditional edge or anything it's going
to be very simple edge so let's put this
draft right here and then let's put the
execute tools right here okay so what
we've done is we just connected we just
drew a line between the responder as
well as the execute tools so next thing
is we need to do the same thing for the
execute tools and the revisor. So this
is going to be a very simple edge as
well, right? So I'm again going to say
add
edge and we need the execute tools name
and then
the revisor. Right? Okay. So we want
this perfect. So we've drew all of the
edges edges as well. Now what happens
next? So this line is connected. So the
next thing is we have to you know right
after the revisor we have two options
right here, right? we can either go to
the human or we can again go back to the
execute tools. So I'm going to write uh
a function something like an event loop.
Basically that function sort of decides
you know if uh you know two iterations
have been done four iterations have been
done you know it's going to check that
and depending on that it is going to
route whether to go here to the human or
it is going to go back to the execute
tools right so I'm just going to copy
paste some uh the method right here and
let me slowly walk you through it. Okay.
So here basically what we're doing is uh
in the event loop again pretty much
every node is going to get the list of
all the history messages right human AI
tool message human AI whatever the
entire thing is going to be received
right here right so basically here what
I'm doing is I am looking at all these
messages and I'm seeing how many tool
messages are
there okay so we know that let's come
back to the diagram
we know that every time this execute
tools component is going to be executed,
it is going to spit out one tool message
with all the data, right? So if if it
were to loop a second time or the third
time or something like that, the count
is going to go up, right? So I'm just
going to say uh count tool visits and
I'm basically calculating how many tool
messages are there in this particular
list. Okay, so that is exactly what I'm
doing. I'm looping through it and I'm
calculating the sum of it. Okay. And now
I am, you know, this is the the
iterations and I'm checking if it
exceeds my maximum iterations constant.
It's a threshold that I'm setting. So I
can just go up here. Let's say I can put
it over here and I can set it to two
because that's what we've decided at the
very start. You can always increase it
as well, but it's just going to take a
little bit more time to, you know, fully
run the process. Okay. So this is a
drawback of this particular section.
We'll look at the uh advantages,
disadvantages and all of that uh you
know at the end of the section. But if
your product you know requires really
quick responses for the user then this
this system might not be the best thing
to to do it. But if your users are
comfortable with you know them taking
their uh they they prioritize quality
over you know the the speed in that case
in that case we can just go for the
reflexion graph. All right. If the max
if the number of tool messages are more
than two, it means that we have to end
the graph or else we can go for the
execute tools again. Perfect. So that is
exactly what I've done here. So if it is
greater than this thing, we are going to
the end or else we are going back to the
execute tools node. So I hope that makes
sense. Now let's go ahead and add that
conditional graph. Okay, so how do we
add that conditional graph? We can just
say add conditional graph and then we
are basically saying that you know right
after the revisor go to the event loop
right so right after the
revisor go to the event loop okay and
then let it sort of branch out into the
end or the execute tools so very similar
to what we did in the previous uh uh you
know topic that we saw. Perfect. So I
think that's it right. So we've
connected the responder to the execute
tools execute tools to the reviser.
We've connected the reviser with the
event loop method that is going to
branch off to the end or it's going to
branch off to the execute tools. And
finally, if you remember, we have to set
the entry point as well, right? So let's
quickly do that. So we have to go back
and then set entry point to draft. Okay,
so because this is going to be the entry
point of the graph. All right, perfect.
So I think that should be it. Let's go
ahead and compile the graph now. So I'm
just going to say graph.compile. And
finally I can just uh you know um create
the mermaid diagram as well just to make
sure that everything is looking good.
All right. So I'm just going to do that.
And also I'm going to invoke the graph
as well. So and next step I'm also going
to invoke the graph with my human
message. So I can just say app do.invoke
and I can provide in my uh string right
here. write about how small
businesses can
leverage AI to grow. Okay, so this is
going to get the response and now I can
go ahead and print the response as well.
So what we have to do is we have to go
inside of the response. We have to get
the last AI message in the history which
is going to be the AI message and then
we have to go inside of the tool calls.
We don't want we are not interested in
the content the main content we have to
go inside of the tool calls and even
that tool calls is going to be an array
right so we have to go inside of the
first array and then go inside of the
args and then we have to go inside of
the answer okay I hope that makes sense
if if this is confusing to you just
print it out and see how the structure
is going to look like and then it would
make sense to you and why not let's
let's also go ahead and print out the
response as well so that we we'll be
able to see the full unabbridged
version.
Perfect. Okay, then let's actually now
go ahead and let's make sure that
everything is looking
good. All right. Um Okay, let's go ahead
and let's see what we get actually. So,
I'm going to go back to our graph and
let's run this
graph. If at all there is any bugs,
we'll try to debug it right
now. So, you can see that this is going
to be the graph as well. You you can
actually pause the video and then look
at you know if everything is looking
good. All right. So we are getting some
issue right here. It is saying that uh
you know the answer question is not
really the output of the answer question
tool call is not really correct. So
let's actually go back to the
chains. Okay. Okay. Okay. Okay guys. So
this particular chain is not really
returning an AI message, right? But
rather it is like you know going through
the parser. So it is not really the AI
message but I just added this thing here
for the validation if everything is
working good. I just wanted to test it
by adding this parser. So what I can do
is I can just remove that and instead of
that I can put it outside of the chain
just to make sure that you know the
validation is also taken care of. So if
at all the LLM is not going to return
the schema in the right format it would
still throw an error but this parser is
not going to be a part of the chain. So
to do that I'm just going to say
validator pyantic tools parser and
inside of the tools I can just provide
in the answer question. Okay, perfect.
So this is also going to throw an error
if at all the output is not really
adhering to the schema. All right,
perfect. So now the chain is going to be
properly ready. It's going to output the
AI message and now it should work. So
let me clear it out. Let me come back to
the reflection graph and then let's run
it again.
Okay. So, we'll actually take a look at
the lang as well. See how what is
happening there. But yeah, you can
imagine that it might take a little bit
of time because you know the the the
first responder is generating this
entire 250 word at this thing and then
it is going to you know the tool calls
are going to happen and there's like
three separate uh search terms that are
being called and then that information
is being fed into the revisor and that
revisor is going to loop uh twice.
Right. All
right, let's see what's
happening. Perfect. Okay, so you can see
that. Okay, guys, this is a little
difficult to read. Um, okay. Let me
actually go ahead and search for
um, okay, I didn't really put another
keyword here for the first print. So,
I'm just going to come up to the very
top. All right. Okay. So, you can see
that we have, you know, the small
business can leverage AI to grow. This
is going to be the final revised you
know the final AI messages final
content. So you can see that the numbers
are also here. So you can imagine that
at the end of the blog post we are going
to have the URLs as well. So let's make
sure that okay perfect. So we have the
references as well right perfect and
then also we have the unabbridged
version right here. Right? The entire
conversation history you can see that we
have the human message the AI message
should be somewhere here and then
another tool message. Okay, let's search
for tool message. Okay, so
tool message. Okay, it's not really
searching. I'm not sure what's
happening. Yeah, so we have the tool
message right here with the content of
the entire, you know, um the URL and the
contents. We've got everything and then
and then again the revisor which is
going to have an AI message. So I'm not
really be I'm not really able to search
for it right here. So let's actually go
back to our Langsmith and then look at
you know what's happening there. All
right, perfect. So this is going to be
the latest run right here. Let's click
on it and let's look at the trace. So
this is going to be the high level.
Totally took a took us 80 seconds. So
you can imagine that it might take a
little bit of time. So if you don't see
anything in the terminal, don't worry it
is going to come. Or you know if you are
worried that something might have gone
wrong, don't worry about it. Just pull
my code and then just copy paste the
code and then you can debug it later.
All right. So here we can see that this
is going to be my input the original
input and it's going to go to the
responder chain right. So this is going
to be the output the first draft. So we
have the answer we have you know
everything except for the uh you know
the references right and you can imagine
that that is going to be sent to the
tool execution. So the next thing you
can imagine is going to have the tool
message. So if I come down here you can
see that this is going to have the tool
message right. So the name, the phrase,
uh the search term and then we have the
URL and the content again the search
term, URL and the content, right? So we
have this entire thing right here and
then going down you can so what's the
next thing right after the tool message
we have to send that to the revisor
chain right so again there is going to
be the AI message but this time we will
have the references as well along with
the you know the numbers okay the
numbers in the content as well. So if I
scroll down you can see that we have the
numbers 1 two and then at the very end
we have the references as well right
along with that we have the search
queries and we are looping through it
one more time right so again it's going
to go to the tool right so we have the
tool right here again it's going to you
know fetch make the internet search it's
going to pull it and then again it is
going to feed the entire conversation
history back into the revisor agent okay
so we have a tool message and then
another AI message and then another tool
message. Okay, so totally now we have
three different tool messages in our
list. And right now because it's three,
it is not going to loop through it
again. It's going to end it. So that is
why we have the final AI message right
here. And this is exactly what we need.
This is such an enriched blog post. You
know you can see that small business can
leverage AI to grow by enhancing
customer service, optimizing operations.
You know um we have all the you know
references here as well. You know where
exactly it took the data from. So you
can actually you know copy one of these
things. These are like properly legit
right? It pulled these in real time. So
you can actually search for this thing
and you can see that it basically pulled
everything from this particular uh blog
post and it does the same thing for a
lot of other things as well. And again
there is going to be such queries but
since we are stopping the uh the looping
right here we stopping the graph it is
then going to exit out of the graph. So
this is going to be the final thing that
we want. Okay. So please do go through
each and every single one. If you don't
uh if you're not able to understand it
and uh if at all you have any questions
at all please feel free to you know ask
me in the comments or you know um Claude
I believe is going to be very helpful to
you as well. ch is going to be very
helpful helpful to you. So that is it
for the reflexion agent system. I hope
you learned you know a lot of things
about how to parse how to ground the
response of the LLM, how to parse it,
how to you know put it all together in a
graph and I'm hoping that you are able
to you know see how you can actually
customize these systems based on your
own use case. Right? So the goal is not
to you know just copy it but rather to
understand the fundamentals so that you
can actually you know build out your own
systems right so the reflex I just
wanted to end this particular section
with the disadvantages and you know the
advantages of the reflexion system. So
you can imagine that if your product or
if your company you know the customers
don't really care about speed as much
then please go for the reflexion system
because the data is going to be grounded
in live data and also the improved
quality compared to the reflection
system that we saw in the previous uh
section. Okay. So I hope that you
learned a lot. I'll see you in the next
section.
Hello guys, welcome to the section. In
this section, we are going to deep dive
into state in LAN graph. Okay, so why do
I want to do that? So, so far in the
previous sections that we've seen, we
have used the message graph, right? It's
basically just a list of messages. But
in the future, if you build complex
applications, complex graphs, complex
systems, you might want to keep track of
multiple other properties. you don't
really want to restrict yourself to just
a list of messages, right? So in that
case, you might want to uh you know you
would have to learn about state graph.
Okay, so in that so that is what I've
planned. In the next section, we will
look at how to build out the react agent
system using land graph. That is going
to be a little bit more challenging
because there's going to be a lot more
properties that you'll want to keep
track of in the state. But before we do
all of that, I want us to just focus on
state in land graph. Thoroughly
familiarize ourselves with state in LAN
graph and then once we are confident,
we'll move on to the next section. So
let us look at some slides. State in LAN
graph is a way to maintain and track
information as an AI system processes
data. Right? So think of it as a systems
memory allowing it to remember and
update information as it moves through
different stages of a workflow or graph.
So here are some concepts that we are
going to be covering. So we will first
look at what is state graph you know
what are some basic state structures and
then we will progress to a bit more
complicated state uh structures and then
we will see what are the different ways
to update that state. So there is a
manual state transformation that we can
do and we can also do a declarative
annotated state transformation. So I'm
going to go back to VS code and you can
see that I've already created a folder
right here called state deep dive. I'm
going to create a new file called let's
say you know the first thing I want to
start off with something very simple. So
I am going to call it basic state py. So
let me show you a simple graph that we
are going to be building. So this is
what I've planned to explain this. So
all that we're doing here this is a
simple counter. Okay. So this is going
to be the state which is just going to
have one property called count. Initial
value is going to be zero. Okay. So we
have the start node and as soon as the
increment node is hit, the count is
going to be made to one. Okay. So once
it is made to one, it is going to go to
the should continue method. So this
should continue method is going to have
a condition. We can set whatever
condition that we want. But I'm going to
set the upper limit to five. Okay. So
basically I'm saying okay increment keep
on incrementing it until five is reached
and as soon as five is reached go to the
end. Okay. So by doing this we are going
to be able to see how we can actually
maintain a state like this. So so far
we've only seen message graph which is
going to be a list of messages and you
know we we weren't really able to build
custom states with that but right now we
are going to learn how to do that. So
let's come back. So the first thing that
I'm going to do is I'm going to define
the blueprint for
our uh dictionary for our object. Okay.
So blueprint as in okay this is this is
going to be the name of the property and
this is going to be int which is going
to be the data type of that particular
property. So to do that I can just say
class and I can call it whatever I want.
I can call it simple state or something
like that. All right. And now we have to
import from type dict. Okay. So I can
just say from
typing import type
dict. Okay. So I can actually look at
you know we can see that type dict
creates a dictionary type such that a
type checker will expect all instances
to have a certain set of keys and the
consistent typing as well. So basically
it enforces something like this. So here
we're saying that you know this should
be a count and this should be an
integer. Okay. So whatever object or
dictionary that we are going to be
dealing with in the graph it is going to
have this particular structure. So that
is what it means. Now let's come back.
So let's actually go ahead and build out
this particular node right here. So we
are going to be following the same
structure. We are going to build out
each of these nodes and then we are
going to connect it together using state
graph this time not message graph. All
right. All right. So I can just uh let's
just keep it very simple. Let's call it
uh increment. Okay. This is going to be
the node and this is again going to get
the state. If you remember every state
every node in a graph is going to get
that uh input. Okay. So since we are
going to be dealing with a state graph,
it is going to get the entire dictionary
with this particular structure. Okay. So
we can just call it simple state. And
this node is again going to return the
same simple state with modified values.
That's it. All right. So basically what
this is going to do is this is going to
return another object. The same object
whatever this is returning is going to
replace the existing global state. So we
can say something like count. So this
needs to be in strings. So we can say
state count and then we just need to
increment it by one. Okay. So that is
how simple it is. So you can see that
this is a very important learning. Okay.
So any any node you know every single
node is going to get the same state. If
it is a message graph, if we are using a
message graph here we will get a list of
messages. But since we are going to be
dealing with state graph, I will
initialize it later. But since we're
going to be dealing with state graph, we
are going to get a dictionary with this
particular property called count. All
right, perfect. So what is the next
thing that we have to do? So let's come
back and let's actually go ahead and
build out the should continue node. So I
can just say should
continue and this is again going to
receive the entire state and let's
actually write the condition right here.
So I can just say if state count is less
than five. Okay. So if state count is
less than five, we want this control
flow to go back to increment, right? So
I'm going to say return and I can say
continue. Okay. And if this is not the
case, then we can return the end right
here. Okay. So let's actually go ahead
and import from land
graph.graph
end.
Perfect. Actually, you know what? I
don't really want to use end either. I'm
just going to say something like stop.
Okay. So I want to I want us to discuss
about something else that is also
possible. Okay. So I'm going to keep it
very pure this function. I'm going to
say continue and stop. Okay. All right.
Now that we have this node as well as
this node done, let's actually go ahead
and create the graph and let's add these
nodes to the graph and then let's you
know do the conditional edges as well.
So to do this now, I'm going to say
graph equals state graph. Okay, I'm not
going to be using the message graph this
time. So let's actually go ahead and
import the state graph right here. And
in the state graph, we have to provide
the blueprint that this state graph
needs to create for the object for the
dictionary. So we have the blueprint
right here. So let's actually go ahead
and provide that. Perfect. So now let's
go ahead and add the nodes right here.
Okay, so the first node is going to have
increment as the name and then let's
pass in the method as well. All right,
perfect. So what is next? So now we need
to add a conditional edge, right? It can
either go here or it can go to the end
as well. So let's say graph dot add
conditional edge. And we're saying that
as soon as the increment is done, go to
this should continue method. So this is
going to do this thing. All right. So
let's actually I'm going to do something
here. Okay. So it is going to go to the
should continue. So but here you know
we're calling it continue and stop. We
are not really calling it increment and
end right. So we can also do something
like this. So this takes in a third
argument. So in this argument you know
this is a method. This is an object. And
inside of here I can just say if this
should continue returns continue then it
means that you need to go to increment.
Okay. So this is what it means. If it is
continue you need to go to increment. If
it is what's the next one? If it is stop
we can go to end. Okay. So I just wanted
to add it here because I wanted to
provide a bit more syntax exposure for
you. But if you don't want to, you know,
deal with something like this, you can
just go for, you know, saying end right
here and saying increment right here and
removing this. Okay, so this also should
work. So let's actually, you know, go
back. This looks perfect. So so far
we've added the node, we've added the
conditional edge as well. And then
finally, we just need to add the entry
point, right? So we can just say graph
dot set entry point. And that is going
to be the increment node. And now
finally let us go ahead and compile this
graph as well. So I can call it
app compile. And then finally we can
invoke it. And right now we are going to
provide a dictionary. Okay. So the
initial state is what we need to provide
right here. Okay. So we can just put it
separately as well. The initial state is
going to have account of zero. Right. So
that is something that we can provide in
this state right here. All right.
Perfect. So initially it is going to be
zero and this is going to return the
result. We can go ahead and print the
result as well. So if I run this
file perfect you can see that we get
count is equal to five. Okay. So all
this other additional you know logs that
you're seeing warnings and all this has
to do with lang. Let's not worry about
it right now. But right here you can see
that the count has been incremented to
five. So you can imagine that what
happened is that it first went to
increment and then it looped five
different times and then finally the
should continue directed it to the end.
But the main takeaway from this
particular section is that instead of
having a list of messages we can now
define pretty much our own custom state
anything that we want. So in this case
we went with count. Okay. So all that we
did was we defined the blueprint. We
defined the blueprint right here using
the type and then all that we're doing
is providing it in the state graph.
Okay. So now every single node right
here is going to you know is going to
have access to the entire object the
entire dictionary that the state graph
is going to create using the same schema
using the same you know um blueprint
that we specify right here. So in
conclusion, you can actually imagine
this state, this object to be like a
global state that is available for all
of these nodes to modify, update, right?
So if you're a front-end developer, if
you're a React developer and you've
worked with Redux, let's say, you know,
it is kind of very similar to it, right?
All right. So very similar to Redux as
well. There, you know, we don't mutate
any of the values. we actually create a
new object and then we update right so
that is called immutability and that is
exactly what we are going to follow in
LAN graph as well so you can see that
here we are actually creating a new
object and then we are merging it with
the existing object okay so perfect so
in the next section we are going to
increase the complexity just a little
bit we are going to be adding you know a
second property to the global state or
maybe even a third property okay I'll
see you in the next
section all right guys so in this
section Let's actually, you know, let's
go ahead and create another file and
let's call it, let's say, complex
state.
Complex
state.py. Okay, I'm just going to copy
paste everything right here. Whatever we
did in the previous section. So, I want
to add another property to this, you
know, blueprint. And basically what I
want to do is I want to keep track of
the sum of whatever count that that the
system goes through. Okay. So initially
the sum is going to be zero. So let's
actually go ahead and put that down here
as well. So the initial state the count
I mean the sum is going to be zero. And
so in this particular node right here in
this node right here. So if the sum is
going to be zero and the count increases
to one, the sum is going to be 1. So
right now the sum is going to be one.
And if the count were to be increased to
two, then it's it's going to be 1 + 2,
then the sum is going to be three. Okay.
So you can imagine that if there's five
more iterations, the sum is going to be
15. So let's actually go ahead and
implement it. So in addition to changing
the count property in the state, I'm now
also going to update the sum property.
Okay. So how can I do it? So in this
object that I'm returning, I'm also
going to update the state. So first off,
I need to get the current the latest sum
value and then I also have to, you know,
get the next count value and then add it
to the existing sum. Okay, so since
we're repeating it, I can just go ahead
and say new count and then I can just
like move this outside.
Okay. So I can also do something like
this and this should work perfectly
fine. All right. So let me go ahead and
quickly run this as
well. All
right. Okay. So coming up you can see
that we have count as five and sum as
15. So you can imagine that we can add
as many properties as as we want and
this is the whole point of a custom
state. It is completely in our control.
Right. Perfect. Let's also do another
thing. Let's also have a history.
Basically, I just want to keep track of,
you know, what are the different counts
that we've encountered and I want it to
be in a list format, right? So, in this
case, I can just say it should be a list
of ints. So, for the list, I can just go
here to typing and then, you know, get
the list as well right here. Okay. So, I
can do the same thing. So initially
coming down in the initial state we want
the history to be an empty uh list right
here and now I get access to the history
everywhere. So you know I can easily get
it from here and then I can update the
history as well. So the new history
value we want it to be you know we want
the latest count to be appended to the
list right so state history right so
statehistory is going to be an empty
array because if you come down here that
is what we've said right here okay this
needs to be an empty array and we can
actually you know append two arrays
together by using the plus operator okay
so we can actually put whatever thing
things that we want here and then two
lists are going to get merged together.
So what do we want right here? We want
the new count again. All
right, perfect. So let's actually give
this a world as
well. And if I run it, you can see that
don't worry about these things, but you
can see that we have the count as five,
sum as 15, history as 1 2 3 4 5. Okay.
So that is how simple it is to manually
update the state. Okay. So in this
section basically how we've updated the
state is that we have updated it using
the manual method. Okay. So we're
manually we're doing all of the
calculations right here and then we are
updating it manually. But there is
another way of doing it using annotation
as well and that is something that we'll
be covering in the next
section. Hello guys. So uh in the
previous section we looked at the manual
state transformation um which is where
we are basically updating the state
inside of the node and we're doing all
of the calculations inside of that
particular node. But there is slightly a
different way to do it as well. So that
is called the declarative annotated
state transformation. It is not it is
not going to be as daunting as it
sounds. So let's actually look at what
what it is used for. Okay. So in the
manual way you know we just dealing with
one node right here and let's say you
know we are calculating the sum right so
this is going to be the the the the
operation right but let's say there's
like a four or five or 100 different you
know nodes that do the same you know
summing operation so you can see that
this is going to get repeated a lot
right so basically just to cut down on
that there is a slightly more you know
better way of doing it and that is
through annotation so I'll show you what
that means so I can just say annotated
and right here we can actually you know
provide landing graph basically tell it
give it some metadata and tell it how to
update the state in the future so I can
say
annotated okay so the first this thing
is going to be the type okay and then
second in the second uh option we can
actually provide it how to update uh the
state in the future so right now what
we're doing is we're just summing it
right so we are taking the previous
value and then we're adding the adding
it with this new computed value. Right?
So all that we have to do is we can just
say operator do add. Okay. So we don't
really have the operator. So I'm just
going to say operator. So now that we've
added it here, we can actually go ahead
and remove this part. Okay. So
essentially what we're saying is uh this
this annotated this thing is going to
keep track of the previous value and
then anytime there is a new value that
is going to be returned in the place of
this property. Okay. So anytime that is
being returned add this to the existing
value and that is what this particular
method means. Okay, let's try to do the
same thing for history as well. So we
have the previous value right here. Um
we don't really want to you know keep on
repeating for other nodes as well. So in
that case we can remove this and we can
actually you know say
annotated. Okay so annotated. So again
this is still going to be a list but uh
if there is there is an existing list
that is being maintained and if there is
a new list that is being you know uh
added how do we update it? We have to
concatenate it now right we have to
merge it. So we can just say operator
dot concat okay so you can see that it
it provides a bunch of different methods
you can see that call we've got a lot of
different methods. So in this case I'm
just going to use concat. All right and
that is it.
So if I were to run this, you should be
able to see the same exact thing, no
different. So this would have slightly
simplified your code a little bit,
right? So you can see that you know this
is a lot more clearer now. And then all
the you know the actual you know
intelligence as to how to update the
state is maintained here. Okay. In this
particular annotated and we're using the
operator. So there might be some uh you
know some operations that might not
actually be available in the operator.
In that case you might have to do it
manually right here. But yeah this is
always available for you to use. And the
reason why I wanted to cover cover all
of these different uh patterns is
because you know we will be using some
of this in the next section where we are
going to be building our very own react
agent that we saw in the start of the
course and we are going to be building
it using LAN graph and that is going to
give you a much more deeper insight as
to how things work and also that is
going to give us a lot more flexibility.
Okay. So using lang chains classes we
don't really have a lot of control
infinite loop problems but if we have
the control with us if we build it using
lang graph that is going to give us a
lot of control and we eliminate some of
the drawbacks of using the uh the lang
chains default classes. So I'll see you
in the next
section. Hello guys welcome to another
brand new section. In this section we
are going to be building the react agent
using LAN graph. Okay, so in the
previous section we learned everything
there is to know about state in LAN
graph. How to use custom state. So we
are going to be employing all of that to
build out our react agent. So I know
that we did you know implement the react
agent using lang chains you know uh out
of the box classes like initialize agent
method and agent executor classes. But
in this uh you know section we are going
to eliminate you know the agent executor
and we are going to have full control
over the looping that happens between
the LLM and LAN chain between LLM and
LAN chain we are going to have full
control because this time we are going
to be using our graph data structure we
are going to be using LAN graph so
before we proceed forward let's quickly
jog your memory about you know what is
the react agent what pattern does it
follow so if you remember this
particular slide so you can see that you
know the LLM M first thinks let's say
you are giving the LLM a problem to
solve it first thinks about the user's
prompt or the problem and then we have
action the LLM decides if it can answer
by itself or if it should use a tool so
once it decides once it makes that
decision it is then you know if it
decides that it is going to use a tool
the action input is going to be the LM
provides the input arguments for the
tool right so once the input argument is
provided uh lang chains you know out of
the box has it just like stops you know
the streaming of the LLM. It tells the
LLM shut up. It just basically takes the
think action action input. It is then
you know going to call the tool execute
the tool get the response and with all
the entire history it is again going to
send it back to the LLM and now the LLM
is going to observe the result of the
tool and if the entire problem is solved
at that particular point uh this is the
final answer or else the same loop is
going to
continue. So the problem that we faced
was that we did not really have control
over that uh you know default class that
lang chain provided for you know running
the react agent right because sometimes
we went into the infinite loop problem
we do not have the control to ourselves
right so that is exactly what we're
going to be solving using LAN graph okay
so before we actually start implementing
you know LAN graph let's actually take a
quick peek into this method Okay, so let
me actually come back to the first
introduction where we built out the
React agent. So here you can see that
we've used the initialize agent method.
Okay, so let's actually you know take a
quick peek into what is exactly
happening. What are the components that
sort of you know come together to build
out the react agent. Okay, so that is
what I've written here. So in lang chain
we use the initialize agent as an
all-in-one solution, right? So that is
exactly what we've used right here. It
basically just takes in the tool the llm
you know the the prompt as well the
react prompt we're telling the method
use the react prompt uh inside of it you
know the react prompt would have been
written and then we are setting the vost
to true as well right so this is what
we're using as an all-in-one solution
but it combines two key components so
the first one is going to be the create
react agent method okay just on a very
high level think that you know this is a
method that does one thing and then
along with that we also have the agent
exec computer class. Okay, so this is
exactly what we are going to eliminate
for LAN graph. Okay, so this is what
actually controls the looping between
lang chain and LLM between lang and llm.
This looping is being done by agent
executor and that is exactly what we are
going to be replacing using lang graph.
All right, so create react agent. So
what is it? So you can basically think
of this method to be that method that
creates the
agent. Obviously you know everything's
got to come together to create the agent
including the looping. But for now just
think that this is what is going to
create the agent. So what does it do? It
just takes each tool's name and
description. Okay. So you can imagine
that we would have provided a few tools
uh you know to solve the problem. Right?
So let's come back to the code. We see
that we have the search tool right here.
We have the get system tool right here.
And this entire tools is provided to
this particular method. Right? So let's
see what it does. So it basically takes
each tool's name and description. Okay?
So we have the name right here and we
have the description right here. Okay?
All these things are not going to be
taken at all. These things are not going
to be considered at all. Just these two
things are going to be taken and then it
formats them into a standardized way the
LLM can understand. Okay. So basically
it's going to convert it into a string
and then it inserts them into specific
placeholders in the react prompt
template. Okay. So let's quickly look at
the react prompt template. So here you
can see that we have a placeholder for
tools and then we have a placeholder for
tool names. Right? So that is exactly
what this method is going to do. It is
basically just going to take the name
and description of various tools. It's
going to structure it. It's going to
parse it. It's going to make it into a
proper string and then it is going to
combine it with the prompt react prompt
template so that you know when when we
send it to the LLM the LLM is going to
have full context about what are the
tools that it can work with all right so
not just that okay it doesn't just do
the merging it also makes the LLM call
and okay it makes the LLM call so the
LLM is now going to proceed forward with
you know generating what is this it is
start it is going to start generating
the question thought action action
input. Okay. So these four things it is
going to start generating and the minute
the action input is received and the
minute we actually start go going to
this observation. Okay. So the minute
this is coming to an end the streaming
that happens between this particular
method the streaming that happens
between this particular method and the
LLM you know that is interrupted. Okay.
So we don't really want what happens
after that. Okay, we don't want to we
don't want the LLM to continue because
the LLM is not going to have the result
of the tool, right? So what I mean by
this is up until this point, okay, up
until this point, you know, the LLM
needs to generate the action input. As
soon as the action input is received,
the LLM is going to stop it. Okay, uh
this particular method is going to tell
LLM to stop streaming. Okay, your work
is done. I now have the action input.
it's going to get that action input.
Okay, so that is what it does. It makes
the LLM call. It takes the LM's response
and then it parses it. Okay, so this is
an example. Let's say okay, so first off
the LLM is like you know thought action
action input. The thought could be I
need to find out when SpaceX's last
launch was and calculate the days since.
Okay, so if you remember that this was
the first you know prompt the problem
that we provided to the LLM, right? So
it is first thinking to itself and then
it is the LLM is deciding I want to use
the search tool and then it is also
providing the input to the tab search
tool. Okay. So let's say this is up
until this point this method is going to
allow the LLM to generate and as soon as
this is done and the observation is
started cut and then the data is going
to come back to this particular method
and finally once it has these three
things right here once it has these
three things right here it it is going
to parse the response of the LLM into
one of these two classes okay so the end
this method is just going to return two
It is either going to return the agent
action class or the agent finish class.
So let's actually look at what this
agent action and agent finish looks
like. So this is a lang chain class that
represents an action the agent wants to
take. It typically contains something
like this. So this is how the complete
you know parsed output is going to look
like. It's going to be an agent action
class. It is going to have the tool tool
input and log. The tool is going to be
what tool to use the input to the tool
and then why it needs to why the LLM
thinks that it needs to use this tool.
So this three data is essentially what
these three things are going to contain.
The thought is the reasoning and that is
exactly what is going to be put in the
log and you know the action is going to
be which tool to take that is exactly
what is going to be converted into this
tool input. And then finally we have the
action input and that is going to be
this tool input. Okay. So it just like
parses it and creates this particular
class agent action class. Okay, I really
hope that makes sense. So in case that
you know the LLM is actually you know
does not need to call another tool. It
thinks that you know I don't really need
to call a tool. My answer is done. My my
work is done. I have all the information
that I need. I have the final answer. In
that case this particular method the
create react agent method is going to
return the agent finish class. So this
represents the agent completing its task
with a final answer. It typically
contains you know this is going to be
the output and this is the log as well.
Okay. So now I really hope that you
understand what the first component the
first key component of react agent does.
So this is going to be 50% of the work
that is done by this initialize agent
method. Okay. So let's look at the
second key component of this initialize
agent method and that is going to be the
agent executor. So let's see what it
does. It takes the agent from create
react agent. Okay. And then manages the
execution loop. So you can imagine that
you know the create react agent method
is going to output parse the the agent
action and the agent finish. But
something's got to control the
uh the looping
between the lang chain class as well as
the LLM. Right? So after the LLM is
done, the the uh the control needs to
come back to, you know, the lang chain
lang chain's got to parse it. It's going
to call the tool and as soon as the tool
is called, you know, again, all of the
data needs to be sent back to the LLM
along with the history. So all of that
looping is being done by the agent
executor. I hope that makes sense. So it
basically takes the agent that is
created from the react create react
agent method and manages the execution
loop. It receives the user's question
and feeds it to the agent. Okay. And
then it identifies which tool to run
based on the agent's output agent action
or if it is no tool required then agent
finish. So it executes the tool and
captures the result feeds the result
back to the agent for the next decision.
It continues this loop until the agent
produces an agent finish
class returns the final answer then to
the user. Okay. So to put it very short
agent exeutor is what is actually going
to be controlling the looping between
the R system as well as the LLM. So this
is where we actually lost control a
little bit because of the infinite
looping problem. So if I actually go
back and go inside of the initialize
agent, you can see that it is actually
returning the agent executed. Okay. So
this is why you know this is not really
in our control at all. Okay. So if we
mess up with the prompt or we mess up
you know if we don't provide a tool or
anything then it's going to malfunction
and we don't really want that. So this
is exactly where there is a drawback and
this is what we are going to replace or
rather eliminate using langraph. So we
are going to gain control of the
looping. So let's actually look at some
of the key advantages of using LAN
graph. So LAN graph basically turns the
hidden blackbox loop into a visible
editable workflow. So you can now add
custom nodes, modify the flow, insert
additional logic. So this is going to be
the LAN graph graph for the React agent.
And then we have the reason node and the
acting node. So together is what is
React, right? reasoning plus acting. So
let's actually look at what this graph
means. So the reason node does what
create react agent did? It thinks and
decides. If the reason node outputs an
agent action, then act node executes the
tool. The results from the tool flow
back to reason node for the next
decision. When the agent has the final
answer, it takes the right path to end.
This visualization makes the black box
of agent executor transparent and
modifiable. So I really hope that this
introduction makes sense. If it does
not, don't worry. We are going to be,
you know, dissecting it and we are going
to be looking at it in much more depth.
But yeah, in the next section we are
going to be looking at how we can build
out the reason node right here. So I'll
see you in the next
section. Hello guys. So in this section,
let's go ahead and build out the the
chain or the runnable that is required
to build out the reason node which is
very similar to what we did in the
reflection reflexion systems as well.
Right? So I'm just going to go ahead and
first build out I mean create a folder.
Let's call it uh React
agent and then I'm going to call this
the let's say agent uh
reason runnable. All right. So let's
think about what we need right here. So
all that we need is to you know provide
all of the properties that the create
react agent method is going to need. So
let me go ahead and paste a few imports
right here. Pretty standard stuff. We
have the open AI chat model. that this
is the method that we are going to be
using the create react agent the tavly
tool and you know we have the llm as
well so what is the next thing that we
need so we can just go back to the first
section and then let's see what we did
right here so we still going to need the
same tools that we have to provide right
so we have the get system time tool
basically I'm going to replicate the
same example that we saw we don't want
to reinvent the wheel because we're just
learning right so I'm just going to copy
you know everything right here including
including the search tool as well as the
get system time tool. So let me come
back put it over here. Perfect. And
finally we are going to use this
particular create react agent method and
let's look at what are the things that
it needs. So first thing is it requires
the tools and then it requires the lm as
well and then it requires the prompt.
Okay. So the prompt that we need to
provide is going to be the react prompt.
Okay. So for that I can just go to you
know hub. Okay. So I can just say react
prompt. Okay. This is going to be the
react prompt template. I can just say
hub dot. Okay. Let me just go ahead and
paste it. Yeah. So this is going to
basically pull the triac prompt from the
hwchase 17 lang chain you know um uh the
website. Perfect. So let's actually go
ahead and provide this here as well.
Okay great. So looks like this method
has been provided everything that it
needs. So this method is going to return
a runnable which is going to do all of
those things that we saw in the uh the
slides before right. So if I go in here
you can see that this is going to return
a runnable. So we can call this the
react
agent runnable let's say. Okay perfect.
All right guys so we have created the
runnable the reasoning runnable. In the
next section, we are going to, you know,
build out the state that we need to
build out the system. So, in the
previous section, we looked at the
custom state and we can actually put in
pretty much any property that we want to
achieve this particular system. So, for
the react, we are going to need a few,
you know, we are going to think about
what are the states that we need. So,
we'll look at that in the next section.
All right guys, so in this section, let
us look at what are the state properties
that we need to achieve this react agent
system. So you can see that I've already
gone ahead and created a file called
react state.py and I've imported a few
things like the operator. I'm I'm pretty
sure that you know what operator is by
now. We have the annotated class. We
have the type dict and then we have the
union. Okay, we'll talk about what these
things mean. All right, so this is going
to be the agent state. We can call it
anything that we want but I'm just going
to call it agent state or react agent
state. We importing the type dict and
these are the three properties that we
need to achieve. These are the three
state properties that we need to
actually you know you know achieve this
particular system. So the first one is
going to be very simple. The first one
is going to be the human message the
initial human problem. Okay. So for for
example my problem initially was I
wanted to you know this was my prompt
right so this is going to sit right here
and you know initially we're just going
to put it right here and it is going to
stay for the rest of the uh process. All
right so next thing is going to be the
agent outcome and then we have the
intermediate steps as well. So let's
look at what this agent outcome is. So
this agent outcome if you remember is
exactly what this runnable is going to
output. So if you remember this
runnable, so if I go in here, you can
see that this runnable is going to
return either an agent action or the
agent finish. So I can actually search
for agent action. So you can see that it
returns a runnable sequence representing
an agent. It takes as input all the same
input variables as the prompt passed in
does. It returns the output either as
agent action or agent finish. Right? So
this output is going to be put in right
here. Okay. So initially it is going to
be none but you know once the LLM wants
to use a particular tool after the
parsing this agent action can sit here
or the agent finish can sit here. Okay.
So the union is basically like you know
it can either be this or it can either
be this or it can either be this. That
is what this means. But yeah initially
it is going to be none. And next we have
the intermediate steps. And this is
basically just to keep track of the
entire history of all the existing
problems that have been solved. We don't
want the LM to keep on solving the same
problems again. Right? So if if there is
a small pro part of the problem that's
been solved already, we save it and then
we keep it right here as a list of
pupils. Okay? So anytime there is a new
tool call and the output of the tool
call is done, we're just going to add it
to the existing list. So if this does
not make sense to you, don't worry. I
actually have sought the help of claude.
I've just like told it exactly you know
how basically I told it okay I want you
to give me the exact sequence of how the
state changes as the looping happens in
the right sequence. So this is exactly
how it's going to go guys. Okay so
initially when the application starts
with app.invoke invoke. Okay, so let's
say this is going to be the initial
human prompt, right? So initially the
agent outcome is going to be none and
then the intermediate steps initially is
not is going to be an empty array as
well. Okay, so in the first iteration,
okay, the flow starts at the entry
point, right? So if you come back to the
uh the graph, so this is going to be the
reason node, the entry point, right? So
what this is going to do, this is going
to uh you know, it's going to call the
method. This is going to sit inside of
the create react agent. Don't worry
about it. But yeah, this is going to
process the input and it is going to
parse and generate the agent action for
the search tool. Right? So this is going
to be the updated state. So as soon as
the react node is done executing, it is
going to return the agent action. So
that agent action is going to sit
inside. We are going to update the state
and put it in the agent action the agent
outcome property. So this agent action
is going to have the tool this tool to
use tool input. This is going to be the
search term and then the log. Okay. So
right here this is going to return it.
And now we are going to put the data in
the agent outcome. So what is the next
step? We need to send it to the act
node. The act node is what is going to
actually execute the tool. So now it has
all the data. It needs it knows okay it
knows exactly you know u what tool to
use what should be the input to the
particular tool. Right? it it has all
that information. So what what's going
to happen next? The conditional ledge is
going to happen. Okay, it's either going
to go to you know the uh the act node or
the end node. So it sees the agent
action. It is not agent finish. It sees
the agent action. So it is sending it to
the act node. We'll actually build all
of this later in the graph. But it is
going to send it to the act node and
then the execute tools. the tools are
going to be executed and then as soon as
the tools are executed the intermediate
steps is going to get populated. Okay.
So we in the tupil so this is going to
be the first pupil in the list. In this
tupil the first part we are going to
have the agent action that was generated
before and also we have the output.
Okay. So we have the output of the tool
execution. Okay. Here it is going to you
know all the output of the you know the
search results is going to sit right
here. So this is going to be the
intermediate steps. Perfect. So as soon
as this is done the tool execution is
done. What happens? So as soon as the
tool execution is done we are going to
send all of that information back uh I
mean the control flow is going to go
back to the reason node and then all
that history is going to be again sent
back to the LLM in that agent
scratchpad. So now it has solved a
particular problem. One problem it has
solved. Now it needs to solve the next
problem and for that it might use a
different tool. Right? So we have the
second iteration and then we have the
agent reason node again. Okay, it's
going to get updated. Now get system t
get system time needs to be called not
the other tably search. So this time
okay the same thing happens the act node
is going to execute the get system time
tool and then it is going to append it
and then the final llm call is going to
happen in the agent reason node. Right?
So this is going to be the updated state
and then finally uh what happens the
agent finish is going to be you know
outputed by the reason node and that uh
so what happens the agent finish is
going to come right here and it's it's
not going to go to the act node but
instead it is going to the end node.
Okay so that is exactly this is exactly
why we need these three different
properties. So don't worry about how
this is all coming together right now
but for now just know that these are the
three properties and why we actually
need those three properties. Okay. So
that is what I want you to take away
from this. Okay. So in the next section
we are going to be building out the
nodes. Okay. So this node and this node
we are going to be building it out. So
I'll see you in the next
section. Hello guys. So in the previous
section we built out the runnable that
this reason node is going to depend on.
In this section let's actually go out
and build these nodes. Okay. So you can
see that I've already gone ahead and you
know I've created this file called
nodes. py and we have the reason node as
well as the act node right here. So
let's actually let me take you through
it step by step. So right here you can
see that we are going to use the react
agent runnable that we wrote in the
previous section and all that we are
going to do is we are going to invoke on
it using the agent state. Okay. So all
the values all the these things that we
see right here these are reserved
keywords and this you know runnable is
going to know exactly what to do with
those particular names. So don't change
anything. So you will have to follow the
exact input agent outcome intermediate
steps. Okay. Everything's got to be
perfect. And now this runnable is going
to look at the state. It's going to take
all of the properties, merge it with the
React prompt, and then it's going to do
all of that magic including calling that
LLM and then passing the output of the
LLM and then having that particular
agent outcome. If you remember what is
going to be the agent outcome, this
thing is going to return the agent
action class or the agent finish class.
Right? So here we are going to update
the agent outcome. Right? So right here
we have the agent outcome that is going
to be an agent action or the finish. So
as soon as this node is executed, it is
either going to have agent action or
agent finish. Only during the first
initial, you know, instance, it is going
to be not uh it is going to be none. But
after that, it is always going to be
action or finish. All right. All right.
So let's come back to the graph. So
now let's let's assume that it is going
to be agent action. Now it is going to
come to the act node, right? So at node
we are going to again basically extract
what is that agent action that needs to
be done and now we've written the tool
executor here as well right so here we
are providing the tools and we have the
tool executor all that we're doing is
we're invoking that uh you know we're
providing that agent action and then
whatever output that we're getting we
are again we are going to be appending
it to the intermediate steps. Okay, so
if you remember the intermediate steps
is going to be a list of pupils. Each
pupil in the first part it is going to
have that agent action or the agent
finish and again it is going to have the
output of that particular tool
execution. Right? So in this case the
output is going to be here. We are
stringifying it and then we are putting
it here and whatever we return here in
this array in this list it is going to
be merged with the existing intermediate
steps list. Why is that? That is because
if you come back to the state, we are
using the operator do add. Okay. So now
I hope that you you see that everything
is coming together, right? Perfect. So
now we have the reason node as well as
the act node. So the next step is going
to be very simple and fun to be honest
because all that we're going to be doing
is putting it all together in a graph
and you know drawing these edges and
then running it. So we will build out
the graph in the next section. So I'll
see you there.
Hello guys, I'm from the future. I had
to uh rewrite the act node alone because
in the latest version langraph version 3
the the tool executor class that we had
used inside of the act node that has
been deprecated. It does not exist
anymore. So we are going to do it
without using any of these methods.
Okay, without using the tool executor
class. Okay, so it's going to be very
simple. So this line we've already
written. The only change that we are
going to do is without using that tool
executor, we have to invoke the right
tool. Okay. So if I come back here.
Okay. So we already have the list of
tools that we have available. I'm just
going to uh take it right here. Okay. So
we have this available. So from the
agent action I am all that I'm doing is
I'm I'm going to take the tool name and
then I'm going to take the input. Okay.
So we can easily extract the information
from the agent action. So this is going
to be step one. Okay. So from step one
we are getting information about which
exact tool the LLM wants to call. Okay.
So the LLM might want to call get system
time or it might want to call the search
search tool. Okay. So we have both of
the information right now. The next step
here is to basically get access to
whichever tool that we need at that
particular moment. So I'm saying I'm
basically just looping through the tools
and I'm checking okay which tool name
matches with the tool name that the LLM
wants to call and I'm finally getting
the tool function getting access to the
tool function. Okay, so I'm basically
getting a reference to this or this.
Okay, so that is it. So once I get a
reference to either this or this that I
want in the act node, I can then invoke
it. That is it. Very simple. We don't
need tool executor uh in this case at
all. Okay. So once we have uh that
particular tool I'm just going to invoke
on it right here. Okay. So why do we
have this if else? Very simple. If we
are dealing with get system time and we
have like another few of these things a
b whatever in that case we will have to
spread it out. If not we don't have to
spread it out. Okay. So that is what I
have done here. Okay. And then finally
we are going to get the output and then
we are going to add it here. Okay. So
it's a very simple uh alternative. Also
I don't really want to rely keep on
relying on these you know langchain
classes that they providing for two
reasons. One thing is we are not
learning you know exactly how to do it
without these classes because they
introduce breaking changes especially in
production you know we keep on having to
change code which I don't really like.
So yeah this is a this is a much more
simpler way of doing it and that is
exactly why I wanted to uh introduce I I
just wanted to make this particular
section. So thank you. I'll see you in
the next section.
Hello guys, welcome to the final section
in this topic where we are going to be
putting together all these different
nodes uh together to form the final
react graph. All right, so you can see
that I've already gone ahead and created
a file called react graph and I've
already populated the code as well. So
let me walk you through it step by step
what I've done. So I'm just doing a very
standard import of all the nodes that
we've written. We we have imported the
agent state as well the global state
that we have written right and then
we've also imported the state graph so
that we can create the graph and then
just some classes like agent finish and
agent action just to you know type it
basically okay force type it basically
right all right so let's actually go
back to our graph so the first thing
that I'm doing is I'm basically just
giving it a constant name for these
nodes so that is exactly what I've done
here so we have the reason node and the
act node Okay. So, so if I scroll down
here, all that I'm doing is I'm creating
a graph using state graph and I'm
providing the blueprint that we've
created as well. Right? If you remember
the blueprint is going to be, you know,
the input is going to be a string. Agent
outcome is going to be one of these
three things. Immediate steps is going
to be a list of pupils with each pupil
having the agent action as the first uh
part and the string as the the output of
the agent the output of the tool
execution as the second part. Right? So
coming back and that is exactly what
we're passing in here and I'm creating a
simple graph. All right. So first thing
is I'm adding the react node I mean the
reason node and the act node. So you can
see that I'm adding the reason node I'm
adding the act node and I'm also setting
the entry point to be the reason node.
Okay. So I'm saying that right after the
reason node is done executing we want to
basically check if this particular
outcome let's go back to the state. if
this outcome has the agent action or the
agent finish. So if agent action is
present then it means that we have to go
to the act node if uh you know agent
finish is present we just have to end
it. So for that we need a should
continue method and that is exactly what
I've written here. So you can see that
so I'm saying that right after the agent
node is done executing I'm adding a
conditional edge and it is the the
control flow is going to the should
continue method. So the should continue
method again like every node in the
graph it is going to get the entire
state as the input right here. Okay. So
for now I'm just going to look at the
agent outcome because that is what is
going to contain the agent action or the
agent finish. So I am checking in the
state is it an agent action or the agent
finish. So I'm checking if the agent
outcome is an instance of the agent
finish. So if it is an agent finish then
the control flow needs to go to the end.
Okay. But if it is not if it is not then
we have to go to the act node which
which is down here. Okay. So for our
prompt it is going to go to the act
node. Right? So that is exactly what's
happening here. And I'm also adding an
edge between the act node and the reason
uh the reason node right. So right after
the act node it should always come back
to the reason node. Right? And that is
exactly what I've added here. That is
it. Okay. So nothing out of the
ordinary. All of this is something that
we are already familiar with. Okay. And
finally we are going to compile the app
and then we are going to invoke the app
using the initial state. Okay. So right
now going back to the state we are just
providing the input. Right. How many
days ago was the latest SpaceX launch?
So we have provided the input. It is
always very good practice to provide the
initial state for everything else. So
I'm just going to you know put the
uh okay what's happening? Okay. So I'm
just going to put the agent outcome to
none which is going to be the initial
state. Initially u the LLM hasn't
suggested anything. And then we have the
intermediate steps and that is going to
be an empty list. All right. And finally
I'm just going to print out the final
state right after the end node is
reached. All right. And then I'm also
going to print out something inside of
the agent outcome. We'll be able to see
why we are doing that you know once we
look at this particular result. So let
me actually go ahead and run this
while let's give it a few
seconds. Okay. So perfect. You can
actually see that we are getting the
entire uh state right here. So we have
the input and this input is what I
provided right here. Right? So this is
going to be the input and then we have
the agent outcome which is going to have
the agent finish. So if you remember
only because we have the agent finish
did the control flow even come to the
end node right so that is why it is
ending the cycle and then we are getting
the result. So in the agent finish in
the return values you can see that the
latest SpaceX launch was 2 days ago.
Okay. So this is going to be the final
answer. Okay. So how we actually got to
this point we will actually look at the
lang logs. Right. But for now you can
see that this is going to be uh you know
uh this is going to be the final answer.
So you can see that this is how it
actually arrived at this answer. So
today is March 7th, 2025 which is true.
It is March 7th, 2025. It was able to
call my, you know, get get system time
tool that I provided. So the latest
SpaceX launch was 2 days ago, right? And
then we also have all of the
intermediate steps that it went through
as well. So we don't really even have to
go to Langmith. So you can see that in
the intermediate steps, first thing is
there was an agent action. it the LLM
suggested that we use the Tavili search
results tool and then this was the input
and then you know this was the reasoning
behind using that tool by the LLM. Okay.
And then we have you know basically the
same thing happened a couple of
different times and then finally I'm
just going inside of the agent outcome.
I'm going inside of the return values
and I'm printing the final result. So
this is going to be the final output.
The latest SpaceX launch was 2 days ago.
Okay. So, if you have any problems
understanding this code, simply just go
ahead. I've I'm pushing all of this code
to my GitHub repo. So, just go there,
you know, pull it and then, you know,
you can uh test it out right away. So,
in the next section, let us look at, you
know, the lang tracing so that we can
better understand it.
All right guys, so in this section, let
us look at uh the lang tracing so that
we better understand uh what are the
different steps that our system went
through. So I ran it a couple of times
but let's let's just go to the latest
one right here. So let's actually look
at you know you can see that this took a
total of 13 seconds right so this is
going to be the initial state and this
is going to be the final state that we
already looked at in the terminal. So
let's actually go through you know what
the steps that happened to arrive at
this final output. Okay. So initially
you can imagine that this is going to be
the initial state that we provide and
the control flow is going to go to the
reason node. So let's click on this and
you can see that this took a total of 4
seconds right. So this gets the entire
state and then this you can imagine is
going to output the agent action or the
agent finish. So the agent action is
what the reason node is outputting. So
it is suggesting you know use this
particular tool you know put this
particular tool input and as soon as
this is done the control flow should go
to the should continue method and that
should direct it to the act node. So
let's actually look at if the should
continue is coming. Yeah. So we have the
should continue right here and that is
directing it again to the act node.
Right. So it is going to the act node
and yeah so you can imagine that the act
node is only going to take the outcome
the agent outcome right so it is only
going to take the agent outcome and then
it is going to actually call the tool
right and then it's going to take that
output and then it's going to append a
pupil to the intermediate steps and that
is exactly what it is doing right here.
So you can see that the first element of
the first pupil is going to have the
agent action and the tool input and all
of that and it is also going to execute
this tool. So let me actually go back to
our
um uh graph right here or rather let's
go to the nodes. So you can see that it
is going to invoke that particular tool.
It's going to take that output and then
it is going to in the second element of
the pupil it is going to put the output
as well. So that is exactly what is
happening right here. So the first one
is going to be the agent action and then
the second one is going to be the output
of the agent action tool execution. So
we have that entire you know um tavly
search call that is being made. So what
is the next step again? So right after
this is done the control flow goes back
to the uh reason node. So you can see
that we have the reason node again right
here and then the same step sort of like
continues. The same step continues. The
act node comes and then finally the
should continue is going to get the
agent finish. You can see that agent
finish and finally the output the
control flow is going to the end. And
that is it for building a react agent
using lang graph. I hope that you are
able to see what is possible with lang
graph and these patterns you can sort of
extend it to a lot of different use
cases as well. So if at all you you know
face any struggles at all go ahead pull
my code. If you have any problems even
despite that, please do go ahead and ask
me in the comments. I'll be more than
happy to help you out. I'd be more than
happy to connect with you on let's say
LinkedIn even. So yeah, that's pretty
much it guys. Uh in the next section, we
are going to be you know looking at you
know um another aspect of langraph and
we are going to be looking at
persistence. We are going to be looking
at rags. I'm not really sure which one
I'm going to be covering but I'm pretty
excited for what is ahead and I'll see
you in the next section.
Um hello guys. So I'm excited about this
section. In this section we will you
know um take on chat bots. We will look
at uh incrementally we'll sort of
increase the difficulty and we'll
introduce more concepts. Okay. So these
are what we'll be covering in the next
few sections. So the first thing we are
going to be you know building a very
basic chatbot. A basic chatbot meaning
it does not have any memories. It does
not you know it cannot make any internet
searches or anything of the sort. you
give it something, it spits out a
response, right? Uh we also have chatbot
with tools. We're adding tools like you
know various tools in order to get
accurate data. Uh and then we will
introduce memory. Okay. So persistence
and memory, we will cover those
concepts. And then we have the human in
the loop uh concepts. We will cover
that. Um that will build on top of
whatever we learn in the memory. And
then we have you know chat bots with
more complex state. we'll explore a
little bit more use cases so that you
understand you know uh all the edge
cases that are possible and finally we
have time travel. So without wasting any
more time let's go ahead and build out a
very basic
chatbot. Uh hello guys. So this is what
we are going to be building. So you can
see that uh this node just has a start
and then we have this node called
chatbot and then we have an end. Okay.
So basically you can think of this
chatbot node as just a model just an LLM
that sort of like you know uh takes in a
human message from the start I mean um
when we invoke it we pass the initial
state right so yeah it just takes in the
initial state and then it passes it to
the chatbot node processes it and then
sends it uh and then ends the graph
that's it very simple so this basic
chatbot does not have any memory it does
not have any tools either right so these
are the some of the new things that
we'll be learning. We'll learn about
another method. So far, we've been using
the app.invoke method, right? We'll also
learn about, you know, there's another
method called stream. We'll learn about
that. We'll learn about, you know,
looping in the chat. And finally, we can
also use uh a free lama model using the
Grock interface. Okay. So, if you do not
know, Llama is an LLM that is created by
Facebook and it is hosted in a service
called Grock. Okay. So basically from my
experience I've seen that you know the
response is much more faster compared to
open AIS. Uh and also it is free. So
we'll definitely explore that as well.
So without wasting any more time let's
jump back to our VS code. So right here
you can see that I've created a folder
called chatbot and I've just named it as
basic chatbot. Okay. So these are some
very standard imports that I've done.
The only thing that I've done
differently is I've used the gro chat
model which lets us access some of these
llama models. So if you want to know
where you know where I pulled this from,
I can just go to chat gro. Okay, that is
going to be a chat model inside of lang
chain. So I can just click on this
thing. I think uh let me go to this
particular class and you can see that
this is what we have to install. Okay,
so just say pip install langchain- groc
and then we just need to set the API
key, right? So this is going to be the
name of the API key. You can you can set
it by going to
grock.com. Let's go to dev console and
then uh I think yeah, right here in the
API keys, you can just go ahead and
create an API. Okay, so that should be
it. And I'm just going to load all of
the environment variables. Okay, so we
have all of this ready. Let us now go
ahead and build out this very simple
graph. So um uh so let's go ahead and
build out uh let's think about what we
need in order to run this right. So
whenever we talking about a chatbot um I
mean it's it's it needs some memory and
that memory is going to be a list of
messages right so it needs to uh you
know it needs to be a list of you know
AI message human message system message
or whatever right we are not going to be
doing anything different it's all
something that we've already done before
so I'm just going to go ahead and say
something like you know basic let's say
basic uh chat
state okay so I I can use the type dict
again And here it's going to be
messages. Okay. So this object is going
to have one property messages. And I can
use the same annotated and I can say
this is going to be a list of messages.
And then how are we how are we going to
you know append uh a new message to the
existing list. We can use uh you know
the concat operator that we've seen
before. But uh LAN graph also provides
this method called add messages. Okay.
So if I hover over it, you can see that
it merges two lists of messages updating
the existing messages by ID. So you can
see that this is also something that we
can use. Perfect. All right, this is all
that we need for the state. Now let's go
ahead and what's the next thing? Let's
go ahead and build out this particular
node right here. Okay, so I can call it
chatbot. Uh doesn't matter. Uh I can use
this basic chat state. This is what we I
mean we this is going to be the state
and this is going to be the type of it
right. So what do we need to do here? So
all that we have to do is uh you know we
are going to get a list of messages from
the start. In this case it's going to be
very simple. The list is just going to
have one human message. It just needs to
take that list and then invoke it in the
LLM. Right? So all that we have to do is
let's return the
uh this is going to be the um the the
structure of the state right. So here
all that we have to do is let's go into
the state. Okay, this is going to have
the messages inside of it. Uh okay, so
how do we do it? So llm dot
invoke and then we can pass the state
right here. Okay, so I hope that makes
sense. Uh and whatever is the response
is going to be an AI message, right? So
we are passing we're passing the entire
list of whatever conversation you know
history that it could have. We're
passing it all inside of this and this
is going to return an AI message and
that also needs to be an array. Okay. So
this entire thing is going to return the
AI message and that needs to be an array
so that two arrays can merge together.
So we have to do something like this.
Okay. So I really hope that makes sense.
So the chatbot node is done right. So
now let's go ahead and create our state
graph. Okay. So that we can actually add
all of these nodes. So we've already
created the chatbot node. We just need
to add this thing and then connect it
and set this as the entry point. All
right. So what we can do is we can say
uh graph equals state
graph. Let's pass in this particular uh
schema. All right. And uh let's go ahead
and add all the nodes. So in this case,
it's going just going to be the chatbot
node. All right. And then we can also
set the entry point to this
chatbot. And then finally, uh we can
also add an edge, right? We can add the
edge from chatbot to end. Okay. As
simple as that. So we've added this as
the chatbot node. We've connected this
and this together. and we've set the
chatbot node as the entry point. Okay,
so now all that is left is to compile
the graph. So let's go ahead and
compile. And next up, we want to write a
for loop, right? So this is something
that constantly requires the users, you
know, um we need to be able to basically
have a conversation in the terminal uh
like a very similar like a chatbot,
right? Like how we would talk to, you
know, claude interface, web interface or
chatg's interface. That's how we want to
talk to it, right? So in that case we
can just go for the while loop right. So
inside of this while loop we want to
first add an end condition. Let's say if
the user were to say exit or you know
something like that we need to come out
of the loop or else we just need it to
continue looping right. So in this case
I can just say user input. I'm first
going to collect whatever the user is
typing in there. I can use the inbuilt
method input method and uh the message I
want to basically show in the terminal
is just user something like this right
all right and now I can check if the
user input okay so is one of you know it
could be many things it could be you
know either exit or end or buy or
something like that okay end okay
something like this so if it is any of
these things I'm just going to break
okay that is it if it is not the case
Then we just need to invoke the graph
right. So I can just say app dot invoke
and we have to pass in the initial state
right here. And the initial state is
going to be the messages. And right here
uh you know initially we need to pass in
the human message right. So we need to
pass in the human message and that is
going to have the content but we are
going to get that content from the user
message. Okay. I hope this makes sense.
Now I'm just going to you know uh I'm
just going to say result of the entire
you know this thing and finally we are
just going to print the result. Okay so
it's very simple uh on every single
iteration we're we're going to restart
the entire graph. It is very similar to
just stopping the program and starting
it again. Right? So it's just going to
start chatbot end. Okay another loop
start chatbot end. Okay. So let's come
back and let's run it and let's see if
everything is working
fine. So you know I can uh say something
like hi I'm
Harish and great you can see that the
graph has completely executed once. So
it started from here I passed in the
human message. Hi, I'm Harish. And then
coming down here, you can see that in
this list, we have the AI message,
right? The AI message saying, "Hello,
Harish. Nice to meet you. Is there
anything I can help you with?" Right?
So, it only has one human message, one
AI message. So, we start off with one
human message. It comes here, it adds
the AI message to the list, and then it
ends it and then it shows the result.
Very simple, right? But if I were to you
know do something like you know if I
were to let's say ask okay what's my
name? Okay, what do you think would be
the answer? Right, uh because we're
restarting the graph, okay, this is
going to be the initial state that we're
providing, right? This is going to be
the initial state that we're providing.
The AI does not have context about what
my name is right now. Okay, so if I were
to press enter, okay, this is where I
put it. And here you can see that I
don't have any information about your
name. I'm a large language model. Okay,
so there is no actual memory maintained,
guys. I really hope that you understand
what's going on here. Every single time
the iteration of the while loop is
running, we're starting the graph a
fresh. There is no memory at all. There
is no persistence at all. Okay. And also
if I were to you know say something like
you know what's the
weather in Bangalore let's
say I mean it's not going it's not going
to be able to understand it's not going
to be able to give an answer. I'm not
able to access real-time information
about the weather in Bangalore. Right?
there is no tools available for the LLM
to use either right so uh in the next I
I hope you understood what are the
limitations of this particular you know
basic chatbot in the next section uh we
will introduce the ability for the
system to access the internet okay so
that is one thing that we'll do it's
very simple it's very basic but once we
have both of that then we'll have then
we'll introduce persistence we'll
introduce memory and we'll see how to do
that Okay, so I'll see you in the next
section. Um, hello guys. Welcome to this
next section. Uh, in this section, we'll
just equip our chatbot with the ability
to call tools. All right, so you can see
that I've already gone ahead and created
a file and then I've done some imports
right here. Um, it's it's not really
very different from what we did in the
previous section. I've just had I mean
I've just basically copy pasted
everything. But in addition to that, I'm
just going to use the Tavly search. um
this thing again and I'm going to create
a list of this thing. Okay, so I can
just say search tool and then I can put
this in here. Okay, just to be a little
bit more descriptive and as usual we're
going with this particular model again.
All right, perfect. So the next thing
that I'm going to do is I'm going to uh
you know initialize a variable called
llm with tools let's say and then I'm
going to use the bind tools method that
this provides. And here I can actually
go ahead and pass in all these different
tools. Okay. So if you do not if you're
not familiar with bind tools
basically when we make the API call to
the LLM. Okay. So normally we can just
say something like you know what's hi
how are you or something like that. But
if we are to you know ask a problem that
you know the LLM would not be able to
solve it by itself. Okay. So we can
actually make some tools available to
the LLM. Okay. So in that case what the
LLM is going to do is it's going to look
at all of the tools that are available.
It's going to see okay can I answer it
by myself or do I need to suggest to use
a particular tool. So let me give you a
very simple example how it works. Okay.
So it's going to be very similar to
doing just llm.invoke. It's going to be
very similar. In addition to that, we
are just making some tools available to
the LLM to decide if it wants to use a
tool or if it can respond by itself. So
I can just say you know I can just say
LLM with tools invoke. So I can say
something like hi my name is Harish and
let me go ahead and print this out. So
let me run this. You are going to see
some errors and all because that is from
lang. Don't worry about it. But if you
come up here this part you can see that
uh you know um it says okay nice to meet
you Harish is there anything I can help
you with right so this is a perfectly
valid response no different from you
know using just llm.invoke invoke but
because we've bind we've binded some of
these tools okay so what this is going
to do is let me clear this out and uh
let's tweak the prompt a little bit okay
so I can say something like what's the
weather in
Bangalore so in this case if I were to
run this let's see what happens now okay
so in this case you can see that the
content is empty right the content is an
empty string and if you come this entire
thing is a dictionary right so So if I
come down here in the tool calls you can
see that there is one object which is
suggesting to call a particular tool
that we have provided in the tavly I
mean that we have provided right here.
Okay so I know this is a little
complicated to look at. Let me just
actually you know put it in a neat
format. Okay guys so basically what I've
done is I've just copied this thing and
I've put it in a JSON file so that we
can actually look at it uh you know in a
proper way. So you can see that uh we
have the content which is going to be
empty. Okay, let me just uh you can see
that total of you know six properties
are there. The content is going to be
empty. We don't really have to consider
these things. These metadata and some
additional information. But here inside
of the tool calls you can see that there
is one object inside of which the LLM is
suggesting to use this particular tool
and is also providing the query. Right?
It is also providing the query. So that
is something that we're getting from
this bind tools. We're just making some
tools that are available and the LLM can
decide. Perfect. All right guys, so now
that we've understood this bind tools
method, now let us go ahead and build
out our graph. Okay. So let me show you
you know what we're going to be
building. So uh it's it's going to be
very similar but right after the
chatbot. Okay. So right here this is
where the llm.bind tools lives, right?
So what's going to happen is let's say
if I were to ask something like you know
what's the weather in Bangalore. So this
is going to output a response which we
just looked at right. So the if the the
content is going to be empty but the
tool calls property is going to be
having exactly what tool to call. So
basically we're making a decision right
there. So we're checking if there's a
tool call available or not. If there is
no tool call available, it means that we
can just you know end the graph. Okay.
So the LLM it means that the LLM has all
of the information it needs to answer
the question. So in that case we can
just end it right there. But in case
there is no content there is only tool
calls available. In that case that
method that condition method that we are
going to write is going to make a
decision. Okay. It decides whether to go
to the end or if it is going to go to
the tools. If the tool calls are
available, it's going to go here and
this tools method is going to execute
the tool. Okay, so it's going to execute
the tool. It's going to get the response
from let's say Tavary search. Okay, so
it's going to create a tool message with
all the content and it's going to append
it to the list and it's all going to be
provided to this particular LLM. Okay, I
really hope that makes sense. And then
once the LLM has all of the full
information including you know
information about the weather in
Bangalore or something like that it is
then going to you know output the
content. It's not going to make another
tool call because it now has all of the
information. So in that case it's just
going to give the content you know the
current weather is this is this and then
the flow is going to go to the end. So
now that I've given you a bird eyes view
of what we're going to be building let's
go ahead and code this out. All right.
So as for the actual node, it's not
going to be very different from what we
did previously. So I'm just going to
copy this particular chatbot put it over
here and uh okay, we can change this and
then instead of using this llm, we have
to use this llm with tools. That is the
only difference. All right, perfect. So
what is the next thing? So we've written
this. Now you know there is going to be
a conditional. there's going to be a
message that decides if the control flow
should go to the tools node or the end
node. Right? So let's actually call this
something like uh tools router. Okay,
let's call it tools router. Okay, this
is going to be a function. This is again
going to get the list of everything,
right? And uh okay, so what this tools
router is going to do is you know let me
just copy paste some code and let me
walk you through it. All right, so let
me walk you through it. So first off we
just getting the last message from the
list of messages. Okay. So why am I
getting the last message? Because if the
control flow is coming here it means
that the last message is going to be
because of this chatbot that is always
going to be an AI message. So I'm just
getting the last AI message and then I'm
just checking does the last AI message
have a property called tool calls and
I'm checking if the tool calls property
also has a list greater than zero. So if
I come back to the response you can see
that this is present and this list the
items the length of it is greater than
zero right. So I'm just checking that.
So if it is true then I it means that we
have to go to the tool node right we
have to go to the tool node. Okay so
that is what this method I hope that
makes sense. Now let us actually go
ahead and build out this particular tool
node. This is actually much more simple.
Lang graph provides a very very easy to
use method that we can use that takes
care of like 90 95% of the work. So uh
so we have tools okay so let's call it
tool node itself and then we are going
to use this class called tool node that
just takes in the tools that are
available okay so this tool node comes
from langraph.p pre-built. So let me
actually hover over it and let me show
you what it says. It is a node that runs
the tools called in the last AI message.
Okay. So if this is going to be the last
AI message this entire thing, it is
going to run all of the tools that are
available in the last AI message. Okay.
So u it can be used either in state
graph with messages state key okay or a
custom key as well. So we always have to
use this particular keyword messages.
Okay. So this tool node actively looks
for messages and looks at the last AI
message and it goes and executes all of
the tools that the LLM has specified. If
you want this tool node to look at
something else. Okay, let's say you know
I don't want this particular messages
key. Instead I want messages
uh you know let's say okay it's a
different name right. So if that is the
case then we have to provide we have to
tell the tool node to look at a
different list and not the default
messages list. Okay. So how can we
specify that? We can actually specify it
by saying messages key. Messages key.
Here we can actually specify this thing.
That's it. That's as simple as that.
Okay. It's going to look at this
particular pupil. It's going to look at
this particular sorry uh list and then
it's going to look at the last AI
message that is
present. But yeah, in this case, we can
just remove it. And let's actually Okay,
so yeah, the tool node is also done. So
what have we done so far? So we've built
out the chatbot node. We've built out
the tools node. We've also built out the
tools router method that is going to
decide if it needs to go to the two node
or the end. Okay. So now all that is
left is to just like create the graph
and add all the nodes and connect it all
together. Right? So here's what I'm
going to do. I am going to you know go
ahead and create the graph. So I'm going
to say state graph. Let us pass in the
basic
chatbot. And now add node. Let's go
ahead and add the chatbot first.
chatbot. The second node that I want to
add is going to be this particular tool
node, right? So I can just say tools
because in the diagram that's what I've
specified, right? Tools. And I can just
go ahead and put this right here. And
what's the next thing? So we've added
this and this. Now we have to, you know,
connect it. So we know the entry point
is going to be set entry point is going
to be the
chatbot and as soon as the entry point
you know as soon as this chatbot node is
executed we have to make a decision
whether to go to tools or end. So in
this case we have to set conditional
edges. So we have to put the
chatbot and then we have to put this
tools router here. Okay, I hope that
makes sense. And once the tools router,
okay, that I think that should be it,
right? So the tools router is going to
decide. Okay, so if we've named it tool
node right here, let's just go ahead and
name the same thing here as well. Okay,
perfect. Okay, so I right after the
tools router is executed, it is going to
go to the tool node or the end. Okay, so
let me check if we've written all of the
edges. So right after the chatbot, you
know, it comes to the tools, but we've
not really added this particular edge
yet. So let's go ahead and add a a
normal edge. So this is going to go from
tool
node to
chatbot. Okay. Okay. So now let's go
ahead and compile this graph. So I'm
just going to go ahead and just copy
this bit of code. Uh it's not going to
be any different. So we're compiling it
and then we are running the same thing
again. But the only difference is that
this time we have tools available. All
right. So let's go ahead and run it. So
here I'm just going to say something
very simple first uh just to test it
out. Hi I'm
Harish. It's going to be like hi Harish
how are you? Or something like that.
Okay so you can see that the AI message
says that hello Harish it's nice to meet
you. Great. Um, still the memory is not
really added yet. But now let's actually
go ahead and force it to use our
tabularly search tool. So I can say what
is the current
weather in Bangalore. Okay guys, even
though it looks like you know it is
continuing uh the graph is actually
getting executed from the start with a
different initial state. Okay. So it's
not going to remember anything even
though it looks like you know it is
continuing. Perfect. So what is the
current weather in Bangalore? So in this
case great so right here let's go up.
Okay. So we have the user message right
here. Okay. So what what should happen
first is the human message. Okay. Let's
come back to the state. Okay. Uh I mean
let's come back to the graph. So we have
the initial human message and then the
chatbot which is going to be the LLM
call. It is looking at the what is the
current weather in Bangalore and then
instead of actually providing the
content itself, it is suggesting to use
a tool. Right? So we've got a human
message, we've got an AI message with
just the tool call property filled up.
So right after that the control flow is
going to come back to the tools. this
node this tool this node is going to
execute based on the query right based
on the query the search query that the
LLM provided so this is going to output
what this is going to output the tool
message so we've got a human message AI
message tool message and then the tool
message this entire list is going to be
sent to the LLM again right and now it
is going to give the final answer so we
should see all of these things so Uh
what do we have right here? So we have
the human message. What is the current
weather in Bangalore? And uh coming down
we we should find the AI message. Right
here you can see that the content is
empty. Instead of the content we can
actually find the tool calls and inside
of the tool calls the LLM is suggesting
to use this tably search. So right after
this you should see what a tool message.
So if I search right here. Okay. So we
have the tool message right here and
right here you can see that this is this
entire thing is going to be the response
of the tavi search tool. So we have the
URL and the content URL content a lot of
these different things right and finally
the control flow is going to go back to
the chatbot to the LLM right so if
I okay so we should be able to find
another AI message somewhere here okay
great so the AI message so it is saying
the current weather in Bangalore is
partly cloudy with a temperature of
32.3Â° okay so now the LLM has full
context about the uh the weather. So it
is giving a final answer. Okay. So this
is uh in a nutshell you know basically
adding tool calls to a chatbot. Okay.
But still you can imagine that if I if I
were to say something like what's my
name right? Okay it's not going to uh
you know if I come down here you can see
that I don't have any information about
your name. Right. So it doesn't actually
maintain you know um uh the context
about what happened previously. So that
is something that we will tackle in the
next section. So I'll see you
there. Uh hello guys. So in the previous
section we spun up a very basic chatbot
and we also built chatbot with tools.
But we also noticed that you know the
previous conversations that we have the
chatbot does not remember because
persistence have not been introduced
yet. Memory has not been introduced yet.
So that is exactly what we are going to
solve using something called
checkpointers in LAN graph. So when you
build a basic chatbot using land graph
you run into an immediate limitation. By
default your chatbot has amnesia. This
we've already seen. Every time a user
starts a conversation the bot has no
recollection of previous interactions.
This happens because without memory
management each invocation of your graph
is completely independent. This is where
the concept of checkpoints in langraph
come into the picture. So let us look at
what exactly are
checkpointers. A checkpo pointer in
langraph is essentially a way to save
the state of your agent or workflow at
specific points during
execution. Think of it like saving your
progress in a video game. When you reach
a checkpoint, the current state of
everything is saved. If something goes
wrong later, you can always return to
this saved point. You don't have to
start over from the beginning as well.
Right? So in the context of lang graph
nodes and workflows, nodes are the
individual steps or components in your
workflow. This we already know.
Checkpoints basically save the complete
state after a node finishes its
work. If an error occurs in a later
node, you can resume from the last
checkpoint rather than starting the
entire workflow again.
This is particularly useful for complex
workflows where processing takes
significant time or resources. You want
to implement retry mechanisms. And in
this case, all that we're going to do is
introduce persistence across sessions or
server restarts. So to introduce
persistence and memory into your graph,
understanding checkpointers is only half
of the puzzle. You also need to
understand you know what is a thread ID.
Okay. So together both of it together we
can actually introduce persistence into
our graph. So we will implement it later
on in our code. But for now let's
understand why what is a thread ID? Why
is it even
necessary? So a thread ID is simply a
unique identifier for each specific
conversation or workflow execution. You
can think of it like a unique session ID
for a user. A conversation ID that
groups related messages together. So if
you've ever used the web version of
chatc or claude every single chat that
you create or every single conversation
this thing that you create has its own
thread ID right. So that is how we can
actually separate out these
conversations. So that is what a thread
ID is. So if if at all you know it's a
group chat or something like that that
group chat is going to have it is its
own thread ID. So I hope that makes
sense. Let's move on. A thread ID is
necessary because you might have
multiple conversations or workflows
running
simultaneously. Each needs its own
separate saved state. The thread ID
helps the system know which saved state
belongs to which
conversation. Without thread ids, all
your conversation would share the same
state which would cause confusion and
errors. So, so far we know that to
introduce memory and persistence into
our LAN graph application, we need two
things. We need to provide a
checkpointer and we also need to provide
it a thread ID so that it can link both
of this together. Right? So let's
actually go ahead jump back to our
visual studio code here. I've basically
just created a file called chat with
in-memory checkpointer. So all that I've
done here is basically just implement
the basic chatbot that we already saw
right here. Okay. So we just have a
state, we have the chatbot node and then
end node. So we have start chatbot end.
That is it. Right. So that that is all
that I've done here. I've added the
chatbot node. I've added the link
between the chatbot and the end. I've
set the entry point to chatbot. Compiled
it. And we have the final application
right here. And all that I'm doing is
I'm uh I'm invoking this application
using this initial state. Okay, I'm just
providing hi I'm Harish and then I'm
trying to use the same application again
and I'm passing in something else. Okay,
what's my name? Okay, it's very simple.
But usually what happens without memory
this graph gets invoked from the start
to an end and you know it doesn't really
remember what happened previously. So
that is the issue that happened. So let
me quickly run this for you and show you
that memory is indeed not working. So
yeah you can see that this is going to
be the first invocation using I mean
I've provided this thing right here,
right? Hi I'm Harish. Okay, I'm
providing this thing. So you can see
that hi I'm Harish and in this AI
message nice to meet you Harish great
but if I were to use the same app and
invoke again and you know use a
different you know what is my name if I
were to ask that it should ideally
remember it right and it is saying that
I don't have any information about your
name right so the the persistence is not
really happening right so let's actually
go ahead and uh introduce that so we
know that we just need to provide a
checkp pointer and a thread ID so to
introduce the checkpointter all that I'm
going to do is I'm going to import the
memory saver checkpointter. Okay, so I'm
just taking it from LAN graph checkpoint
memory memory saver. So this is an
in-memory checkpointter. This is not
really put in an external file or an
external database or anything. It's an
in-memory checkpointter. Okay, we'll see
further you know what are the
differences but okay all that I'm doing
here is I can just say memory and then
invoke this. Okay, so if I hover over
this you can see that it says it is an
in-memory checkpoint saver. This
checkpoint saver stores checkpoints in
memory using a default dict. Okay. So
this is really uh this we can use this
for you know debugging and testing
quickly but then in production
applications you would have to use you
know an SQL or Postgress uh
checkpointter. So we will look at more
about that later but for now all that
I'm going to do first is I'm going to
come down here and when we compile the
graph you can see that it actually asks
for this checkp pointer and we can
actually provide this memory in memory
check pointer right here okay so if you
remember this is the first thing that we
have to do and the second thing is we
also have to tie it with a particular
thread right a thread ID so what I'm
going to do is just going to come down
here and I'm going to say config okay
And this is going to be an object. And
inside of this object, okay, so this is
the exact structure that we uh this is
the exact structure that we have to
follow. So we have
configuration. Okay. And this
configuration property inside of this
dictionary is going to have another
object inside of which we have the
thread ID right here. And we can
actually, you know, put one or two or
whatever we want. Ideally, we want it to
be something that, you know, is very
unique. You can use the UYU ID package
or something like that. But for now, I'm
just going to save it as one. And what I
can do is I can come down here and
wherever the graph is being invoked, I
can just say config equals config. That
is it. Okay. So this is going to be the
first place where we're invoking the
same graph and we're also invoking it
here. So if you want to tie this and
this together, the first and the second
one together, if you want the
conversation to continue, all that we
have to do is provide the same config
here as well. Okay, great. So that is it
guys. Let me go ahead and run this file
so that I can show you you know the
persistence. Um, okay. So, okay, I
messed up one word right here. Okay,
this is not supposed to be
configuration. It's supposed to be
configurable. Configurable. Okay, sorry
about that. Let's run this
again. All right, great. So, we have the
first time we're invoking the graph
right here. And then we have the second
time we're invoking the graph right
here. Right. So, the first time, you
know, we are passing in the human
message and the AI message is getting
generated, right? So, if I come down
here, you know, the first time we are
invoking this human message, we are
passing in and then an AI message. So,
we already have two messages, right? So,
since we are passing the same config
here as well. So when I invoke when I
pass in this initial state this initial
state it actually needs to be the third
message and not the first message right.
So that is exactly what we will be able
to see in the final states of you know
both of these graph invocations. So
right here you can see that first is the
human message. Hi I'm Harish AI message
is saying that you know nice to meet you
Harish. Great. So the next time I invoke
the same graph right here you can see
that we see the first message right the
first message even though we are passing
in this what's my name as the initial
state we still see the previous messages
that is linked with this particular
config okay with this particular thread
ID so we have the hi I'm Harish nice to
meet you this is retained and then what
happens then I am again providing my
human message okay where is it okay
human message what is my name and the AI
is able to say your name is Harish okay
so I I really hope that you're able to
see you know how persistence works but
you still might have some questions
about how exactly is the storage being
saved in this memory saver in this
checkp pointer right so I've actually
prepared that as well so you can see
that this is how basically roughly how
the structure looks like so you can
basically think of it as a dictionary
with you know different thread ids Okay,
for for in our case it's going to be the
thread ID of one, right? So you can
equate that with this particular thread
1 2 3 and inside of which we have
multiple checkpoints. Okay, checkpoint
one, checkpoint two, checkpoint three,
checkpoint 4. So inside of which we have
the messages here, right? So initially
I'm just providing hi Mish. So the a
right after the first node's execution
okay whatever is the output of that
particular node it is going to append to
this particular messages list and then
after the AI response AI responds with
something we are going to add that AI
message uh to this thing and that is
going to be checkpoint two and then you
know there could be uh you know another
human message saying that okay what's
the weather in New York or something
like that that is going to be added and
that is going to be checkpoint 3 so
checkpoint 4 checkpoint 5 so every
single time a node is done executing
whatever is the response of it. It could
be a tool message, it could be a human
message, it could be an AI message, it
just gets appended with new checkpoints.
Okay, so that I I hope that that makes
sense. Okay, we you can see that we also
have the tool message and in addition to
it, we're also you know maintaining
basically that checkpointer is also
going to maintain what was the current
node, what was the next node so that if
there is something that goes wrong, we
know exactly where to you know resume
the execution and if at all you want to
look at the latest checkpoint uh you
know there's there are methods that land
graph makes it available for us to use.
Let me just you know comment this out.
So this is the graph that you're working
with right this this app is what we're
working with. we can just say get state
and we can pass in the config uh that we
are interested in. Okay, so basically
the thread ID that we are interested in.
Let's go ahead and print
this. Okay,
so now you should be able to see the
latest this thing. Okay, so this is
going to give you a snapshot the
snapshot of the latest checkpoint. Okay,
so here you can see that you know there
is a human message and there is an AI
message and there is another human
message and then there is an other AI
message. Okay, so that's it and then a
bunch of other things. Uh let actually
let me just put it in a neat format.
Okay, so yeah so this result is what it
looks like. Okay, so basically it's
going to you know let me just uh Okay,
so this is how it looks like. Okay, so
you can see that we have uh you know a
bunch of different messages right here.
Okay, so the first message is going to
be a user message. I'm seeing this thing
that's got an ID as well. And then the
AI is saying this thing um you know
basically behind the scenes, you know,
human message and AI message gets
converted to these roles and content.
Okay, so we have the role content. Okay,
we've got some other metadata as well.
And then this is the third message.
What's my name? And this is going to be
the fourth message. Okay, your name is
Harish, right? So, and then in addition
to that, we also maintain some other
things like the config, you know, the
thread ID. We also have the metadata,
right? Okay, we've got some additional
things that we don't really have to
confuse ourselves with right now. But
yeah, so we have all this data being
printed from this particular latest
checkpoint state. Okay. And I also want
to do another thing right now. This is a
little artificial, right? uh I've
basically just you know invoked the same
graph two different times so that I can
sort of demonstrate this uh you know
persistence in action. So let me
actually put it in a while loop so that
it's much more natural and we can
actually have a conversation in the
terminal. All right guys, so all that
I've done is I've just converted you
know those two different invocations
into a while loop so that we can
actually extend it and basically have a
conversation basically have a long
conversation. And finally, I'm just
going to, you know, uh, to show the AI
message, I'm just going to take the last
message, which is going to be an AI
message, and I'm printing out the
content. And that is it. So, let me run
this. So, you can see that, you know,
um, hey there, my name is Harish. I know
I'm using the same example again and
again, but, uh, yeah, my name is Harish.
I just wanted to remember my name,
right? So, uh, my name is Harish. And,
so nice to meet you, Harish. Great. Is
there something I can help you with? And
uh now I can say what's my name? Right?
So it should be able to say your name is
Harish because the entire history is now
maintained. Perfect. So you can actually
test it out yourself. So what I want you
to do is you know um basically try to
invoke this app this LN graph
application using different thread ids
and you would be able to see that you
know um if if it is a different thread
id the the conversation history would
not be available and it would start from
scratch and also guys there is one big
disadvantage when it comes to using an
in-memory checkpoint so what this is is
it's an in-memory checkpointer okay so
basically if we where to stop this
program right here. Okay. So if I were
to say exit all right and if I were to
run it again and if I ask the same
question again what's my name? It is not
going to have that information because
the memory gets wiped out. So this is
exactly why langraph provides uh you
know this SQL lightsaver. This is
something that we can use to sort of you
know save all of that memory externally
uh you know and not in memory. And you
know it also provides a posgress saver
as well. Okay. so that you know even
after we end the program we can we can
always like continue it and that is
exactly how things should work in
production right so that is what we are
going to be looking at in the next
section so I'll see you
there uh hello guys so in this section
let us now go ahead and replace our
in-memory checkpointer with the SQLite
checkpointer so that we can you know
introduce true persistence meaning we
can you know shut down the server or you
know we can close the application and
still we can resume the conversation
Right. So I've gone ahead and created
this new file right here called chat
with SQLite checkpointer. I've basically
just copy pasted everything that I had
in my previous file. But right now
instead of this memory saver, we are
going to use the SQLite. So I'm going to
go ahead and you know uh import another.
Okay, we we don't need this anymore. We
can go ahead and import the SQite. So I
want you to go ahead and install this
package so that we can actually go ahead
and import it. So let's do pip install
this particular
package. Let's give it a few
seconds. All right, great. So now you
can see that the the the warning has
gone. Now let's go ahead and import the
SQLite saver. All right, great. So
what's the first thing that we have to
do? So we have to first import the
SQLite 3 library that is uh that is
basically a part of Python itself. So
whenever you want to work with basically
SQLite you first create a connection
string right so that is exactly what
we're going to do let's say SQLite
connection and uh we can say SQLite
dot connect and then here we can
basically okay provide the name of the
database uh file that we want to create
okay so I'm going to call it something
very simple so I guess we can call it uh
checkpoint dot
SQLite. Okay, so this is a pretty
standard thing that we do in pretty much
any Python application if we want to
work with um SQLite. Okay, so now we
have to introduce the checkpointter. So
to do that, basically all that I'm going
to do is I'm going to replace the memory
saver with the SQLite saver and then I'm
just going to provide this connection
string inside of this and that is it.
Okay, everything else should work
perfectly fine. So let's go ahead and
run the file. So the minute it hits this
particular line, it's going to create a
database file and then it is going to
after every single node execution, this
checkpointer is going to take control.
It is going to append new checkpoints
and it's going to save the state, right?
So let's go ahead and run this
file. Okay. And great, you can now see
that there is a file that is being
generated right here. There is a
database file, right? So let's actually
go ahead and you know right now the
checkpointer is not really taken effect
right it hasn't really worked yet it
just created the database file so let's
go ahead and say something like you know
uh hi I'm Harish so this way it's going
to actually save the checkpoint so let's
see what
happens all right so it looks like we
are getting this error the SQLite
objects created in a thread can only be
used in the same thread all right so
guys this issue is a pretty common issue
So basically how LAN graph works is that
it's going to use multiple threads you
know different different threads
whatever thread is available in order to
you know work with the SQLite uh
database but SQLite database only
prefers to work with one thread
whichever thread that created the SQLite
database that is a thread that it allows
to make modifications as well. So what
we can do is we can actually explicitly
tell SQLite to not freak out if some
other thread were to go and make
modifications. So to do that I can say I
can say the check same thread and set it
to
false. Okay. So this should get rid of
the error. So let's actually go ahead
and run this again. Let's delete this
off. Okay. So now it should hopefully
work fine. Okay. So I'm Harish. Okay. I
messed up my name but yeah. How can I
assist you today? What's my name? Okay,
you can also see that there's multiple
different files that are generated.
They're all part of the same uh this
thing. So, what's my name? And you can
see that you know uh the persistence has
been truly you know introduced. So, I
can easily just exit out of it. Okay.
So, right now the application is closed
but still the database is persisting
right. So now I can again ask uh you
know I can basically you can run this
file again and I can say what's my name
and it should be able to pull it from
this particular DB file. Okay. So we can
also go ahead and take it up a notch. We
can actually go ahead and explore this.
So I'll also show you how we can
actually take a look at. So right now
you can see that it is not really
accessible. So one way to do that is to
go go ahead and install uh the SQite. U
there is an extension here. You can go
ahead and install it, but I had some
issues with that extension. So, what I
prefer working with is this DB browser
for SQLite. You can go ahead and
download it. This is what I clicked on
to download it. And if I open it, this
is how it looks like. Now, I can just go
ahead and, you know, uh, open the
database. I can just drag and drop this
particular file. This file right there.
Let me show you how it looks like. Okay,
great. So, I've actually opened that
particular DB file in my uh, you know,
DB browser for SQLite. And right now you
can see that there are two tables that
our LN graph application our SQLite
checkpointer has created. So inside of
this checkpoint you can actually click
on this and go to browse data and you
can see that we can also see all of the
different uh you know um messages that
have been added. So you can see that
this is the for the checkpoint ID
checkpoint ID 2 whatever. And then if I
click on this thing okay so this is
going to be the metadata right. So if I
scroll down, scroll to the very right,
you can see that the content is going to
be I'm Harish. I says I'm saying I'm
Harish. And right here there is a loop
happening. We don't have to worry about
that. And then the AI is saying hello
Harish. How can I assist you today? And
then the same input happens. And then
you know another loop happens. We don't
have to worry about that. And finally
okay um I ask okay what's my name? and
then it's saying so basically you can
see that all the messages are being
stored in this table called the
checkpoints table right so I can also do
something where I can just go to the you
know I can basically execute uh an SQL
command and then sort of remove delete
all of the rows from this table so the
way that I can do that is I can just say
delete from uh I think checkpoints right
checkpoints and if I run this file okay
so you can see that nine rows have been
affected so most most of it has been
deleted. And if I come back to my VS
code, so if I now ask the same question
again, okay, what's my name? It is not
going to be able to answer it because we
have removed all of the rows. So let's
check it. Great. You can see that you
know I'm not able to recall information.
Great guys. So I hope that you now
understand how you know the SQL
lightsaber works, how in-memory
checkpoint works. So now that we
understand how checkpoints in memory
works, let us now use all of these
concepts and introduce a human in the
loop scenario. Okay. So I'll see you in
the next
section. Hello guys. So so far we've
learned a lot about checkpoints and
memory. Let us now use some of those
concepts and let's start exploring human
in the loop. So let's look at some
slides. So a human in the loop workflow
integrates human input into automated
processes allowing for decisions
validation or corrections at key stages.
This can be especially useful in LLM
based applications where the underlying
model may generate occasional
inaccuracies. So some of the use cases
include reviewing tool calls. So humans
can review, edit or approve tool calls
requested by the LLM before tool
execution.
We can also use it for validating the
LLM outputs. So humans can review, edit
or approve content generated by the LLM
or you know we can use it to provide
additional context as well like for
example enable the LLM to explicitly
require human input for clarification or
additional details or to support
multi-turn conversations. So next let's
look at some of the design patterns that
usually are used with human in the loop.
So there are three different actions
that you can do with a human in the loop
workflow. So the first one is approve or
reject. So we can pause the graph before
a critical step such as an API call to
review and approve the action. If the
action is rejected, we can prevent the
graph from executing that step and
potentially take an alternative action.
This pattern often involves routing the
graph based on the human's input. So
here's a very simple diagram. So here we
have you know a node here another node
here and then some other nodes here. So
right here we can actually decide the
human can decide if you know it need if
the flow needs to go to this particular
node or this node. So depending on the
human's approval or rejection the graph
can proceed with the action or take an
alternative path. Let's look at another
one. So review and edit state. Okay. So
in this case a human can review and edit
the state of the graph. This can be
useful for correcting mistakes or
updating the state with additional
information. And you know another use
case could be reviewing tool calls which
we've already seen. A human can review
and edit the output of the LLM before
processing. This is particularly
critical in applications where the tool
calls requested by the LLM may be
sensitive or require human oversight. So
you you can actually imagine that there
could be some tools that could be
expensive and in that case you we can
have the graph ask the permission from
the human and if the human is okay with
it then we can go ahead with you know
going to the particular tool node or
else we can go to another node in the
graph. So if you remember the first
design pattern that we saw was approve
or reject. Okay. So that would probably
be the easiest for us to as a beginner
for us to learn. So basically you you
know uh the graph flow execution just
comes in a certain direction and
depending on whether the human is okay
depending on whether the human says yes
or no we either direct the flow in the
left side or we direct the flow in the
right side. So I've just prepared a very
simple you know intuitive example. So
you can see that we have basically the
graph that we are looking at right now
is like a a very simple agent that you
know creates LinkedIn content and if the
human is like happy with it, it goes off
and you know talks to the LinkedIn API
and then just like you know creates a
draft or if the human is not happy with
the draft that is generated the human
can provide feedback on it and then the
agent is going to you know uh you know
take that feedback and then iterate on
it and make it better and the loop is
going to continue. So you can see that
we have the start node and here is where
we are actually going to provide the you
know this is the topic that I want you
to create a LinkedIn post on right. So
uh this node when it hits this node the
llm call is being made a post is
generated and as soon as this post is
generated we are then going to interrupt
this the flow of the graph it's going to
stop the flow of the graph right here
and then we are going to you know
basically provide uh show the user the
post the draft that is created and ask
the user is it good or not okay so if
the user says yes in that case we're
just going to go ahead and post uh you
know post it to LinkedIn or rather uh
you know create a draft and communicate
with the LinkedIn API, right? But if the
user were to say no, then this node
comes and then we are going to collect
feedback from the user. Okay? So we can
we can have the user say okay make this
shorter, make this longer, make it more
funnier or something like that and then
we are going to feed it back to the
generate post and it is going to iterate
on it. Right? So um initially we're just
going to do it very quick and dirty. We
are not going to dive deep into
langraph's methods or anything of that
sort just so we can actually understand
how simple it actually is. Right? So you
can see that I have actually gone ahead
and created a folder and a file and I'm
calling it using input because in this
particular example we are going to be
using the input the python input method
and that is going to be you know
something that all of us would have
already been used to working with right.
So the first thing is obviously just a
state with a very simple you know
messages uh property. We've provided the
add messages reducer function here as
well. And then we are first defining the
generate post node. Right. So this is
something that we would have already
seen before. It just takes in the list
of messages and invokes on it. Right.
Right. So let's come back. So the
generate post is done. So as soon as the
generate post is done, what we want to
do? we want to actually you know get the
feedback from the user. So you can if I
were to come down here you can see that
so right after the generate post is done
I'm going to go to this particular node
called get review decision. Okay let's
go back to the get review decision. So
all that I'm doing here is that I'm
going to take the last message which is
going to be the AI message which is
going to contain the LinkedIn post right
and then what I'm basically going to do
is I'm going to print it in the terminal
right okay this is going to be the
current LinkedIn post and then I'm
showing the post content as well and
then I'm basically asking the user do
you want me to post to LinkedIn yes or
no very simple right so depending on
whatever the answer is we are then
directing the flow of the graph to
either either the post method u the post
node or the collect feedback node. So if
you remember right here we are either
going to the post node or the collect
feedback node. Right? So if I were to go
to the post node I'm just basically
saying that you know I'm just uh showing
that this is going to be the final
LinkedIn post and then I'm just going to
print it. Okay? We are not actually
going to make an API call or anything of
that sort. So we're just saying the post
has been approved and is now live on
LinkedIn. Okay. So we're just keeping it
very simple because we're just learning
the concept of human in the loop. Right?
So if this yes is not going if if the
user were to say no in that case we are
going to come to this collect feedback
and right here you can see that we have
another input right here which is going
to ask the user how can I improve this
post right. So the human could say
something like you know make this
shorter or make this more funnier or
something like that and then we are
going to append this to the existing
list and as soon as this is done we are
going to loop it back to the generate
post again. So if you see that I would
have added an edge from collect feedback
to the generate post right. So that is
going to run the same loop again and
then eventually if uh you know the user
is finally happy with it they can just
go ahead and post it to LinkedIn. Right?
And finally you can see that we are
providing the initial state which is
going to have one human message which
just says write me a LinkedIn post on AI
agents taking over content creation. So
if I run this okay so you can see that
the post has been generated and printed.
So you can see that okay this particular
you know uh node it comes inside of this
particular node and it prints the
current LinkedIn post and then we see
the uh the complete post right it hits
this particular line and it is going to
ask post to LinkedIn yes or no right so
if I were to say no uh it's going to ask
okay how can I improve this post and I
can say something like you know make it
you know four lines max okay so you can
see that now it's like you know four or
five lines right so now if I were to
Okay, let's say I'm happy with this
particular post. I do now want the agent
to go ahead and post it as well. So now
I can say yes and that is going to post
the the final thing, right? So final
LinkedIn post. This has been approved
and is now live on LinkedIn. Okay. And
finally we are also you know logging the
printing the the the final state after
the end of the graph. Right? So we have
a human message an AI message and then I
would have provided the feedback
somewhere. So I would have provided make
it to four lines max and then we have
the final AI message somewhere here.
Right? So we have AI message. So in this
example you can see that we are using
this input Python method but that is not
the right way of doing things. Okay. So
I've included this particular example
because you know the way that we are
going to be doing it with LAN graph's
interrupt method. We are going to be
looking at it later but it's going to be
very similar to what we're doing here.
Okay. So I just wanted to give you sort
of like a bridge between you know what
we're used to and what we're going to
learn. Okay. So let's look at some of
the drawbacks of using input because
this is not how we're supposed to do it
and then we'll sort of segue into
learning about another method a class
that langraph provides called interrupt.
Okay. So let's come back to the slides.
So the drawbacks of input uh involve you
know it freezes your program completely
until someone types something in. You
know it only works in the terminals,
right? It is useless for web
applications. If your program crashes,
all progress is lost. It can only handle
one user at a time. It only lives in
your terminal
session. So this is why we use a special
method that langraph provides called
interrupt. So what is interrupt and why
do we use it? It is a special langraph
function that pauses your your workflow
nicely. Saves your program state so it
can continue later. It works in web
apps, APIs and other interfaces. It
handles multiple users and sessions at
once. It survives program crashes and
restarts. Lets humans take their time to
respond. It is required for any serious
human in the loop system. So here are
two ways wherein you can use interrupts.
So in the first case you can see that
during the compile step we can provide
the interrupt before and we can actually
specify before which node do we want to
interrupt this application. So here we
are saying that if there is going to be
a tools node in our graph it means that
right before the tools node is going to
be executed we can actually interrupt
the graph. Okay, so that is what it
means and langraph also provides this
interrupt function with the command
class that we can use together. So you
can see that this sort of using it is
going to be very similar to what we did
with the Python input method. So instead
of the input method, we can just replace
that with this interrupt method and that
is it. So whenever the execution hits
this particular line, the program is
going to get interrupted or basically
just paused or exited from the graph.
And now uh you know after we get the
response from the human we can just
resume it using this command class.
Okay. So how can we resume it? Where is
the memory stored? It is me the memory
is stored with the checkpoint. Okay. So
the checkpointer is going to have
information about exactly where the
interrupt happened and depending on what
we pass in here it is going to resume
it. It is going to continue from it.
Okay. So we'll look at a lot more
examples about you know how we can
actually use this interrupt method. So
in the next section let us use this
interrupt method and then build out some
human in the loop workflows. So I'll see
you
there. Hello guys. So in this section
let us understand what the command class
is, what is the interrupt method using
very simple examples and once we
familiarize ourselves with it let us
then go ahead and convert the LinkedIn
agent that we built using the right
methods using the interrupt method and
the command class. All right. So first
in this section let us look at what is
the command class what is the utility of
it. So what this command class allows us
to do is to create edgeless workflows.
And this is how it works. Okay. So let's
say this is a node. This is a very
simple node. So instead of you know just
updating the state as we've been doing
so far, what we can do is we can return
this command class and this takes two
keyword parameters. So in this go to
keyword parameter, we can just specify
go to this particular node right after
this node is executed. Okay. So you can
see that the next node to transition to
that is as simple as that. But what if
you know a node needs to update the
state as well. So so far every node that
we've been writing it also updates a
state right. So we can also specify the
same thing here as well. So right here
we have the update keyword parameter.
Inside of this we pass in the same
object. This is very very this is no
different to what we've been doing so
far which is uh just returning the state
object. Right? So if we return a
response right here, it gets appended to
the messages list. Very simple. So I
hope that makes sense. So to better
understand this, I've prepared an
example. So right here you can see that
this is the this is a very simple node
uh graph I mean. So what we normally do
is we just create these nodes uh you
know and then we add these nodes to the
graph and then explicitly we will
mention add edge add edge right but this
time we are going to do the same thing
using the command uh class without using
any of these you know explicit add
methods that we've been using so far. So
let me go to VS code. So here you can
see that I've already gone ahead and
created a file. So all that I'm doing
here is that you know we have a node
which is just going to print node A.
Okay. So we are not going to be you know
making any complicated LLM calls. We're
just trying to understand what the
command class does. Right? So we have a
node A. We're printing node A. And then
right after node A what do we want to
do? We want to go to node B. Right? So
that is exactly what we're doing. We're
returning the command class. We're
saying go to node B next. At the same
time, I'm also, you know, just to better
drive the point home, I'm also going to
update the state as well, just to show
you that that is also possible. So,
right here in the state uh schema, you
can see that we have something called
text right here. Right? So, I'm just
basically just going to take the text
and then I'm appending this letter A to
it. All right. So, right after the node
A is going to be executed, we are going
to go to node B. Node B does the same
thing. It's printing it and then it is
saying go to node C next. And this time
before we go to node C, we are also
going to append this letter to the
state. Okay. So if the initial value
were to be an empty string, so right
after this, it's going to be A. And when
it comes to B, the state is going to
have just A letter alone. And by the
time this is done executing, we'll we
will have appended the B letter as well.
And the same thing for C as well. And
then we're just going to add the node.
And uh the set entry point also needs to
be done. And that is simply it. Okay. So
I'm just going to leave it as an empty
string right now. All right, great. So
you can see that node A is done
executing, node B is done executing,
node C is done executing as well. And
then the final response has A, B, C. I
hope that makes sense. So I hope you
understand, you know, the utility of the
command class and the go-to keyword
parameter that it takes in it. It's be
personally it's a lot more readable for
me. uh it gives me a lot more
flexibility on you know the logic that I
put inside of my nodes. So given that
this is a very simple graph this would
be a very nice way to learn the
interrupt method as well. In the next
section we are going to be adding the
interrupt method we are going to
interrupt the graph at a certain point
and then using the human review we are
going to direct the graph towards node C
or node D. Given that this is a very
simple graph it would be a perfect
candidate to learn about the interrupt
method as well. So I'll see you in the
next
section. Hello guys. So in this section
we are going to learn about the
interrupt method. So this is the graph
that I'm going to be building. So here
we have start node A and node B. Inside
of node B, you can imagine that we are
going to have a command class that can
either go here or here. Okay. So we have
two different options right here. So in
order to make the decision whether to go
to node C or node D, I'm going to seek
the help of a human. Okay. So I'm going
to add a break point in the node B. I'm
going to pause the execution of the
graph. And then I'm going to make I'm
going to ask the question to the human,
do you want to go to node C or node D?
Depending on the human's, you know,
whether they say C or D, it's going to
then go to C or D and then end it. Okay,
so I hope you're excited. Let's go to
the VS code. So I have created another
file as well for resume. So the reason
why I've named it resume is because we
are just going to put a break point.
We're just going to pause the graph
execution and then we are going to
resume from it. We're not going to do
anything else complicated. We are not
going to you know update the state or we
not going to branch off or anything like
that. So before we jump into the code,
let's look at some of the operations
that are possible with interrupts. So
the first one is obviously resume,
right? So we can continue the execution
with input from the user without
modifying the state. Okay. So that is
exactly what we're doing here right. So
we are going to you know pause the
execution right here and we are going to
get an input from the user and then
depending on that we are resumeuming the
execution. That is the first one. We can
also do update and resume as well. So we
can also do update the state and
continue the execution. No different. So
you can imagine that the command class
that we have it can also you know go to
the next node we can also you know
direct the flow of the graph to a
certain node but at the same time we can
also update the state as well right so
that is also something that we can do
and the third thing is rewind or time
travel basically so go back to a
previous checkpoint in the execution so
what I mean by that is so in this graph
let's take a very simple example so
let's say you know there's a node A node
B the user decid writes they want to go
to node C and then end right so the
graph is actually ending completely
right but we can also do time travel so
uh after the graph is done executing if
we want to actually go back and then
choose a different option for whatever
reason maybe we might have made a
mistake right it it could be anything so
we can actually go back to node B and
then you know simulate the same
interrupt again and then this time you
know go to node C I mean node D and then
end So how can we do that? We can do
that because of checkpo pointers, right?
So after every single node execution
there is a checkpoint getting created,
right? So we can actually just take the
checkpoint and then continue from that.
So that is also another uh you know
possibility. Along with that we can also
have branch. We can create a new branch
from the current execution state to
explore alternative paths. It's kind of
very similar to what we just saw. And
then we can also abort. We can cancel
the current execution entirely as well.
So each of these operations give you
different ways to control the flow of
your graph when it is interrupted. Okay.
So I hope that makes sense. Now let's
actually go ahead and see interrupts in
action using this very simple example
that I just showed you. Okay. So this
we're going to be using this particular
example and uh let's actually go through
it. So um it's no different. So the
previous code that we saw I've just
basically copy pasted everything as is.
All that I've done is added this memory.
Okay. So why do we need memory right
here? So if we are going to you know
interrupt a particular point in the
graph, basically what it's what is
actually happening is it's going to exit
out of the graph. Okay. But we need some
sort of information about where it got
ex exited, right? Where the break point
happened. So the memory is going to keep
track of where the interrupt happened so
that the next time we can resume from
that particular interrupted point. Okay.
So that is why we want this memory saver
right here. All right. So the rest of it
is pretty very very much the same. We
have node A and then you can see that
inside of node B I have this interrupt
method. So you can see that I've
imported it from langraph types. So all
that I'm doing here is I'm just asking
do you want to go to C or D? uh type C
or D right so we have the human response
u and then depending on whether the user
says C or D we are going to go to node C
or we are going to go to node D okay at
the same time I'm also going to update
the state just so you know it is we are
learning and we are seeing possibilities
so we can also do it if we wanted to so
because we are a node B we are appending
B in each of these places and the
initial value is going to be just an
empty string all right okay trade. So
the same thing continues. We've got node
C, we've got node D, and then we are
adding all of these nodes together. And
then we are providing the memory as
well, a config
object. Okay. And then finally, okay, we
are also providing the initial state.
Okay, so the initial state is going to
be the value with an empty string. Okay,
that is it. So if I were to invoke,
okay, so I'm not running this just yet.
So if I were to invoke this graph, what
what do you think is going to happen? So
it's going to you know print out node A
and then it's going to print out. Okay.
So the execution is going to come from
come to here. It's going to print out
node A and then it's going to go to node
B. It's going to print out node B. Okay.
So this much we know. So after this node
B is printed out, it is going to
encounter this input interrupt method.
Right? Right here. It's going to look at
this particular line and it's going to
exit the state. Okay. So this interrupt
okay this is the area where the
interrupt happened that information is
put in the checkp pointer right that is
that information is going to be added
and it's going to exit from the graph
okay that is it so if I go ahead and
execute this file so you can see that we
have node A and node B printed coming
from here okay so as the graph went
through these two different nodes we are
printing both of this and we also see
this information right here okay so
since we set the stream mode to updates
we are going to see the output of every
single node uh you the state return
right so node A successfully returned
the value of A this is correct and then
the node B did not get a chance to
completely execute because the interrupt
happened right so that is exactly why we
see this interrupt right here so in here
you can see that the value says do you
want to go to C or D type C or D okay so
in ideally in production applications we
would surface this information to the
user in the UI in the front end so that
you can get that response and then
resume the graph from there. But right
right now we're just going to do it very
quick and dirty. But in the next section
in the next few sections we are going to
be building uh you know we are going to
be seeing how to build production grade
graph applications. But this is great
for learning. All right great. So right
here we are getting the uh the the
latest checkpoint. You can imagine that
the checkpoint is going to have
information about you know what is the
exact state at that particular point.
But in addition to that it is also going
to have information about you know which
node the next node it should go to right
all those flow related uh things as well
it is going to contain right so so I'm
going to use this next property and that
is going to give you information about
the next node it needs to go to so if I
come back to the diagram so the
interrupt method we put it right here
right so because we put it right here
it's going to stop the execution right
here so the next time we resume it it's
going to start off from node B. So that
is exactly why we see the node B right
here. All right. So what are we going to
do right now? We now need to you know
give get the user to give a particular
value. It could be C or D. Right? So
depending on that it's going to go here
or here. So how can we do that? We can
now use the command class. Okay. So we
can say app invoke command and we have
to use this exact keyword parameter.
Okay. So we this here we can pass in
pretty much any information that we
want. We can pass in an object. We can
pass in a lot of different properties
but right now I'm just going to keep it
very simple. I'm just going to pass in
the uh a single letter alone. Okay. So
whatever I pass inside of here the graph
is going to resume wherever it left off
wherever it uh you know it inter it got
interrupted and this value is going to
go and sit
inside of here. Okay. So whatever is
going to be the response of this we are
sending in this command resume. So if I
come down here and if I run this file
what do you think should happen? So we
are getting D right here. Right? So it
should come print D and then go to end.
Right? So if I run this you should now
be able to see that we are printing node
D and we also see the node B and node D
got executed right here. Node B and node
D. And as you know B would append a B
letter and D would append D right here.
So that's why we have B right here and
then we have D. And now I want to
explore one more use case. What if we
made a mistake coming to the D node? So
right now because I passed in the resume
value to be D. It came to D and then it
ended. Right? The graph ended
completely. It ran its entire execution.
Right? It went through the entire thing.
So what if I wanted to sort of come back
to this node B and then if I wanted to
provide C now in this case it's not
going to be possible right because the
graph is already ended we are using the
same thread ID as well so there is
nothing to actually resume there are no
interrupts right there is nothing to
resume actually so if I just go here and
if I change it to C and if I try to run
it you we are not going to get anything
okay why are we not going to get
anything because we we don't have
anything to execute anymore. The graph
has already ended. So one way to get
around this is to change the config
thread ID. So that in that case you know
you can start over from uh from the
start. But in case you want to use the
same thread ID, what you might have to
do is we might have to do something
called time travel. It's very simple.
It's a pretty straightforward concept.
So I'll explain that in the next
section. So I'll see you
there. Hello guys. So in this section
let us look at the other way of using
interrupts. So if you remember uh we
we've already done this way of doing
things right. So where we use the
interrupt method and then we get a
response and then we based on it we make
a decision inside of the node right
there is also another way of using
interrupts which you can still see in a
lot of code bases which is to use it
this way in the compile step using the
interrupt before you know keyword
parameter. So we are also going to
explore this and in addition to this
let's look at another use case of human
in the loop. Okay. So if I were to go to
the human in the loop documentation land
graph. So you can see that we've got a
couple of different design patterns
right. So we've already seen the approve
or reject right depending on what the
human is saying we can either go to this
node or this node which we've already
seen. Let's also you know explore
another uh use case which is to review
tool calls. Okay. So where could this be
useful? So you can imagine that there
could be a scenario where this
particular tool execution could be
expensive or it could be something that
is sensitive. It could be an API or it
could be like a tably search uh tool or
it could be some expensive tool
something like that that could
potentially require a human approval
right that that could be a valid use
case right and this is something where
we can use human in the loop as well. So
we have an LLM and the LLM is suggesting
that we use a tool and based on that the
human can actually approve before it
goes to the tool executor step. Okay. So
that is exactly what we are going to be
looking at in this section. So once we
do this we can also take a look at the
multi-turn conversation. So let us now
explore the review tool calls use case.
So you can see that I've already gone
ahead and created a file called
approval. So what I have right here is
going to be absolutely no different to
what we saw in the chatbot with tools
section. But if you don't remember it,
don't worry. This is what we did in that
section. So we have the start, model,
tools and end, right? So if the human
were to say something like you know what
is the capital of India or something
like this, the LLM is already going to
know it. It's already going to know the
answer. So in that case, you know,
there's not the the tool condition is
not going to go to the tools. It's just
going to go to the end. But in case you
know it requires some real-time
information like what is the current
weather in Chennai or something like
that in that case this LLM with bind LLM
that is bound with the tools. So this
LLM is going to not give the answer but
instead it is going to give information
about which tool to call if you remember
right. So it's going to go to tools the
the tab search tool or some other tool
is going to execute it's going to send
the response the tool message back to
the LLM and the LLM can make an informed
decision. Right? So and then finally
it's going to come to the end and that
is exactly what we are going to be
building here. The only difference is
that we are going to interrupt it before
this tools node ever the the execution
before it ever hits the tools node. We
are going to interrupt it. we are going
to get a human approval before making
the tavly search call. All right. So let
me walk through the code step by step.
So we have the tavi we are putting it
inside of the tools list and we finally
have the llm with tools and as usual we
just need one messages uh you know
property in the schema. We have the
model which is just going to invoke on
all the list of messages. Very simple
right? So what should happen right after
the model executes? Okay, so right after
the model is done executing, right? We
have it right here, right? So right
after the model is done executing, we
need to go to a router, a tools router.
The tools router is just going to take
the latest AI message. It's going to
look at the tool calls and it's going to
see if there is any tool calls the LLM
wants to wants us to do, right? So if
there is some tool call like you know
tavly search tool call if the LLM wants
us to call the tavly search we are then
going to direct the flow to the tools
node and the tools node we can use this
pre-built you know tool node you know
class and we just need to provide in all
these tools right there. All right so
that is going to execute that particular
tool and it's going to create
automatically create a tool message
append it to the messages list. All
right. So after that is done, we are
again going back to the model. The model
now has information that you know it
would not have had otherwise. Tavi is
returning that new information and based
on that it is going to make an informed
decision and then come to an end. So
this so so far this is something that
we're already super familiar with.
Nothing new here. The only new thing
that we are going to learn here is look
at this compile step. Okay. So as usual
we are going to provide the memory
because if you remember anytime we use
an interrupt we need to provide a memory
right checkpointer basically not a
memory checkp pointer and next this is
new this is where we are providing this
interrupt before keyword parameter and
we are providing which node before which
we need to exit out of the graph right
so before we even get a chance to come
here we are going to exit out of it okay
so that we can actually review the tool
call that the LLM is suggesting okay
based on that we can approve it in
addition into this. We also have an
interrupt after. So you can see that we
also have an interrupt after right here.
Right? So this interrupt after also is
going to be very similar to interrupt
before. But interrupt after is something
that exits the graph after this tool
node is executed. So that could be
another use case wherein we can actually
make it mandatory that the user should
be able to review the output of a tool
call, right? Just so just to make sure
that everything is looking good, right?
But in this case, we are just going to
do the interrupt before. And if I scroll
down here, we have the config object,
right? We are specifying the thread ID.
And then all that we are going to do is
invoke the graph. Okay? So in this time,
we are not going to use the invoke
method. We can use the stream method.
This time the only difference between
app.invoke and app.tream is that
app.invoke returns a value after you
know the graph has exited. It could be
because of an interrupt or it could be
because the end node has reached but
stream is something that it basically
just returns uh it's a generator
function that emits events after every
single node right so that is exactly
what I'm doing here uh this events is
something that we can actually iterate
through loop over and then every single
time a node is done execution we are
going to get an event right here and
that event is going to contain all the
different messages at that particular
point we're just going to take the
latest one and 3D printed. Okay, it's
very simple. Okay, we can also set the
stream mode to values because we are
only interested in the values and not
the entire object. All right, great. So,
if I were to now let's run this. Okay,
let me just clear all of the outputs
because I ran it before. But let's go
ahead and run this file, this code
block. Run
this. Okay, so everything is looking
perfect. Let's run this as well. All
right, great. So this is as you know
that this is the question I'm asking. So
as expected the LLM is saying that okay
you have to go ahead and use this tab
search results right that is what we've
provided. So it's also providing the
query here as well. Okay Chennai current
weather is what it wants to search for.
All right great. Now coming down let's
actually go back to the diagram. So the
model has successfully you know provided
the AI message and now we are
interrupting out of it. Okay, we are
coming out of we are exiting the graph,
right? So if I were to now, you know,
look at the next node that this latest
snapshot is going to go to, what should
we see? We are supposed to see the
tools, right? So if I run this, you can
see that we have the tools which is next
in line. So how can we resume it? It's
going to be very simple. All that we're
going to do is, you know, we can say
app.invoke none, appstream none, no
difference. And then we can actually go
ahead and run this again. So this time
it is going to you know go ahead and
execute the tavly search tool right. So
you can see that this is going to be the
result of the this is going to be the
tool message the result of the actual
tool execution. And once that is done we
are going to feed it back to the LLM.
Right? So we have the AI message and you
it's doing an informed answer. So right
here we've we successfully have
interrupted the tool call. Right? Okay.
Okay, we can apply it to a lot of
different use cases where you know it
could be something sensitive that we
have to sort of you know make sure that
a human is approving or you know it
could be the result of a tool call that
the user has to the human has to confirm
they have to quality check or something
like that using the interrupt after. So
I hope you now understood how simple it
is to basically use the interrupt in the
compile step as well. So we can also
provide in as many nodes as we want and
uh it's going to be no different. Okay,
perfect. So in the next section we are
going to you know let's scroll down. We
are going to look at this multi-turn
conversation as well. So I'll see you in
the next
section. Uh hello guys. So in this
section let us look at an example where
we make use of interrupts in a
multi-turn conversation and we are going
to be using the same LinkedIn
postcreation agent example. So as you
can see that I've already created a file
and I've populated the code as well. So
let me walk through the code step by
step. So looking at the state first we
have the LinkedIn topic. Okay. So this
is something that we can populate at the
very start of the graph execution. we
have the generated post which will get
populated every time uh you know uh the
the model you know creates a post and
then we can append it uh to this
particular list and the same thing for
the human feedback as well. Okay. So the
first node that we are going to be you
know entering into is going to be the
model obviously. So you can see that
here we are using the LLM to generate a
LinkedIn post with human feedback
incorporated. Right? So coming down I'm
just going to extract the LinkedIn
topic. I'm going to extract this human
feedback and then I'm going to first
create a prompt. Right? So this is going
to be the the main prompt. Right? So I'm
providing the LinkedIn topic right here.
I'm providing the human feedback. I'm
just going to take the latest human
feedback because you can imagine that we
are appending everything. We don't
really need to append everything in the
human feedback. We just need to maintain
the latest one. But yeah, why not? We
can always have a list so that we can
later look at, you know, what are the
different human feedbacks that came in,
right? So I'm just going to take the
latest one. If there is no feedback that
we were able to see, then no feedback
yet. Okay. So this is the main uh
prompt. I'm simply just saying generate
a structured and well-written LinkedIn
post based on this particular given
topic. Consider previous human feedback
to refine the response. So this is going
to be the prompt. I'm providing the
prompt and this is the message. It's
going to be a very simple. You are an
expert LinkedIn content writer. So as
soon as I get the generated post I'm
just going to add it and I'm also going
to add the feedback as well. Perfect. So
what are we doing after this? Okay. So
if I come down here you can see that
right after the model we are going to go
to the human node. Okay. So what do we
have to do? Right after the content is
generated we have to you know go to the
human node to basically you know show
the message to the human and ask the
human okay is this good? Do you have any
feedbacks or do you want to continue? Do
you want to finish the execution? Right.
So let's go to the human node. So the
human intervention node, it loops back
to the model unless input is done.
Great. So we are awaiting human feedback
right here. So we're just going to take
the generated post and then we are going
to surface this to the user. Okay. So
we're basically just saying this is
going to be the generated post. Okay. So
this is in an object format so that we
can actually show it in a neat way in
the UI, right? And then we're showing
the message provide feedback or type
done to finish. In that case, okay, the
human feedback has been received. This
is not going to be a done, but instead
it is going to be an actual feedback. So
if it is done, in that case, we are just
going to you know go to the end node.
Okay, so go to end node. If you
remember, we can also use the command
class to write do edgeless graphs,
right? We can actually direct the flow
of the graph right within the node,
right? So, but we are not going to be
doing this. In this case, we have an
actual human feedback that we are going
to append to the human feedback. Right?
So, this time we are going to go back to
the model. So, we are going to do the
same thing again and we are going to now
incorporate this particular feedback.
Okay? So, this loop happens a couple of
different times. Okay? How muchever
times the human wants to refine it.
Okay? So once the user actually says
done in that case it's going to come to
the end node where we are finally going
to show the generated post. We're going
to show the final human feedback as
well. Why not? And then we are going to
end it basically. Right? So that is what
I've done here. Scrolling down we're
just adding all of the nodes. We're
adding some initial edges and then we
are finishing it with the end node. All
right great. So coming down next we have
to uh you know take care of the
interrupt mechanisms right so we have
the checkpointter we are compiling the
graph we are creating the thread config
and then now we need to collect the
initial state so if you if you uh
remember this initial topic needs to be
present at the very start and in this in
this case we are getting it from the
input method and the other two is going
to be just an empty list and finally we
are going to invoke the graph We're
using the stream method and from the
chunks that we're going to get in every
single event, we are going to look at
the node ID and we're checking if the
node ID is interrupt or not. Okay, so if
we've encountered an interrupt, we are
entering into a while loop right here.
Okay, so this while loop is going to
stay true until the user says done.
Okay. So if the user keeps on providing
feedback, we want it to keep on running
and then we are going to provide this
user feedback in this resume. All right.
So let's actually go ahead and run this
file. So let's say you know I want to
see something like you know AI agents
taking over content
creation. So right here you can see
that. So this entire thing is going to
be the LinkedIn post that the model has
provided. But one thing that I see is
that this is too verbose, too long. You
know here we can actually provide a
feedback right here. Okay. So what
feedback can we provide? Keep the entire
post to less than five lines. Okay. So
let's say something like
this. So now you can see that it's it's
not more than like three or four lines,
right? So now I can probably say too
short. Uh make it
longer and funnier
maybe. And now it's going to make it a
little bit more funnier. Right. So now
I'm not going to read it but you can
actually read it if you want. So and
that is it. So it's going to keep on
looping until we say done. And if you
say done and now we can see the entire
history. So what is all of this? Uh so
in the in the final end node we are
printing the generated post the last
final generated post. We're printing the
last human uh feedback and that is all
that we see right here. So I hope that
you now are starting to see why human in
the loop can be powerful. If you have
any questions don't hesitate ask me in
the comments or you know you can always
go to the source code pull it and you
can play around with it try to break it
and uh yeah so uh that's it for this
section. I'll see you in the next one.
Uh hello guys, welcome to this brand new
section where we are going to be uh you
know combining retrieval augmented
generation or rags with LAN graph. So we
are going to be building uh rag enhanced
you know AI agents. Okay. So u I this is
not going to be an introductory video to
rags because I expect you to know it
already because I've already covered
rags extensively in my lang chain grass
course. Okay. So if you have not watched
it please go ahead and watch it. So
let's anyway get a quick refresher about
what rags is. Uh basically a rag system
is going to comprise of two different
parts right. So the first part is going
to be the knowledgebased construction
and the second one is the query
processing. So let's quickly talk about
why we even need rags. So we use rags
because we want to give additional
information to the LLM. So let's say we
have a lot of different documents. I run
my own private company and this let's
say there's a lot of documents. In that
case, the LLM is not going to be aware
of the contents of my private documents.
So in that case, I can actually make
these documents available to the LLM to
query. Okay. So that is the whole point
of rags. So let's go through the
knowledgebased construction quickly. So
let's say these are my private documents
and you can see that 1 million doc
tokens is a lot of information, right?
So the first thing that we have to do is
we are chunking it up. Okay? So we're
taking a huge document and then we are
chunking it small into smaller smaller
documents. So you can see that I'm
chunking it and each chunk is going to
have 1,000 tokens. So it's much
manageable. It's much more smaller.
Right? So this is what I'm representing
right here. We have multiple different
chunks. So now that we have chunked up
all of our texts, the next thing that we
have to do is we have to put them
through an embedding model. Okay? So an
embedding model is going to take normal
text like normal English. It's going to
take that and it is going to embed it
and it's going to convert it into a
vector representation or to put it very
simply it's going to convert it into a
mathematical representation. Okay. So
this chunk is going to convert is going
to be converted into a mathematical
representation. This is going to be the
uh the corresponding uh version and this
chunk is going to be converted into its
own mathematical representation. the
same thing for every single different
chunk. So now we have the English
version of the chunk and we have its
corresponding mathematical
representation as well. Right? So now
that we have both of this information,
we are going to put it all. Okay, we are
going to put all of this information
both the original version as well as the
mathematical representation. We are
going to put everything in a database
called the vector database. So now that
we have all of the chunks and its
corresponding vector embedding versions
in the vector DB, now we can actually go
on to doing the second part. Okay. So
how does the second part work? Let's say
there's a bunch of different documents.
We've already put it in the vector
database and as a user I want to query
something. So I'm asking like you know
some question about my own company and
even that is going to be converted into
a mathematical representation. So and
now that we have both this information,
the query as well as the data in the
vector embedding format, this retriever
component is going to based on this
particular question, it is going to
fetch all of the relevant chunks which
could potentially have the answer to
this particular query. Okay, so it's
going to fetch all of this. It's going
to fetch the corresponding chunks as
well. And then finally, this retriever
is going to, you know, have the the the
chunk one, chunk two, and as well as the
query. Okay, so this is going to be in
English. Okay, this is not going to be
in the vector embedding format. And then
finally, we are going to send it to the
LLM. So this is chunk one, chunk two,
chunk three and question. Okay, so this
is how we are going to send to the LLM.
So the LLM is going to look at the
question. It's going to look at all the
relevant pieces, relevant document
chunks and then based on this, it's
going to give an informed answer. So
this is a very quick intro to rags. So
let's quickly see it in action as well.
Okay. So I've gone ahead and created a
file called basic. Okay. So right here
you can see that I've imported document
uh I've imported the open AI embedding
model. I've also imported chroma which
is the uh the vector database that we
are going to be using right now. Okay.
So later on when we're building
production applications I'll be making a
lot more videos courses on that uh where
we are going to be using superbase and
doing a lot of cool stuff. But since
we're just learning right now, Chroma
works perfectly fine. Okay. So now we
are going to be using the open AI
embedding model. And so right now I have
an array of documents. Okay. So we have
like six different documents. And all
these documents talk about this
particular gym. Okay. So you you can
consider you know we had huge amounts of
text and then we just chunked it up into
six different smaller documents. So that
is what I'm sort of simulating uh that
we have. Okay. So in this document we
have the page content and metadata. So
this is going to be the about section of
the gym. So peak performance gym was
founded in 2015 by this particular
person Marcus Chen with over 15 years of
experience and professional. Okay. So
there's a bunch of different information
and we also have you know the timings of
the gym. Okay. So we have a bunch of
different information about a gym. Okay.
So we have ours.txt. We have membership.
Okay. Different information about the
different tires of membership. Uh we
have classes. Okay, these are the
different classes. Uh, Zumba, spin
cycling, yoga, crossfit, etc., etc. We
have the trainers. Okay, so these guys
have five years of minimum years of
experience, right? So, all of these are
chunked up neatly. Okay, facilities,
right? So, all of these are chunked up
neatly right now. And now we are going
to create we are going to create uh you
know uh the vector database and we are
going to embed everything and put it in
the DB and that is exactly what this
line does. Okay, so we are providing all
of the documents. Okay, so we are
providing all of the documents here and
then we're also providing the embedding
model that we are going to use as well.
Okay, so finally we have the database
created as well as this DB now has
information about all the different uh
you know embedded versions of all these
different documents as well. All right,
great. Coming down. So in this
particular instance of the database, we
actually get access to the retriever
method. So if you remember from the
diagram, we have the retriever
component, right? So this component is
what is going to you know based on the
query it is going to go fetch the
relevant chunks from the DB right so
that is what this method does. So right
here we are just giving a few
configurations about how this retriever
should behave. So right here we are
setting the the the search uh the amount
of uh chunks it should retrieve to
three. Okay. So it's going to collect
three different chunks from the database
that it thinks is similar in meaning
semantically. And then we're also
setting the search type to MMR. So this
means maximum marginal relevance. It
basically just makes sure that the
chunks that are retrieved are different
from each other. So we don't want
similar chunks to be retrieved. Okay. So
great. So we have the retriever. We've
configured it. Now we can actually use
this and invoke something. Okay. So I
can say who founded the gym and what are
the timings. Okay. So if I run
this, okay, retriever is not defined.
Okay. Let me just go ahead and run these
code blocks again. Okay. So now it's
going to take some time because it is
taking all of these documents. It is
then embedding it, right? And then
putting it in the DB. All right. Okay.
Great. And then now if I run this, it
should work. Okay. Perfect. So you can
see that based on our query, we wanted
to know about okay who founded the gym.
So basically we wanted the about
section. Okay. So basically the
retriever is going to look through this
particular text and it's going to see
that Marcus Chin is the founder of this
gym. So that is why it semantically
queried this particular chunk and then
regarding the timings as well. So
hours.txt right. So 5 to 11. Okay, it
queried that as well. And then there is
another uh thing that it queried as well
which we do not want at the moment. All
right, great. So the retriever works
fine. Coming down. So right now we are
creating a template. Okay. So this is
going to be the main template that we
are going to be sending to the LLM. So
if I come back to the diagram. So right
here you can see that we are you know we
are structuring this in a certain format
and we're sending it to the LLM. Right?
So when this what do we want? We want
the context. So this is going to be the
user question and these chunks. Okay.
Chunk one, chunk 2, chunk three. These
are going to be the context. Okay. So
that is what we are going to put right
here. So answer the question based on
the following context and then we have
the final users question as well. All
right. So we are preparing the prompt
here. Perfect. So what is the next
thing? So we have another util method
that we've written right here. So the
retriever is going to it's going to
fetch some um chunks, right? So it's
going to fetch some chunks and each of
the chunk is going to have the the
regular English text as well, right? So
all that we're doing is we're getting
all of the we're getting access to let's
say you know we have three different
documents. So we are sending all of this
here. We are just taking the page
content. Okay. We don't want the
metadata. We just want the page content.
And then we're basically just joining
and we're just like giving a couple of
lines in between. Okay. So that is it.
All right. So this is a helper method
that we're using. Okay. So this is going
to be the last chain. Okay. So this is
going to be the QA chain. So the first
thing that we have to do is we have to
populate the prompt template. So as we
know the prompt template has context as
well as question. So that is the first
thing that I'm doing. I am populating it
right here. So we have the prompt and
then right here before the prompt we
have to provide this particular object.
Okay. So in the object in the dictionary
we have the context and I am able to get
the context because I am doing retriever
invoke and I'm sending this x. Okay. So
whatever we send right here is going to
be available as x that I'm providing
right here. Okay. So this particular
method is going to okay. So this thing
is going to just get the relevant chunks
and this is going to format it in a
certain way and we have the context
ready and the same thing for the
question as well. So this thing is going
to be the question. Okay. So this is a
lambda function. It just gives you
access to you know it basically allows
you to write a function in a single in a
single line. All right. So now that the
dictionary is ready we are now you know
sending it to the prompt and now the
prompt is also going to be ready. So
right after the prompt is ready. So
right after all these values are
populated, we're sending everything to
the LLM. Okay. So we're using the lang
chain expression language right here.
Okay. So if we use this, we don't have
to say invoke invoke every single time.
Okay. So and then finally we are just
extracting the content out of the AI
message that we get back. Okay, great.
So let me actually go ahead and run this
particular code block as well.
Okay, so it looks like we're getting
some problem. Okay, let me actually go
ahead and run this as well. Okay, so we
missed out on running this. All
right, so right now you should be able
to see the owner's name. Okay, so the
owner of peak performance, Jim, is uh
this person. And then these are the
timings as well. Okay, perfect. So this
uh section was a very quick you know we
we are not using lang graph. We're not
you know building agents here. This is
just a quick refresher for rags just to
jog your memory. In the next section we
are going to be u you know building an
agent and we are going to you know allow
the agent to make use of rags. Okay. So
that is something that we'll look at in
the next section. So I'll see you
there. All right guys. So welcome to the
second section in rags. Uh in the
section we are going to be building out
this particular graph. So we are going
to be building the
classificationdriven retrieval system.
So basically what this is going to do is
only if the question is on topic. Okay,
we are going to make use of rags. If the
question is something unrelated, we are
not going to the LLM is just going to
say, "Okay, I don't know. I can't answer
this question." So, where could this be
used? So, let's say, you know, I have a
chatbot on my company's website and I
want the users to only be able to ask
questions about my company. Okay? So, if
the user were to ask something about
some other company, the LLM should not
respond. Okay? So, that is what this uh
workflow is going to do. So first off we
are going to have this topic decision
node. This node is just going to look at
the the user question and then it's
going to figure out is this on topic or
off topic. Is the human asking questions
about something that is relevant about
my company and if it is on topic it is
then going to go look through the
documents that I have provided and then
it is going to generate an answer based
on that. Okay. If it is off topic then
it's going to give a boilerplate
response saying something like you know
I can't answer the question. Okay. So
this is what we're going to be building.
So coming back to the code. So what we
have here is uh I've just basically copy
pasted uh you know the documents. We are
going to be you know working with the
same example. All right coming down. So
we have the embedding function the docs
and then we have the same database that
I've initialized. I've initialized the
retriever. I've provided the same
configuration here as well. So if I were
to ask something like who is the owner,
what is the timing? It is going to Okay.
So let me quickly run this again. All
right. It is going to take some time.
All right. Done. So now it should give
some information about who is the owner
and what are the timings. Okay. Great.
So coming down. So this is also
something that we have seen, right? So
we are preparing the template right
here. We are providing the context and
the question. Okay. This is also no
different. And then we have the rag
chain. Okay. So we just have the prompt
and once we have the prompt and that is
completely ready and then we are going
to send it to the LLM. Okay. So we have
the chain ready as well. Okay. So let us
uh scroll down. So this is where we are
going to start you know building the
graph. So in this graph we are going to
have three different um properties that
we are going to maintain in the state.
So initially it is the list of messages
and then the list of documents right. So
this retriever is going to you know
retrieve a list of documents right and
then we are going to use that particular
property in order to send it to the LLM
later. So we are storing that as well
and then we also have another property
for you know on topic or not. Okay. So
right here we can see that this is
either going to be true or false. All
right coming down we now are going to
define a pyantic model. Uh if you've
forgotten you know what structured
outputs are how to get structured
outputs from the LLM. water pedantic
models. Simply scroll up in the course,
you would be able to find that one video
that I would have made uh right before
the reflexion agent system. Watch it to
better understand this. But yeah, we are
defining the pyantic model right here.
So we are giving a description boolean
value to check whether a question is
related to the peak performance gym.
Right? So this is going to give either
true or false. Okay. So we are providing
a a description as well. Okay. So
basically we're saying okay give a
boolean value. Okay, true or false to
check whether the human question is
related to the peak performance gym. All
right, and this is the property that I
want the LLM to return in a structured
format. So, it's going to be score and
we are further giving a description. Uh,
is the question about the gym? If yes,
give yes, return yes or else no. All
right, so it's going to be yes or no.
Okay, perfect. So, it's not going to be
true or false. It's going to be yes or
no. Perfect. And then coming down we
have the questions classifier which is
going to be this first node right here.
Right? So we are going to get the human
message and then I'm saying you are a
classifier that determines whether a
user's question is one of the following
topics. Okay. So is it something
regarding you know the gym okay? It's
not something else outside of it. So if
it is coming under any of these
different things then if the answer is
about any of these topics respond with
yes. otherwise respond with no. Okay, so
this is going to be the prompt and then
we're also using the structured output
right here and and we're providing the
model as well. Okay, so we are providing
this particular pedantic model as a tool
to the LLM and we're forcing the LLM to
always give this output. Okay, so always
give this output in this particular
structure. Okay, so it should have a
score of yes or no. All right. So then
we're invoking we are passing in the
question right here and whatever result
we are going to get. We are setting it
to the state on topic. Okay. So we are
finally done with this particular thing.
All right. So now we have to decide if
we have to go to this side or this side
based on the result of the on topic.
Right. So right here we have the router
for that. Okay. So on topic router. So
we're taking this particular we getting
reference to this thing and then we're
checking is it yes or no. If it is yes
then we are going to the on topic chain.
Okay. So we're going the on topic route.
Okay. But if it is not we are going to
the off topic route. All right. So let's
look at you know the on topic route.
Okay. So in on topic we have the
retrieve and the generate answer. All
right. So coming down we have the
retrieve. We're getting the the human
message. We're using the retriever.
We're getting all of the documents and
then we're setting it in the state.
Okay, perfect. And what's the next
thing? So once we've set it in the
state, we now have to send that
information to the LLM along with the
user's question. So coming down, we are
getting reference to both the user's
question as well as all of these
documents that have been queried by the
retriever. So once we have both the
information, we are then providing both
of that and then we are uh invoking the
chain. Okay. So whatever response that
we get from the LLM, we are then going
to append it to the list of messages.
Okay. So here we are not using the
reducer function that we use in the
state. We are doing it the very simple
way. We are just appending it right
here. All right.
Perfect. All right. So finally once we
get this answer we are going to come to
an end. Right? So generate answer and
then we are coming coming to an end. But
what if the question that the human
asked was off topic. Okay. So in that
case we have to go to the off-topic
response. Right? So this is going to be
the off-topic response. Here I'm just
you know hard- coding an AI message
saying that I'm sorry I cannot answer
this question. Okay. So we don't really
even need an AI to do it. That's an
extra API call that we're saving there.
So I'm just going to return this
particular AI message and right after
this we are going to end it. Okay. So
this particular graph I've built it
right here. Uh and then you can pause
the screen. All of these are some things
that you should be already familiar
with. So right after the topic decision,
we are going to the router which is
going to decide if it's going to go to
the on topic route or the off topic
route. Coming down. All right. So
finally right after the generate answer
we are going to the end. So if it went
to the off topic then we are going to
the end as well. Okay perfect. So let me
actually quickly go ahead and run all of
these blocks right here. So let me run
this this as well.
Okay. All right. So now I think Okay. So
we get the exact same um structure that
we see right here. Okay. So everything
is looking perfect. Let's scroll down
and let's finally go ahead and invoke
the graph. So right here I'm saying who
is the owner and what are the timings.
Okay. So I'm just making the prompt a
little bit complicated. So let's see if
the right chunks would be retrieved and
the LLM would be able to answer it.
Okay. So let's give it a few seconds.
Perfect. So in the final uh state object
you can see that we have the messages,
we have the documents and we have the on
topic. So in the messages as usual the
initial human message and then the final
AI message is also present. Okay. So
this is what we want. The owner of peak
performance gym is Marcus Chin which is
correct. Okay. This is going to be the
timings as well. Okay. Okay, so the LM
was able to properly you know answer the
questions based on the context we
provided. So what were the context day
documents? So you can see that the uh
retriever you know um collected the
membership text hours and the about as
well. So the about is where you know the
information about the owner is present
and regarding the timings we have the
hours. Okay. Okay, so it properly
queried the relevant chunks and provided
it to the LLM. And you can also see that
this final on topic is also set to true.
Okay, because this question is related.
Okay, so this question is related to
these things. Okay, so that is why we
have gotten this on topic to true. So
let's actually test the system on
something that is off topic as well. So
you know I am going to you know say what
does the company Apple do? Okay. So that
is going to be completely different from
what my gym is all about. Right? So in
this case if I run it, it should ideally
say I am sorry I cannot answer this
question and that is because on topic is
no. So that is it for the
classificationdriven retrieval system.
In the next section, we are going to
take a look at you know how we can
actually provide a rag node as a tool to
the agent so that the agent can you know
invoke the tool anytime it wants to.
Okay. So that is another way of doing
the same thing and we'll explore that as
well. So I'll see you in the next
section. Uh hello guys. So in this
section let us look at how we can
provide a rag tool uh to the agent so
that it can actually call that tool
whenever it needs to. Okay. So, uh you
can see that I've created I've populated
all of the code here. Let me walk you
through it step by step. So, this
particular section it is the same thing
that we've been using uh for the past
few videos. All right. So, we're
creating the database as well. We're
setting the retriever absolutely no
different. So, let's come down. So, this
is where we are going to learn about
something new. Okay. So, you can see
that I am now creating two different
tools right here. Okay. So we are
creating a retrieval tool and an
off-topic tool. Okay. So if at all the
agent wants to make use of you know rags
and it wants some private information in
that case it can use this particular
retriever tool. Okay. So let's look at
what it what it's all about. So this
create retrieval tool let me hover over
it and you can see that it creates a
tool to do retrieval of documents and
this takes in a couple of arguments. So
you can see that it takes in the
retriever itself. Okay, which we've
already written. We we we can give the
name of the tool here that is going to
be passed to the LLM. Okay, the LLM can
decide okay it wants to use this
particular tool and we are providing
some description as well. Okay, so that
is exactly what we have done here. This
is a pre-built tool that lang chain
provides. Okay, so we can go ahead and
use it. I'm going to hope that they are
not going to change the name and
deprecate it. But yeah, so we have this
create retrieval tool. Okay. So here we
are providing this particular retriever
that we have initialized and I want this
to be the tool name as well. Okay. And
then this is the description. So this is
very important. So this is information
related to gym history and founder
operating hours, membership plans,
fitness and all of these different
things. Okay. So basically what's going
to happen is if we ask if the user asks
the agent a question like you know who
is the owner of this particular gym? uh
in that case this tool is going to be
available to the LLM. It can actually
say okay I want to use this tool to
fetch this information and then it can
call this particular tool. Okay. So this
is going to be the first tool and the
second tool is the off topic tool. Okay.
So what is this going to do? This is
going to catch all questions not related
to the peak performance gym's history.
Okay. So all of this. Okay. So if at all
the user asks some other question like
you know what we saw earlier uh if it
asks questions about some competitor's
gym or something like that in that case
forbidden do not respond to the user.
Okay perfect. So we are having both of
these tools right here. So coming down
you can see that this is going to be the
state right. So in this case you can see
that the state is much more simpler. We
just need to maintain the list of
messages. This time we don't really need
to worry about this on topic or not
because the LLM decides to call the
tool. We don't have to keep track of it.
Okay. So coming down so this is going to
be the first node. So don't this is not
a complicated structure. This is
something that we've already seen. So if
I scroll down here, this is the
structure that we're working with. Okay.
So if the human you know asks a question
that requires a tool call, then this
particular model is going to this uh
control is going to go here. If not then
it's going to go directly to the end.
Okay. So that is what we're doing here.
That is the agent or model. Right? So
we're providing all of the tools right
here to the model and then we're
invoking that particular human message.
Okay. So then we are appending this
thing. Great. Now coming down uh we have
the should continue. Right. So the
should continue is what is going to
decide if the control flow should go to
the uh the the tools node or it should
go to the end. Okay. So here we're just
checking if the tools tool calls
property if it is present in that case
we are going to tools or we are going to
go to end. All right coming down we're
adding all of these nodes and then we
are using this tool node as the tools
node and then we are adding the
conditional edge right here. So right
after the model go to this particular
should continue. Okay. And then we're
adding the final edge from tools to
agent as well. And we're compiling it.
And this we have already seen. And uh
scrolling down. So right here I am going
to provide a question like this. Okay.
What is Apple's latest products? So
clearly this is off topic, right? So uh
this agent uh has two tool two tools
that it can use right. So the first tool
is going to be the retrieval tool. But
clearly this is off topic. So we also
have this other tool called offtopic
tool if you remember. So this off-topic
tool right if I scroll up here this off
topic tool is going to you know catch
all of the questions not related to the
peak performance and in that case it's
just going to return this particular
message right so ideally the agent
should call the off topic tool okay so
let's see what it
does perfect okay so you can see that we
have the first human message what is
Apple's latest product and you can see
that the second AI message is going to
It has actually called a tool, right? So
the content is empty. So if I go inside
of the tool calls. Okay. So where is the
tool
calls?
Um tool calls. Okay. So inside of the
tool calls in this array, you can see
that it wants to call the off topic
tool. Okay. So it wants to call this off
topic tool. Okay. So now the control is
coming to the tool node and that is
going to return this particular content
forbidden do not respond to the user. So
this tool message is appending to the
list of messages and now coming this
control is going to come back to the
agent right. So it's going to come back
to the agent and the agent is now saying
I'm sorry but I can't provide
information on Apple's latest products.
If you have any questions okay feel free
to ask. So coming down you can see that
this is a query that is actually on
topic right so I'm asking for who is the
owner of the gym and what are the
timings right so in this case this
question is going to be something that
is on topic so you can see that I've
already run this code block and these
are the outputs right but there is one
thing that I want you to notice so the
human message is correct AI message is
correct as well but right here you can
see there are two different tool
messages and the reason why we have two
different tool messages here is because
this LLM is actually suggesting two
different tool calls. So if I scroll to
the right, okay, so we have the tool
calls right here. Right? So in these
tool calls, you should be able to see in
this particular array, you should see
two different objects. So the first
object is for the first tool call. And
right here, you can see that the LLM is
suggesting the query owner of the peak
performance chim. And that is very
interesting because if you look at the
question we are asking for two different
data. So who is the owner and what are
the timings right? So firstly it is
asking for the owner. So this is going
to be a completely separate tool call.
Right? So we are al also using the
retrieval tool. Retriever tool if you
remember. And then this is going to be
the second tool call. In this second
tool call the argument for the retriever
tool is going to be the operating hours
of the peak performance chim. So this is
going to fetch us uh chunks related to
the timings. So that is exactly why we
have two different tool calls as you can
see right here. So based on all these
data it is now the AI is going to answer
the owner of peak performance gym is
Marcus Chin a former this thing and then
the operate hours is as follows. Okay.
So both of those questions have been
answered. Okay great. So I hope you now
understand how we can actually provide
the rag tools to the LLM for it to use
anytime you know it needs to fetch from
the documents from the vector database.
So uh you can you can basically imagine
that both of these different methods the
classification as well as the rack power
tool calling the outcomes are very
similar right. So it is basically doing
the same thing. The only difference is
that in the classification we have more
control okay because uh because we are
actually maintaining the state like on
topic off topic but here we we have a
little less control because the LLM is
actually deciding if it wants to use a
tool or not. Another advantage that I
see with the classification like with
more control that we have is that we can
we can see you know these are the
documents that were fetched and we can
actually like you know uh shape exactly
how the data is presented but in the
tool calling it's it's a little bit more
difficult. So that is the only drawback
that I see with rack power tool calling
but yeah you can always use whichever
that you deem fit in your application.
So in the next section we are going to
be looking at something really
interesting. So so far everything that
we've done is not really production
ready. In the production applications
users expect the application to remember
what they asked for earlier. They expect
the system to remember the conversation
history. So in the next section we are
going to be building a multi-step
reasoning system that is actually
foolproof that you can actually deploy
in businesses landing pages and it would
work perfectly fine. So I'm pretty
excited and I'll see you in the next
section. Uh hello guys. So welcome to
the last video in this rag section. In
this section we are going to be building
a system that is actually going to be
production ready. So this is a system.
Okay. So I've called it advanced
multi-step reasoning system. So uh
before we go through the code, I'm going
to show you the flowchart exactly how it
looks like. Okay. So uh I know it looks
a little daunting, but it's actually
pretty simple. All right. So, so you can
see we have the start node and then we
have this new node called the question
rewriter. So, let's look at what this
actually is. Why do we need this
question rewriter? Okay. So, let's say
we are going to be building this uh
system for a gym. Okay. So, let's say
the the name of the gym is going to be
peak performance gym and an end user of
that gym is going to come on their
website and ask this particular
question. What are peak performance gyms
hours? Okay. So in this case what's
going to happen? What do we normally do?
We take this particular user query and
then we send it to the retriever. The
retriever is going to go and fetch all
of the results and then along with the
user's question and the context it's
going to go to the LLM and it's going to
respond with an answer. Pretty good.
Okay. So this works perfectly fine. But
let's say the user asks a follow-up
question like you know what about
weekends. So in this case we cannot
actually use this particular string
alone and you know tell it to the
retriever to go get chunks because this
alone does not make any sense. It can
only make sense along with the entire
context of the conversation history.
Right? So if you look at this and this
together this makes sense but this alone
it does not make any sense. Right? So in
this case we have to rephrase this
question a little bit so that all the
context is contained in this this thing
so that the retriever has enough
information to go and fetch the relevant
queries. Okay. So basically without
rephrasing the follow-up query this
query lacks context on its own and would
likely return irrelevant results if sent
directly to the retrieval system. Right?
So the rephrasing node basically okay
what this does is it transforms this
what about weekends into what are peak
performance gyms weekend hours. So that
is why we have this node at the very
start. Okay I hope that makes sense. So
after this we have the classifier very
simple this we've already seen before.
So what does this do? It's basically
just going to see if it is an off-topic
query or an ontopic query. Okay. So we
don't want our system to answer user
questions when the user is asking
something about random things, right? So
in that case it's going to go to the off
topic node and then go to the end. So
there's there we can just provide a
boilerplate response like I cannot
answer the question. So something very
simple there. But in case it is an on
topic uh query so in this case we are
then confidently proceeding forward with
fetching the chunks from the vector
store. So that is what this retrieve
node is going to do. Okay. So it's going
to go and fetch like you know we can set
K is equal to four. In this case it is
going to fetch four different chunks
right. So right after this retrieve
node. So we have four different chunks.
So what this retrieval grader is going
to do is it's going to loop through go
through each of these different chunks
and it's going to check if that chunk is
relevant to the user's question. Okay.
So since we said k is equal to four you
know one or two chunks might not even be
relevant. Right? So we're just like
checking, we're just grading each of
those chunks. Okay, is this relevant or
not? Is this relevant or not? Is this
relevant or not? So there's a couple of
different edge cases that might come up.
So there might be a case where at least
one node is actually relevant and the
LLM can use it to answer the user's
question. So in that case, we are going
to go to the generate answer and then
end it. But there is another use case.
What if not even one query is actually
relevant to the user's question. So in
that case we are going to go to this
refine question node and this is going
to refine it's going to add more
information to the question. It's going
to you know change it up uh and make it
better so that it it is going to give
more information to the retriever and
maybe some better chunk is going to be
retrieved from the vector database.
Okay. So again we are going through this
you know there might be situations where
you know no matter how much we change
the question you know relevant chunks
are just not being retrieved in that
case we are just setting a limit to
three different loops okay so we don't
want it to go for you know infinity
that's going to blow out all of our
token so in that case we just set the
max limit to three in case it actually
fetches something that is useful in that
case we are going to go to this node or
else we are going to keep on looping
through it and if the three is reached
if the limit three is reached it means
that you know it is a hopeless question
we cannot answer it in that case we are
going to go to this cannot answer node
and that is just going to say I I don't
have this information right now or
something like that okay so or we can
also like you know um since it is still
on topic we can actually you know route
it to a human agent and do something
like that in this particular node okay
so this is what we have we have canot
answer generate answer and then refine
question and this takes care of all of
the different edge cases that we might
potentially come up that might
potentially come up in production. Okay,
so uh I hope that made sense. I hope
this introduction made sense. Let us now
jump into the code and go through the
code. All right guys, so you can see
that I've already gone ahead and I have
created this file called advanced
multi-step reasoning. So uh this first
code block is going to be no different.
So I'm just going to reuse the same
examples that we saw. Uh so we have the
peak performance gym. Okay. So, uh we
have four different like five different
documents. Great. Okay. So, we are
initializing the DB as well as the
retriever. Perfect. Okay. So, coming
down we are initializing the model. Here
we are using GPD40. And then in this
template this time we are going to
write. Okay. So, we're saying answer the
question based on the following context
and the chat history especially take the
latest question into consideration.
Okay. So this time we are providing the
the entire history the conversation
history okay in the final LLM call that
we make and then we are going to provide
the context as well this context is
going to be the retrieved chunks by the
retriever and then we have the the
question okay this could be the
rephrased question uh you know that we
make okay so we are making we're keeping
this template ready okay and then we are
creating the rag chain okay so this is
actually repeated we don't want it to be
repeated all right so we have the rag
chain coming down. So this is where uh
you know we have the agent state. We are
defining the agent state. So you can see
there's a lot more properties that we we
are going to be maintaining to achieve
this particular system. All right. So
first thing messages right. So list of
messages we need this right. So to
maintain you know the conversation
history we need to maintain all of the
messages. Great. So we have the
documents. What does this use for? This
could be all of the documents that are
retrieved by the retriever. And then
this could be the boolean. Okay, this
could be like you know um yes or no. Is
the user's question on topic or off
topic? Uh the rephrased question the
question that the LM we can rephrase it
if it does not really uh you know
contain the full context. So in that
case we are going to rephrase and put it
here. And then we have this proceed to
generate as well. So why why would we
want this? So if I come down here so you
can see that you know if proceed to
generate is true in that case we can
just go you know to this particular node
and then end it. If it is not ready in
that case we might have to refine it or
you know in case that the count is
reached we might have to go the this
route. All right. So that is why we
might want this. So this rephrase count
again this looping. Okay. So if it
reaches if it is more than three or
something like that in that case we
might have to exit out of the graph
right and then this is going to be the
original latest users question as well
that we are keeping. So we have the
rephrased question and the original
question. Okay so I hope that makes
sense. Coming down we have the grade
question pantic model. Okay so I hope
that you know why we use a pantic model.
So this pyantic model can be used to you
know get structured data from the LLM.
So in this case what data that we want
is going to be this you know we we just
want a score. Okay. So the score is
going to be a string and we are giving
the description is the question about
the specified topics. Okay. So if yes
then return yes. If no return no. Okay.
So this is going to be the pedantic
model which we will soon use uh you know
we will soon use it down here. So yeah
let's now look at the question rewriter
node. Okay. So let's look at this first
node. Okay. All right. So we're just
printing entering question rewriter with
the following state. So you can see that
at the very start we are resetting a lot
of these state variables. So we are
resetting the documents. We're resetting
uh on topic or off topic. We are
resetting the rephrased question proceed
to generate or rephrase count. Okay. So
why do we do that? If you can imagine
you know for every single prompt every
single question the user is going to ask
the state the graph is going to run from
start to end. Okay for the second
question it's again going to run from
start to end. Okay so we are going to be
using a checkpointer as well. So it's
going to remember what happened but
still it's going to start from start to
end. So if there's a second question, we
want to reset all these different
things, right? We don't we want to start
the graph fresh. I mean with the memory
with the conversation history of course,
but the rest of it we want to start it
from scratch. So that is why we are
going to set all these state variables
except for question and messages.
Messages we don't want to remove, right?
Because we want the messages to continue
forward, right? All right. So coming
down. So right here we're checking if
the messages is not in state or message
is not there. We're just going to
initialize it to an empty list. If we
provide the proper initial states, all
of this would not be needed. But I'm
just going to make sure that every edge
case is handled and it is actually
robust. So coming down, we are checking
if the question is present in a state
sorry if the question is not really
present in the list of messages in that
case we are appending to the uh message
list. Now so we're checking if the
message list is greater than one. Okay.
So if it is greater than one it means
that the graph has already run the first
time and now it could be the second or
the third or the fourth time right. So
in this case we might actually want to
rephrase the users's question properly.
Okay. So coming down here else if the
graph is actually running for the very
very first time if the conversation is
actually like you know just starting in
that case we don't need to rephrase any
question in that case we can just like
put the uh the state question right
here. But in case it is the second or
the third, we might actually want to
make sure that the user's question has
the full context. Okay. So we might want
to rephrase it. So that is why we are
coming into this particular block. So
coming down here, you can see that we
are looking at the entire list of
messages and then we are extracting
everything except the last message
except the last human question. Right?
So we have the conversation history
right here and then we are also taking
the current question and then we have a
list of messages. In the system message
we are saying you are a helpful
assistant that rephrases the user's
question to a to be a standalone
question optimized for retrieval. So
along with the system message we also
have to provide the past conversation
history as well as the last human
message. Okay. So that is what we're
doing here. So we are providing the past
conversation history as well as the
latest uh user question. Okay. Okay. And
then finally we are invoking it. So in
this response we are going to get a
rephrased question. Okay. So that
question is going to be rephrased with a
lot more context. So we are going to do
response. We are going to strip it all
off you know all the unnecessary spaces
trailing spaces initial spaces and all
of that. Okay. Just to make sure that
you know everything is looking tidy in
the production. And then we are going to
just like print it out and then set this
new better rephrased question to this
initial state. Okay, great. So we are
now done with this question rewriter
node. So what is the next thing? So
right after the question rewriter node
we are then we then have to look at the
prompt look at the question and then
check if it is still on topic or off
topic. So coming down. Okay. So I'm
saying that you are a classifier that
determines whether a user's question is
about one of these following topics.
Okay. So it needs to basically be about
the gym, right? The gym history,
founder, operating hours, membership
plans, fitness classes, personal
trainers. Okay? So if the question is
about any of these topics, respond with
yes. Otherwise, respond with no. Okay?
So we need a very structured output in
this case, right? So we we need a clear
yes or no from the LLM for this
particular from this particular node. So
I'm providing the uh the the user's
question okay in the human message and
then I'm creating a prompt out of it.
Okay. So I'm providing the system
message and the human message right
here. Okay. And then before I actually
you know just send it to a normal LLM
I'm not interested in that because I now
want a structured output. I want it to
be in this particular structure. I want
a score property and a particular value
yes or no. Okay. So I'm going to make
use of the with structured output
method. So now I can pass it inside of
this and then expect to get the exact
score and I'm also going to make sure to
strip every you know trailing spaces etc
etc and then I'm going to assign it to
this on topic property in the state.
Okay. So now this is going to be either
yes or no. So coming back to the
diagram. So if it is no in that case
okay it is not on topic in that case
it's going to go to this particular uh
node or else it's coming here. So we
need some sort of like a router right.
So that router can actually direct you
know the direction the flow of the
graph. So that is exactly what we have
right here. So we have the uh onoff
topic router. Okay. So we're entering
this thing and then we're just going to
take that on topic. Okay. If it is yes,
we have to route to the retrieve node or
else we have to route to the off topic
response. Okay, so we have to go to the
retrieve node or the off-topic response.
All right, so let's look at the retrieve
node. So what what should this retrieve
node do? It is all that it's going to do
is invoke it's going to provide this
rephrased question to the retriever and
then we now have the list of documents
and then we are so I'm just going to set
it to the documents property in the
state. All right, perfect. So what's the
next thing? So we Okay, let's come back
to the diagram. So right after the
retrieve is done, we now pass it to the
retrieval grader. So this grader is just
going to you know return true basically.
Okay, we have like let's say four
different chunks. It's going to go
through each of those chunks and then
it's going to say true or false. Okay,
is it good enough or not? Right? So we
need a structured output for that as
well. So that is what I'm giving right
here. We we have another score. Okay. So
in this case, if the document is
relevant to the question, say yes. If
not, say no. All right. So, so here I'm
saying you are a grader assessing the
relevance of a retrieved document to a
user's question. Only answer with yes or
no. If the document contains the
information related to the user's
question, respond with yes. Otherwise,
respond with no. Okay. So we are again
going to be using the structured output
method so that we can actually force the
LLM to use this particular tool to
output in a structured way. All right.
So now that we have the structured LLM,
the supercharged LLM with the tool.
Okay. Now we just need to loop through
all of the documents that have been
fetched in the previous node. So we have
the doc right here and I'm just in the
human message I'm just saying okay this
is going to be the user question and
this is going to be the retrieved
document. Okay. So we have the current
document. We're just providing the page
content. We don't really want the
metadata, the source. We're just
providing the page content. Okay. All
right. Coming down. So this greater llm
it is either going to return yes or no.
We're just checking that. So if it is
yes, in that case we are going to append
it to the relevant docs. So we are
creating a new list. Okay. So we want to
create a new list and we only want to
keep the relevant things alone. So we
are going to append it right here. So
once this looping has been done, we
we're setting the new renewed qualified
list of chunks to the documents again
and then we are going to you know check
whether we need to proceed to generate
or not. Okay. So if the relevant talks
at least has one relevant this thing we
can then confidently go ahead and go to
the generate answer. Right. So that is
what we're checking right here. All
right. So what's the next thing? We now
have the proceed router. Okay. So
basically to check do we have to go here
or do we have to go here or do we have
to go here. So we need some uh you know
router right in the middle and that is
what we have right here. Okay so we're
entering the proceed router and then we
are getting the proceed to generator. So
if this actually at least has one okay
it's greater than zero in that case we
can go ahead to the generate answer
node. But in case proceed to answer is
false and you know the rephrase count is
already greater than two in this case.
Okay maximum rephrase attempts have
reached cannot find the relevant
documents. Okay. So now we have to go to
the cannot answer node. Okay. So we have
to go to this cannot answer node. And
then we also have the final case. If
both of these cases like the rephrase
count is still less than three at the
same time the proceed to generate is
false. In that case we have to refine
the question. Okay. So this is how we
refine the question as well. So if you
see that you know we are getting the
rephrase count. Okay. So if it is again
we're just writing the same logic here
and so we're taking the rephrased
question and now in this new system
message we are saying you are a helpful
assistant that slightly refineses the
user's question to improve the retrieval
results. Provide a slightly adjusted
version of the question. Okay. Okay. So
the human message is also going to be
like okay this is the original question
provide a slightly refined question. So
we're providing that we're creating a
prompt and then we're invoking it and
then we're stripping it and getting the
content out of it. So this is now going
to be the refined question that we are
updating the state with. Okay. So the
refra question we are updating in the
state. Okay. And the count we're also
incrementing as well. Okay. Because we
don't want to loop through it you know
more than three times. And then finally
if everything goes well if at least
there is one chunk that is relevant to
the user's question we have we want to
go to the generate answer. So we need
three different things we need to
provide right. So we need the list of we
need the conversation history that we
need to provide to the LLM. We need to
provide the rephrased question the
actual question that the user user wants
to know and then the documents the
retrieved chunks. Okay, only the
relevant qualified retrieved chunks from
this documents state. So we're going to
provide all these three things in this
rag chain. So if you remember this rag
chain, we're coming up here. So you can
see that we are providing the history,
we are providing the context and the
question. Correct? Okay. So let's go
down again. Okay. So we are providing
all these different things and we are
getting a final response and whatever
response we get right here we are just
going to show it to the user because we
are coming to an end. Okay. So before
that we are just going to update the
state as well. The history always keeps
on updating right. And then we have the
cannot answer node. Okay. If the
retriever still cannot fetch the right
chunks for some reason or you know the
documents are not really enriched
properly in that case we are coming to
the cannot answer node here. Um I'm just
going to add this uh you know AI message
saying that I'm sorry but I cannot find
the information you're looking for. And
then again we are going to end the
graph. Okay. So we've looked at the
generate answer. We've looked at cannot
answer. Now uh we can look at the off
topic response. This is something that
we've already written and we are just
saying I'm sorry I cannot answer this
question because it is off topic right
and that is it guys that is it for all
of the nodes we've defined all of the
nodes. The next thing is we just need to
so coming down we are using the
checkpointer. So why do we need a
checkpointer? So the user needs to be
able to you know ask follow-up questions
and all right so in that case all the
state needs to be you know remembered.
So because once uh you know for every
single question we're going to run
through the graph completely from start
to end. So it needs to remember what
happened in the previous run. Right? So
that is why we need a checkpointer. We
are using a very simple memory saver.
And now we are just adding all of the
nodes. Okay. So all these nodes that
we've defined we're just going to add
all of it. I've used the same names as
the node names. Right? Okay. So whatever
the uh the uh the function name that
I've used I've used the same things
everywhere. And then I'm going to add
the edges as well. So the question
rewriter has to go to the classifier
next, right? It needs to go to the
classifier. And as soon as the
classifier is done executing, we need to
route to retrieve or off topic, right?
And then the there's another condition
here for you know right after the
retrieval grader is done, there is
another routing here as well and that
takes care of that. So coming down,
there's a bunch of different edges that
I'm adding here as well. So these are
something that we would have seen a
million times by now. But yes, so
finally I'm going to compile it. Provide
the check pointer. All right, perfect.
So this is going to take some time. Uh
so because there's a lot of different
notes that we've written, it's going to
take some time. All right, perfect. So
you can see that it almost took me 1
minute and 10 seconds. So yeah, this is
the same graph that we saw in the
slides. So we have the question rewriter
classifier retrieve and then we have all
these different edge cases handled as
well. All right, perfect. So uh this is
what I'm going to do. So first off I'm
going to check the first edge case which
is going to be if what happens if the
user asks an off topic question. Okay.
So in this case the user is saying what
does the company Apple do? It's
completely unrelated to the actual
business the gym business right. So in
this case if I run it so let's look at
what it is saying. So it is saying that
entering the question rewriter with the
following state. Okay. So since there is
only one state right now one item in the
list right now it's not going to rewrite
anything. And then it's coming to the
question classifier right this
classifier is saying the on topic is no.
Okay. So this classifier is saying the
on topic is no. In that case it needs to
go to the off topic response through the
router that we had right here. Okay. So
it's coming inside of the router. The
router is directing the graph to the off
topic response and that is exactly what
we see right here. So the AI message is
saying I'm sorry I cannot answer this
question. Perfect. So we can also see
that the off on topic is no and then the
proceed to generate everything is false.
Right? Okay. Perfect. So let's look at
another case here as well. So in this
case you know uh I'm going to ask
something
uh you know what is the cancellation
policy for peak performance gym
memberships. Okay. So what is this edge
case? So this edge case is if I scroll
up in the documents there is absolutely
nothing that is present in the document
that talks about cancellations at all.
Okay. So there is nothing about
cancellations in this case. So it is
still on topic. Okay. Okay, so it is
still on topic but at the same time the
retriever is not going to retrieve any
relevant chunks at all. So it's actually
going to look through three different
times and it's going to give up. Okay,
so that is what this case is going to be
all about. So let me actually you know
um run this. You can see that I've
already I've gone ahead and used a
different thread because we don't want
to confuse everything in the same
thread. So if I run this okay great. So
let us see what it's saying. So right
here. Okay, so this is going to be the
rephrased question. Okay, great. Uh, and
then the on topic is also going to be
yes, right? So coming down, it's going
to retrieve four different documents. So
this retrieve node is going to retrieve
four different documents and then the
grader is going to kick in. So let's see
what the grader is doing. So the grader
is looking at the first chunk and the
result is no. Okay, so this chunk does
not answer the user's question. This
chunk does not answer the user's
question. No, no, no. Okay. So now it is
actually going to you know the proceed
to generate is also set to false. So it
is actually going to loop back. Okay.
It's going to loop back to the retrieve.
Okay. With a a different with a
different question. Okay. With a refined
question. So in this case the refined
question is going to be you know what is
the me membership cancellation policy at
peak performance gym. Okay. So you can
see that it changed it a little bit. So
here it is. What is the cancellation
policy? Here it is the what is the
membership cancellation policy. Okay. So
it basically made it a little better. So
coming down it is again going to
retrieve four different documents. The
answer is still no no no because we
don't have any documents to support
that. Okay. So finally what happens
after like two different iterations. It
is the proceed to generate is still
false. So it's not going to come in
here. It has already exhausted the
looping as well. So now it is going to
go to cannot answer. Okay. So it is
going to go to cannot answer and then it
is going to end it. Okay. So we have the
I'm sorry but I cannot find the
information you are looking for. So
let's look at the third edge case. Okay.
So here okay I'm using another thread.
So this is going to be a new
conversation. So the user is saying who
founded peak performance. Okay. So this
is something that is relatively very
easy. And if I come up here to the this
thing you can see that peak performance
gym was founded by former Olympic
athlete Marcus Chen. Okay. So this is
easily answerable. So coming down. So
this thing is easily answerable. But
let's say the user actually asks a
follow-up question. When did he start
it? Okay. But this query alone does not
have a lot of context. So we need to
rewrite it with the previous question in
the context. Right? So let let me run
this
thing. Okay. So in this case uh great.
So the first time uh it retrieved four
documents and then this one this about
chunk is going to be yes. So this is
actually a relevant chunk. Right. So in
this case it's going to proceed to the
generate answer node and then the final
answer is going to be peak performance
gym was founded by Olympic athlete
Marcus Chen. Okay. So this is correct.
So let's now run this uh let's use the
same thread id as this thing and then
let us now run when did he start it. So
now what would happen is that this
question rewriter is going to kick in
it's going to rewrite the the prompt
properly. Okay, so that this retriever
has more context because it is kind of
dumb, right? So if I run it right now,
okay, so the question rewriter, so you
can see that the rephrased question now
is going to be when did Marcus Chen
start peak performance gym. Okay, so
it's not going to be he anymore. It's
going to be Marcus Chen. Okay, so in
this case again uh it's going to
retrieve four different documents. Okay,
peak performance gym was found. Okay,
it's going to fetch this thing and then
finally we have the generator response.
it was in 2015. And that is it guys. I
do hope that you learned a lot. Do pull
the code and run it by yourself. And I
will see you in the next
section. Uh hello guys, welcome to a
brand new section. In this video, we are
going to be looking at multi- aent
architectures. So let's look at a few
slides. So we already know that an agent
is a system that uses an LLM to decide
the control flow of an application.
As you develop these systems, they might
grow more complex over time, making them
harder to manage and scale. For example,
you might run into the following
problems. The agent has too many tools
at its disposal and makes poor decisions
about which tool to call
next. The context grows too complex for
a single agent to keep track of. Another
problem could be there might be a need
for multiple specialization areas in the
system. So we might need a separate area
for planning and research and math. To
tackle these problems, you might
consider breaking your application into
smaller, more independent agents and
composing them into a multi- aent
system. So these independent agents can
be as simple as a prompt and an LLM call
or as complex as a React agent. The
primary benefits of using multi- aent
systems are modularity. Separate agents
make it easier to develop, test and
maintain agentic systems.
Specialization, we can create expert
agents focused on specific domains which
helps with the overall system
performance. And then control, you can
explicitly control how agents
communicate as opposed to relying on
function calling. So here is an image
that I pulled from the documentation
where they are talking about the
different agent multi- aent
architectures that are possible. So if
you look at this is the single agent
right? So we know what an agent is. If
we give tools to an LLM that becomes an
agent. Okay. So this is a single agent.
So the next type is the network. So you
can see that we have four different
agents and they're all communicating
with each other. So this one agent is
solving its part of the task and then
once it is done depending on what the
next part of the problem is it is
handing control to the next agent. So
they all have access to each other. So
at the next step you can see this is a
supervisor multi-agent architecture. So
here you can see that we have one
supervisor at the very top. Okay. So the
supervisor's only job is to orchestrate
you know the team. So what we have below
here is we have you can think of it as
like three employees. Each employee is
specialized to do one particular thing.
So the first agent could be uh like a
coding agent. The second could be a
researching agent. The third could be
like a validating agent or something
like that. So depending on where the you
know how much of the problem is solved
let's say it's a three-part problem in
that case the supervisor is going to
make use of this agent to solve the
first part and then the control flow is
going to come back to the supervisor and
then the second part is going to be done
by this agent and then the control flow
is going to come back to the supervisor.
So one thing that you have to note is
that these agents don't communicate with
each other. They always report back to
the supervisor and the supervisor has to
decide which agent to call next. Okay.
So this is the supervisor multi-agent
architecture and the next thing is we
have supervisor as tools. Uh so it's
basically the same thing as what
supervisor does except all these
different agents these three agents are
provided as tools to the LLM. So
depending on where how much of the
problem is completed the LLM can decide
to call a particular tool which in this
case is an agent. It can provide input
to the tool and then you know structured
input to the tool. So and then the agent
or /tool can be executed and the next
step is the hierarchical. You can see
that this is very similar to supervisor
but it's just an extension of it. Okay.
So there might be situations where you
know this one supervisor might not be
enough. In that case we go for you know
multiple supervisors and in this case
it's going to be a hierarchical multi-
aent architecture. So it's very similar
to how big companies work right. And
then finally we have custom where you
know there is no clear definitions. So
right after this agent is done you know
there are two different options
depending on the answer output of the
agent we can you know direct the flow to
this agent or this agent. So there this
is a little bit more disorganized and
they have their own applications as
well. So with regards to actually
learning how to build multi- aent
architectures it's very important for us
to first understand how subgraphs work.
So let's look at some slides. Basically,
subgraphs allows you to build complex
systems with multiple components that
are themselves graphs. A common use case
for using subgraphs is building multi-
aent
systems. So the main question when
adding subgraphs is how the parent graph
and subgraph communicate. In other
words, how they pass the state between
each other during the graph execution.
So there are two scenarios. The parent
graph and subgraph share the same schema
keys. In this case, you can just add a
node with the compiled
subgraph. But if the parent graph and
the subgraph have different schemas, so
in this case, we add a node function
that invokes the subgraph. This is
useful when the parent graph and the
subgraph have different state schemas
and you need to transform the state
before or after calling the subgraph. So
we are going to be looking at an example
where we're going to be building a very
very rudimentary basic subgraph and then
we are going to embed that into a parent
graph and we're also going to see these
two different scenarios as well. So this
is going to give us a very strong
foundation that we can build on top of
and then we can build complex multi-
aent architectures. So let's jump to
Visual Studio Code. So right here you
can see that I've already gone ahead and
created a folder called multi- aent
architecture and I've also created a
file called subgraphs and I have
populated the code as well. So let me go
through it step by step. All right. So
as usual, I'm just going to import some
needed modules like you know land graph
you know we're going to be using the
llama model this time. Okay. So human
message and we're loading the env file.
Okay. We are using the tab and then we
have the tool node. Okay. So in the
first part of the code what I'm doing is
I'm basically you know creating a very
simple graph. So if I scroll down here.
Okay. So this is the graph that we are
building. This is something that we've
done it a million times by now. We just
have an LLM. We have a tool node. Okay.
So if the user and one of these tools
are the Tavi search tool. Okay. So if
the user asks some question that
requires an internet search in that case
the the control flow is going to come to
the tool node and then this is going to
you know give an informed answer. Okay.
So this is what I have built right here.
So as you can see right here in this
child state. Okay. So in the first part
I'm going to be building the child
graph. Okay. not the parent graph. I'm
going to be building the child graph.
Okay, so this is the child graph. So
right here, okay, all that we want is
the messages list. I'm initializing the
tab search tool. I'm initializing the
LLM. We are binding it with this
particular tool. Okay, and then this is
the LLM. This is the model which is
going to invoke whichever human message
is going to be, you know, passed in,
right? So we have the agent and right
after the agent. Okay, so right after
the agent we have the router. So we're
checking if the last message has tool
calls and it has some tool calls uh that
the LLM is asking for. So if it is
greater than zero in that case we are
going to go to the tool node. So we are
using the pre-built tool node class that
the langraph provides. So so this tool
node is going to execute the tab search.
It's going to return back the control to
the agent and the agent which is the LLM
which is going to have the response of
the of the tably search and it's going
to give an informed answer. Okay. So
it's going to come back to the agent and
that is going to come to the end. Okay.
So this is exactly what I have built
right here. So I you can also see that I
have named the this thing a subgraph.
Okay. So just to make it clear that what
we're building is going to be a smaller
graph that we want to embed into you
know a parent graph. Okay. So this is uh
a very basic example that we can work
with. So if I run this you can see that
this is the graph that we get. So if I
run this you can see this is what we get
right. So this is going to be the
initial human prompt. And in the AI
message if I scroll down here you can
see that this is going to be the query.
So the LLM wants to invoke a is calling
a tool and then the tool is also getting
executed. Tavly search is getting
executed and then you know it is also
you know the AI message it is saying
that you know Chennai weather today
temperature. So it wants a bit more
information. It is not satisfied with
it. So it is again you know um calling
the tool and then we are finally getting
the current weather in Chennai is misty
with a temperature of
34.4Â°C. So what we have done so far is
that we have built a subgraph. Now let's
build let's create a parent graph and
then embed the subgraph into the parent
graph. So when we want to embed a
subgraph into a parent graph in the
slides we saw two different cases,
right? So you can see that we have two
different scenarios here, right? So if
the parent graph and a subgraph share
the same schema keys, so what do we mean
by schema keys? So this particular
messages property in the state right. So
if a parent as well as a child have the
same message in that case we can just
add a node with the compiled subgraph.
Okay. So this is going to be very
straightforward. So let's look at how we
can do that. Okay. So right here in the
subgraph we're compiling the app. So we
have the search app. Okay. So I'm just
going to call it this could be like a
search agent. Right. So coming down. So
this case one is going to be the shared
schema and we are directly embedding it.
All right. So this is going to be the
parent state. It has the same messages
key as well for whatever reason. Okay.
So here is what I'm doing right here. So
I'm going to keep it very very simple.
So we just have a start node and then
the search agent node and the end node.
Okay. So the search agent is what we've
already built up here. Okay. So you can
think of this as the parent graph. Okay.
Okay, so we're creating a parent graph,
a new new graph using this new state
which happens to have the same schema
key, right? So we have the new graph.
We're adding the search node. Okay, so
this is going to be this particular app.
Okay, so we've compiled the app right
here, right? So this is exactly what we
are providing as a node to the parent
graph. Okay, so we have we starting with
the start, right? After the start is
done, we're going to go to the search
app and then ending. All right. So we
are compiling it and we are providing
the message. Okay. How is the weather in
Chennai? And basically it's going to do
the exact same thing. Now I can invoke
the parent app and then I can pass in
the same message and whatever we pass in
here is going to be made available in
the search agent as well. Okay. So the
control flow is going to go to the
search agent and whatever the search
agent is going to be outputting it is
going to be available as a result right
here. Okay. So if I were to change it to
let's say a Bangalore okay if I run this
file if I run this code
block okay basically the control flow
should go inside of the search agent so
if we go to the lang and we look at it
it's going to go inside of this okay so
or else how are we going to get this
right so the current weather in
Bangalore is expected to be fair with a
maximum temperature whatever right so
this is how simple it is to embed a
subgraph into a parent graph as far as
we have shared schema okay so if shared
schema we just like put the direct uh we
just compile the the subg graph and then
we just put it as a node in the parent
graph. All right, coming down now let's
scroll down. So we have a case two as
well where we have different schemas.
Okay, so we're invoking with a
transformation. So what does this mean?
So if they have different schemas in
this case you have to add a node
function that invokes the subgraph.
Okay, so we just have to have an
intermediate step. Okay, that is all
that we doing right here. So coming down
we this is where we are going to define
the parent graph with a slightly
different schema. Okay, so this is going
to be different uh this this is going to
have a different schema from what the
subgraph is expecting. Okay, so we have
to sort of match we have to make it both
compatible. We have to make this
subgraph and the parent graph compatible
with each other so that they can be
embedded. So uh so what I'm going to do
is I'm going to define a new node for
that. Okay, so you can see that this
parent graph is just going to have you
know three different nodes. Okay, we
just going to have the start and then we
are going to have an intermediate node
called search agent and then we are
going to have end. Okay, so this is
going to be the intermediate node where
we are going to you know make it
compatible the subgraph and the parent
graph compatible. So if I come down here
okay so this is the uh the subg graph
right search app. So we know that this
only takes in a list of messages. So
that is why we are going to make use of
we are going to extract the query from
here and then we are going to send it
okay as part of the messages array okay
when we are invoking the subg graph okay
so this way okay we are going to get a
response right we are going to get the
final response which is again going to
be a list of messages from the subgraph
so now all that I'm going to do is okay
so you know that uh in the parent graph
we just have the query and the response
so I'm going to extract the response
response from the result of the
subgraph. So in the last AI message, I'm
just going to get the last AI message
and then take the content alone and send
it and then attach it in this particular
response and that is it. And then I'm
going to end it. Okay. So now if I just
ask you know how is the weather in
Chennai? We just have the response.
Okay. In Chennai the weather is expected
to be warm and sunny. Okay. So this is
how simple it is to you know embed a
subgraph into a parent graph. Okay. If
it is a same state just compile it and
put it as a node. If it is a different
state in that case you just need to
write an intermediate node like what we
did right here. you know pass the data
in the exact format that the subg graph
is asking for get the result and
whatever result that you get you can
just update it based on whatever schema
that we have defined for the parent
state so I hope you've learned how to
embed a subgraph into a parent graph and
then we've also seen two different
scenarios as well okay so I'll see you
in the next section where we are going
to be building a multi- aent
architecture so I'll see you
Hello guys. So in this video let us look
at how we can implement the supervisor
multi- aent architecture. So in this
architecture we define agents as nodes
and add a supervisor node that can
decide which agent node should be called
next. So we can use the command class to
route execution to the appropriate agent
node based on the supervisor's decision.
So this is what I want to build. So you
can see that we have a supervisor agent
at the very uh top and this supervisor
agent has four different agents
underneath him. So we have the enhancer
agent, we have the researcher agent, the
coder agent and the validator agent.
Okay. So if the user prompt if the
supervisor agent thinks that the user
prompt is not really clear, if it is not
vague, if it is a little vague, in that
case the supervisor agent can always
employ it can always use this enhancer
agent to make the prompt better it to
rephrase it better. Okay. So so right
after the enhancer is done with the
enhancer's work, the control flow is
going to come back to the supervisor
agent again. So now the supervisor knows
okay the prompt is better. Now it's
going to look okay so does this
particular prompt require research from
the internet in that case it's going to
you know it's going to hand it off hand
off the control to the researcher. So
once the researcher is done it's going
to come back and the same thing for
coder as well. So if there is a
particular problem that requires some
code to be written in that case you know
the control the supervisor can hand off
the control to the coder agent and the
coder agent can actually use an actual
tool. Okay, that tool we are going to be
looking at that tool is going to
actually uh execute code. Okay, it's
going to take the output and uh give it.
Okay, so that is also something that we
will look at. And then finally we have
the validator agent. Okay, so right
before ending this workflow, the
validator agent is just going to check
if the user's question and the final
answer is actually relevant or not.
Okay, so this is what we are going to be
building out. So let's actually without
wasting any more time jump into the
code. So right here you can see that
these are some very standard imports
that I'm doing right I'm importing you
know pedantic because there are some
structured outputs that we might have to
do and then we have human message the
tavly tool the command class to direct
the flow of the graph we have the state
graph and then look at this part okay so
there is actually a pre-built graph that
langraph provides for us to build a
react agent okay so uh if you remember a
couple of uh you know videos before um
in the react agent we actually built out
the react agent from scratch right so we
can always use that particular graph and
then embed it into this bigger graph we
can always do that but if you don't want
to do that that is absolutely fine lang
graph provides a pre-built class okay a
method so this is going to you can see
that it creates a graph that works with
a chat model that utilizes tool calling
so you can just provide okay the prompt
problem statement you can provide the
tools and then that is going to start
you know looping through the uh the
thought action observation thought
action observation it's going to return
the final answer so uh in this uh in
this video let's also explore that okay
great and then we have the IPython we we
are loading the environment variables
and then this is going to be the second
tool that we are going to be using so if
you remember I told you that this coder
agent is going to have one tool right so
that tool can actually execute code so
this is going to be that particular tool
okay so it's going to be the tool for
running Python code in a ripple. So if I
go in here, you can see that it is a
tool for running Python code in a
ripple, right? All right. So I'm taking
it from lang chain experimental. You
probably would not have it installed
already. So go ahead and install that as
well. Perfect. All right. So coming down
these are very uh so we're initializing
the LLM and then we're initializing the
Tavly search and then we're initializing
the Python ripple tool as well. Okay. So
if I go ahead and run this. Okay. Okay.
So you can see that we are getting five
as an output. Okay. So you can imagine
that there might be some cases where the
LLM just writes the code and then it
might ask you know it might actually use
this particular tool to you know better
get accurate answers. All right. So I
hope this makes sense before I continue
forward with explaining the rest of the
code. So you can see that I've already
gone ahead and created the supervisor
node. Okay. And then I've created the
enhancer node. uh the research node, the
code node, right? And finally, we must
also have the validator node as well.
So, I've already created all of these
nodes. But before I actually move on to
explaining each and every single node, I
first want to show you the graph. Okay?
So, this is the graph that we have. So,
this is very similar to what I showed
you, right? So, we have the supervisor
node up here. We have the enhancer. So,
if the supervisor wants the prompt to be
enhanced, it's going to come here and
then it's going to uh report back to the
supervisor. Okay? So if the supervisor
needs research okay so it's going to go
to the researcher node and right after
the output is done okay the control flow
is going to go to the validator okay so
what this validator agent is going to do
is it's going to look at the output of
the researcher and then it's going to
check does this answer the user's
question if it does we are going to
return it if it does not return it we
are going to send it back to the
supervisor maybe there is another
problem that we have to solve maybe
there's a coding problem in that case
the coder is going to you know give the
answer and then we're again checking
validating that does this have the
answer to the user's question if it does
not have the answer then we are again
going to send it to the supervisor okay
so this is going to be the graph so as
you can see I've got a couple of
different examples example prompts okay
so this is one problem and I've got
another problem here weather in Chennai
and then I've got another problem give
me the 20th Fibonacci number okay so
let's let's actually you know you can
imagine that this particular code is
going to make use of the coding agent,
right? So, let me actually go ahead and
run it and show you the
output. Okay, so print is not defined.
Okay, and that is not running because
okay, let me just copy copy and put it
over
here. Okay, because I don't want to run
that particular block right now. Okay,
so now this should work. All right,
great. So, we actually have the answer
right here. So let's actually see what
happened. So the workflow initially
transitioned from the supervisor to the
coder. Okay. The supervisor correctly
figured out that this particular problem
requires the coder agent. So that is why
the supervisor is first saying the user
is requesting a specific numerical
result from a well- definfined sequence.
Okay. The Fibonacci sequence which is
best served by calculating the value
using a coding implementation. The
problem is clear and structured making
it appropriate to move it directly to
the coder for computation. Okay. So if I
had like you know given some vague
prompt in that case the supervisor
probably would have gone for the
prompting agent. Okay. So uh you know
this particular agent enhancer agent but
since it's already well defined it is
going to direct it to the coder. So now
it is coming to the so you can also
imagine why are we using human message
here. So we are using human message as
the output of the supervisor because we
sort of like simulating that each of
these agents are actual human beings
right the supervisor is a human all
these different people are different
agents human agents that's why I'm using
human agent here all right so the next
thing is the coder is going to output
this thing okay the 20th Fibonacci
number is 6765 so we we can't really
look at the tool called right now so for
that I am going to go to langsmith okay
so I'm going to go to the first uh run.
Okay. So you can see that okay this is
what we provided right give me the 20th
Fibonacci number. So the first one was
was the supervisor. The supervisor you
know directed the control to the coder
right. So you can see that go to is
coded. So the next thing is the coder.
So right here you can see that uh the
coder agent is actually calling the tool
and it is providing the input query for
the tool as well. So it basically wrote
the uh you know Fibonacci calculation
program. It also provided 20 instead of
this because that is what the human
initially wanted right that was the
initial prompt. So it's provided this
thing and then finally you can see that
it is going to you know coming down you
can see that it is calling the Python
ripple tool right. So it's calling the
Python ripple tool and the Python ripple
tool is outputting 6765 which is again
going to be fed to the agent. Right? So
it's again going to be fed to the agent.
So if I come down here you can see that
it is actually saying the 28th Fibonacci
number is 6765. And finally the
validator is also confirming that this
is actually good. Okay the answer
correctly identifies the 28th Fibonacci
number addressing the user's request. So
now that we understand you know what
problem this architecture solves. Let's
quickly go through you know uh the
different nodes that I've written. So
initially we have the supervisor node
that I've written right here. Okay. So
I've defined the supervisor node and I'm
also structuring the output of the
supervisor node. Okay. So I'm telling uh
give me back two different properties.
So I need the next agent to direct the
flow to and then I also need a reason
why you think that the next agent is it
could be enhancer or researcher or
coder. Okay. So I'm just giving a
description as well. So use enhancer
when the user input requires
clarification. Use researcher when you
need additional facts. use coder when
some problem solving is required. Okay.
So accordingly it's going to produce
either this or this or this and it's
also going to give some reason why it
thought that okay let's say we need to
go to coder. Okay. So detailed
justification. All right. So this is
going to be the prompt. I'm providing a
lot of different uh information like you
know you are a supervisor managing a
team of three agents prompt enhancer
researcher and coder. Your role is to
orchestrate everything. Provide clear
concise rationale for each decision.
These are the three team members, you
know, the prompt enhancer, researcher,
coder. These are your responsibilities,
right? I'm saying analyze each user's
request and agents response for
completeness, accuracy, and relevance.
Route the task to the most appropriate
agent at each decision point. Maintain
workflow momentum by avoiding redundant
agent assignments. Continue the process
until users request is fully and
satisfactory resolved. Okay. Your
objective is to create an efficient
workflow that leverages each agent's
strengths while minimizing unnecessary
steps ultimately delivering complete and
accurate solutions to users request. So
this is going to be the system prompt.
Okay. So this is going to be the first
system message and then I'm going to
append the conversation history. Okay.
So right here and then I'm going to you
know um use this particular you know
structured output. I'm providing this
pantic model and why am I doing that?
because I want the output of the
supervisor to always have this next and
reason. So that is what I'm extracting
right here. So once I have the go to and
the reason as well. Okay. Okay. So the
reason I can just provide it I can just
add it as a message in the list messages
list and then I'm going to use this go
to to use uh I mean I can put it in this
go to uh keyword parameter and I can use
the command class right so that I can
control the direction I can control the
flow to whichever agent that the
supervisor wants to go to. All right so
that is it for the supervisor node. So
coming down we also have the enhancer
node. The enhancer node basically okay
this is what it does. It improves and
clarifies the user query. It takes the
original user input and transforms it
into a more precise actionable request
before passing it to the supervisor. So
this is going to be the system prompt.
Okay, you're a query refinement
specialist. Okay, you are going to
analyze the original query, identify the
key intent. What is the user actually,
you know, asking for? And then we have
resolving any ambiguities or confusions
without requesting additional user
input. expanding underdeveloped aspects
of the query with reasonable
assumptions, right? Restructuring the
query for clarity and actionability. So
we um we're also saying that never ask
questions back to the user. Instead,
make informed assumptions and create the
most comprehensive version of the
request possible. So we are going to do
the same thing again. This is going to
be the system prompt. We are providing
the entire history again. And now we are
going to you know invoke the LLM.
Whatever is going to be the output, I'm
just going to append it as a human
message. And I'm also going to provide a
name this time. Okay. So I'm going to
provide the name enhancer. The same
thing I would have done in the
supervisor as well. Okay. So I'm
providing the name as supervisor. But we
are not going to be using AI message. We
can do it. Nothing is going to happen.
But I'm just going to consider all these
different agents as humans. So that is
why I'm using the human message and then
I'm providing the name as well. All
right. So right after the enhancer, what
do we want to do? we always want to go
back to the supervisor right we can't we
can't go go to the end right after the
enhancer we always need to go back to
the supervisor so that is why I have
added this as a default value all right
I hope that makes sense let's move on so
now we have the research node so the
research agent node gathers information
using the tavly search it takes the
current task performs relevant search
and returns findings for validation okay
so here I'm going to be using the react
agent
As I said already, we can use the graph
that we've already built or we can use
this pre-built class that langraph
provides as well. Okay, so here I'm
providing the LLM that I want to use the
tools that are available to the React
agent and then this is going to be the
initial prompt. Okay. So I'm saying
identify key information needs based on
the query context. Gather all the
information. Optimize the findings.
Okay. Wherever you know you you want to
site the sources when possible you can
do that to establish credibility.
Focusing exclusively on information
gathering. Avoid analysis or
implementation. Provide thorough factual
responses without speculation where
information is
unavailable. Okay. So this is going to
go crawl the internet. It's going to you
know do a couple of loops. Okay. And
then it's going to return the entire
state again right here. So that is what
this particular invoke is going to do.
And this is going to return the entire
state with the messages list. Right? So
we're just going to take the last AI
message that this particular agent would
have returned. And then I'm going to
take the content alone and I'm going to
append it to our list. Okay? So to our
messages list, I'm going to append it
right here. Okay. So right after the
research agent is done with its work, we
are now going to the validator. Okay. So
this also we are hard coding right here
because if the research agent has done
its job properly, in that case if the
validator gives the sign off, okay, if
it gives the green light, we can just
end the workflow. Okay. So now let's go
to the validator. Okay. So uh validator
is somewhere down here. In the middle we
also have the code node. Okay. It's very
simple. You're a coder and an analyst.
focus on mathematical calculations,
analyzing, solving math problems. Right?
Right. So,
calculations. Okay. So, we are again
providing the Python rebel tool as well.
The same thing. And whatever result that
we get, we are going to take the latest
AI message, right? And then I'm going to
add it as a human message and append it
to our list. And again, I'm going to go
to the validator. Okay? I hope that
makes sense. And finally, we have the
validator. Okay? So, we have the
validator node. We are using a couple of
prompts for the validator. Okay, so
we're saying that first review the
user's question, the first message in
the workflow and then review the answer
which is the last message and just make
sure that and we're just saying that if
the answer addresses the core intent of
the question, okay, finish it or else or
else route back to the supervisor. Okay,
so that is what we're saying right here.
So either it needs to output supervisor
or finish. Okay, so that is what we are
providing right here. And in addition to
providing it right here we are also
using a paidantic model and we are f
further you know forcing it to only
provide supervisor or finish. So
specifies the next worker in the
pipeline supervisor to continue or
finish to terminate the reason for this
decision here as well. So as I told you
we only need the first users question
and the last answer right. So that is
exactly what we're doing right here in
this node. So we're taking the first
human question. Okay. And then we're
taking the last answer that is
supposedly the final answer. So we're
going to take the user's question and
the agent's answer and we are going to
provide all of this to the LLM. And then
we're also you know providing the
validator pyantic model as well. So
right here if the LLM returns go to is
finished in that case we can end the
graph. But if for some reason it does
not provide finish, okay, for some
reason this final answer does not have
the full answer to the user's question.
In that case, we need to go to the
supervisor again. So that is what I've
put right here. All right. So I hope
that makes sense. So far we've built out
all the different nodes. Now all that is
left is to combine put it all together
into a graph. So that is exactly what I
have done here. I'm adding all these
different nodes and then I'm compiling
it as well. Okay, so that is why we have
this final graph right here which looks
pretty good. So coming down, let's
actually go ahead and implement this
other example that we have. So the user
is asking for weather in Chennai. So in
this case, you know, the researcher
agent is going to kick in, right? You
can imagine because researcher is the
one access to the tab search tool,
right? So let's actually go ahead and
run this.
Okay, great. So, you can see the
supervisor is handing off control to the
researcher. Why? The user has requested
information about the weather in
Chennai, which requires collecting
current data. The task is best handled
by the researcher who can gather updated
information. So, this is the reason that
the supervisor is providing and using
the command we are directing the uh the
flow to the validate I mean the
researcher. Now, what is the researcher
saying? The current weather in Chennai
is as follows. It is providing the it is
calling the Tavly search tool. It is
providing the information right here.
And then finally the validator. Okay. So
the answer provides a detailed current
weather this thing. Okay. So that is it.
So we have the final answer right here.
And that is it guys. This is how we can
build out the supervisor agent
architecture system. So um do go ahead
pull the code run it by yourself. Try
putting in different prompts. So there
might be some prompts that that might
not work and that might require some
tweaking uh in the prompt. Okay. So that
might require some changing prompts here
and there according to the problem that
you are trying to solve. And that is
something that I will leave you to
explore. And that is it for this
particular section. I will see you in
the next
one. Hello guys. So in this video let's
actually try to understand streaming in
depth. So in the past few videos we did
use uh you know streaming here and there
but we never really deep dived into it
to an extent where we can actually build
you know production grade applications.
So I'll tell you what I mean. So this is
an application that we are going to be
building uh for the final capstone
project. So I've called it perplexity
2.0 because we are sort of mimicking
what perplexity does in their UI wherein
you know if I were to ask something like
uh or let me just say hi I'm Harish. In
this case, it's going to, you know,
stream the tokens. So, you can see that
as and when the words are generated by
the LLM, all these tokens are being
streamed to the front end. And also, you
know, if I were to ask a question like,
you know, um, what is the current uh,
weather in Bangalore, let's say. So, in
this case, you can actually see all of
the tool calls that are being done. So,
you can see that it's searching the web.
It's this is the query that the LLM has
suggested to the Tavly search and then
this is the response of the Tavi. It is
actually pulling all of the information
from these four different websites and
then finally it's writing the answer
right. So we can actually see that all
these different things are being
streamed to the front end and we can
actually see what the LLM is doing. So
the it's it's very important because you
know just imagine if there is no
streaming happening the user is not
going to know what is happening. So for
a good 20 seconds, 30 seconds, the user
is just going to stare at the screen. So
this is exactly what we're going to
learn in this particular video. So uh
you can see that if we're building a
responsive application for the users,
realtime updates are key to keeping them
engaged. Common use cases are, you know,
workflow progress, right? So we need to
get state updates after each graph node
is executed, LLM tokens as they're being
generated. So that is exactly what we
saw right here. And then custom updates,
right? So when we talk about streaming
in langraph so we have the stream and a
stream methods. So the stream and a
stream are sync and async methods for
streaming back outputs from a graph run.
So there are several modes you can
specify when calling these methods. So
we can say graph.tream. This is actually
very similar to how we say graph.invoke,
right? So we say graph.tream. We pass in
the uh the initial state and then right
here look at this. So right now we pass
in values but uh the common modes are we
can either pass in values this streams
the full value of the state after each
step of the graph and then we also have
stream mode updates. So basically this
streams the updates to the state after
each step of the graph. So let's
actually look at a pictorial
representation of the difference between
stream mode updates and stream mode
values. So if we set stream mode
updates, we are only going to look at we
are only going to see what is the value
that has been updated. Okay. So we know
that the messages. Okay. So this is node
1 2 3. So in node one it just updates A.
Okay. Initially the list would have been
empty. So node one just updates A. Okay.
Node two basically would have appended B
to the list. Okay. So that is why we are
just seeing B and the node C just
appended C to the list. So that is why
we are seeing just a b and c. And when
we look at mode values okay so after the
execution of uh the first node we are
going to get the entire state at that
particular point which is going to be a
list with the item a and then after the
execution of node two we are going to
see the entire state at that particular
point and the same thing for node 3 as
well. So let us actually put this in
practice. You can see that I have
already gone ahead and created a file
called stream events. Uh so right here
you can see that this is going to be the
picture that this is something that
we've built already right so we have the
start node and then we have the llm
sitting right in the middle and then we
are just providing one tool which is
going to be in this case the tably
search tool and then the tool node. So
let's actually go and look at the graph
structure. So we have the agent state
right here which is just going to
contain the list of messages. We are
initializing the tabularly search and
then we have the tools list right here
and then we are also you know
initializing the LLM and then we are
initializing the we are binding the
tools with the LLM as well and now we
have the LLM with tools right so right
here we are defining the model node we
are just going to invoke and send in the
entire list inside of this and then the
tools router is basically just going to
take the last AI message and then it's
going to see if there is any tool calls
uh needed at that particular point and
then if there are any tool calls that
the LLM wants to do in that case it's
going to direct the flow to the tool
node uh node and for this tool node we
are using the tool node pre-built um
method class right here where we are
also going to provide the list of tools
as well all right so we are just
connecting all these different things
and finally we have a graph like this
this we have already seen all right so
everything is looking good so coming
down so in this case we are actually
going to put the stream method to So
right here you can see that what I'm
doing is I'm saying appstream and then
I'm providing the input. So the input
message list is going to be what the
first human message is going to be what
is the current weather in Bangalore.
Okay. So I'm also setting the stream
mode to values. So if you remember when
we set stream mode to values. So what
that is going to do is right after the
execution of every single node it is
going to emit an event. Okay? It's going
to emit an event and in that event we
are going to see the entire state at
that particular point. So I am looping
through the events right here. Okay. So
this event is going to be an iterable.
It is a generator. So anytime there is
an event that is being emitted that is
being yielded from the generator, we are
going to get this event right here and
we are going to print it down here. So
let me actually go ahead and run it and
let me show you what it means. All
right. So you can see that initially we
have this one human message alone. Okay.
So that is the only thing that exists.
So why are we getting this particular
object? We are getting that object
because the start node just got
executed. So at this particular point,
the list only has one human message. And
then after the execution of this LLM
model, what should be there? There
should be two items in the list, right?
We have the first human message and then
we should have an AI message with an
empty content content field and then we
should have a tool call item because
this is a query that requires uh the LLM
to use the tavly search tool right so in
this case you can see that this is going
to be the first item and then if I come
down here you can see we have an AI
message as predicted the content is not
there because we actually have the tool
calls right so we can actually see the
argument here as well that the LLM
suggesting so it is suggesting okay use
this particular query current weather in
Bangalore right so that is all that is
present right here okay so it only has
two different okay it's pretty long but
it only has two items in this particular
list right now so what next what should
be the next thing next the control is
going to come to this tool node this
tool node is going to take that query
and then it's it's going to call the
tavly search tool and that is going to
output a tool message So that is exactly
what we see right here. We have the tool
message. This is going to be the output.
And finally it's going to come back to
the model and then finally there is
going to be an a message. So that is why
you know in the last last event that was
emitted the last event that was emitted
here. You can see that we would have the
final AI message as well. It's going
pretty long but you you get the idea.
All right. I'm not even going to try at
this point. Okay. All right. So great.
So we have the AI message right here,
right? The current weather in Bangalore
is a sunny is sunny with a temperature
of
30.1Â°C. Right? So we have four different
events that were emitted and each of
these events had the full state after
the execution of each of those nodes. So
I hope that that made sense. We can
actually go and we can actually say
something like messages right here. And
this is this would probably give you you
know just the messages alone. Now let's
actually go ahead look at the updates.
Okay. So if you remember updates is just
going to give you what is that new item
that's been added to the list right so
it is also going to provide you which
node actually did the change as well so
let me actually run this and show it to
you so in this case okay so you can see
that the first change was made by this
particular model node and then it
updated something okay so it just added
this AI message at the very start okay
so that is all that we see right here so
this model node just added this AI
message and this tool node just added
this tool message and then this again
this model node added this AI message
which uh which has the final answer
here. So let's actually take it to the
next step. So if you remember what we
did in the UI, we didn't just we were we
are not interested in getting the entire
state after the execution of every
single node. That's not what we're
interested. We are interested in getting
every single token that is being
generated by the LLM. Right? So that is
what we did right here. that is how we
can actually achieve this sort of a UI
right so to help us with that there is
there are other methods okay so let's
look at that so let's actually look at
some slides as well so in production
applications we usually want to stream
more than the state right so in
particular with LLM calls it is common
to stream the tokens as they are
generated so we can do this using the
aream events method which streams back
to uh events as they happen inside the
nodes okay so we are not interested in
what happens at the end of the node. We
are interested in what happens within
the node. Right? So each event is a
dictionary with a few keys. Okay? So uh
in this case we are every single event
is going to be addict and it's going to
have an event property name data and
metadata properties. So the event is
going to be this is going to be the type
of event that is being emitted. The name
is going to be the name of the event.
Data is going to be the data associated
with the event. And we are also going to
have some metadata about which
particular node emitted that event as
well. I hope that makes sense. Let's
actually now go ahead and put this
method to use. So uh right here coming
down you can see that I have used this
particular method right here. So I'm
going to be using something very simple
right here. You know message hi how are
you? We are not going to be using any
tool calls at this particular point
because we are learning. So I'm saying
apparam events passing in this input.
This also takes in two versions. Okay,
so we've got version one and version
two. I'm going to be using the latest
version. And all that I'm going to do is
I'm going to loop through all of the
events that are getting emitted and then
printing it. Okay, so make sure to use
this particular async because this is
going to be an asynchronous operation.
All right, so in this case, let's
actually go ahead and run it and you
should be able to see that you know the
events are actually getting generated.
You can see that there's a lot of events
that are getting generated. We can
actually use all these different things
and then accordingly we can show it in
the UI. So right here you can see that
we these are the event types right. So
these are the event types. So onchain
start onchain end on chat model stream
right? Okay. So onchain end. So these
are the event types and what we are
interested in is the streams the event
that are emitted from the LLM. So these
are the things that are emitted from the
LLM. Okay. So on chat model start and
then on chat model stream. So inside if
you if you look at the data that is
associated with this on chat model
stream you should be able to see in this
particular chunk we should be able to
see this AI message chunk. Okay. So this
is going to be the response. So you can
see that it is saying hello I'm here and
ready to help you with any information
or questions. So this is going to be the
response of the LLM, right? You should
be able to find all these different
tokens that are emitted inside of the
data uh in the in this particular type.
So if you wanted to we we can also
always use these particular types right
we can always use these particular event
types and then isolate it and yield it
isolate it and send it stream it to the
front end. Uh so what I mean by that is
right here you can see that I'm only
interested in this particular event
called on chat model stream and then I'm
just going into event data chunk and
then I'm printing it. Okay. So in this
case if I run it it's going to be the
same uh query as well. So if I run it
you should be able to see just that
being printed. Hello I'm just a computer
program so I don't have feelings but I'm
here right? So if if you want it to be
in a same line, this print method also
provides this end. So right here we can
just say uh we can just give an empty
string. Okay. So in this case, what's
going to happen is uh it's not going to
go to the next line. Instead, it's going
to concatenate everything. So right here
you can you would have actually noticed
that you know it did not generate word
by word. And the reason why that happens
is because you know it actually this
print method by default it's going to
you know store some uh chunks together
only then print it. Okay. So to override
that we can actually use this flush true
and this is just going to flush uh
everything from the buffer as and when
you know some value is actually stored
in it. So in this case if I actually go
ahead and run it you can actually see
word by word it is being generated. So I
hope this makes sense. We can actually
use these events. We can actually use
the metadata. So if I actually go to the
right side. Okay. So inside of the on
chat model stream. So this is the data.
This we know we can actually also go to
the metadata. Okay. So we have the
metadata right here where we also have
information about which particular node
emitted. Okay. Which particular node is
responsible for this particular event.
So in this case the langraph node that
is responsible for this event is the
model node. Okay. So we can also use
this particular name the uh this
particular langraph node and we can also
add that in one of our conditions and
then yield we can isolate that and yield
to the send uh stream it to the UI.
Okay. So it is completely in our
control. We can actually make use of all
these different you know um properties
like event data metadata etc etc. So by
the end of this course we are going to
be actually putting all these different
concepts to use and we are going to be
building this particular application. So
I'm pretty excited and I'll see you in
the next video. Uh hello guys, welcome
to this brand new video. In this video
we are going to be building a full stack
application using lang graph in the back
end. We are going to be using fast API.
We are going to be using React in the
front end and we are going to achieve
something that is very similar to what
Perplexity does in the UI. So I'm not
sure if you've used Perplexity before,
but it basically you know it it's
basically like a very uh smarter version
of Google. You can think of it like
that, right? So if I were to ask some
question that requires the AI agent
powering perplexity to actually make an
internet search the user I can actually
look at what is happening on the UI. So
you know I can see something like you
know it's searching the web right now.
It is actually reading through all these
URLs. It is actually giving a response
as well. So I I would be able to see all
these different things. So that is
exactly what we are going to be building
as well. In addition to that all these
you know it's going to remember our
previous conversations all these
different words as and when the tokens
are being generated it is going to be
streamed to the front end. So the user
is not going to wait for anything. So
let me actually quickly demo it for you.
So let's test out the first thing. Okay.
So there's a few errors right here. We
don't have to worry about it. But uh
let's you know just say you know hi my
name is Harish. Right. So in this case
it's basically just going to you can see
that every single word is actually being
streamed and it is shown in real time.
So it says hi harish how are you? How
can I assist you today? In this case you
I can ask like what was my name
again. So it should be able to tell okay
your name is Harish because it remembers
the previous conversations. So this is a
very small part. Now let's actually ask
ask the agent something that is a little
more tricky. The agent now has to
actually make a Google search, an
internet search. Uh only then it can
actually respond to it. So let's ask
something like uh when is the next Space
X launch? Okay. So right here it is
actually going to you know search the
web. So you can see it's searching the
web. This is the query and then uh it is
actually reading from all these
different URLs, all these different
pages. And then finally it is saying
that the next SpaceX launch is scheduled
for Sunday, April 6th. So this is going
to be a very exciting fullstack
application. So in the back end uh in
land graph we are going to be learning
how exactly to build this graph. It's
going to be very simple. We are going to
be learning checkp pointers for memory
for persistence. We are going to learn
how to actually set up streaming. And if
for some reason you are not a front-end
developer, you don't want to you're not
interested in learning about React etc
etc. That is absolutely fine. you can
just go ahead steal my code, copy paste
it and it would just start working out
of the box. So I'm pretty excited about
this application. So first thing let us
now go ahead and start building out our
agent
graph. So let's now go ahead and build
out our LAN graph agent graph. So once
we build out our graph, we can then wrap
our graph using an API endpoint and then
we can connect it with our front end.
Right? So you can see that I've already
gone ahead created a folder called
Publexity 2.0. I've created two folders
client and server. So right now the
client and the server is empty. So let's
now first go ahead and build out our
server side code. So I'm going to open
it using my Visual Studio Code. So let
me just zoom in a little bit. So the
first thing that I'm going to do is I'm
going to install virtual environment.
And we need the virtual environment
because we want all our packages that we
are going to be installing later on to
be isolated in this one particular
location. We don't want it to be
available globally. So to initialize my
virtual environment I'm going to say
python 3. I'm saying python 3 here
because I'm working with a Mac right
now. If you're working with a windows in
that case you can you can just say
python. And then I'm going to say dash
m. This stands for module. I'm using the
venv module. And the folder that I want
created right here I want it to be named
as Venv as well. So right here in a
while you can see that we have the Venv
folder created. All right. So the next
thing is we have to activate this uh
virtual environment. So right here you
can see that the the path is not really
pointed to VENV. It is still pointed to
base. So in order to activate this uh
since I'm working with a Mac again I'm
going to say source. Okay. I'm going to
go inside of the venv and then I'm going
to go inside of this bin and then I'm
going to invoke this activate file.
Okay, so this is going to activate my
virtual environment. As you can see
right here in Windows, it's a slightly
different command. You can look it up.
Okay, it's pretty simple. All right, so
now that we have the virtual environment
initialized and activated as well, now
I'm going to go ahead and I am going to
create an app dot uh Jupyter notebook.
Okay. And now there's a couple of
packages that we might have to install
uh in order to build out our graph. So
I'm going to go ahead and install uh
first thing is I'm going to install lang
graph. We absolutely need lang graph.
And then I'm going to be using lang
chain open aai chat model. Okay. We can
always use other chat models. We can use
uh you know Gemini. We can use uh you
know gro to access llama models. But in
this case I'm going to be using the uh
langchain openai chart model. And then
we need the lang chain core package. And
then we also need python.env because we
want to you know make the environment
variables available inside of our uh
Jupyter notebook right and then we also
need fast API and then we also might
need lang chain community because in the
langchain community we can actually get
access to you know uh a lot of tools
like you know the tab search tool etc
etc. So if at all we need something else
in the future, we can always come back
here and then we can install it. So
let's go ahead and install
it. All right, great. So now that it has
been installed, let's go ahead and
import a few things. And guys, before we
start coding this, I expect you to know
LN graph uh already. Okay, so I expect
you to know langraph lang chain, the
fundamentals of it. I expect you to know
how to build a react agent. I expect you
to know the basics of streaming. I
expect you to know you know how
persistence works etc etc. If you do not
know any of these fundamentals I have
created a course that I'll attach the
link in the description below. That
course is going to take you from
absolute beginners to you know prolevel.
We are going to be covering you know
what are the different types of agents
how to build a react agent how to do
streaming how to do persistence how to
you know what are checkp pointers how to
do human in the loop you know how to
build complex architectures like you
know the supervisor agent. So I would
have covered everything. So I would
expect you to know it already. If you do
not know it, please go ahead watch that
and then come back to this video. All
right. So now let's actually go ahead
and start importing our uh you know
modules. So the first thing is we are
going to need um from typing. Okay. So
from this we are going to need the type
dict and the annotated and the optional.
So basically why do we need these
things? We need these things because we
want to define the state of our agent
graph, right? And then we are obviously
we are going to need lang graph and
inside of the graph module we are going
to need the add messages method. This is
going to be the reducer method which is
going to let us easily update the graph
state and then obviously we are going to
need the state graph and then the end
node. Okay. So what's the next thing and
the next thing is we are going to need a
chart model. uh basically chat models
are uh tools that you know langchain
gives us which lets us interact
basically like interfaces which lets us
interact with LLMs right so in this case
I'm going to be using open AI's chat
model if you don't want to pay for it
you can always go for you know Google's
chat model or you can go for you know
llama's chat model all right so just go
go to Google search for you know chat
model Google and that in that case it's
going to give you a different chat model
you can import it and use it but in this
case I'm going to be using the lang and
chain open AAI and I'm going to be using
the chat open AAI chat model. All right.
And then obviously we are going to need
the ENV as well because we want to load
all our environment variables in this
file. And then we are going to need uh
the Tavly search tool. The Tavi search
tool is what is going to let our agent
access the internet. So in this case we
can actually go to Langchain community
and then we can say tools and inside of
this I can you know access the tab
search. So I can say tab search results.
So the last thing that I want is the uh
you know um the ability to persist our
graph. So what I mean by that is we need
you know memory introduced. So I'm going
to be using a checkp pointer that is the
go-to way of introducing persistence in
our graph. Okay? Because we want our
application to remember what the user
said previously. Right? So in this case
I'm going to be using lang graph dot
checkpoint
dotmemory. Okay. So from this I'm going
to be using the memory saver. And then
after this we also would need the UYU ID
because we will have to create unique
ids for each user session. So in this
case we might need the UU ID. Let's use
UU ID 4. All right. So we are done with
most of our imports for now. So now I'm
going to go ahead and I'm going to
create an ENV file. And right here we
need to add a couple of uh you know
environment variables for you know the
Tavi search. Okay. So for the Tavi
search results, we might need the API.
For the chat open AI as well, we might
need the API key. So for the open AI API
key, I'm going to go to
platform.openai.com. I can go inside of
uh the profile and I can go inside of
API keys right here. And now I can go
ahead and create a secret API key. So I
can just say perplexity and I can go
ahead I can have the permission set to
all. And I can go ahead and copy this.
And the same thing for Tavly search as
well. So I can just go to this
particular link, log in and then I can
just go ahead. You can see that I've
already created a few tavly you know API
keys. I can just copy this thing come to
the env file. So what would be the uh
the the environment variable name. So if
I go inside of the chat open AI, you can
see that it requires this particular uh
environment variable set. Okay. So just
put the API key that you would have
copied there inside of single quotes or
double quotes right here. And then the
same thing you will have to do it for
the tably search as well. So if you come
inside of here you can see that this is
the environment variable. Okay,
environment variable key. All right
guys, so I've already populated all the
environment variables in my file. So in
this case I'm going to go ahead and load
all the environment variables. And the
first thing that I'm going to do is I'm
just going to make a simple LLM call and
make sure that everything is working
fine. So in this case I can just say
this is going to be the model. So I can
say uh the model that I want to use is
going to be
GPT40. And now let's go ahead and invoke
this model. And we can say something
like um you know give me a joke about
cats let's say. Okay. So let's get the
response right here. Then let's also
Okay. So let's try to run this. So you
can see that running cells with this
virtual environment requests the IP pi
kernel package. So let's go ahead and
install that as
well. All right. So now you can see that
it is running and it should give us a
joke about cats. All right, perfect. You
can see that we are getting an AI
message back. We saying that why was the
cat sitting on the computer? So
everything is working fine. If at all
you're getting an error saying that you
know insufficient balance or something
like that, just go ahead and top it up.
You can go to the billing and you can
top it up. uh it's going to last you
anywhere from 3 to six months. I paid it
several months ago and I and you can see
that it's still you know I've still got
a lot of uh balance. All right, perfect.
So now let's get rid of this and now
let's also actually go ahead and test
out our tavly search. Okay, so I'm going
to initialize our tavly search. So in
this case I can say search tool. Okay,
so search tool tavly search results and
in this case I can just set the max
results to four in this case. Okay. So
now I can just go say search tool. Okay.
What's happening? Search. Okay. So I can
just say search tool.invoke. Okay. I can
say what is
the weather in um let's say what's
what's the weather in Bangalore. So in
this case this is also going to give us
a proper response. Okay. So I'm just
going to test out all the environment
variables. Make sure that everything is
working fine.
All right, perfect. So now you can see
that we are getting an array of, you
know, four different objects. Each of
these objects are going to have title,
okay, and the URL and the content. Okay,
you can see that there's quite a lot of,
you know, content. Basically, the stavly
search is very similar to a Google
search. Okay, it searches for this
particular query. Uh this is the URL
that it pulled all this particular
content from. So this is the query and
this is the URL that it pulled all the
content from. Right? Okay. Okay, so
we've got a lot of different information
which then later the agent can actually
use. Okay, so the agent can actually
make use of this particular tool
whenever it needs to, you know, needs
that particular data, live data and then
it can actually make some informed
decision. All right, perfect. So now
that this is also working fine, I'm just
going to clear this out again and then
we can just say tools and right now
we're just going to have this one search
tool in the list of tools. All right, so
what's the next thing? So next thing is
we can actually go ahead and initialize
our memory saver as well. Okay. So this
is going to be the checkp pointer that
is going to introduce persistence in our
chat application. Now let's actually go
ahead and create a an LLM instance that
is going to have access to these tools.
So I can just say llm with tools. Okay.
So I can just say model dotbind tools
and then I can pass in all these list of
tools right here. Okay. So what
basically what this is going to do let
me actually put it in a separate code
block. Okay. So what this is going to do
is now if I were to actually you know
llm with tools llm with tools. Now if I
ask a question right here that is
actually going to require some live
data. Okay that is going to require live
data. In that case the LLM is not going
to give an immediate answer but instead
it is actually going to call the search
tool. Okay. So if I were to ask
something like you know what is the
current
weather in Bangalore. Okay. So in this
case it is going
to let's see what it actually returns.
Okay. Tools is not defined and that is
because we have not run this. All
right. So in this case you can see that
the AI message does not have any
content. But instead if I go to the tool
calls. Okay. So where is the tool calls?
Let me actually search for it. Tool
calls. All right. So in this tool calls
you can see that in this array there is
going to be one tool call. So the LLM
actually wants to call the Tavi search
results JSON because we've provided it.
We've bound all the tools right here.
Okay. So it is it actually wants to call
this particular tool and this is going
to be the query that the LLM is
suggesting as well. Okay. So now we have
the LLM with tools instance as well.
Okay. Which is perfect. Okay, so what's
the next thing? So now let's go ahead
and build our graph. Okay, so let me
copy paste some code right here. All
right guys, I've copy pasted a bit of
code right here. So before I actually
walk you through this particular graph.
So I'll show you what is the structure
of the graph that we have built. Okay,
so if you've actually followed along
with uh my LAN graph course, okay, this
is something that we would have done a
million times already. So you can see
that we have the start node at the very
start and then we have the model. Okay,
so the model anytime the user question
is something that requires the model to
make a tool call in this case the
control flow is going to go to the tool
node. Okay, so in the tool node this
particular tool node is going to make
use of the tably search and then it's
going to you know take the query that
the LLM has provided. It's going to make
the internet search and then it's going
to return the response back to the LLM.
Okay. And if the internet search is
sufficient in that case the LLM is going
to you know end this particular graph.
Okay. So this is what we have built
right here. So if I scroll up here you
can see that we have you know this is
going to be the initial state. So to
achieve this particular thing we just
need the messages list and we are going
to be using this particular add messages
reducer function. So if you do not know
how this reducer function works, if you
do not know the different types in which
we can update the state, you know all of
these things I would have covered in the
course. So do check it out. All right.
So, so initially we have the first uh
node which is going to be the model
node. Okay. So if I come back to the
diagram, you can see that that is going
to be the entry node right here. Right.
So we have this particular entry node.
So this node is going to get the entire
state right here. And we are going to be
using the llm with tools. We are going
to invoke and we are going to pass in
the entire list of messages and then we
are going to update the state with it.
All right. So uh also another thing that
I want you to note here is that we are
going to be using the async await
keywords and why do we need to do that?
Because when we are dealing with
production applications and when there's
going to be a lot of concurrent users in
that case you don't want to block the
thread. Okay. So we want multiple
threads to be working concurrently. So
in that case we will have to use async
await and we will also have to use the a
invoke method and not the plain invoke
method. Okay. The functionality is going
to be very similar. All the all that
we're doing is we're going to be using a
invoke and adding async and await in all
our nodes. Wherever there is an async
operation, wherever there is an LLM call
that is being made or an API call is
being made, we have to do async await
there. So right after this particular
model is executed, we are actually going
to look at this AI message right here.
Okay, so that is what this tool router
is going to do. Okay, so it is basically
just going to take the latest AI
message. Okay, and then it's going to
look at the tool calls. Does it have any
tool calls or not? If it has some tool
calls, in that case it is going to route
the flow of the graph to the tool node
node. Okay, so or else it's it's just
going to go to end. So let's come inside
of the tool node. Okay, so this is going
to be the custom tool node that handles
tool calls from the LLM. Again, we are
going to take the you know the last AI
message. Okay, so this is going to take
the last AI message and then we are
going to go inside of the tool calls and
from that tool call we are going to get
the name of the tool call. In that case,
it's going to be the Tavly search
results JSON because that is the only
tool that we've made that we've provided
to the LLM. We can also get the args. So
in this args, this is where the query uh
you know the internet search query is
going to be located. So we are going to
get a reference for that as well. And
then we also need the tool call id. And
then we are going to execute the search
tool with the provided arguments. So we
are going to provide all the queries and
all the uh you know everything that is
inside of the args. And then this is
going to give us the search results. So
we are just going to stringify it and
then we are going to add the tool node
in the tool message and the tool name as
well. Okay. So now we have the final
tool message that we are going to append
to the list of tool messages and then we
are going to update the node as well. So
this is going to be the uh function uh
the tool node function definition. There
is also another pre-built you know uh
tool node class that langraph provides
because this is such a common thing
right okay when whenever we are working
with agents uh you know calling this
particular code block is going to be
such a common thing. So langraph
actually wrote a class and then just
gave it to you for us to use. So if I
actually go back to you know this is
part of the langraph series that I had
worked on. So here you can see that in a
lot of different sections uh we would
not have actually written this entire
thing. We would have just used uh this
tool node right here. Okay. So this lang
graph pre-built import tool node and uh
you know I would have just like in
instantiated this tool node passed in
the tools and this would have been the
tool node. Okay. So exactly what we've
done right here. Okay. So we can either
do this or that but I'm doing it this
way because this gives you a lot more
you know transparency on what is
happening inside. But if I actually go
inside of this particular tool node, you
can see that this is basically what
we're doing right here. Okay. So we're
taking the latest AI message. We're
going inside of the tool calls and then
we are invoking it. And that is it.
Okay. So that is exactly what we are
doing right here as well. Perfect. So
coming down, we are going to be using
state graph. Uh we are building the
graph. So the first node is going to be
the model node. Okay. So that's going to
be the entry point. And then we also
have the two node. And then we're adding
a conditional edge. So right after the
model is executed. So right after the
model is executed, we have to look at
whether we have to go here or here. So
that is why we have this tools router
right here. Okay. So tools router is
going to decide if this tool calls is
present. At least one item is present or
not. If something is present in that
case, we have to go to the tool note or
else we have to go to the end. Okay. So
I hope this makes sense. And finally
during the compilation step we are going
to provide the memory the checkp pointer
right here. So that is it. So now let's
actually go ahead uh let me run this as
well and let's now go ahead and put this
to test. So I can say
graph.invoke. Okay. So in this case you
know in the messages array I can use
this human message. Okay. So I can ask
something like um how are you doing?
Okay. Okay. So let's just test it
out
response. Okay. So we are getting an
issue right here. Okay. So this is
because Okay. So this is because we are
using the uh the checkpointer right
here. Right. So in this case since we're
using a checkpointer a checkpointer
always requires a session ID. Okay. So
you know the the graph needs to know
which session is it storing the uh
particular state for. Right? So in this
case we have to actually go ahead and
create the uh the config object. Okay.
So let's say config inside of this we
have to say
configurable. Okay. In this in this we
have to say thread sorry thread id and
right here we can say one or two or
three and then we can provide this
config right here like this. Okay. So
now everything should work fine. Okay.
Okay, so it's still not working and that
is because Okay, so to fix this we just
need to say a invoke and then we have to
await this as well. And now let's also
you know go ahead and change it to let's
say five or something. And now let's say
you know let's introduce ourselves and
then let's ask what my name is so that
we can also test out the checkpointer.
So hi I'm Harish.
Okay. So in this case, okay. So you can
see that we have the first human message
and then the AI message is saying hello
Harish, how can I assist you today? So
let's actually test out the persistence
if it is working or not. So I can say
what is my
name. So now you can see that all the
past conversations are being carried
forward, right? So we have the first
thing that I sent and now it is actually
you know remembering that my name is
Harish. Okay. Okay. So now let's also
try to uh you know ask a question that
requires the agent to make an internet
search. So in this case I can say uh you
know when is
the next space x launch. Okay. So in
this case it would actually have to go
uh you know uh utilize the tab search
tool and it would have to fetch uh
internet results. Right. So you can see
that right after this particular
question you can see the AI message to
content is empty. In in this case it is
actually you know suggesting to call a
tool right. So when the tool calls you
can see that uh it is saying that you
know call use this particular tool and
pass this particular query inside of
this particular tool as
well. Okay. And then finally what
happens is we have tool message. Right.
So this is going to be the return uh the
response from the tavly search. This is
going to have all the information. You
can see it has pulled information from
next
spaceflight.com. It pulled it from, you
know, a lot of different URLs, right?
See, it pulled from space launch
schedule.com as well, right? And then
finally, the last AI message is going to
have the next SpaceX launch is scheduled
for this particular date. Okay? So,
everything is working fine. And finally,
I also want to show you how streaming
works because uh uh you know this is not
what we want, right? We actually want
every single token as in when it is
being generated to be streamed to the
front end, right? So what I mean by that
is so whenever I ask a question here uh
you know let's let's say what is the
current weather in Chennai itself. If
you remember every single word was like
streamed to the front end, right? It
wasn't like you know abruptly there. So
this is going to give you abruptly. This
is not what we're interested in. We want
data to be streamed. So let's also get a
quick rough you know idea about how
streaming works as well and then we can
all put it together later on. Okay. So
I'm going to create another block. So in
this case I'm going to be using I'm
going to be saying graph in this time.
This time I'm not going to be using this
a invoke or invoke that is going to
abruptly give you the final response.
That is not what we want in this case.
Langraph provides another method called
a stream events. Okay so this is what we
are going to be using. So let me
actually show you how this works. So in
this case also we can use an input.
Okay. So where is the input? Okay. So we
can actually copy the same input. So
let's put it here. So in this case I'm
just going to introduce myself here. Hi
I'm
Harish. All right. Okay. So what's the
next thing? So similar to what we do
right here we are providing the config
right. We have to do the same thing here
as well. Okay. So I can give it a
different thread ID this time. I can say
six this time. All right. And now what's
going to happen is this is going to
return us a generator uh function. Okay.
This is going to give us uh something
like you know we can say events. Okay.
So anytime there is a an event getting
emitted, we can actually loop through
this iterable and then print it. Okay.
So I can say for event in events and
then I can print this particular event.
Okay. So I hope that makes sense. So
this is going to be an iterable. Anytime
there is an event getting emitted, we
are going to this function is going to
run. All right, this this particular
block is going to run. So now if I run
it, I hope that everything is looking
good. Okay, we have not provided the
config right here. Okay, so we have not
provided the config. And one more thing
that I forgotten is we also have to
provide this version. Okay, by default
it's going to be V2, but still let's
also go ahead and provide the version as
well. Okay, so there is version one.
Okay. And there is version two. There's
a lot more information being emitted in
version two. And that is what we are
interested in. So let's actually go
ahead and run this and make sure that
everything is looking good. I'm not sure
if we we actually have to await it here
as well. Okay. I'm not sure if we have
to do it here or here. Uh let me
actually run it. Okay. So we are getting
the error. So in this case it is saying
that we have to actually use the async
await. Okay. So right here we are not
using that right. So I can do something
like this. So I have just put the async
for event right here. So we're using
async for to iterate over the async
generator. All right. So now you notice
that if I were to now run this, you
should be able to see a lot of different
events that are being generated. So you
know that this is the question that I'm
asking or rather I'm just saying I'm
harish. In this case you can see that a
lot of these events are being generated
right here, right? So you can see
there's a lot of events. So you can see
that we have onchain start. Okay. So we
have onchain on chat model start on chat
model stream. Okay. So all of these are
event. Okay. So this is a particular
event item. This is another particular
event item. Right. So in one particular
event object we would have the event
type. We we would have data. Right. We
would have another property called
metadata as well. These are some things
that I would have deep dived in the
langraph uh course but I'll just give
you a quick uh you know um overview as
well. So you know it's it's I'm not able
to search for it. Meta data. Okay. So
you can see that we have metadata here
that is going to give you information
about which node this particular event
was emitted from. Okay. So we have all
these different information through
which we can actually isolate uh these
events and then we can yield it to the
front end. Right? So I can actually do
something like this. So I can say you
know if event is equal to on chat model
stream because we're only interested in
this particular event type. Okay. Okay,
I can just isolate that particular thing
alone and I can go inside of the data.
So instead of the data, I can go inside
of the chunk and then I can just extract
the content alone. So if I clear this
file and if I change it to let's say
seven and if I run this, okay, you
should okay, you should it's it's pretty
fast. Uh so give
me a 100word, you know, essay on um
climate change, let's say. Okay, change.
So you should be able to see the words
that are getting populated. Okay. So,
okay. So, you can see that every single
word was getting, you know, streamed.
Okay. So, this is what we're interested
in, guys. All right. So, next up, we are
going to transfer all these files. Okay.
So, when we are dealing with production
applications, we are not going to be
working with Python notebooks, right?
So, we are going to be creating a plain
Python file. We are going to be, you
know, importing fast API. we are going
to create an endpoint uh around this
agent so that the front end can start
accessing it. Okay, so that is what
we're going to be looking at
next. Uh hello guys. So in the last part
we built out our agent graph but we
built it out using the Python notebook.
So basically what I have done right now
is that I've created a new file called
app.py. I basically transferred all of
these code okay to a separate okay
Python file. Okay. So no difference at
all. I have imported the same exact
imports. Okay. So you can see that I'm
initializing the memory, initializing
the state. Okay. So we we have the model
node. We have the tools router and then
we have the tool node as well. And then
I'm connecting all these different
things together. So up until here we've
completely already seen. Okay. So coming
down this is the new code that I have
added. Okay. So it's going to be very
simple. uh all that I'm doing is I'm
importing fast API okay from fast API
I've installed that as well okay so
coming down you can see that I've
initialized the fast API app I'm adding
some middleares okay so this is
basically just to you know for our
browser for our client to be able to
communicate with the server endpoint
without any issues without any cause
issues I have added these things all
right so here look at this thing so we
have the chat stream this is going to be
our endpoint so whatever new message
that the user in the front end is going
to enter that information is going to
come right here. Okay. So that is what
we are uh we are accepting in this
particular argument and the second
argument is going to be the checkpoint
ID. Okay. So this checkpoint ID uh this
is going to be provided as a query
parameter. Okay. So if at all the client
does not provide a checkpoint ID that is
completely fine. In that case we are
going to default to none. Okay. So we
are going to default to none. But if
they do provide something in that case
we are going to accept it and okay so
let's let's pay attention to what we are
doing right here. Okay. So if it was a
simple if you're not thinking about
streaming or anything like that we can
just go ahead and you know we can just
say graph invoke or a invoke in this
case we can just provide the message
inside of this and then we can uh you
know um use this particular checkpoint
ID as well. It's going to be very simple
but since we are interested in streaming
right now. Okay. So in fast API we are
going to be making use of this concept
called server sent events. Okay. Uh so
okay so it's going to be server sent
events. Okay. Server sent events. This
is for your own education. You can
definitely Google it, research more
about it, learn more about it. But
essentially serverent events is slightly
different from websockets. Okay. So what
is the difference between serverent
events and websockets? Web soockets
essentially you know establish a
consistent connection between the server
and the client and both the client can
communicate the server as well as the
server can communicate with the client
right it is birectional but in our
application in our project that is going
to be an overkill what we need in our
chat application is going to be server
sent events so what this means is that
the server can actually like communicate
with the client okay so it's going to be
one directional not birectional. This is
what we need. So the way that we can do
this in fast API is that we can use this
streaming response class that fast API
provides. Okay. So streaming response uh
so the first thing that we have to
provide in here is going to be the
generator function. So the generator
function is just going to you know emit
events. Anytime it receives something
it's just going to emit it. And then the
second thing this is very very
important. Okay. So in the media type we
have to set it to text event stream.
Okay. So this is going to be the
protocol that we are going to be using
the serverent event protocol. All right.
So let's actually take a look at this
generator uh you know um function. Let
me now expand this thing. All right. So
so first off we are going to check if
the checkpoint ID is there or not. So if
it is a new conversation we have one if
block right here. If it is an existing
conversation, we have another else block
right here. So let's look at the first
uh block. Okay. So the first thing is we
are going to generate the checkpoint id
and then we are going to construct the
config object and then we are going to
be using the same aeream events method
that we just looked at you know in the
IPI notebook right. So we are going to
be using the aream events. We are going
to provide the message that is received.
We are going to use the same thread ID.
Okay. And then this is how we are going
to be yielding. Okay. So that this is
what uh you know this is a generator
function is going to emit the events or
we can also say it's going to yield
events. So if we put yield right here.
Okay. This is what we are going to be
sending back to the client. So we are
going to be looking at exactly how the
data looks like uh using uh a swagger
docs. Okay. Fast API provides a very
easy way for us to actually look at you
know exactly what is being emitted from
the API. So we'll take a look at that.
So here you can see that we are uh the
type is going to be checkpoint and we
are going to be providing the checkpoint
ID. So you might think that why are we
using all these slash basically just to
you know escape this particular double
quotes. Okay. So we can't use double
quotes within double quotes without
escaping it like this. Okay. So that is
why we are yielding it like this. And if
you're wondering why we are saying data
right here and why we are saying /ashn
/ashn right here. So we have to say that
because uh that is basically just how
the server sent event protocol works.
This is how the browser is going to know
that this is going to be a server sent
event. Okay. So we just need to follow
this. All right. So now we've seen this
particular if block. Let's now move on
and let's understand the else block. So
let's say this is going to be a
continuing conversation. In that case we
are going to be using the checkpoint ID
that we get from right here. Okay. So we
are getting it from right here. So the
the server knows exactly which session
to continue the conversation. Okay. And
then we the same thing is again
continuing. Okay. The same thing and
coming down uh right here you can see
that we are getting reference to the
event type and then based on that we are
doing a a couple of things. Okay. So for
every single different event type, we
are structuring it in a certain way and
then sending it to the client so that
the client can differentiate between
each event type right. So the first
event type that that we are interested
in is the onch chat model stream. This
is going to be a very basic uh this is
going to be the most common thing that
we are streaming. So what I mean by that
is let's come back to our Jupyter
notebook. So if you remember okay so let
me say okay I've just said that give me
a 100word essay on climate change. In
this case, you can see that this is
going to be you know the events that are
going to be emitted when the LLM is
actually generating something something
right. So this is when the LLM is
starting to generate and this is when it
has started generating already. So that
is why we see you know the empty array
empty string climate change refers to
significant long-term changes in global
or region. So this is what I am going to
be grabbing next. Okay. So how can I
grab it? So you can see that I'm passing
the event data chunk inside of this
particular method. It's it's just going
to be a fallback method. Let me show you
what it means. But I'm going into event
data and chunk. So this is going to be
the entire event. I'm going to go into
data. I'm going going going to go into
chunk. Okay. So I'm grabbing this AI
message chunk. I'm passing that inside
of this method. All this method is going
to do is it's just going to check if
this chunk is an instance of AI message
chunk. Okay? So there might be some
situations where you know there could be
something going wrong. In that case I'm
just going to you know um you know show
an error. Okay. So that is all that this
is doing. So we're grabbing this
content. So this particular content I'm
grabbing that particular content and
then I'm just going to you know escape
single quotes and new lines for safe
JSON parsing is just you know this is
safe content and finally I'm going to be
yielding this particular data to the
client. Okay. So uh we are going to have
okay so we are going to have data and
the slash and slash and this is just
protocol okay the server sent events
requires you to add this and this at the
start and at the end and this is going
to be the JSON that we are going to be
sending to the client okay so this is
going to have the type okay it has the
uh escape uh characters okay because we
are using double quotes within double
quotes but the type is going to be
content and the content itself is going
to be this particular save content.
Okay, so we are emitting this thing.
Similarly, we are also emitting another
event type called onch chat model end.
Okay, so we're emitting some data. Why
are we doing this? So, let me actually
show you why we are doing this. U so in
this case, I'm going to um you know
change the thread ID and I'm going to
say I'm going to make use of you know
I'm going to force the agent to use a
tool call. Okay. So, so that I can show
you why we're doing that. So, I can say
when is the
next Space X
launch. Okay. So, if I run
this. Okay. So, we can see that the LLM
has started to generate we. So, right
here you can see that uh you know all
these on all these on chat model streams
the content is not going to be there.
Okay. because uh you know if I come back
to the uh end okay so this is where the
LLM is done generating something so in
this case you can see that the content
is going to be empty and it has
completely shown that we have to call
this particular tool call okay so you
can see that it is saying that use this
particular query use this particular
tool and then do a an internet search
okay so this is what we are grabbing
okay so we're grabbing this thing so
that in the UI you can see that we are
showing this particular Okay, we are
showing this particular thing. Okay, so
searching the web and the next SpaceX
launch date. So in order to show both of
these things, we need to grab this on
chat model end. Okay, so sometimes the
tool calls might be there, sometimes the
tool calls might not be there, but we're
only isolating when there might be tool
calls. So that is exactly what we're
doing right here. So we're checking if
the tool calls are present or not. Okay,
if it is not present, it's going to be
an empty array. and then we are looping
through it and then we're checking if
there is at least one tool call where
the tavly search is needed. So if it is
not there no problem at all but if it is
present in that case we will have to
start showing this searching the web all
this okay so that is why we are emitting
a different uh event so what I'm doing
I'm just going to grab the query alone
so you can see that we're signaling the
search is starting we are again escaping
uh this is some parsing that we're doing
and then we are yielding a different
type okay so here we saw that we were
you know yielding a type content and the
content name In this case, we are going
to be yielding type search start and
then we are passing the query as well.
So this is how the client can actually
differentiate between you know what is
what right? So we might want to show a
different UI or something like that. In
that case the client can easily
differentiate based on this particular
type. So in this case it's going to be
search start. All right. So what's the
next thing? So we are also emitting this
on tool end. So what is that? So if I
come down here we we have this on tool
start and on tool end. So it's only in
the on tool end where we are having the
full response of the tably search tool.
Okay. So this is where we are seeing the
entire you know the four URL URLs that
it pulled the data from. So this is
where we are able to see everything. So
this is why we don't want the on tool
start because you know we are not going
to have this particular information at
that point. So that is why we are
grabbing a hold of this ontool end
event. So that is why I'm I'm taking the
on tool end event and I'm also making
sure that the event name is tavly
search. Okay. So there in the future if
you want to extend it there could be
multiple tools right in that case uh you
you will have to you know add these
conditions as well. Okay. So I'm
grabbing the output. So this is going to
be the output. In this case it is going
to be a list. Right. So the response of
the tably search is going to be a list
of four different items. Each items is
going to have the title, it's going to
have the query, etc., etc. Okay, so that
is exactly what I'm going to loop
through and I'm just going to grab the
URLs. Okay, because for the UI, I'm just
going to show the URLs. So that is why
I'm just providing the URLs. Okay, I'm
converting the U urls to JSON. And
finally, we are just going to be
yielding it. So in this case, the type
is going to be search results. Okay,
earlier it was search start and now it
is going to be search results. All
right. And finally, we are just going to
send an end event. Okay. So, this is
going to be the complete server side
code. So, what I'm now going to do is
I'm going to, you know, make sure I'm
just uh let me just close this up. I'm
going to run this file. Okay. So, I can
run it by saying uvcon. Okay. So, uon is
going to be you know if you've worked
with NodeJS, we've got nodemon, right?
Okay. Similar to that we've got uvicon
in fast API and we are going to go into
this particular app py. So that is what
this thing stands for the first part and
then the second part is going to
reference this particular variable.
Okay, where is it? So we are going to
reference this particular variable.
Okay, so this is going to be the fast
API instance. Okay, so we're getting a
reference to that and then we're also
saying
d-reload because if at all we want to
make some changes immediately we want
the server to be restarted
automatically, right? So that is why we
are saying reload as well. So let's now
run it and I'm going to be testing I'm
going to be testing this API endpoint.
So I'm going to be testing this using
swagger docs. So if you do not know what
swagger docs is, it's basically like
Postman but uh you know it is something
that fast API provides out of the box
that we can actually use to uh test our
endpoints. Okay, make sure that
everything is working fine. So to access
it I can just say docs and this is going
to give you a very nice UI about all the
different endpoints that we've written.
In this case we only have this one
endpoint that we've written. So
chat_ream and then messages. So if you
click on it you can see that it
automatically shows you okay we have to
provide this particular message which is
going to be required and we can
optionally provide the checkpoint ID as
well. So I can click on this try it out.
So in this case I can say something like
what is the uh let's say um current
weather okay weather in Delhi okay so if
I do that you should be able to see all
the different events that are
emitted okay it's taking a while okay
great and right here you can see that
the first event that was emitted
contains the checkpoint ID so the client
can actually grab a hold of this
remember it for future future requests.
All right. So, so since this is a query,
this is a query that requires tool
calls. So, in this case, it is going to
return some empty content. Okay. So, in
this case, it's not going to do anything
on the front end. And then uh you can
see that search has started, right? This
is uh this is we on the client side we
can grab this thing and then we can show
a nice UI like this one. Okay, search
has started and we can also you know we
also have access to the query, right? We
also have access to the query. All
right. So once the search is done
executing, once the tavali is done
executing, we are also providing getting
another event where we are getting the
all the URLs. Okay. All the five URLs,
four URLs I think and then we can make
use of that as well and we can show it
right here. Okay. So I hope that you are
able to see how it's all coming
together. And just one hiccup guys, one
thing that I ran into when I started
when I wanted to run this code. One
thing that I ran into was that uh there
was a certificate issue that I kept
getting when you know the agent wanted
to run the Tavly search. Okay. So this
is going to be the fix guys. So in
readme so all that I'm all that I did
was I just installed certify and then I
just put this particular line in the
terminal as well and that fixed the
issue for me. All right. So let's move
back. So what happens next? So right
after we show all the URLs, okay, then
we can actually then we have access to
the type content again, right? So we can
now okay get all these different
keywords, okay, all these different
tokens that are being generated. So it's
all going to be part of the type
content, right? And then we can actually
show it right here very neatly. So I
really hope that you are starting to see
how it all comes together. So in the
next part of this video, we are going to
be building out the React front end. So,
I'm pretty excited. I'll see you
there. Uh hello guys. So, in this
section, we are going to be building out
our UI using uh React and more
specifically, we are going to be using u
a React framework called NextJS. And
again, if you're not a front-end
developer, that is completely fine. Uh
you can just pull this code, you know,
clone this repository and then just put
in the commands that I tell you to put
in. Okay? Okay, so you just need to run
this npm rundev and that is going to
spin up the server and everything should
work perfectly fine. Okay, but if at all
you are interested in front end, okay,
so this particular part is for you. If
you're not interested, always like you
know skip over it. But it would still I
believe it would still help uh to know
on a very high level how things are
working on the front end. Okay. So, uh
to start off with I'm going to be using
as I said I'm going to be using NextJS
which is going to be a framework of
React. Okay. So, if I go to React.dev
you can see that you know next.js is uh
something that React recommends as well
and this is the command that I ran.
Okay. So, basically open a directory and
then run this particular command and
that is going to give you this
boilerplate code. Okay. So, once you
have this uh this is where everything is
going to start. Okay. So this app folder
is going to be present in this brand new
project structure. Okay. Inside of this
you would also find a
page.tsx. You know this is going to be
the file where the topmost component uh
in your entire application is going to
sit. So what I mean by that is uh if I
just go and okay so if I just you know
let's let's say I get rid of all of this
and I just say something like hello
world.
So if I save this and I come back to the
UI, you can see that it says hello
world, right? So let me actually go
back. Let me undo all of the changes.
And now you can see we have the uh the
earlier UI, right? So this is going to
be the topmost file and this home is
going to be the topmost uh container
that that is going to be the starting
point of our application. So if I scroll
down to the very bottom in this return
part, you should be able to see we have
the header and the message area and the
input bar. So what are these three
things? This header is just going to be
the static. Okay, it doesn't do anything
actually. Okay, it's just going to be
there for the aesthetics, right? And
then we have the message area and this
message area is for you know whatever
shows up in this particular window.
Okay, so if it is a user, it should come
on the right side. If it is not a user,
it should come on the left side. So all
these things are going to be taken care
of by this particular component, the
message area component. Okay. So we we
don't want to go too deep in the weeds
and try to figure out what is doing
what. Uh if you want to do that, just
pull the code and then you know you can
take your own time with it. But since
this is not going to be a front end
focused course, uh I'm going to be
skipping over it quickly. So we have the
header message area and then finally we
have the input bar. Okay, so this is
what we're talking about. So anytime we
type something in here, we are going to
be you know grabbing a hold of that
current value and then we're setting it
the setting the current value right
here. And then anytime an enter is
pressed in that case we are going to
fire this particular event handler
called handle submit. Okay. So this is
the UI part. Let's scroll up and let me
show you what we're doing right here
step by step. We are maintaining the
list of messages right here. How can I
help you? This is something that we are
hard coding right here. Okay. So we are
assigning it uh an ID a message ID and
then we also having this is user
property and that is going to be false
in this case right because this you the
AI is only asking this particular
message right so it's going to be false
and the type is going to be message so
there's going to be different types of
uh you know data that is going to be
populated here right so in some cases it
could be the search query some cases it
could be the search results in all those
cases the type is going to be different
accordingly we can you show different UI
for that. Okay, so before we move on to
that, let me show you you know what are
the other things that this object could
possibly hold. So this is going to be a
TypeScript interface. You can see that
you know ID is going to be a number
which we've already seen. The content is
going to be string which we've already
seen is user and then the type and then
is loading. Okay. So, right after the
request hits the server and before the
LLM actually starts generating
something, we are showing a typing UI,
right? So, if I say I am Harish, okay,
so in this case, you can see there's
going to be a typing UI, right? So, that
is going to be maintained by this is
loading. Okay? And then we have this
search info, right? So, this is going to
contain stages, query and URLs. So,
stages is basically like, you know,
initially we could be searching the
internet. Okay. searching could be a
stage and then uh you know reading could
be the next stage and then writing right
okay so these are the different stages
why are we maintaining it I'm just
maintaining it because uh to show the UI
in a certain way okay so let me show you
what I mean so I'm going to be using the
same query that we've been using so I'm
going to ask you know when is the next
space x launch okay so in this
case okay so you can see that we are
searching the web that is going to be
one stage right and then we are uh you
know reading it and then we are writing
So it's all going to be part of this
entire thing is going to be part of one
particular AI message response. So
that's how we can think about it. Okay.
So it's it's all going to be part of one
message. So we'll look at that. So okay,
so this is going to be the main method.
Okay. So 99% of all the logic is going
to be maintained at this one handle
submit method. Okay. So what happens?
Whatever happens when we you know type
something and we press enter. So this
particular event is going to get
triggered. So coming down so this
current message okay so whatever the
user is currently typing that is going
to be maintained here and then we are
going to create a new message id okay
we're just going to take the grab the uh
the maximum ID that is present in this
existing list and then we're just adding
one to it okay so that is all that we're
doing and the next thing is we are just
grabbing the current user message and
then we're updating the state right here
okay so this is going to be a
synchronous operation so immediately we
can just add uh this new item in the
list of messages. So you can see that
you know I'm harish if I say this thing
immediately we expect it to show up
here. Okay, so that is what this
particular this thing does and then
let's look at what what we're doing.
Okay, so we're clearing it out because
we don't want it to linger on here.
Okay, we want that to be cleared. So
that that's why we're clearing this out
and then this is where you know the
actual uh you know the streaming
receiving all the streamed chunks is
going to happen. Okay, so inside of this
try block. Okay. So the first thing that
I'm doing is I'm going to create uh you
know a loading um you know so what we do
is when we type something here we can
see that you know there's something
loading right. So essentially what we're
doing is we're creating this AI response
ID. Okay we're setting the content to
empty string. The user is false so that
it shows up on the left side and not on
the right side. The type is again going
to be messages. We're setting the is
loading to true. Okay. Because the
chunks would not have started to come
yet. Right? And then search info. none
of these stages would have started to
happen. And the next thing is we are
constructing the URL. Okay, so we know
that this is going to be the API
endpoint, right? So this is going to be
localhost 8,000 exactly where our server
is listening to, right? So we are going
to grab this particular URL and then we
are going to construct this entire URL.
We are passing in the user input as
well. So whatever message that the user
has entered that is going to be set
right here. And then we're checking if
the checkpoint ID is present or not.
Initially the checkpoint ID is not going
to be present. So we are going to come
down and then this is very important
guys. So we know that we are dealing
with server sent events on the server
side right. So how is the client
actually going to you know know that you
know events are getting emitted. Okay.
So something is being streamed. So how
do I capture it? So the browser actually
gives you this particular API browser
API called event source. Okay. So this
is going to be very important. So we are
using this to connect the serverent
event endpoint using event source. Okay.
So we are providing the URL right here.
We are initializing it. Okay. So any
time there is any event at all being
consumed or received on the client side
this particular event handler is going
to get this particular call back
function is going to get triggered.
Okay. So this is going to be this is
where most of our logic is going to be
done. Okay. So in case there is an error
this particular block is going to be
executed but you can see that that is
going to be the end of it. So let's look
at what is the first thing that we're
doing. Okay. So from this event we are
going to get whatever data that was sent
from the server side. Okay. So we're
extracting the data from that. We're
parsing it and then we now have the
object right here. Right now we're
checking what type is it. Okay. So
there's a couple of different types that
we're checking for. So now you can see
that we have like five different six
different types that we are checking
for. We had the checkpoint type. We had
the content type, search start type.
Okay, so if it's checkpoint, we're just
going to store it on the client side
using this particular, you know, uh use
effect. Okay, so that for the next
followup question by the user, we can
just attach it. Okay, so that's what
we're doing right here. Okay, so we can
just
um where is it? So if it is there, we're
just going to attach it in this
particular query param. So this way the
server is going to know uh you know
which particular session uh it is and it
can continue the memory from it. Right?
So coming down. So this is going to be
data type checkpoint and then we have
the content. So if you're dealing with
the content we are just going to
whatever data is being received we are
going to you know add it to this
streamed content variable. Okay.
Initially it's going to be an empty
string. We're just adding adding adding.
So you know if it says hi harish how can
I assist you today? So initially uh the
content is going to be high and then
okay it's going to append it okay and
then it's going to add it add it add it
to this particular streamed content and
then as and when it is coming we are
also updating the state right here okay
so that we can actually reflect it in
the UI and then let's move on to the
next thing so we have data type search
start right so along with this okay
let's search for the search start in
this thing so you can see that along
with the search start we are going to
provide the query as well right so
whatever the internet search query. So
we are going to grab a hold of the
query. We are going to add it in the
state as well. Okay. So the URL is still
going to be an empty string. And then uh
as usual we are going to update the AI
message with the search info. The same
thing for the search results as well. So
we're looking at you know the data urls.
Okay. And then we are going to update
the URLs right here. So we are going to
get the URLs from here. Okay. So if I
search for the search results, you
should be able to see that we are
sending the URLs, right? So we're
grabbing a hold of that and then we are
updating the state again. So the minute
we update the message there, we are
going to show it in the UI. And then
finally we have the end uh event. So if
we get this end event, we are just going
to close the event source. Okay? So we
don't have to keep it open more than we
absolutely have to, right? So we are
going to immediately close it. And that
is it guys. So uh if you are interested
in you know what we are putting inside
of the components it's just going to be
some you know tailwind CSS it's just
going to be some you know animation
we're using some animation etc etc and
even if you're not a front-end developer
guys and even if you don't you know
understand this code it's going to be
pretty simple just copy paste it to
chachip or claude ask it to walk you
through it and I'm sure it would start
making sense so that is it for the
front-end code guys so the Next part uh
is going to be we are going to be
deploying our uh server code. We are
going to be deploying our AI agent. So
I'll see you in the next
section. Hello guys, I'm excited.
Welcome to the last part of this course.
Uh we are going to be deploying our
application. Okay, so so far we built
the server side code, we've built the
graph, we've tested it locally, but uh
eventually we will have to deploy to
production and that is exactly what
we're going to see. Um there's a quite a
lot of different ways to do it. My
favorite way is to do it with Docker.
Okay. So this is going to be uh you know
the the backend code. I haven't touched
it at all since the last time we saw it.
Uh so the only two things that I've
added to this uh repo are this
particular docker file and this docker
ignore. Okay. So this docker file is
going to have all the instructions that
is necessary to put the project together
in a different server. Okay, in a public
server. So I expect you to know some
amount of docker already. So here is
what I'm doing. I'm basically going
ahead and pulling a python image and
then we are setting this work directory
to be app. So inside of the docker
container we are we are telling okay
create an app folder and then we are
going to copy this requirements txt okay
into that particular app folder and then
we're also installing this requirements
txt okay and then we are going to copy
the rest of the application code
including everything right okay so we
don't want this ipy ynb so we can
actually go ahead and delete it I'm just
going to keep it around in case we might
need it for something But uh actually
when you're running when when you're
building this image uh okay we can
actually go ahead and delete it as well.
So we're essentially copying the rest of
the things and then moving it inside of
the app directory. And also we don't
want to move the contents of the
environment file as well. So that is
exactly why we have this docker ignore
file as well where we're going to put in
all the different files that we want
docker to ignore. Right. So we are going
to copy the application code and then
finally we are going to expose the port
8,000 and then this is going to be the
command to run the application. Okay. So
these are things these are some things
that you would recognize right uon app
right. So this is what is going to spin
up our container. All right. So once we
have the docker file and the docker
ignore setup done. So now it is time for
us to start building our image. Right.
So once we build out our Docker image,
we can then, you know, use a a cloud
provider. So I'm going to be using
render. Okay. So you can use digital
ocean if you want. I'm going with render
because uh they offer a free tire and um
it does have some downtimes like you
know if you don't use the server it is
going to you know take some time to spin
up again. So there are some you know
drawbacks but since we're just learning
this is going to be ideal for us. And
then for the different steps that we
have to do like you know building the
docker image testing it locally right
okay pushing to docker hub you can see
that I'm actually using my own API keys
right here. So I want this to be
extremely easy for you as a viewer to
follow along. So I'm going to be
removing all these API keys as soon as I
upload the video. But yeah this should
make it extremely easy for you to follow
along. Okay. So the first step that we
have to do. Okay. So you can see that
this guide contains exact commands with
your credentials for building, testing
and deploying your perplexity app. So
that's what I'm calling it. So
personalized docker and render
deployment guide. So step one is going
to be we have to build the docker image.
And if you're wondering why we have
different commands for you know Apple
silicon and the rest of it and that is
because uh you know Apple silicon has a
different processor and if we build the
image in a different processor uh we
cannot actually use render because
render only accepts you know this AMD 64
processor. So that is why we are you
know using this and then we are making
it standard. Okay. So let me actually
copy this thing. This is going to be the
first command. Okay. Okay. And then I'm
going to uh copy this entire thing and
then put it right here. And that is
going to go ahead and start building my
uh docker image. So this might take some
time. And if at all you know you're
using Windows or Linux or Intel max in
that case just say docker build-d
perplexity latest and a dot here as
well. Okay. So why do we have this dot?
We're just saying that it is in this
particular root directory where we have
this docker file. So we're basically
just giving it the location to our
docker file. All right. So uh while this
is taking some time, let's go ahead and
uh let's uh move on to step two. So
whatever image that we built right here,
we are going to run it. Okay. So we are
going to you know run it as a container
and we are going to test it. We have to
say docker run. So this is how we run an
image. Okay. So this is how we run a
container and we are saying dashd. This
is a detached mode. Okay. So we don't
want to worry about all the logs.
detached mode and then we have this -p.
So what this means is uh we are
specifying the ports. So the container
itself is going to have ports. Okay. So
that is what we are saying that okay in
the container use the 8,000 port and
once the container is running if you
want to you know tap into if you want to
access that port inside of that
container you know use the local host
8,000. So that is what we are saying
right here 8,000 8,000. So this 8,000 is
going to be the port of the container.
This 8,000 is going to be the port of
the host machine. All right. And then we
are going to set the open AI API key. We
are going to set the Tavly API key. And
then we are going to give uh this thing
this container a name called Plexity
container. Okay. And then the image that
we are going to be using is perplexity
latest. Okay. So the same thing that we
mentioned right here. Okay. So let's
actually go ahead and copy this and let
me see if it is done. Okay, great. So
you can see that uh the image has been
built. So now let's go ahead and run
this image. Okay, great. So now you can
see it has given the container name. So
if I actually go back to my docker
desktop. So right here you can see that
we have the container running right
here. Right. So we have perplexity
latest which is currently running on
port 8,000. Right? So I can just come
back to Chrome and I can go to localhost
8,000. Okay. So now you can see that if
I actually go to docs, you can see that
we are actually able to access the
endpoint and everything, right? So this
is actually amazing, right? So you can
see that we don't actually have any
server running right here. So we're
directly listening to this particular
server. it is actually the container
that is running our entire Python server
including you know all the uh the LAN
graph files as well right so now we know
that uh the container is working fine
now let's go ahead and push it so let's
actually look at what is the next step
right here okay so the next step is
basically to push to docker hub so now
let's actually go ahead and uh we can
also stop this particular
uh container okay we don't want it to be
running because we now want to push it
okay so it's been stopped and then we
can also you know do this particular
command as well. So now let's go ahead
and login okay to docker hub so that we
can actually push it to docker hub. So
once we have it available in the repo in
the docker hub, we can actually you know
go inside of a public server and then
pull it and then we can build the same
image there as well. And then we can
have our application be available
globally. Right. So I'm going to go
ahead and say docker login. Okay. So I
can say docker login authenticating with
existing credentials. Username harish.
Okay. So it's because I've already
logged in. But if you have not logged in
uh it's going to you know you can just
provide your uh credentials and it's
going to log you in. Okay. So what is
the next thing? So the next thing is we
have to tag our image with your docker
hub username. Okay. So this is going to
be the command docker tag perplexity
latest. Okay. And then this is how we
can actually do it. So let me copy it
put it over here. And now let's go ahead
and push the image to docker hub. So we
say docker push. And this is going to be
the username of my uh you know docker
hub. So if I go here to docker
hub okay so you can see that this is
going to be my username right. So
harishneil is going to be my username.
So that is exactly what it is showing
right here. For you it is going to be
something else. And as you know this is
going to be the name of our image right.
So this is going to be the tag as well.
So let's copy this entire thing and then
let's go ahead and push it to docker
hub. So you can see that it's preparing
waiting. So now it's actually going to
be publicly available for you know the
world right. So let's actually wait for
a little bit. Okay great so it is done
pushing. So if I actually go to docker
hub right here and great you can see
that it has been pushed less than a
minute ago. Right. So this is what we
want and now this is actually available
publicly. So the next step is going to
be very simple. Okay. So let's actually
go back to our artifact. So we are going
to be deploying on render, right? So I
can go to my
dashboard. Okay. So I can click on add
new and I can click on web service.
Okay. So right here I can click on
existing image. So in here I can
actually provide the docker image URL
and for you uh the username is going to
be different. For me this is going to be
the image URL. So let me copy this and
put it over here. Okay, great. So you
can see that there is a tick mark right
here. So I can now go ahead and click on
connect and then you can create a
project if you want. I'm not going to do
it. I am going to go for the free
version right here. I can leave this
region as it is. So coming down, this is
where we have to provide our environment
variables. So we have to provide the
open AI API key and we have to provide
the tab as well. So I'm just going to
quickly go ahead and copy that. So I'm
just going to copy this put it over
here. This is going to be the open AIS.
This is going to be the tab. All right.
And that is it. So now all that I have
to do is deploy this. So right now
what's happening is uh you know render
is automatically pulling the image from
uh docker hub because it's now been made
publicly available. It's pulling it and
you can see that it is actually you know
uh running it. It's building it in its
own server and it's going to make our
application available to the
world. So you can see that the service
is now live and now you can see that up
here it provides me the new uh API
endpoint. I can just copy this and I can
actually go ahead and run this and you
should be able to see that we can
actually access the the swagger docs of
our deployed application. Okay, in this
particular endpoint. Okay, so now what I
can do is I can just copy this endpoint.
Come back to my client side code. Okay,
so right now it is actually pointed to
our local host, right? So instead of
this I can actually you know replace
this. Let's remove this docs. Okay,
let's uh save this file and let's
actually now go ahead and see if uh you
know it everything is working fine. So
in this case I'm going to say you know
I'm Harish. How are
you? Okay, so now you can see that it is
actually working guys. So our
application is successfully deployed on
render. So if I actually inspect it and
show it to you in the networks tab, you
should be able to see calls going to
render right now. So I can say you know
uh what is the weather in SF. So in this
case you can see that all the calls are
actually going to perplexity right.
Right. Okay. So, it's going to onren
render.com. Right. So, perfect. So,
everything is working as expected. So,
if you want to go ahead deploy the
front-end code as well, that is pretty
easy. There's a bunch of tutorials out
there on how you can deploy a NextJS
application on Versel. And if at all you
want to steal the code, you know, I
don't mind at all. Please go ahead, pull
the code, uh, you know, clone it and
then change the UI a little bit and then
go ahead and put it in your list of
portfolio projects. So, I don't mind
absolutely at all. Okay. So, and that is
it, guys. I hope you learned a lot and
I'll see you in the next video.