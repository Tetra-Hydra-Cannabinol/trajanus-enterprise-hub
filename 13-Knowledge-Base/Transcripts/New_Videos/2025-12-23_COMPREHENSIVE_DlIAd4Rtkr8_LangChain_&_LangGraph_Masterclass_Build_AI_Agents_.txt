Title: LangChain & LangGraph Masterclass: Build AI Agents & RAG Pipelines
Channel: LearningHub
Video ID: DlIAd4Rtkr8
URL: https://www.youtube.com/watch?v=DlIAd4Rtkr8
Duration: 409:48
Level: COMPREHENSIVE
Ingested: 2025-12-23
Source: yt-dlp Auto-Generated Captions
==============================================================================

WEBVTT
Kind: captions
Language: en
Hello everyone, welcome to this brand
new series on Langen, the framework
that's changing how we build intelligent
context aware applications using live
language models. If you have ever
wondered how chatbots, AI agents, or
retrieval based systems actually work
behind the scenes, you're in the right
place. In this series, we'll go beyond
just theory. We'll build real world lang
project step by step from simple
conversational bots to AI systems that
can use tools reason with data and even
connect with external APIs and
databases. By the end, you'll have the
skills to design your own AI assistant
or integrate LLM into your apps
confidently. But before we dive in,
let's understand what Langen really is.
Langen is a framework that bridges LLMs
with real data and real tasks. It helps
developers manage prompts, memory,
retrievalss, agents, and chains all in a
structured modular way. Think of it as a
toolbox that turns a large language
model into powerful goal oriented
system.
In the upcoming videos, we'll explore
langen concepts like chain, agents,
retrieval, and vector stores and much
more. Each topic will include hands-on
coding demos and project based learning.
So if you're excited to build real AI
apps with Langen, make sure to
subscribe, hit the bell icon, and follow
the C series from the start. Let's
unlock the true potential of language
models together. I'm Object, and this is
the Langen series. Let's get started. So
in this video, we're going to have our
first interaction with an LLM. So for
that, first of all, I'd like to create a
virtual environment. So open a terminal
in VS code. I've opened a folder in VS
Code. Uh and then open a terminal. So
we'll create our virtual environment
virtual env.
And I'm going to activate this virtual
environment and install my libraries
inside this. Okay. So the virtual
environment is created and now I'm going
to install
some libraries here. So pip install
langen
google genai
and I'll also install langen
community
and let's also have python
do env or env. So let's install these
three libraries.
And while this is being installed I'm
also going to create a requirements file
requirements.tx txt and then mention
these three libraries name here. So the
first name is langchain
community.
Second is langchain google genai. Well,
you might be wondering why we're
installing
Google geni because uh the
Google
Google geni models or like the gemini
models are kind of free. The flash
models are free. So we will be using
those models for our LLM interaction.
I'm also going to create av file here
file
and inside this file we'll have our uh
Gemini API key which will be defined as
Google API key and then you can paste
your Google API key in
in this place here. So paste your key.
Uh I'm going to replace
replace my key here when I'm going to
run the application here. Okay. So for
the first first program or for the first
interaction, we're going to create a pi
file. But I would like to organize
everything inside this single folder
only for the future videos too. So let's
just create a new folder and then
type lm interaction
and then inside this I am going to
create a file called chat model
uh_jemini
dot
py okay so we are inside our pi file and
then this will be our first
python program which which will also be
our first program to interact with
Gemini model. So for that I'm going to
use the langen ecosystem. As you know
that this is a series of langen. So we
will import some libraries from langchen
google_jai
import
chat google generative AI and then
another library is from env import
uh load.env.
So I'm going to call this function load
env. This is going to load our Google
API key uh from the env file and then
I'm going to define my model. My model
as I said will be a Gemini model. So for
that I'm going to use a Gemini
2.5/model today.
And let's also invoke our model here. So
model.invoke let's provide a prompt or
like we can define a prompt here in
string. So I'll say what is the capital
city of USA
and then I'm going to invoke my model
with this prompt here.
Let's save this in result. The model
output will be saved in the result. And
then I want to print out the
[clears throat] result here.
So let me clear this.
So now if I run my application here, I
can directly run my Python file from
this button here. So run Python file. It
activates my virtual environment and
then the Python file is being run. And
you can see my result here. But we don't
only get the result, but we get a lot of
things inside this. We get the answers.
We also get some metadata from the
response. uh the number of tokens and
tokens and then uh stuff like that. What
we only want here is our content part.
So I'm going to hit result content. I'm
going to clear this again and then run
it. And we can see our result here. The
capital city of USA is Washington DC.
Well, this is our very first interaction
with a LLM here. So throughout this
throughout the course of this video,
we're going to use um Gemini models as
well as some hugging bas models wherever
applicable. We saw our very first
interaction with an LLM. But I'll show
you one problem that this LLM call has.
So let me run this again
or like let me
run this again. We got the answer but
I'm going to change the prompt here. So
hello my name is Abishek.
What is your name?
So if I run this
it gives me an output. So I'm a large
language model and AI. I don't have a
personal name like human does. You can
just call me I assistant if you like.
And then in the initials of this message
it's written hello Abishek how can I
help you today? So now if I type
do you remember
my name
and then run this
we'll see the problem here.
As an AI I don't have memory of past
conversation or personal information
about you. So no I don't remember your
name. So each and every LLM calls that
we make are actually independent here.
So how do we maintain these multi-turn
conversations meaning the contextual
conversation like we have in this chat
GBT or like Gemini models. So we'll try
to implement that here. So for that I'm
going to create a new folder
called chatbots
and inside this I'm going to create a
new file
chatbot.py.
Uh
so let me install a langen core if I
haven't done it. So pp install lang gen
core.
Okay, I've already installed this. I can
also maintain
mention this in the requirement rank
code.
So here I'm first going to import my
Gemini model.
import chat Google generative model and
then from langen core
langen core dot messages I'm going to
import system message human message and
AI message
so in this chatbot we'll try to maintain
that contextual information so that the
AI remember our remembers our name so
from
envo
load.
Okay, I'm going to load my credentials
here. Then I'm going to define my model
which will be chat Google generative AI.
The model name is going to be Gemini
2.5/model.
Now for the first
message I will now define a chat history
here which will be a list and inside
this I'm first going to provide a system
message and the system message is going
to say that you are
you are a helpful
AI assistant.
Okay. So I'll give this in loop so that
I can continue talking with the bot. So
I'm going to
create a variable for user to type their
message.
So this is where user will be typing the
message
and then whatever message the user types
I'm going to append this in my chat
history
so that so that the chat history list
will have the information about what the
conversation is being happening
equals to user input.
Now I also need a break from this chat.
A way to break this chat. So user input
dot lower equals to exit. If the user
types exit and I'm going to break this
or else I'm going to just invoke the
model
using my chatty.
not just going to provide a single
prompt to the user but I'm going to
provide the model the entire contextual
history of a conversation
then the model is also going to reply
back here so that will also be appended
so chat do append and then it will be
appended as an AI message
where the content is going to P result
dot
content.
Okay. And I'm also going to print my B
response here.
So result dot content.
Finally once the conversation breaks out
I'm going to print the entire chat
history. We had chat history.
Now if we run this we'll see that
because of this particular
list here where we have stored
everything or every conversation that we
had throughout this loop the model will
be able to have some contextual
information and at least remember our
name or at least remember what we've
talked about in the past. So let me
clear this output and then run this
again. Save this and run this again. Do
I have any errors? Yeah. So there's a
comma missing here.
Chat history.
Okay. So run run it.
So it's asking for my message. I'm going
to say hello.
My name is Abishek.
What is your name?
Oh, okay. So, I probably gave the wrong
model name here. Okay. So, I made a
mistake here. So, let me clear this
again. I'm going to recolor this.
Gemini- 2.5- flash is the correct model
name. So, let me run this again.
Okay. So, hello.
I'm shape.
What is
the name?
So the model says hello Abishek I do not
have a name. I'm a large language model
trained by Google. So let me also ask a
few more questions. What are you
trained on?
So I've been trained by Google on a
massive data set of text and code.
uh this data set includes a wide variety
of information from the internet and
then and stuff like that. So I'm going
to ask another question. What is the
capital of
India?
So the capital of India is New Delhi.
Now we'll check if the bot remembers our
name or not. Do you
remember my name?
And yes, it does remember my name
because why it remembers our name is if
we hit exit and then see our entire chat
history, we can see that
our entire chat history is being saved
here. So this was the first message that
we gave to the AI assistant. I asked
this question.
The AI replied me with this answer.
I again asked another question using a
human message and then just because of
the existence of this contextual
information
the bot was able to remember what the
conversation was about or like was able
to remember the stuffs that we said in
the past. So in the last video we saw
how we can include contextual
information in our chatbot. So in this
video we're going to talk about
something. We know what simple prompts
are. We've already given prompts to our
chatbot. But in this video we're going
to introduce a concept called chat
prompt template. So chat prompt template
in lang chain. It is a tool that helps
you design and organize prompt for
chatbased AI models in a structured way.
So instead of writing the entire message
each time, uh you can actually give a
pattern that includes different roles
like system, human and AI along with
placeholders for dynamic values. So such
placeholders are actually later filled
with real data when the prompt is
actually used. So what this does is it
makes it easier to maintain uh reuse and
modify prompts uh without changing the
main logic of your application. So if I
say it in simple terms, it's like
creating a reusable message template
that guides how our chatbot or AI
assistant uh should behave and respond
in a conversation. So I'm going to
include my chat chat prompt template in
my next program. So I'm going to create
a new file here.
Prompt
template.py.
So
have our import langen_core
dotprompts
import chat prompt template.
I'll have my model to lang google geni
import chat google generative AI and
I'll also have my messages here. So from
langen
uh I'm not sure if I'll use this or not
but we'll keep it for now. messages,
input,
system message, human message, and AI
message.
Okay, first of all, what we're going to
do is I might not invoke the model at
all. I'm just going to show you uh how
our prompts are created using chat
prompt template. Okay, so this is
actually chat prompt template, not chat
message prompt template. So I'm first
going to create a template here. So chat
template
equals to chat prompt template.
I'm going to create a list inside this.
And then [clears throat]
for our system message I'm going to type
in
type in this part and then you are a
helpful
domain expert. So the domain is going to
be a placeholder which will be filled
later on whenever we execute the stratum
template. And for the human message
I'm going to type
explain in
simple terms
simple terms the concept of
topic and then the topic is also going
to be a placeholder here. So whenever I
create a prompt, what I'm going to do is
I'm going to use my chat template dot
invoke
and inside this since we have two
different placeholders here. So I'm
going to provide the value of those
placeholders. The first one is domain.
So we'll call this quantum quantum
mechanics or quantum physics
or something like that. And then the
next is our topic. So I'm keeping the
topic as warm.
Okay. Now if I try and print this
prompt, what I'm going to see is a
generated prompt from this chat prompt
template here where we have these two
messages along with their placeholders.
So let me clear this and then run this
again.
So if I run this I can see my
I can see my prompts here. So the first
one is a system message. You are a
helpful quantum physics expert because
the placeholder is being filled here and
then uh the human message explain in
simple terms the concept of wormhole. So
if I need to invoke this I can directly
define my model here. So if you define a
model then you can directly invoke your
model using this
prompt here. So model invoke and then
prompt. I I think you can do this by
yourself and then check out the result
here. What I wanted to show you in this
video is a new style of generating our
prompts using this chat prom template.
When building a chatbot, one of the
biggest challenges is making it remember
the conversation. You don't want to
treat AI or you don't want the AI to
treat every every user message as a
brand new question. So that is where the
message placeholder comes in. Uh it's
like a dynamic container for a chat
history. Instead of manually combining
all previous messages into a single
string every time you generate a prompt,
you just put a placeholder in your
template and lang fills it automatically
uh with the actual conversation that
happened so far. Uh so the placeholder
doesn't just dump text, it preserves the
role of each message whether it came
from the human, the AI or the system. Uh
so this means that uh the AI can
actually uh identify between
instructions, user prompts and is and
its own pass responses. So that is what
allows multi-turn conversations to feel
natural that we see in chat GPT or
Gemini interfaces that we use. So
another powerful aspect of such is
reusability. So you can define a single
chat template with a message placeholder
and it works for any number of previous
conversations that we had. So whether
the conversation has two turns or 20
turns, the placeholder always inserts
the right history in the right format.
So this also keeps the template clean
and you don't need complex logic to
actually stitch the chat together or
like put the chat together. So finally
message placeholders uh work seamlessly
with lang chains role based messages
like system message human message and AI
message that we've already seen. So this
combination actually uh uh ensures that
our prompts are structured uh clear and
and also are context aware which is
actually essential when using LMS for
chatbot or customer support agents. Uh
so message placeholder is what lets a
chatbot remember understand rules and
continue
conversation smoothly without you
manually managing
every previous line of dialogue. So that
is what we're going to do in our
upcoming code.
So I'm going to create a new file called
message placeholder
holder.py. What I'm also going to do is
create a new file that actually contains
or saves my conversation here. So let me
write it at chatbot history
uh txt. As I told you that uh whenever
we save our conversation, we save it in
such a way that um that the information
of the message is being preserved. So
I'll put a few messages here. So the
first one is a human message where it
says that I want to request a rep for my
order and the AI message uh says this
thing. So okay I will use this and what
I'm going to do is in my message
placeholder I am going to
load this chat and then create a new
prompt based on my uh new user input. So
first of all I'm just going to use from
line gen core dot promps
import the first one is going to be chat
prom template
it should be promps.
So chat prom template and then the other
is going to be a message placeholder. So
our chat template is going to be
something that we've already seen.
So chat template equals to chat prompt
template.
So this will contain a system message
uh which says you are a very helpful
helpful
customer support agent
and and then I'll also load our
conversation history that we already
have in this file. So for that I'm going
to define a message placeholder with a
variable name and then and then the
variable name is going to be like
uh chat
history.
It's just a variable name. It does not
need to match with the file name
provided here. And then the third is
going to be a human message.
a new human message that will come uh
in this particular place here.
So I'll also indicate this as a string.
Okay. So this is our chat template. Now
I'm going to load a chat history from a
file. So for that I will create a list
called chat history.
So with open uh chat_history
txt
as file
I can load a chat history chat history
uh dot extend
file
dot read lines.
Okay.
Now
uh I will create my new prompt here
based on the chat template that I have
as well as the chat history that I just
loaded. So
inside my chat history
variable
is going to be my chat history list.
chat
history list.
Okay, so this should be inside
curly braces. So chat history list and
chat history along with this I'm going
to also have a query because the query
also contains a placeholder here. So in
the query I'll ask where is my refund.
So I can invoke this using a model too,
but I just want to show you the use of
placeholder here. So I'm just going to
print my prompt or my new prompt here.
Okay. So let me rerun this and then we
can see
uh something's not right. Okay. So this
should be txt.
Clear this again. Then run it.
I definitely made some mistake here.
Oh, it should be chatbot history, not
chat history.
Chatbot history.
Okay, if I load this again, then we can
see uh that the first message is a human
message.
uh you are a very helpful customer
support agent. So far we focused on
making AI conversations more natural and
context aware. So we looked at how the
AI can remember previous messages using
features like message placeholder which
allowed our chat bots to maintain
multi-turn conversations like chat GPT
or Gemini. Uh back then the AI could
generate responses freely which worked
well for casual conversation but the
outputs were often unstructured
sometimes too long inconsistent or
sometimes hard to process automatically.
Now we're moving into the next step
which is structured responses. So
instead of letting the AI respond
however it wants, we want to define a
schema that specifies the exact fields
and types of information we want in the
response. So this actually guides the AI
to produce outputs that are predictable,
organized and easy to work with. So for
example, we can ask it to summarize the
review and identify the sentiment and it
will return the information in a clean
and structured format. So the key idea
is that structured output turns the AI
from just a conversational partner into
a reliable data source. So it
complements the uh whatever we've done
before. The AI still remembers the
context of the conversation but now uh
its responses are also consistent and u
more readable making it much easier to
integrate into applications dashboard or
any system that needs such actionable
data. So let me jump to the code and
then show you what I mean by that
concept here. So
I'm going to create a new folder here
which is going to be structured
structured outputs. Let me give this as
number three.
So inside this I'm going to create a new
file
and then the file name is going to be
structured
output.py.
So here again we're going to be using
the geni model from Google. So import
chat Google generative model. I'm also
going to
load my credentials. So env import load
env. And here I'm going to introduce
something new which is a typed dict
type. So I'm going to load my credential
define my model
which is going to be a Gemini model.
So as I said we want a proper structured
output right? So for that we're going to
define a schema. Let's write review.
Uh
so and the type is going to be typed
tick. What we want is a summary which is
going to be in string and a sentiment
value
uh which is going to be also be in a
string. So it can be a positive,
negative or sentiments like that. So now
what I'm going to do is instead of
directly invoking the model, I'm going
to convert this model output into a
structured model output. So
structured
model equals to model dot with
structured output
and then I'm going to provide my review
class here. So what this does is
whenever this model is invoked it tries
to base its output on this particular
structure provided here. Okay. So let me
define my prompt. I'm not going to use a
prompt uh chat prompt template. You know
how to use that. So I'm just going to
give my prompt here in a simple
approach. So let's say this hardware is
great
but
the the software feels
kind of bloated.
So many
boilerplate
boilerplate apps and my phone keeps
hanging
when I play PUBG.
Okay. So this is going to be my review.
uh and then based on this I'm going to
invoke my model with a structured model
dot invoke and then provide my prompt
here
and then I'm going to print my
response. So now if I try to run this,
let me run this.
So what I'm going to see is I'm going to
see a structured model output in this.
So as I said we
gave the model a schema
to base its response upon and then the
response is just based on the schema.
Here we have a sentiment a sentiment and
then the summary. So the sentiment is
negative and then the summary is just
hardware is great but software is
bloated with bullet coursees and the
phone hangs when playing PUBG. So it is
just trying to summarize this particular
provided review prompt here and then the
response is quite well structured. Well,
if I try to do something like this. So
[clears throat]
if I give a new prompt and then
uh okay um I'll also show you what
happens if we don't provide such
structured outputs here. So my prompt is
going to be uh
generate
sentiment and
summary of
the review below.
Review given
the review is.
So if I paste my review here
or I can just do this. Uh [snorts] I can
hit new prompt and then
use this as f string
and then include my
review using this prompt placeholder.
Well, if I invoke this with not my
structured model but with my model, then
I'm going to see that my response is
going to be very much unstructured which
I which I don't actually want. So, I'm
not going to print this for now. Let's
not print this. Uh, we'll just print it
with the previous upper that we've done.
Okay, something's wrong. I'm not sure
what. So, let me check again. Uh,
ah, okay. We've not used the function
invoke here. So let me write invoke
clear this and run this again.
I'm not printed by result in fact. So
let me print by result too.
So as you can see uh the response is
quite mixed. Uh it says here's the
sentiment and summary of the review
sentiment mixed to negative positive the
hardware is slightly
highly praised and then yeah stuffs like
that which is not actually structured.
So we can actually
base our LLM output
response [clears throat] and then ask it
to base it on on the provided schema.
Here we can also include the details of
of such schema but we'll do that in our
upcoming video. In our previous video we
explored structured outputs where the AI
was guided to return information in a
specific format like a summary and
sentiment. That approach made the AI's
response predictable and easy to use,
but it was relatively simple. We were
mostly extracting a summary and a single
sentiment value. So in this next step,
we taking structured output to the next
level by introducing rich schemas with
multiple fields, annotation and optional
elements. Uh now instead of just a
summary, we can ask the AI to extract
multiple layers of information from a
single piece of text. the key themes
that is talked about, a concise summary,
overall sentiment, and even detailed
pros and cons. We're also telling the AI
exactly how each field should be
formatted and making it strict so it
follows to the schema precisely. So,
this approach transforms the AI into a
powerful data extractor. uh you can feed
it complex reviews, articles or even
long form text and it will actually
return you a structured machine readable
object that you can directly use in
dashboards, analytical tools or even
databases. So the AI is no longer just
answering your questions or chatting.
Now it's systematically organizing
knowledge in a way that applications can
consume automatically. So by building on
what we learned about contextual and
structured output, uh the following uh
implementation will show you how to
scale from simple structured
uh output to detailed multi- field
extraction making AI truly practical for
real world task like review analysis,
product insights or content
summarization. So let's go for the code
now.
Okay, I'm going to create a new file
inside this
detailed output structured.py.
Uh I'm going to use these same fields,
the same model that I have here. I'll
just paste this but but what I'm going
to do is I'm going to add a few more uh
fields inside the schema here. So let me
first add key themes. Uh and then I'll
also add an import of annotated
and then optional here. So these two
imports will be essential for me. So key
themes will actually be uh annotated
list of strings.
So what we can also do is we can also
write a detail about this field here. So
we can say that
uh
key themes must write down all the
themes
all the important
concepts
discussed in the review in a list and in
the summary I'm I'm again going to use
an annotated key and inside this I'm
going to Right. Must write down must
write down a brief summary
of the
of the review. Uh
close this uh inside my sentiment I'm
again going to use an annotated.
So with annotated I'm going to say must
return
a sentiment value
uh or sentiment of the review
either positive or
negative
and I'll also add pros and cons here.
So,
okay, I I need to close this bracket at
last. Also, I need to do the same here.
I'm also going to add pros and cons. So,
pros is going to be annotated. But this
is going to be a optional list. If the
model does not find anything like pros
and cons inside the review, then it is
not going to put it or else if it finds
something then then it will put it
inside our uh pros list. So this is
going to be optional
uh str and then uh write down
all the pros inside
all the pros inside a list and also
we'll do the same for cons
annotated
optional
list of strings and this will also be
write down all the cons inside
a list. Okay, so I'm going to uh add
some more prompts to this or some more
details to this prompt here. So let me
copy a prompt from somewhere. This
prompt is from Google Pixel phone and it
is quite detailed here. So you can just
pause the video and then type it if you
want. I'm going to perform this word rap
for you so that it'll be easier for you
to type this. So this is going to be my
prompt and then based on this prompt uh
let me remove this and then I will
invoke a structured model here to see my
result.
So yeah so let me go ahead and then run
this code. So if I run this
as you can see with the structured
output uh the model's response is going
to be kind of structured here. So the
key theme is Google 10 pixel pro GPU
performance because it talks about the
GPU performance somewhere around here
with uh talking about its chipsets also
it talks about the tensor G5 chipsets
more gaming performance comparison with
Pixel 9 Pro and things like that. The
sentiment is mixed. It does not say
positive or negative here. So
we can also ask it to fix that. Uh
but like it is okay for now. Uh the
summary is also provided. And then the
pros and cons, I don't think it's
provided because the model could not
find or the or the model could not
identify the exact pros and cons in this
particular review since this is
optional. So the model could not write
it or if we remove optional from here
and then ask the model that it has to
include the pros and cons or the or like
just the pros the model will do that and
then include all the pros of the review
provided. So let's run this again
and then we see that uh summary is
provided.
Uh okay what else is provided here?
Summary is provided key things is key
things is provided sentiment is provided
and then it should also contain pro
somewhere. Let me copy this and then see
it in my
uh copy. Let's open a JSON formatter
window
if we can see the pros or not.
So JSON formatter
paste this paste my JSON here and and
then we see that it still hasn't
generated any pros here but um that's
okay for now still our response is kind
of structured here. So this is how we
can take our structured response to
somewhat another level where we can
include the details of what each of
these keys that we want that we want to
include in the responses mean and then
what format should they be provided in.
In our previous video, we explored
structured outputs where the AI was
guided to extract specific fields like
summary or sentiment. That approach
worked well but the schema was
relatively simple. Uh now we're taking
structured output a step further by
using pyentic models to define a rich
detailed schema. So Pyntic is a Python
library that allows us to create data
models with type validation default
values optional fields and and clear uh
details. So this means that we can tell
the AI exactly what kind of data we
expect for each field and we can trust
that the output will confirm to these
types here. So by integrating pyic with
lang chain structured outputs uh we can
define multiple fields like key themes a
brief summary sentiment pros cons and
even the reviewer's name with strict
validation. Um the AI then produces the
responses that are not just structured
but also type safe, consistent and fully
compatible with Python applications.
So this is especially useful when
dealing with long or complex text. So
instead of u getting unstructured
paragraphs, the AI learns to organize
information into a machine readable
object that can be directly used in
dashboards, analytical tools or
database. And because of Pythics
validation, we reduce the risk of errors
like missing fields or wrong data types
too. So the integration takes structured
outputs from simple field extraction to
a robust, reliable and a developer
friendly approach. Uh it also kind of
ensures that the AI not only remembers
the conversation context but also
returns precise, actionable and
validated
information
making it ready for uh real world
applications. So let's go to the code
now.
So I'm going to create a new file here
or this is going to be uh sorry three
pantic
structured
output.py.
Okay. So let's start.
As always, we're going to use the same
geni model. import go chat Google
generative AI. I'm going to install uh
one more library here which is called
pyante.
So included in requirements.ext text and
then I can directly install this using
this command
and I also okay that is all for now
because I don't have any emails right
now or else we would have to have to
install some other libraries too. So
what I'm going to do is from env
import load env uh from
typing
uh import type
Sorry, import
date. I I'll also include an annotated
literal
uh and also a keyword called optional.
Not sure I'm going to use each and every
one of these though. But still uh we'll
keep it in the import. So I'll import
from pyic import base model.
And then I'm going to load my
credential. I'm going to define my
model.
Uh my model will be
Gemini 2.5 slash.
Now I'm going to define a similar schema
like we did like we did in earlier code.
So I'm going to define a class which
will be called review and the import
will not be a type but it will be a base
model from our py class.
So like I said earlier, I'm going to use
uh
use a keyword called key themes.
And
instead of doing like uh annotated list
of list of our content, what we'll do is
we'll include
a class called field here and then
define this as a field. So this is going
to be a a list of string. Uh its type is
going to be list of string and its
detail will be included in the field
value. So I can include a field and then
write down the details here. So write
down
three key themes
[snorts] discussed in
the review
in a list.
Uh the next is going to be my summary.
My summary is going to be a string. And
then I'll again write a field to write
down the details of that sum of that
summary keyword. Now this is going to be
a brief
summary
of
of the review.
I'll also write sentiment.
And in case of sentiment, we had
something like a mix sentiment earlier.
But I'm going to fix the model or ask
the model to give either positive or
negative sentiment here. So for that I'm
going to use literal. So one will be
positive. One option will be positive
and then the other option is going to be
a negative literal.
So for that I'm also going to write down
the details.
Uh description equals to
return
the sentiment of the
sentiment of the review either
positive or negative.
And this is going to guide me guide my
model to return either positive or
negative here. Now uh I'll also include
a name here. So name is going to be in
string and then I'll keep an optional
keyword here because the names might not
be available
uh in the review but if it is then uh
the model will definitely show the name
too. So the description is going to be
the name of the
reviewer or or I'm going to say write
down
write down the name of the reviewer.
Okay. So this is my output schema that I
want my model to provide my output with.
So I'm going to create a structured
model. So this is going to be model dot
with structured output
and then I'll provide my class and then
uh I'll also instruct this
instruct the model to strictly follow
the provided schema. Now
uh let me provide my prompt. I can copy
my prompt from the previous video or the
previous code that I had. So I'm going
I'm just going to copy everything else
from now on here. So it is going to be a
prompt and then my prompt will be
invoked using the structure model
and the code is complete. So let's run
this code. Let me check if everything is
all right. Okay, it seems okay.
So let me run this. Uh when we run this,
the model output will be based strictly
on the schema as provided earlier.
Okay. So we have key themes. We have
three key themes here. One, two, and
three key themes. We also have a
summary. Uh and then and then okay what
else? Uh we have a sentiment which is
called which is strictly following the
uh literal or the option that I provided
here and then the name is actually not
specified. So if I change this prompt
and then add a few text like add a name
to this reviewed by Abishek then I'll
see
that the model also includes my name
here. It should include my name here.
Let's see the output. So yeah you can
see the name is being included and then
everything else is present based on the
schema that we provided. In this video,
we're exploring a powerful combination
of langen features that help us control,
structure, and process AI outputs in
multi-step workflows. So there are three
key concepts at player. One is prompt
template. Second one is str output
parser and and the third one is
chaining. So first let's talk about
prompt template. So prompt template
allows us to create reusable structured
prompts with placeholders for dynamic
content. Instead of hard- coding every
instruction for the AI, we can define a
template once specifying exactly what we
want and then fill in fill in the
variables as needed. For example, we can
have one template that asks the AI to
write a detailed report on any topic and
then another template that takes the
generated block of text from the
previous prompt and then generate a
concise summary about it. So prompt
templates gives us the consistency and
control over how we interact with the AI
which is crucial for predictable
results. The next concept we have is STR
output parser. So whenever the AI
generates text the raw output can be
inconsistent. It can have extra lines,
unexpected formatting or maybe some
minor variations in the wording. So str
output parser acts like this cleaning
and standardizing tool and it also
ensures that uh no matter how the AI
phrases its output, the output is
converted into a neat predictable string
that we can feed into the next step
directly. So this is especially
important whenever we are building
multi-step workflows one after the other
because one wrong uh output can easily
break the next prompt or the step in the
process. Now whenever we talk about
multi-step workflows
we introduce this concept of chaining
where the output of one prompt becomes
the input to the next. So by connecting
this multiple prompts in a chain, what
we can do is we can perform complex task
that requires several steps. For
instance, let's say we first generate a
detailed report on a topic using one
template, parse it into a clean string
using str output parser and then feed it
into another template to create this
short concise memory.
Now this chain ensures that the flow of
information is seamless while templates
and parsers gives us the control and
reliability at each step. To summarize,
the approach
allows us to uh break down or decompose
this complex AI task into structure and
manageable steps. To summarize these
three key concepts, prompt templates
defines what we want and keep
instructions consistent. STR output
parser ensures clean and reliable
outputs and chaining lets us link
multiple steps together to produce more
advanced result. Now when we combine
these three together we can build these
multi-step AI workflows for real world
application.
So now let's go into the practical
implementation of these three concepts
that we have just talked about.
Okay. Okay. So, I'm going to create a
folder
here
output parsers.
Let's create a file inside this str
output parsers.ai.
So, let's import
a model first. Import
chat Google generative AI. Let's also
import
uh and then we'll import
prompts from langen core
prompt template sorry prompt template
okay we'll do this first we'll show how
we break down the steps and then we'll
link up together uh using chaining at
last So first of all I'm going to define
a model.
The model is the Gemini flash model.
Okay.
Now uh I will generate my first prompt
using prompt templates. So first
prompt
we'll define this using template one.
This is done using prompt template.
The template detail is write a detailed
report on
topic. The topic is going to be my
placeholder. But what will come inside
that placeholder is this input variable.
And this input variable is going to be
my topic.
Okay.
If I need to invoke this, I can invoke
this using this prompt one template one
dot invoke.
Uh
the topic is let's say uh English
Premier League.
English Premier League 2024
or 2023 2024.
Okay. So this is my topic.
If we want to check what kind of prompt
it's it generates, we can run this and
then see
that the that using the prompt template
we can generate such kind of prompts
here. Right? D report on English Premier
League 2023 2024. Simply this particular
topic is going to be replaced here in
the placeholder. Okay. I don't want to
print this. Now I want to generate my
second prompt.
[snorts] Second prompt, we'll define it
using template 2. We'll use prompt
template.
And then the template details is going
to be write a four point summary
on the following.
The placeholder is going to be text.
uh now after this what I'm going to do
is I'm going to define my input
variables which will be my text here. So
based on this I can again generate
prompt two. So prompt two is going to be
invoke from template 2.
Template 2 dot invoke and then my text
will be uh
okay where will my text come from? Okay,
my text has to come from this first
prompt here. So for that I will have to
invoke the model using this particular
prompt here. So model dot invoke. Okay,
let's write this result one.
And then model dot invoke uh this has to
be prompt one and then we need the
content of this one not the entire
response but only the content part or
the text part. So here what I'll have to
put is I'll have to put rest
result one.
Okay. Now based on this I'll have to
again invoke the model
prompt
to uh
prompt two.
But now I can print
my result dot content here. Okay.
So here we've only used prompt template
the first concept that we've talked
about. So we can define a template in
such a way and then generate the prompt
based on uh based on the template that
we've created using prompt template and
then invoke the model. So, but one thing
that you need to remember here is result
one if we only invoke the model and not
print its content. This is not going to
give me a standard string response here.
So for that what I need is I want to
take out the content part of this result
one. But still I might not be sure that
this provides me a string here. For that
I'm going to do str result just to be on
the safer side because the prompt needs
to be a result here or a string here
because the content we pass in this
particular text has to be a string. So
now what we can do is we can print out
result one to here and then we'll run
this model first then we'll invoke
invoke str output parser and jin
later on. Let's also use a divider
so that it becomes clear to us. Okay.
Now if I run this, I don't think I have
any errors. Let me run this.
Okay, let's go down up here.
So
here we have the
here we have the result from the first
prompt. So it is in in fact giving me in
terms of points. So this is a detailed
detailed uh report about
English Premier League. And then from
the second prompt we have a fourpoint
summary about the English Premier
League. It is summarizing whatever text
is being generated upwards here. Okay,
this is working well. But
we could have made this code a lot
shorter here. So how do we do that? Now
we introduce the concept of str output
passes to make sure that our output is
in string and then
based on that we also introduce the
concept of chaining so that we don't
have to write two different invokes
here. So now we're going to use that
particular approach. So I don't need to
clear anything much. I'm going to
comment this out. I'm also going to
comment these two lines out.
Uh I'll have my prompt one.
In fact, I don't even need to generate
prompt one, prompt two. So I'll just
have two different templates. I'll uh
import uh STR output parser here. So
langen code dot output parsers import
str output parser.
Okay, I'm going to remove all of this so
that you don't get confused here. So I
don't need this. I don't need this. What
I'll have to define is a chain. So my
chain is going to be
first
invoke template one then invoke a model.
Okay, I also need to define a parser
here. So, parser object from str output
parser class. Okay, so the model
response needs to be converted into a
standard string format using the str
output parser. Then after that, I'll
have to invoke model uh template 2
first. Then again, I'll have to invoke
the model, pass the prompt from template
to the model, and then use a parser
again. So all of those steps can be
reduced to this single step using the
concept of chain.
Now to get the result what I need to do
is I just need to invoke the chain and
not the model because the model
invocation process is already defined
inside this chain here.
So so
what parameter do we need to provide
here? If we look at template one the
entry point to template one is this
particular topic here. So what we need
to do is we just need to provide a topic
inside this.
The topic can be English Premier League
and then I can simply provide print out
the result inside this and still the
model will work but the model will only
print out the output of this particular
second template here. All of these in
between task will be handled by this
chain. So let's run this.
It will take some time to run because
there are two model invocations in
between.
[snorts]
So as you can see we can directly see
the output of this particular second
template here because everything in
between is being handled by this chain
and the final result is a fourpoint
summary that we get from this particular
topic here. So what is happening here?
Everything that we did earlier has been
happening here too. But the
but since we're only printing the final
result, we're seeing the final result
here. So this concept of chain is very
crucial in implementing such multi-step
workflows that we've talked about
earlier. In our previous video, we
learned how to use prompt templates to
give the model clear instructions and
how to use SCR output parser to clean up
the model's text responses. But what if
we want the model to not just write text
but actually return structured data like
proper JSON objects with specific
fields? That's where the structured
output parser comes in. Uh think of it
like giving the model a blue blueprint
for for its answer. So instead of
letting it respond however it wants, we
define a clear structure like for
example we can say give me three facts
about black holes and each fact should
go into its own field. fact one, fact
two and fact three. So we define this
structure using response schema which
defines uh what each field represents.
Then the structured output parser uses
that schema to make sure the AI's output
follows the exact format we asked for.
Uh usually that is a validation object.
Now, if you worked with LLMs
before or you followed our video, uh you
know that sometimes the model does not
exactly uh follow the instruction
perfectly. So, it might it might miss a
field, forget the brackets or add some
extra text. Now, that's where the output
fixing error or the output fixing parser
saves the day. It automatically
identifies when the output isn't in the
correct format and uses the model itself
to repair or reformat the response until
it matches the required schema. So what
we are trying to do here is we're really
teaching the model to think structurally
to not just like generate sentences but
also to produce clean structured content
uh that a program can directly read and
use.
So this actually becomes powerful when
we want to build uh systems like
knowledge extractors, facts, fact
generators or even uh data analysis
tools where consistency matters. And
just like we did in the previous video,
we wrap everything inside a chain. So
the flow looks like the following here.
So the prompt first defines the task,
the model generate the response and the
parser ensures the output is valid and
well formatted. So the concept brings
together everything we've learned so
far. Clear prompting, structured control
and automatic error correction, meeting
making our AI output uh also ready for
[snorts] uh automation. So now let's go
to the implementation of whatever we
talked in this video.
Let me create a new file called
structured
output parser.py.
Okay, first model.
[snorts] We've been doing this since our
first video. So I I hope you understand
this. Then the credentials.
Then we're importing a prompt template
from langen core.
Then we'll import our output parsers
here. So uh it will again come from lang
code output parsers import structured
output parser
and a response schema.
Okay I did something wrong here. So this
is going to be
oh okay so it does not come from langen
port but it comes from langen. So output
parsers import structure output parser
and
okay something is wrong here. Oh I
should have written from not
import structured output parser and then
a response schema.
Now uh from langchain.output parsers
I'll need output fixing parser to
uh so this library allows the model to
recorrect if there is any error in terms
of it response.
So load env I'll first define my model.
Okay. Now we'll define our schema here
or the response schema that we want.
So basically we're going to pass a topic
and then we will ask the model to
generate three facts and then we'll
create a placeholder using these
response schema for those facts here. So
so for the first one we'll call fact one
and then
uh it detail will be first fact about a
certain topic. Okay we'll say black
hole.
So this will be our first one. We'll
again define a response schema. We'll
have fact two
and then add a description.
So this will be second fact about black
hole.
And then we'll do a response schema
again. name equals to
fact three
description equals to third
fact about black hole.
Okay. So we will define our parser. So
this is going to be a structured output
parser but the structured output parser
will be based on the schema that I have
provided earlier. So structure output
parser from
response is schemas and then I'll and
and then I'll pass in the schema here.
So again at last if the model does not
uh obey us in passing
the in uh generating the output based on
this schema. I'm going to use a output
fixing parser
from
llm.
The llm is going to be our model and
then the parser is going to be a parser
which will strictly adhere the model uh
to the given schema here. So now let me
generate a template.
Uh
so template equals to I'm going to use
this using prompt template. We've
already done this earlier.
So template will be give me
three facts about
topic.
Uh
write something more. Return only valid
JSON instruction.
instruction that follows this format and
I'll provide a format here. Uh let me
enclose this using
string tag
so that I can give multi-line strings
here. So now uh the format
will be uh format or the instruction
format instruction
or we'll we will we'll call it the
response format here. So response format
uh the topic
sorry the input variables at first is
going to be our topic
and then I'll also define a partial
variables for the response format that
we provided here. So response
format and this is going to come from
parser dot get format instruction.
So this particular parser will provide
this schema or the response format to
this template here. Now I'll create a
chain where I'll pass in the template
first then a model first and then the
passer here.
So first of all the prompt will be
generated using template and it will be
passed to the model and then the model
response will be fixed by the parser.
Now I can invoke this chain
using the entry point which is our
template. So I'll need to provide a
topic here
and the topic will be black hole
and then I can print out the result.
Let me clear this and run this again.
Okay, I missed topic here. So, clear
this
and run it again.
There are a few spelling mistakes, but
it's okay. I can see that I got a JSON
format where I have fact one
uh fact two and
where's three? Okay, I have fact three
just like the way that we've asked for
in our schema map. Uh so what is being
done here? The prompt template is
generating
a prompt based on the template that we
provided here. The structured output
parser uh is is asking the model to base
its
response on on the basis of this
provider schema. Here the output fixing
parser is strictly
uh asking the model to follow this
particular schema. Here in the prompt
template one extra variable we provided
is the partial variables. that partial
variables is the
response schema that we provided here
and then we've extracted it from the
parser uh parser object of structured
output parser and then we've generated a
chain and generated the response. So I
hope this runs for you on your end as
well. If you in our previous video we
talked about how large language models
often gives us free flowing text
responses and then how we can use tools
like structured output parser to make
those responses more organized and
consistent. But now we're taking a same
idea into a step further by introducing
a much more powerful and professional
tool called the pyantic output parser.
So what exactly does it do? Now think of
it in this way. So when we ask a large
language model for structured
information like someone's name, age or
city, the model tries its best to follow
our instruction. But sometimes it gives
extra text, sometimes it forgets a field
or sometimes it just formats things
incorrectly. The parenting output parser
uh helps us control and validate those
responses. So it makes sure the models
output isn't just structured but also
it's correct, complete and reliable.
This parser is built on top of a Python
library called Pyntic. And Pyic is all
about data validation. It actually lets
us define what kind of data we expect.
For example, like we can say the name
should be text, age should be number and
then it has to be greater than 18 and
the city should be a word. So when the
model gives us a response, the parser
checks if everything fit those rules or
not. If something is missing or invalid,
uh the parser immediately flags it or
even corrects it when combined with a
tools like out tools like output fixing
output fixing parser. Now if you
remember the structured output parser we
used earlier in the previous video, it
also gave us structured data. So what is
the difference between these two here?
So the key difference is that structured
output parser mainly focuses on
formatting. It helps the model respond
in a defined structure but the pyetic
output parser uh adds this layer of
intelligence and validation on top of
that. So it actually does not just
organize the response but it also checks
the response and ensures that it matches
the exact data type and the constraints
we define. So that means uh we'll get
fewer errors uh cleaner data and a far
more predictable output which is
especially important when you're
building real world application. So the
beauty of this parser is that the once
the model output passes through it, you
can get a clean and reliable data ready
to use in your code or application
directly. Like you don't need to worry
about
converting things to string, fixing JSON
or like checking for missing fields.
Everything comes out neat and validated
exactly the way you want it. Uh so the
pyic output parser takes the concept of
this structured output and then upgrades
it with validation and reliability. If
the structured output was our first step
towards getting a structured response
from LLM then the penting version is a
professional grade tool uh the one that
makes your AI pipeline more stable and
then uh production ready. So that is why
this concept is so much important to
understand. Because as we start building
more complex uh
systems, we will rely on such kind of
validated structured output to make sure
our AI
uh gives us the output in the format
that we expect. So after this brief
introduction, let's go to the code and
see what we have talked about and
implement it in real life.
Okay. So I'm going to create a new file
here.
I'm going to call this pyantic
parser.py.
So
I will have some import. First of all,
our model
then envy
output parser.
Uh you can also use this output fixing
parser if you want but uh I'm not going
to do this right now. I've already shown
you how to use it in the previous video.
So from Pyantic I'm going to import base
model
and then I'm going to import a field.
Okay. So load the credentials
define the model
and after that I'm going to create this
pentic validation class called person.
So here I'll have base model as a
parameter I'm going to include three
things name age and string. So name is
going to be a string. its field and its
detail is
uh the person's
full name.
Then I'm going to have an age. Age is
going to be an integer and I'll also
define its detail. So the age uh has to
be greater than
18 and then it has to be less than uh
let's say 100.
Then I'll also add the detail or the
description.
The person's age
must be greater than 18. Well, I've
already defined that here. Now I'm going
to define a city. It is going to be
string.
Okay. Field
field field and its detail.
So
the city
where the person lives.
Okay. Now I have defined this class. I'm
going to
create an object a pyic parser
pyic output parser and then the pyic
object is the class that we have
defined.
Okay. Now let's create a template
from the prompt template that we have.
So the template is going to be I'm going
to create a multiple string or
multi-line string here.
So the prompt template is giving me give
me
give me the name, age and city of a
of a fictional
fictional place
person. Uh okay what else? Make sure the
age is greater than 18 and less than
100. Also uh return
return the response in following format
and the format is
our uh response format just like we did
earlier.
Okay. What else? I need a input
variable. Input variable is going to be
a place name
and then my partial variables. Partial
variables is going to come from uh this
is response
format and then it is going to come from
parser or the parser object dot get
format instructions.
Okay.
Now what I can do is I can directly
define a chain. So it will first call up
template then model and then the parser.
Uh then after that I can invoke this
chain
using the entry point to this chain
which is a place name.
So let's say Nepal
and then print a result here.
So the code seems to be okay. If you
have any problem viewing this, let me do
a word wrap so that you can see
everything else here. Now let me run
this code and see what we get in our
response. We should get the name, age
and city just like we asked for. Okay.
So if if we see here we see a name
Pracastahal age is 45 and city is
provided here just like in the format
that we asked for. So this is how we use
pyic output parser uh using lang graph.
So this is all for this video. I hope it
runs in your end as well. In our
previous video, we've already worked
with simple chains connecting multiple
steps together where the output of one
step becomes the input for the next.
Now, we'll talk about a concept concept
a bit deeper and talk about why chains
are so useful and how we can make them
more powerful using something called
sequential chains. The biggest advantage
of using chains is that uh they help us
organize complex AI workflows into
smaller logical steps. So instead of
sending one massive prompt and hoping
the models model handles everything
correctly, we break the process down.
Maybe first generate something then
summarize it and then finally maybe
analyze it. This modular structure makes
our workflow much more cleaner, more
reusable and then easier to debug. So if
one part of the chain isn't doing well,
we can just improve that step without
touching the rest. So it's a very
practical way to scale our AI projects.
Now let's talk about sequential chains.
Uh which is one of the most useful type
of chains in lang chain. So sequential
chains work in a step-by-step manner.
The output from one stage is is
automatically passed to the next stage
in order. So think of it like an
assembly line. So each component in the
chain has a specific role and together
they create a complete workflow. So for
example, step one could generate a
detailed article. Step two could
generate summarize it into a few
sentences and then step three could
maybe extract keywords or insights from
that summary. The entire process runs
very smoothly with data flowing
automatically between steps. So now
what's great about sequential chains is
that they makes complex reasoning task
easier to redesign and control. So we
can exactly fix what happens at its each
it stage and then how information moves
between them. And since everything is
modular uh we can mix and match our
components, change a prompt, switch a
model or maybe add a new processing step
all without rewriting the entire
pipeline. So in this video we'll
actually implement a sequential chain
and see how it helps us combine multiple
prompts and models into one connected
process. And by the end of this video
you will be able to understand not how
not how just to build one or not just
how to build one but also why sequential
chaining is one of the most powerful
ideas in line chain for creating
intelligent and multi-step AI
applications. So now let's go to the
code.
So I'm going to create a new folder here
called
uh it's folder number five called chains
and inside this chain I'm going to
create my first chain called sequential
chain.py
pipe.
Okay. So again, first of all, we'll have
a model
and then avo
template. So that is going to come from
langen core.
And then we'll also use this str output
passer that we've already used earlier
too. And then that is also going to come
from langen core.
Load our credentials.
Define our model.
Now I'm going to
provide two template. The first template
will be a prompt template.
We've done a similar example earlier
too, but uh I'm going to use it anyway
here. So, generate
three detailed
uh detailed report
on a topic
topic. Uh I'm also going to use my input
variables. My input variables will be a
topic. So this is going to be my first
prompt that will be generated from
prompt template. The next will be
another prompt that says
uh generate a threepoint summary.
Threepoint summary on following text
and then I'll provide a text here.
Uh
so
my input variables is going to be a
text.
And then uh I already have a model. So
I'll define my parser which will be from
str output parser. And then I'll define
a chain which is a sequential chain
here. So first of all I'll go to prompt
one. Then I'll call the model. Then I'll
go to parser and then I'll go to prompt
two. And then I'll again call a model
and then I'll go to parser again. And
finally I I'll invoke this chain and
then store the output in the result. So
chain do.invoke. I'll define an entry
point for this chain which is topic.
So,
so my topic is going to be
my topic is going to be uh 3II
interstellar.
Okay. Threei 3i atlas
interstellar object which is quite
uh in use these days. So I'm going to
use this topic and then I'll just print
out the result. So, so this here is a
sequential chain here. So, let me run
this and while this is being run, I'll
try to explain you what's happening
here. Okay, something's wrong.
Ah, this is a model name.
So, let me run this again.
So, we have our model. The first prompt
is generated from this prompt template.
The second prompt is generated from this
prompt template. So based on the first
prompt which is passed to the model and
then and then the output is passed
through the string output parser which
means the entire output will be
converted to string and then and then
that particular output will be passed to
this text here uh using prompt two and
the model and then the output will be
passed again by str output parser and
then finally be printed here.
So it might take some time to run
because we've got multiple models here.
So let's see the output.
Okay, we have a three-point summary
about 3i atlas. So the first point is
provided here. The second point is here
and the third point is here. The content
is not that important right now. Uh I
just wanted to show you the
implementation of chains. In our last
video we explored sequential chains
where each step in the chain executes
one after another in a fixed order. That
was great for workflows that all that
always follows the same path. But what
if you want your AI workflow to make
choices along the way? What if the next
step depends on the output of the
previous step? Now that's where the
conditional chain comes in. So
conditional chains lets you design
workflows that branch based on certain
conditions. So instead of a single fixed
path, you can define multiple possible
paths and the chain decides which one to
follow depending on the model's response
or some other criteria. So think of it
like you choose your own adventure book.
Depending on your choice, the story
takes a different direction. Conditional
chains give your AI workflows that same
flexibility. Okay, let's take an
example. So imagine you're processing
customer feedback. So if the feedback
mentions a bug, you might want to send
it to the development team. If it's a
feature request, you might want to send
it to a product team. And if it's a
compliment, maybe you log it and then
thank the user automatically. So with a
conditional chain, the workflow
evaluates the content and routes it
automatically. You don't have to
manually check each case. The chain
handles the branching logic. So the real
advantage of a conditional chain is that
they make your workflow dynamic and
intelligent. So you're no longer limited
to linear processes. Instead, your AI
can adapt its upcoming step based on the
data it sees. Now this is incredibly
useful for real world application where
inputs can vary widely like customer
support, content analysis, data
classification and much more. So in this
video we'll implement a conditional
chain and see exactly how to set up
multiple parts and conditions. So by the
end of this video, you'll understand how
to create a flexible AI pipeline that
responds intelligently to different
situations, making your applications
more robust and closer to human like
decision-m. Now let's jump onto the
code.
Okay, I'll create a new file here
and I'll call it uh
conditional chain.py.
First of all, we'll have a model.
then env
template.
Then I will have
uh the pyic output parser which will
come from length and core dot output
parsers import
identic output parser.
uh then I will use this runnable schema
dot runnable
importable
branch for
flexive branching chain logic and then
runnable lambda for a lambda function
since we using pye model so we'll
include a base model and a field
and also uh let me write this
literal class from the typing function.
Load the library.
Load the model.
Okay. Now,
uh I'll also have one more parcel which
is the str output parser.
So [clears throat]
my parser object will also be created
str out parser. Now I will create a
pentic class here for feedback.
So this will be a base model.
I will try to find the sentiment of the
feedback which will have two options.
And it can be the positive feedback or a
negative feedback.
And then its detail will be given here
field
and description
is the sentiment of
the feedback
feedback provider.
Okay. I will have a parser two which
will be my pyic parser or pyic output
parser
and then the pyic object will be my
feedback class.
Now I'll have a prompt one
which will be my prompt template
uh
prompt
template.
It template will be classify
the
classify the sentiment of
following feedback text into positive or
negative
and then this will be my feedback test
uh feedback placeholder. So input
variables
will be
feedback
and and I'll also have a partial
variable
uh because I'll provide a format
instruction here
uh statement and provide the response
in following format
and then this will have our response
format. Right. So I'll have a partial
variable
uh
uh okay this has to be a dictionary.
So response
format is going to come from parser 2
parser 2 dot get format instructions.
Okay I'm going to use a word so that uh
it will be easier for you to view here.
Okay. This is my [clears throat] first
first prompt here. I'll first define a
classifier chain
which will follow prompt one
model one
or the same model that I have. I'll just
use the model and then parser
and then parser two because I'm going to
use the pyic parser here. Next, I'll
again have another prompt
which is going to say
template
uh write an appropriate
feedback
to this positive
sorry appropriate response
response do this positive feedback
and then I'll provide write my feedback
here. I'll have an input variable.
Uh this will be feedback
and I also have prompt three
that will work for the negative
feedback.
So negative feedback
and then the value will be prompt three.
So here I'm going to declare a branch
chain.
Branch chain as we can see this is a
sequential chain we have here. But based
on the output of this chain now we'll
move on to the branch chain and then
either select one of these uh two
prompts and then generate the response.
Now
for branching I'm going to use runnable
branch
and not this bracket. I need a small
bracket here. So runnable runs I'm going
to declare a lambda function here. So
lambda x x [laughter]
sentiment
equals to
uh positive.
So, so if the sentiment is positive then
uh my chain will be prompt to
model and then parser
parser else
if my sentiment is negative lambda
x [snorts]
x dot
sentiment equals to
negative then my chain will be from
three
from three
model and then a passer
and I need to
declare this as a runnable lambda
uh lambda x. So if else if and then the
last one is else condition here.
So if the sentiments are either of these
positives and negative sentiment then
I'm going to say no valid sentiment
found in my
review or feedback.
Okay, the bracket should not close here.
So this is done.
Now what I'm going to do is I'm going to
combine these two chains into one chain.
So first one is our classified chain and
then the second one is our branch chain.
So we can also combine a sequential
chain and a conditional chain into one
single chain here like this. The first
one that is going to be executed is our
SE is a sequential chain or the
classified chain and and based on the
response of the classifier chain we will
then move to branch chain and then
uh the model will work accordingly. So
we'll invoke this chain dot invoke the
entry point is the classifier chain and
then and then in the classifier chain
the entry point is feedback one sorry
prompt one where I need to provide a
feedback here. So let me provide a
feedback and then see that
the phone is actually
actually amazing.
Let me print the result and see what we
get here. It should actually trigger
prompt two and then we should get an
appropriate response for this positive
feedback here. So let's run this.
So now we see that we get a positive
response and then uh there are uh
the the model classifies this feedback
as positive and then based on it it
generates a response. Now I'm going to
change this. Okay, the phone is actually
horrible.
The UI is stuck and then things like
that.
The UI is stuck.
And then if I run this
so the model is in fact giving out a lot
of things but you know that we can
structure the output uh using the pyic
output parser or this or or like we can
base it on some some form of schema as
we've already done earlier. So you can
uh actually uh implement it yourself. So
here we get that the response.
Okay. So it says that when responding to
negative feedback so it classifies the
feedback as negative here for this
thing. So in this video we saw how we
could implement conditional chains using
runnable branch runnable lambda as well
as how we could combine a sequential
chain and and a conditional chain into
one and then make our uh workflow
automated here. So far in the series
we've explored sequential chains where
each step executes one after the other
and conditional chains where the
workflow can branch based on certain
conditions. Now we're moving into
another powerful concepts in line chain
called parallel chains. So what exactly
is parallel chain? So in simple terms we
can say that parallel chain allows us to
run multiple workflows at the same time
independently of each other and then
combine their results. So think of it
like a team working on various tasks
simultaneously. So one team member is
taking notes, another is taking or like
creating quiz questions and and then
later on someone merges both into a
final report. So by doing things in
parallel, we can save time, make the
workflow more efficient and then handle
multiple aspects of a task at once.
And then that is also key advantage of
parallel chain uh efficiency and
modularity
because each component can focus on a
specific task without waiting for the
others to finish and once all the
parallel task are complete uh the output
can be combined in a meaningful way
creating a complete and comprehensive
result. So what are we going to do in
this video? So our plan for this video
is first we'll take a block of text then
we'll run two parallel task. One will
generate short and simple notes from the
text and then the other will generate a
set of short quiz questions based on the
same text and then finally once these
two parallel tasks are done we'll merge
the notes and the quiz into a single
comprehensive document.
This will show exactly how parallel
chains can handle multiple output at
once and then combine them seamlessly
making our AI workflow both fast and
organized. Now let's jump onto the code.
So create a new file
called sequent parallel chain not
sequential chain parallel chain.py.
Okay. First of all, our language model,
then av function.
Then I need a prompt template which will
we'll get from langchen.prompts.
Let's also use the str output parser.
And we'll have a runnable parallel from
line chain dots schema dot runnable for
our runnable uh sorry for our parallel
chain
to run here. Okay. Now let's load the
credentials. Define a model.
Now first of all I'll define a prompt.
First prompt will be
about
generating a short and
like let's say generate short and simple
uh nodes
for or from the
following or for the following topic.
Sorry. For the following
topic,
we would place a topic in the
placeholder. Our input variables is
going to be topic.
We're not going to enforce any kind of
schema for our
response right now. So, we'll just leave
it to this. Our next template will be a
prompt template.
And this will be about generating
uh generate
five short question answer from the
following
text
or input variable is going to be a text
and then after this I'll have prompt
three
prompt template it.
Um this will generate
uh
okay short question answer is done and
then the summary is also done right. So
we will have prompt three to merge the
provided
notes and quiz into a single document.
document uh
then I'll provide nodes
in nodes placeholder and then base
in quiz placeholder.
Okay. So now let's create an object of
our parser here.
So parser equals to strl output parser.
I'm going to create a runnable chain
parallel chain which will be done using
runnable parallel.
The first component of this chain is to
create nodes and I'll name it nodes. How
we'll run this is we'll first start with
prompt one then pass it to the model
and then uh model and then run a passer
on it.
In the second chain
we'll name it quiz. I will pass
prompt
prompt to
prompt
two
then model and then a passer
passer. Okay.
Now
we'll call our final chain which will be
prompt
prompt three and then
model and then passer.
Now we'll combine the sequential chain
here final chain and then the parallel
chain runnable chain. Here we'll call it
chains equals to first of all we'll
execute our runnable chain and then
we'll execute our final chain.
We'll invoke this chain
chain dot invoke
and then we'll uh provide an entry point
for this one. So the entry point to this
chain has to be a text
right. So a topic or a text here. Okay,
I'll just try to make it uniform. But
it's okay even if we don't make it
uniform. Uh because one is going to go
in topic and then the other is going to
go to text. I think uh I don't need to
make it uniform I guess. So let's
write a text here. I'm going to copy
this text from some someplace else. But
I'm going to I will be using a word wrap
so that you can copy uh copy this or
like use any text that you want from
anywhere.
So this text is a support vector
machines
uh view word wrap. Okay, everything is
wrapped inside this view. Now what I'm
going to do is I'm going to invoke this
simply invoke this text. The result is
going to be stored here. Result equals
to change.invoke invoke and then finally
I'm going to print my result. So this is
my parallel chain implementation here.
Let me run this code and then I'll
explain what is happening
here. Okay, while the code is running,
let's go up. We have prompt one uh which
generates a short and simple note about
a following topic. Prompt two is like uh
generating short answer question from
the following topic. And then prompt
three is about merging the output from
prompt one and prompt two. We have a
parser and then I I have initialized a
runnable chain where we uh run these two
chains in parallel at first. Then the
output of these two chains are are then
merged with the final chain and then uh
a response is created. So if we see here
so basically a complete node is created
where we have support vector machine
nodes. uh the note is created
here on top and then the quiz is created
here on the second and then both of
these are combined into one
comprehensible documents. So this is how
we uh implement parallel chain in
line chain. So I hope this runs on your
end too. If you have any questions or
have any or if you have any queries feel
free to comment down below and I'll try
to help you out. I will see you in the
next video again.
So now we see that we get a positive
response and then uh there are uh
the the model classifies this feedback
as positive and then based on it it
generates a response. Now I'm going to
change this. Okay, the phone is actually
horrible.
The UI is stuck and then things like
that.
The UI is stuck.
And then if I run this,
so the model is in fact giving out a lot
of things. But you know that we can
structure the output uh using the pyic
output parser or this uh or or like we
can base it on some some form of schema
as we've already done earlier. So you
can uh actually uh implement it
yourself. So
here we get that the response.
Okay. So it says that when responding to
negative feedback, so it classifies the
feedback as negative here for this
thing. So in this video we saw how we
could implement conditional chains using
runnable branch, runnable lambda as well
as how we could combine a sequential
chain and and a conditional chain into
one and then make our uh workflow
automated. Here we've explored different
type of chain, sequential chain,
conditional chains, and even parallel
chains. Each of these help us design
multi-step workflows and process
information in a structured way. Now,
let's focus on something simpler, the
LLM chain. So, what exactly is an LLM
chain? At its core, an LLM chain is just
a singlestep chain. It connects a
language model to a prompt template,
letting us easily generate outputs from
our LLM in a structured and reusable
way. Think of it as a building block, a
straightforward chain where we give a
prompt, the model processes it and then
gives an output. There's no branching,
no parallel processing. It is just a
simple clean uh workflow for a single
task. The beauty of LM chain is in fact
in simplicity and reusability. So for
example, you might want the model to
suggest a catchy blog title, summarize
an article and generate ideas for social
media task. So you create a prompt
template for the task connect it to the
model via the LLM chain and then you can
get or you can reuse that chain for any
input without rewriting your logic.
So in this video here's what we'll do.
We'll define a prompt template asking
the model to suggest a catchy block
title for a given topic. We'll connect
it to a language model using LLM chain.
Then we'll run the chain with a specific
topic.
And finally, we'll see how the chain
returns a creative usable block title.
So the following implementation is going
to demonstrate how LLM chain provides a
structured, repeatable, and simple
workflow for generating AI outputs. So
without any delay, let's go into the
code.
So I'm going to create a new file here.
I'm going to name it LLM chains.py.
As always, we'll first use our model.
import
chat Google generative AI
from env import
load.env
from
langchen_core.prompts.
I'm going to import a prompt template
and and the new concept today is from
lang chain dot change import lm chain.
Okay. So first of all load the
credentials.
Uh
we'll load a model
Gemini 2.5
flash.
Then we'll create a prompt
which will be used using prompt
template. I'm going to write a template
for this one. So suggest a catchy
blog title about a topic
and my input variable is going to be
a topic.
Okay.
Now I'll I will define a chain
which is going to be an LLM chain and
I'm going to pass a model to this and
then a prompt
to this.
Okay. and [snorts]
and then based on this let me write a
topic name let's say 3 I atlas
interstellar object
I'm going to invoke this chain so
response equals to chain do invoke I
need to provide a topic here
so topic is going to be my topic
variable
And then basically I'm going to print
the response to this. So everything is
done. Let me clear this. I'm not sure
about this LLM chain. Uh it might be
deprecated or something. Let's see.
Okay. So this is deprecated
uh with the method. Okay. Runnable
sequence. So we've already done runnable
sequence earlier, but it's okay. We're
doing something that has already been
deprecated. But still we get our answer
here. Uh the topic is threei atlas
interstellar object. Here are some
catchy blog titles. Since we have not
structured or directed our uh response
to be something specific that's why the
model is generating this long text here
uh as a catchy block title which we
don't don't in fact want but like uh it
is what the model is uh providing me. So
here's what you can do. You can uh use
some form of response schema or maybe a
parentic class uh to direct
[clears throat] the model to provide
just a catchy block title just a a
combination of three to four words
uh
block title for this particular topic.
So I'm going to end this video for now.
Uh if you have any problem or any page
do comment down below and I'll try to
help you out and I'll see you in the
next video. In our previous videos we've
worked with LMS and change giving
prompts and generating responses and
even combining multiple models to create
structured workflows. Now we're moving
into another fundamental concept that
powers intelligent retrieval search and
reasoning in modern AI agents.
Embeddings. So what exactly are
embeddings? Think of embedding as the
numerical fingerprints of text. So when
we feed a sentence like Delhi is the
capital of India into an embedding
model, it doesn't just read it as words.
It converts that text into a vector
which is a list of numbers.
Each number in this vector represents a
small piece of meaning from the original
text. Together they form a semantic
representation. meaning that two text
with similar meaning will have vectors
that are close to each other in this
higher dimensional space.
So we'll make this intuitive. Suppose if
you take two sentences like Paris is the
capital of France and Delhi is the
capital of India. The embeddings will be
very close because both talk about
capital cities. But a sentence like I
love pizza will have a completely
different embedding far away in vector
space. Now why do we need embeddings?
Embeddings are basically essential
whenever we want our AI to understand
the context or meaning rather than just
a plain text. They help us with semantic
search meaning finding documents similar
in meaning not just by matching
keywords.
It also helps in context retrieval
that means fetching relevant chunk of
information to feed on into an LLM. They
help us with clustering and
classification which means grouping
related ideas together and they help us
with recommendation system meaning
suggesting similar product or content
based on its meaning.
So in this video we'll use hugging face
embeddings specifically all the
specifically the model
in this video we'll use hugging face
embeddings which is a lightweight and
efficient
in this video we'll use hugging face
embeddings and a lightweight model from
the hogging face embeddings so we'll
generate embeddings for a single query
that is the Delhi is the capital of
India and then a list of related
documents. Then when we print them,
we'll see a long list of numbers and
those numbers will be the numerical
representation of our text. But what's
more important is that we will use those
emittings to compare similarities, store
them in a vector database or build
context aware AI agents that retrieve
information intelligently. So main
advantages of these embeddings are huge.
First of all, they allow semantic
understanding, meaning the model can
reason about meaning rather than exact
wording. Second, they make retrieval
augmented generation possible or rag
possible where an AI can look up context
before answering. And the third, they
significantly improve accuracy in search
question answering and knowledge based
systems. So embedding acts as a bridge
between the language and numbers
allowing machines to understand and
compare the meaning of
provided text or document. And as we go
further in this series, we'll see how
embedding power intelligent agents
enabling them to remember, reason, and
respond more accurately. But for now,
we'll see the very basic implementation
of these embeddings. So let's go into
the code.
Okay, I'm going to create a new folder
here
and then call it embeddings.
Uh
after that I'm going to create a new
file. Let's call it hugging face
embeddings.
uh hing face embeddings
do pry. Okay. Now the import is going to
be quite different here. So first of all
I think I'll need to install something.
Let's go to requirements
here. I'll need to install lang chain
auging fishing
face. Okay. I'm going to install this
these requirements as as everything else
is installed. The new library will also
be installed in addition to the other
libraries that we already have.
Okay, this is installed. Let's go back
to our code and then from here we'll
import from
langchain_hugging
face import
hugging face embeddings.
Then we'll re uh input env
uh load env.
And then we'll also import OS. I'm not
sure if this is going to be used or not,
but anyway, let's import it and I'll
load my credential. Okay. So, what kind
of credential am I loading here? So, to
use hugging face embeddings, you need to
visit the hugging face website and
create your API key and then save it in
the environment file here. So you can do
that yourself or take the reference from
the hoggingface documentation too or the
lenins documentation and then uh you can
create that create that hogging API key
and then paste it in your env and and
then that will be loaded here using this
load env function. Anyway I'm not going
to go to that. I've already placed my
hogging face embedding key here. So I'm
directly going to run this. So
embeddings equals to hogging face
embeddings. And then I'm going to use a
model here that I found out in the
hogging face model section. So this
model is sentence
transformers
all
mini LM
L62.
Uh I I might also need to import
sentence transformers here.
So let me write that down. And then
uh
install the requirements install minus r
requirements. txt.
So while that is being installed, let's
continue with our code. As I said, I'll
have a text,
simple text. Delhi is the capital of
India.
Uh,
and along with this, I'll have some
documents
where I'll say Delhi is the capital of
India.
Uh,
Kolkata is the capital of West Bengal.
and
then Paris is the capital of France.
So what we can do here is like I said we
can convert this textual data into
numerical figures. So to do that I'm
going to use embedding model dot embed
query
embed query and then inside this I'm
going to pass this text here. So if you
print this out st result if you print
this out then you'll see that this
particular text will be converted to
numbers okay so let me run this now the
library is now installed I can run this
code now
it will take some time to run because
the model might not be downloaded so it
will download the model first and then
uh run our program here okay so once I
printed did this and the code has run.
We can see that this particular text is
now converted into into this combination
of numbers here. Now this is called
embeddings.
Not only this, we can also embed our
documents here. So if I just type
So if I just type result doc equals to
embedding dot since we embedding
documents so we need to pass in embed
documents and then pass in the documents
in this list. Now if we print this uh
we'll see that we have uh embeddings for
each of these text in a document. So we
basically get a list of embeddings and
then each embedding will
resemble each sentence that we see here.
So I think you can run this program on
your end and then see the result of your
embedding here. uh what we what we'll
also do is we'll also try to compare the
similarity of this particular text with
these documents here. So let's see what
happens. Uh what we'll do is we'll do
result here and then result do here.
Now we'll compare the similarity score
and
and then for the similarity score we'll
use cosine similarity. For that we'll
need to uh import something here or
basically need to install a library
install pip install scikitlearn
and then based on that particular
uh scikitlearn library
we'll import from
skarn.mmetrix
dot dotpise
we'll import cosine simarity
And now to compare our uh
similarity embeddings between our text
and our document, we'll use cos and
similarity. And we'll pass uh
our result here.
The result which is the embedding of our
text and then we'll pass our document
embeddings which is result_doc.
So let me print out the similarity
scores here. So
let me simply print out similarity
scores whatever we get here
and then run this program again. So as
you can see uh the text
is uh the similarity between the text
and then and then the first document is
almost 98%. Similarity between the text
and the second document is almost 47%
and the third and the third document is
all is 27%. Now if I add a a new
sentence like uh I love pizza here we'll
see that this similarity score is quite
low in comparison to the above three
sentences. Let's print it out. Let's
clear this and then run this model
again.
So we can see that
the text Delhi is capital of India is
very much lower similar to I love pizza
which almost 9% similar to pizza. So the
highest highest similarity is these two
sentences. You might get why did not why
we did not get 100% similarity because
here uh there is a slight mistake while
I've typed this word. So if I type Delhi
is the capital of India and then we get
the same sentence here it would
basically be 100%. Let's see if if we
get a 100% similarity between them or
not. So yeah we get a 100% similarity
code between these two sentences. So
this is the importance of uh embeddings
basically converting the text into
numbers represent the semantic meaning
of that particular text uh which is
easier for computer to understand and
everything from now on that we'll do
will be based on these embeddings.
Basically if embeddings were not
possible uh retrieval augmented
generation would not have been uh
possible. In the last video, we explored
the concept of embeddings. How they
transform text into numerical
representation that capture meaning.
Now, we're taking
one step further and actually use those
embeddings to make our model retrieve
and reason over real data. So, in this
video, we'll build a simple retrieval
augmented generation or a rack pipeline.
a system where the model doesn't just
rely on its internal memory but instead
looks up relevant information from
external sources before giving an
answer. So we'll break down step by
step. The [snorts] first thing we do
here is load a document using a text
loader class. So it's a straightforward
way to bring in external data. Uh for
example, a text file, a web page, or
even a PDF. Here uh we're going to use a
txt file. uh which might contains note
research summaries or articles. Next, we
use the recursive character text
splitter. Now, this is a very clever
tool. Uh what it does is it breaks large
text into smaller manageable chunks,
usually around 500 characters each in
our case.
But why do we do that? Uh we do that
because language models and embeddings
work much better when the text is short
and focused.
So by so by uh giving them the byite
side chunk with side overlap we have the
model preserve the context while
avoiding cut off issues especially
useful when dealing with large
documental books. Now once the text is
split we move to the embedding step. So
here we're going to use uh Google's
Gemini emitting model. Each of these
chunks will be now converted into a
vector or embedding just like we saw saw
in our last video. [snorts]
Now these embeddings need to be stored
somewhere efficiently and that's where f
comes in. So f uh stands for Facebook
aim Facebook a similarity search uh and
it is a powerful open source vector
database that allows us to store and
search documents very quickly.
Uh so using F what we do is uh each
chunk of our content is embedded into a
vector and then those vectors are stored
in a f index and f prepares itself to
find the most similar vectors whenever
we search. Then uh we'll go for
something called a retriever. So what
happens in a retriever is it is like a
search engine for our AI agent. So when
we provide a query for example like what
are the key takeaways for our documents
the retriever searches through all those
embeddings inside files and then returns
the most semantically similar search. So
this step ensures that our model doesn't
try to remember everything. Instead what
it does is it retrieves the right
information on demand. Uh so after that
[snorts] we combine all of those
retrieve text chunks into a single
string of context. This context acts
like the background material or nodes
that we pass to our language model.
[snorts] Then finally, we initialize our
Gemini chart model and manually
construct a prompt that combines both
the context and the question. So when we
pass this prompt to the LM, uh it uses
the provided context to generate a
factual context of their answer and then
not something made up from its internal
knowledge. So this workflow is is the
backbone of what's known as retrieval of
media generation or rack.
&gt;&gt; [snorts]
&gt;&gt; Instead of letting the model guess or
holen it uh we ground it with real data
here so the benefits are uh huge the
model
gets more accurate and reliable. Uh it
can also handle custom data sources and
it's far more scalable since we can swap
documents add new data and then do
something like so in this video we've
taken [snorts]
we will take a big step from embeddings
to retrieval. Now uh let's go to the
code.
So I'm going to create a new folder here
and call it rag or basic rag for now.
Let a new file
basic rag.py
what I'm going to do is I'm going to
create a docs.txt txt file
and then uh bring uh bring some
paragraphs of content. Uh here I already
have it. So [snorts] I'll have this
content about AI. I'm going to do word.
You can bring anything else that you
want here. It's fine.
So yeah, this is my uh document for now.
Okay, we'll go back to our basic ad
model. So I'm going to generate my model
first
and then
env
uh I'm also going to load a text loader
from Langen community.
Then I'll have a texter
from Langen.
Then uh let's also import our embeddings
from
Google geni.
Then we'll use vector stores
to store our embeddings here and then
we'll use a retrieval QA. So from lang
chain
of chains
put retrieval not this one
retrieval QA okay
now based on this first first let me
load the credentials load the
credentials
okay I'm providing [snorts]
stepwise
uh information here. So step one, I'm
going to load my credentials. In step
two, I'm going to load the document that
I have. So I'll be
defining a loader and then using this
class here called text loader. My file
name is docs.txt
and then I'm going to load this. So
documents equals to loader.load.
In step three, I'm going to split the
text into smaller chunks.
Smaller chunks.
So for this, I'm going to use a text
splitter.
Recursive text. Sorry, recursive
character text splitter. Uh I'm going to
define the chunk size of each
uh splitted text. Let me give 500. and
then also chunk overlap because uh
we might carry some contextual
information in while breaking those
chunks. So we want some overlap so that
uh two different chunks can have
something common something in common
between them. So I'm going to split my
document using the splitter. So text
splitter dotsplit documents
documents
Okay, now I'm now I'm going to
Okay, this is step three.
I'm going to step four where I'm going
to convert
text embeddings and store in.
So here what I'm going to do is I'm
going to define embeddings equals to
Google gen Google generative embeddings
where the model is model/
Gemini embedding
uh 001. I found this in the langen
documentation itself.
And then I'm going to define a vector
store using phase dot from documents
docs sorry
from documents docs comma
then in step five I'm going to create
create a retriever
which fetches
relevant and documents.
Okay,
retriever equals to vector store dot as
retriever.
And then I'm going to initialize a
model.
Sub model equal to chat Google
generative AI model equals to Gemini 2.5
you can even use hoging face models here
step seven
I'm going to create a
retrieval QA chain
so chain equals to retrieval QA dot
from_chain
type where llm equals to llm and
the retriever equals to retriever
or model here
then uh
I'm going to manually query
manually query
uh
model and retrieve and relevant
documents.
So query is going to be like okay what
is the document about? Let's see.
So it is about AI. So my query will be
very simple. So what are the key
takeaways
from the document
and then based on this query
I can invoke my chain here. So chain dot
invoke query and then finally I'm going
to print my answer.
So print
response. Okay, the code is done. I have
tried to provide you stepwise
implementation of what we talked about
in the rack pipeline. So let me run
this. Let me also save this document and
then run this here.
So the model will provide answer based
on okay I need to something I need to
install something here. So 5G CPU needs
to be installed. Let's write it down in
requirements.txt2
GPU
install this with install minus rates.
txt.
Okay, we can install the CPU version
because my laptop not be equipped with
GPU here.
Okay, so let me run this once again.
And
while this is being run uh the answer
from the LLM is based on the documents
that are provided and not on its
internal knowledge base. So the key so
the key takeaways are uh
are like actually provided here. uh if
we need to ask spec something something
specific from this document like uh what
is this so
doing such tasks such as learning reason
AI comprise okay what kind of technology
that AI comprise let's ask that
so what kind of technology
technologies
does I
comprise of. So if you ask this question
then
the model is going to provide the answer
based on the document and not its
internal knowledge.
So it says uh AI comprise a variety of
technology. It's copied just from this
particular context here. So this is how
we build a basic direct pipeline. I hope
you understood the concept. We've been
exploring how different types of chains
help us connect multiple components
together from simple prompt to model
pipelines to more advanced parallel and
conditional chains. But now we're going
one step further and introducing a
concept that gives us even more control
and flexibility in how our data flows
through a chain. Runnable sequence. So
what exactly is a runnable sequence? So
think of it like a conveyor built for
data. Each step in the sequence takes
the output from the previous step,
processes it and passes it along to the
next one. It's part of the new runnable
interface in lang chain which unifies
how different components like prompts,
models and parsers communicate with each
other. Earlier when we worked with LLM
chain, the structure was was quite
fixed. One prompt, one model, one
output. It worked great for single
single or simple workflows. But when we
wanted to do something more dynamic like
taking a model's output, processing it
through another template, and then
feeding it back to another model, things
start to get messy. That's where a
runnable sequence shines. It lets us
build multi-step pipelines where each
element can be a model, a parser, or
even another chain all connected in a
single streamlined sequence. You can
think of it as a customizable chain
where you fix exactly how data flows
from one step to another. Now, in this
video, we'll be doing something fun to
demonstrate it. We'll start with a
prompt that asks the model to write a
joke about a given topic. Then, we'll
take that generated joke and then feed
it to a second prompt, one that asks the
model to explain the joke. And then
finally, we'll use an output parcel to
cleanly process and present our final
response. So by the end of this video
you'll understand how runnable sequence
works, how it is different from earlier
chain structures we've seen and how you
can use it to build smooth multi-step
reasoning pipelines where the model's
output becomes the input for the next
stage.
All right, so let's dive into the code.
So I'm going to create a folder here
called Runnables
and then uh file called runnable
sequence.py.
Okay. So first of all import lang Google
genai
then I need av
after that I'm going to import prompt
template which will come from langen
core.
Then I will import an STR output parser
that is also going to come from lang
core
from lang
from line 4 dot output passers import
str output passer And after that I'll
need a runnable sequence which will come
from langen dots schema dot runnables
import
runnable sequence.
Okay. So first of all load the
credentials.
I'll define my first prompt where
using my prompt template and then the
template is going to be about write a
joke about
about a topic and inside this my input
variables is going to be
topic.
Similarly write second prompt
again using a prompt template where my
template
will be explain the following
joke
and then I'll type in joke here my input
variables is going to be
joke.
Okay. So my model will be from chat
Google AI.
The model will be Gemini
2.5
flash.
I'll also define my output parser using
str output parser
and then based on that I'm going to
define a chain or a runnable sequence.
So first of all I'm going to go to
prompt and I'll go to model then parser
then prompt two
then model
and then passer again
uh and then result I'll just invoke this
runnable sequence
book.
So the topic
is going to be a monkey like and then
and then finally print a result here. So
this is how you implement runnable
sequence. You might see that this is
similar to the concept of chains that we
used. We if we had used chains we would
do it in this way. So change [snorts] it
would be prompt
model
uh cursor
then prompt two
and model
and then parser again. So this is how we
would implement chain but here we're
doing it with runnable sequence. Let me
run this code.
It will take some time because we have
multiple model calls here.
One one model call is this part and then
the other model call is this part and
then without the completion of the first
model call uh we cannot go to the second
model here. Okay. So couldn't feel the
love anymore. So the humor breaks in.
Okay, it's only printing me out the
explanation about the joke. But uh but
it's okay. Uh uh it's okay because uh
I've got what I wanted at the end of
this runnable sequence. In the last
video, we explored runnable sequence
where each step in the chain passed its
output to the next creating a smooth
step-by-step flow of data. But now we're
flipping that idea around and diving
into something that works side by side
instead of one after another, which is
runnable parallel. Simply put, runnable
parallel allows us to run multiple
sequences or tasks at the same time in
parallel. While runnable sequence is all
about order and dependency where where
step two awaits for step one, runnable
parallel is about independence and
speed. Each branch runs simultaneously
on the same input producing multiple
outputs at once. So imagine you're a
content creator and you have one topic,
say AI, and you want a tweet, a LinkedIn
post or maybe an Instagram caption about
it. So instead of generating them one by
one, Runnable Parallel lets you generate
all of them in a single call. So that's
exactly what we'll be doing in this
video. We'll create two separate
prompts. one to generate a tweet and
another to generate LinkedIn post about
the same topic. Then we'll wrap them
inside a runnable parallel. So both
prompts are executed at the same time
using the same model. The final output
will give us both results neatly, one as
a tweet and then the other as a LinkedIn
post. So the real advantage here of
runnable parallel is efficiency. When
you're building complex AI workflows
like summarizing document, generating
multiple content format or running
multiperspective analysis, runnable
parallel helps you process everything
faster and more efficiently uh without
waiting for one task to finish before
starting the next. So in this video
we'll see runnable parallel in action,
understand how it complements runnable
sequence and also explore how combining
both can help us build truly powerful
multi-step multi-output pipelines. So
let's get started and then go to the
code.
So I'm going to create a new file
called runnable parallel
py.
Uh I'll be using the same import. So I'm
just going to copy it from here. Okay.
So these three imports are the same. Uh
these four imports are the same. And
then I will need to uh call up runnable
parallel and runnable sequences. So
langen dot schema dot runable
import
runnable
uh sequence and then a runnable
runnable parallel. Okay. So this is also
same uh I'll have prompt one which tells
me
or which is about
generating a tweet. So generate
a tweet
about
topic
and then the input variables here
is a topic
and another prompt is also the same. So
I'm just going to copy this.
So prompt two is generate a
LinkedIn post about a topic.
So this is going to be prompt two.
I'll define my model.
So model equals [snorts] to
representative AI
model= to Gemini
2.5
have a parser.
So parser equals to str output parser
and after that I'm going to implement a
parallel chain
using uh runnable parallel. So a real
parallel chain equals to runnable
parallel and inside this I'll have two
different runnable sequences.
Okay. So the first one will be named tw
and inside this I'll have a runnable
sequence.
uh tweet
it is going to be a runnable sequence
where I'm going to pass in prompt
one model and then passer and similarly
I'll also have second
runnable sequence
it will be about two model and parser
now uh going to invoke this par chain
par chain.invoke.
Uh the topic is going to be
uh
we will do runnable parallel
in lang
and then I'm going to print the result.
So this is how we implement a runnable
parallel in lang. uh we'll need uh
chronable sequences for individual
parallel events here.
So let me run this and then see the
output. Again it is going to take some
time to run because I have multiple
runnable sequences here. So these two
need to be implemented but they'll be
implemented simultaneously at once and
then I'll get the final output here.
Uh also if we need to maybe bind our
output in some for some some form of
schema we know that we've already done
that. Uh we can do it using py or maybe
uh maybe by defining some for some form
of uh schema and then we have done it in
our earlier videos. If you haven't seen
that uh you can go back and then check
it out and try it yourself. So here we
have a tweet and then down here we'll we
also somewhere have a LinkedIn post uh
that we need to find but at least it's
there somewhere uh it is there somewhere
around there and then uh in in case of
LinkedIn post I think I'm getting some
code to code to here like these these
lines I think are the codes here so
anyway we have implemented a runnable
parallel uh in this particular program.
Today we're diving into one of the most
essential building block of any Langen
project which is document loaders. So if
you've ever wondered how we bring data
from the real world like PDFs, text
files, websites or CSVs into our AI
pipeline, this is where it all begins.
So document loaders actually act as a
bridge between raw data and intelligence
system. So what exactly are document
loaders here? So think of them as a
specialized tool that read and import
data from different sources into langen
standardized format called documents. So
each document contains not just text but
also
metadata things like file name, source,
URL or page number which later helps us
organize and query data more
efficiently. In simple terms, they are
the data injection engines of blanken.
And today we'll look at five most uh
five of the most commonly used ones.
Text loader, pipe PDF loader, directory
loader, web based loader, and then CSV
loader. So first of all, we'll start
with the text loader. As the name
suggests, this one deals with plain text
files. So it's perfect when you have
.txt documents, maybe transcript logs or
notes that you want to feed into your
system. where uh it it opens the file,
reads the text and wraps it into a
length and document object which is
quite lightweight, fast and ideal for
structured plain text data. The next is
a PIP PDF loader. one of the most
popular loaders especially in editable
based AI system because PDFs are
everywhere in research paper, invoices,
reports, books and they often carry a
lot of valuable information. So, PIP PF
loader extracts the text from each page
and converts them into a structured
documents keeping track of the metadata
like page numbers and file sources. And
then it makes it incredibly useful when
you need to query or summarize specific
sections of a PDF rather than treating
it as a single long text. Now, what if
you have hundreds of files sitting
inside a folder and that's where
document loader comes in. instead of
manually loading each file uh sorry
directly loader. So directly loader
scans through your folder and
automatically loads every document that
matches your design pattern. For
example, all all txt or PDF files. It is
perfect for large scale projects like
when you're building a chatbot trained
on a company's entire set of initial
docu or internal documents. So it also
saves you from a lot of manual effort
and ensures consistency across all your
data sources. Now uh this is where
things get interesting because not all
data lives on your local machine.
Sometimes the information you need uh is
scattered across web pages, blogs or
online articles on the internet and then
that's where the webbased loader comes
in. uh this loader fetches the data
directly from a web page URL, extracts
the text content uh cleans it up and
then turns it into a usable document. So
it's extremely handy for web scraping or
live data retrieval. For example,
pulling FAQs from a website, news
content from an article or product info
from an online store. With web based
article, you're not just limited to
static files. uh your AI can now learn
from live evolving web content from the
internet. And finally, we have a CSV
loader. So that's the one built for
structural table data like spreadsheet
data sets or analytical reports. Instead
of reading the file as raw text, CSV
loader processes each row of your CSV
file as a separate document allowing the
model to understand and retrieve uh
structured information easily.
especially useful when combined with
retrieval or Q&amp;A uh components where you
might want your AI to answer questions
like which product has the highest
highest sales last month or like who's
the top uh growing employee and and then
all sorts of stuffs like that from a CSV
based data source. Uh so as we have
completed uh these five these five
document loaders so each loader solves a
unique problem here. First
text loader helps with plain text. PIP
PDF loader handles structured PDF
documents. Directory loader manages bul
data. Webpage loader brings in dynamic
online content. And CSV loader handles
structured text. Now we're going to see
the implementation of each of these
loader in our code.
Okay. So first of all I'm going to
create a folder
uh what is it number nine and I call it
document loaders
and then we'll start with the first one
which is our text loader.py.
Okay. I'm going to uh create a few files
here. Let's uh
okay for this we'll use the docs uh docs
txt file that we already have and then
we'll load this. Let me word so that you
can see the content of this what it's
written here. I'm not going to use the
model uh I'm just going to use the
loader and then show you by loading the
documents. So from line chain community
dot document loaders import
text loader.
Okay. Uh now you need to create a loader
object from this text loader. So text
loader and then provide a file name
cricket uh sorry docs.txt.
And then if you need to provide an
encoding uh you yeah you you can also do
it. So let's do it. Encoding UTF8
and then once you do this you can print
out your docs here. Um so first of all
let's do loader.load
and then you can print out your docs
here.
Okay. Uh what did I do here? Let's exit
this and then run this again.
So as you can see the content inside
this docs folder has been loaded in
loaded from my file here and then the
source is docs.txt. What you can also do
is you can also see the type of this
docs.
So if you print the type of this docs
then you'll see
that the type is list here in in the
list you have uh document which encodes
the metadata that encodes source uh and
then inside the document we also have
page content here. So you can directly
also use this. So print docs dot sorry
docs
zero dot page
page content which will directly print
you the content inside the text that we
have or you can also print out the
metadata
doc0 dot
metadata and then you'll be able to see
the meta which is the source that we
have. Okay. So this is
our first loader that we have.
Now we'll move to our second loader
which is our PI PDF loader.
New file.
So PIP PDF
loader.py.
Here I'm going to copy a simple PDF file
from somewhere uh and then [snorts]
paste it here. Okay, you can grab any
PDF file that you want. Uh,
and I'll go to PIP PDF
loader and I'll import from
langen
community document loaders import
by PDF
loader. Now
initialize the loader
object
and my file name is uh dl curriculum
PDF. So I'm going to use that same file
name
PDF
just say docs equals to uh
loader dot load and then print out the
doc.
So whatever is present in this document
will be loaded and then printed out.
Okay. So we need to install PIP PDF. So
let's do this
pipf.
Okay. and then uh type install. We'll
just do pi pig. Everything is else is
installed here.
And now that is installed, we can run
our program again which is by PDF
loader.
Where is that?
[snorts]
And you can see our PDF page content has
been loaded and it is loaded in the same
format which contains a a list and then
that contains a document. Document
contains some metadata inside this and
then along with that it it also has its
page content. So you can follow the same
process as as we did earlier here to
print out the page content as well as
the metadata. Okay. Now we'll move to
the third type and then our third type
is going to be our directory loader. So
for that I'm I'm going to uh
copy some PDFs inside a directory. So I
basically have these AI uh AI books and
a financial report. uh and then using
using this PDF inside this directory I'm
going to load the content of this
particular books folder here. So let me
open a new file. I'm going to call it
directory loader.py.
Uh you can choose any any kind of PDF
inside a folder and then that will work.
Okay. So from langen community dot
document loaders
import uh directory loaders
and then we'll also do pi pdf loader
because we have pdf content inside this.
So loader equal to directory loader
directory loader and then we'll provide
a path to this. So path is a folder
called books
and then inside that I have uh contents
like a file name dot PDF
and then I'll also define a loader class
and my and my loader class is going to
be a pi PDF loader. Okay. So
what I can do is I can do uh
loader dot lazy load
and then for
document
in docs
we'll only print the meta data here. So
document dot meta data. Okay, let me
clear this out and then run this again.
So, I have four different
uh PDFs here and
and for those four different PDFs,
my document loader has has started
loading whatever it can find inside it.
So since there is a lot of metadata that
I have, so it has been loading uh every
metadata it can find inside my
uh folder here.
So it's loading each and every page.
That is why the metadata uh print is is
going on. But I think you understand
what we tried to do here.
So I'm going to do terminate this.
And now we'll move to our
fourth [snorts] one or our fourth loader
which will be our webbased loader. So
let me create a new file
web base loader.py.
I'm going to import webbased loader from
langen community
dot document loaders import
the [snorts] web base loader. Okay,
now I don't need anything else. I'm
simply going to copy a URL about uh
about a car review that I could find on
the internet.
So from this
I'll use loader
equals to web based loader and then I'm
going to pass the URL
docs equals to loader.load
and then I'll print out the docs here.
So this will open a website that
contains a car review uh on it and then
I'm loading I'm trying to load the text
present in this particular URL here. So
let me run this.
Okay. So it also needs beautiful soup
because it will it'll I think go for web
scripping of this. So reinstall PS4
here.
I'll also mention it on the
requirements.py. Okay. So since this is
installed, so let me go and then run
with base loader again. We clear this
and run this.
It might take some time because scraping
might take some time here. And then you
can see that all of the text inside this
are loaded and then it's again in the
same format. So so the format is
uniform. It has a list inside it. It it
contains a document inside. we we have
metadata and then there are a lot of
other contents in the metadata as well
and then the page content actually
contains the text content of that
particular website. Now finally uh I I'm
going to uh work on the last last type
of loader which is a CSV loader. So for
that I'm going to paste a CSV file here
on my on my project folder. So you can
see the CSV file that I have.
It's just a comma separated file that
contains some data. You can use any kind
of CSV file that you want. And now I'm
going to go to document loader. I'm
going to create a new file. Five CSV uh
CSV loader.py.
Okay. Now again the import is the same
from langen community.
We import document loaders. uh import
CSV loader and then the loader is going
to be an object of CSV loader. Inside
this I'm going to provide a file path.
File path is going to be what is my file
name. So social network ads dot CSV. Let
me rename this and then write data dot
CSV so that it'll be easier for me to
write down the file name here. So data
dot CSV
and data equals to loader do.load upload
and then if I print data here I will be
able to see my CSV file content in this
let me run this and you can see it's
again pre being presented in a similar
format inside the list there is a
document and there is a metadata then
inside it it contains source and page
number here uh sorry page content here
so this is how how how we use uh these
five document loaders
this is an essential part of our
retrieval augmented generation system
because uh on the very first step we
need to load uh we need to ingest our we
need to load our data uh into our rack
system which can be used as a knowledge
base for our rack project. So this is
how we use it. I hope you understood the
concept and also the implementation
works for you. So imagine you're
building an AI assistant that needs to
remember everything you ever taught it
from long documents, research papers, or
even notes scattered across multiple
files. Now, you wouldn't want your
assistant to go through all those
documents every single time you ask a
question, right? It would be like asking
a librarian to read every book in the
library before giving an answer. And
that's where vector store in line comes
into play. They're like the memory banks
of AI system. So far in this langen
journey, we've talked about how to load
data and how to split it into smaller
manageable pieces. We've also seen how
to generate embeddings, those numerical
fingerprints that represent meaning
rather than just words. But once we have
all those embeddings, we need a place to
store them efficiently and a way to
search through them intelligently. And
that's what a vector store does. You can
think of a vector store as a smart
semantic database. Instead of storing
plain text, it store those
highdimensional numbers vectors uh that
capture the meaning behind the text. So
when a new query comes in, the system
converts it into an embedding too and
then compares it with the stored vectors
to find the ones that are most simply in
the meaning. So even if your query
doesn't use the same word as your data,
it can still find the right answer
because it understands what you meant,
not just what you said. So for example,
if you ask who is known as Captain Cool,
a traditional keyword search might not
help unless that exact phrase exists
somewhere. But a vector store contains
or the vector store understands that
Captain Cool is semantically related to
maybe Mahindra Singh Dhoni
because their embeddings are close to
each other in vector space. That's the
real power of vector database. They let
your AI think in terms of meaning and
not in terms of matching words. So in
langen we can use several vector stores
like f chroma or pine cone among others.
Uh in our case in this video we using
chroma which is lightweight and works
perfectly for local experiments. It
stores all those embeddings
persistently. So even when you restart
your application your AI doesn't forget
what it has learned. And when you
connect this to your retrieval pipeline
everything falls into place beautifully.
The documents get embedded stored in the
vector database and when a user asks
asks a question the system fetches the
most relevant chunks uh not because of
matching keywords but because of the
shared meaning here. So those retrieved
chunks are then passed to your language
model as additional context helping it
generate accurate and informed answers.
So vector stores are one of the key
reasons uh modern AI systems feel so
intelligent. They make your assistant
capable of remembering, reasoning and
retrieving information just like a human
instantly recalling the most meaningful
pieces of information it has seen before
and without vector store your AI would
simply be guessing and with them it's
reasoning based on the memory.
So in short uh we can say that vector
stores turn static knowledge into
searchable understanding giving your AI
a memory that's fast, scalable and
deeply semantic. So now let's go for the
implementation of search vector store
tool.
Okay. So I'm going to create a new
folder again here.
Let's call this folder 11.
Call it vector store.
Okay. Inside this I'm going to create my
first file.
Vector store.py.
So uh let me use the langen hogging face
embeddings. So
for this part so like this langen
hogging face embeddings let's also
import chroma
uh which is our vector database that
we're going to use. So I don't think
I've imported this. So let me install
pip install
lang chain chroma
and I'm also going to mention it on the
requirements.
So lang chroma
from langchain chroma
import chroma
uh once it is installed uh the error
will then go okay chroma then we'll also
[snorts] import document
So from lang dot schema import
document. Okay. Uh
I'm also going to load my load my
hogging face credentials here. So from
env import
load.env
and load env function is called.
Let me define my embedding model here.
So I'm going to copy this
uh from my previous code. So this model
will be used as hoging base embedding. I
think we've already used this in our
previous code too. So this is not new
for you. So here I'm going to create
some some document. Let me just copy
these uh these are too longs for me to
type. So I have some document related to
some IPL players.
So let me just copy this and then use
word. You can pause the video and then
see what is being typed here. So now
document one contains page content
related to Virat Kohli and his team
called Royal Challenge Bangalore.
Document two is about Sharma and his
team. Document three is about MS Dhoni
and his team. Document four is about
just Bumrah and his team. And then
document five is about Rabinda Jada and
his team. So these are the documents
that I need to store in my vector
embeddings. Uh so here what I'm going to
do is I'm going to combine all of this
in a single list. So doc 1, do 2, do 3,
do 4 and do 5.
Okay. Now let me initialize my vector
store. This will come from chroma here.
Chroma. And inside this
I [snorts] need to define my embedding
function. My embedding function will be
my embedding model that I've defined.
my pers directory.
Let's just name name it chromad. A new
folder called chromad will be created in
my project and then my embeddings will
be stored inside that.
Then after that I'll need a collection
name.
So the collection name will be sample.
[cough]
[clears throat]
Now since my vector store is initialized
and now I'm going to add my documents to
the vector store. So vector store dot
add documents and then I'm going to add
the collection of document that I've
created here.
Uh-huh.
So what else? Let me retrieve vector
store.get
get
and then uh let me in some in including
words here. So embedding should be
included whenever I retrieve from vector
store. Document should be retrieved
whenever I fetch from vector store and
then metadatas should be retrieved.
Okay. Now I will have
my query here.
Let's say uh who among these are
ballers.
So I have some players here or cricket
players among them. My query is who
among these are ballers. So what I'm
going to do is I'm going to run a vector
search. So vector store dot run a
similarity search.
My query will be my query here. And then
I will fetch a top three result from
this particular query.
So and then store everything in result.
And then let's print out our result
here.
So let me run this. Uh
my documents will be embedded using this
embedding model here and then be stored
in this vector database. uh and from
that vector database uh I will simply
run a query and then run a similarity
search in that query and then get the
relevant result here. So let me see uh
the first one Mumbai Indian Jaspid Bumra
is a baller that's nice. Uh the second
one Mumbai Indian Rohit Sharma is also
shown as a baller and then and then the
third one Robinda is also shown as the
baller. So Virat Kohli and then Msi are
excluded from these search list and then
based on the similarity it's found that
V Rohit Sharma Bumra and then Robin Aar
are ballers here. Let me also run uh
another query here if it's okay. Uh
we can also do something like this. So
we can also print our result based on
similarity search with scores. So if we
see this then then we can we should see
that the result rohit sharma should have
a very low similarity score here. So
similarity search with score.
Okay. Uh our query will be our query
and then and then we'll have a toply
result here.
So let me run this again and we should
see that the simatic score for Rohit
Sharma should be quite less than uh
Bumra or Ravinday
because Rohit Sharma is [snorts] not in
fact a baller. Sometimes he does bowl
off spin uh but not much. So let's see
uh play for okay the first one is the
spin bumra. So this is shown.
Okay. Wait. Okay. I've not printed the
result. Let me also print the result.
Let me omit omit the previous result.
And then let me print it here.
And then let me com comment this out.
And I just want to
see the result.
And then you might be wondering where
this chroma DB is. It should be well
within here sometime. So yes, you can
see the folder called chromadv and then
uh that and then using that particular
uh chromad vector database my result is
being pulled here. So just with bumra
has a similarity of something like 1.01.
So again it's saying just with bumra and
then it's saying just with bumra again.
So, so the top result is
top result is only Bumra and then not
the other bowlers here. So, this is how
uh vector store works. It converts our
data
or documents into embeddings and then
store it and then based on the query we
have provided we can fetch
uh following contents as well as uh what
we can do is we can uh get the most
similar result or like run the
similarity search on our given query
against the stored documents. So I hope
you understood the concept for vector
store. It is just like a just like a
database but instead of storing your
data it stores the embeddings of your
documents. You've loaded a ton of
documents onto your langen pipeline.
PDFs, text file, web pages, you name it.
But now comes the big question. How do
you exactly find the most relevant piece
of information when the user asks asks a
question? And that's where the retrieval
comes into play. Retrievers are like the
memory search engine of your engine
system. They don't generate new
information. Instead, they fetch
relevant documents from your existing
knowledge base. So whenever a user query
comes in, the retriever looks at all
your index data and returns only the
chunks that are most relevant to that
query. In simple terms, think of
retrievers as your AI AI's librarian.
You ask a question and it doesn't hand
you the entire library. it quickly find
the exact pages or paragraphs that
matters the most. Now, under the hood,
retrievers often work with vector
stores. Whenever you add documents, they
converted into vector embeddings,
numerical representation that capture
the meaning of the text. Then, when a
question comes in, it's also converted
into a vector and the retriever compares
it with all stored vectors to find the
closest matches. This is how your system
ensures semantic relevance.
It's not just matching words but
understanding the meaning. Langen offers
various types of retrievers. Some are
simple keyword based ones, others are
embedding based and then others are even
specialized ones like multiquery
retrieval or contextual compression
retriever that enhances the retrieval
quality using alms. Retrieverals are uh
essentially be or retrievers. The
retrievers are essential because they
bridge the gap between your static
document and your conventional AI. They
ensure your LLM has contextually rich
relevant data to reason with without
needing to retrain or fine-tune the
model itself. And in this video, we'll
get hands-on with one of the most
commonly used retrievers, the Wikipedia
retriever, where we'll see how to pull
realtime factual information straight
from Wikipedia to enrich our AI
response. So now let's go to the video.
Okay. So, I'm going to create a new
folder again here.
Uh, I'll call it Okay. What is the
size? Okay. 12. So, let's me create a
folder 12 dot
retrievers. Uh, I'll create a new file
inside this. Let's call it Wikipedia
3r.py.
Okay. So from langen community
retrievers import
wikipedia retriever uh
then I'm going to define my retriever
object here. So ret object r equals to
wikipedia retriever.
I'm going to get the top key results and
then the language I can also say the
language and then the language has to be
English. So my query is uh let's say
Indian premier
league
and then uh what I can do is I can use
my reviewer object to invoke this query.
Uh I'm not sure how many documents I'll
find but let's print out print out the
length of the document that I'm going to
find here. So lend the docs. Let me run
this.
Okay. So I need to install Wikipedia. So
let's go to our requirement. Add a new
library.
Save it. I'm directly going to install
this without going to the
requirements.py
or other requirements.xt. So let me
rerun this again.
Uh where is that? Wikb.
So let me run this.
It will take some time because it will
again uh go to go to Wikipedia and then
uh get the results from there. So yeah,
this is my
uh since I'm since I've only asked for
top two results. So it is giving me two
results. And then for this one I can
print each each and every of my results.
So for I do in in enumerate
uh docs
I can print is the result. So f uh
result
I + 1 and then uh print content
and inside content I can print
uh
doc dot
page content.
Okay. So if I print this again I will
have my two documents as well as its
content too.
And then that is straight from some
Wikipedia pages about this particular
topic here. So this is how we use
retrievers. Uh in the next video we'll
see we'll see something else uh
or like we'll see uh more more in the
retriever section and then we'll also
use it on the local data or local
documents. In the last video we explored
what retrievers are and why they are
such an essential part of langen. Now
let's see how they actually come to life
with the help of something called a
vector store retriever. Think of a ve
store as a smart database that doesn't
just [snorts] store text, it stores the
meaning. We've already talked about
this. Every sentence, paragraph or
document we feed into it is converted
into a numerical representation called
an embedding. And then these embeddings
capture the semantic meaning of our
text. So instead of searching for exact
words, retriever searches for similar
ideas. So in this video we're using
Chroma app which we've already used
earlier which is one of the most popular
open source vector databases out there.
Uh
so once we add our documents into Chroma
it handles all the heavy lifting storing
indexing and efficiently searching
through our vetoriiz data. Now to
generate these embeddings we'll be using
hogging face embeddings a special model
that we've already used in the past. uh
the model will transform each document
into a highdimensional vector allowing
Chroma to understand the relationship
between different pieces of text and
then when we convert our chroma vector
store into a retriever uh what we're
really doing is giving our application
to application the power to search
semantically. So when you ask a question
like what is chroma used for uh it
doesn't just look for that exact phrase
but in but it's fine sentences that
means something similar even though
they're using different words. So the
combination of hoging face embeddings
and chroma retriever forms the backbone
of the retrieval augmented generation
rack system that we've been talking
about and then a simple form of rag
we've already implemented in a previous
video. So by the end of this video,
you'll see how this retriever transforms
ordinary text into a scalable
intelligent knowledge base that your AI
can reason over effortlessly. Now
without any delay, let's go to the code.
Uh so I'm going to delete this Chroma
database for now because uh it already
contains embedding uh previous
embedding. So I'm going to create a new
file here inside retrievers and then
call it vector store retrievers.py.
So let me import chroma first. So from
langchen
chroma import chroma
from langchen_h hoging face
import hogging face embeddings from
langen_core
dot documents
import document
okay so uh I'll also need to load env so
from
env import load env
let me into load env load the hogfest
credentials here I'm going to create a
document I'll just copy paste it uh from
somewhere so these will be my source
documents here let me wrap this up okay
so these will source documents here we
have uh these four these four contents
uh
each defined inside a document
Then uh we'll initialize our embedding
embedding model.
So this will be our hogging face
embedding model. We'll use sentence
transformers and then a model inside
this sentence transformer. So what we're
going to do is we're going to create a
vector store.
We've already done this again. So we'll
use comma dot from documents
from underscore
documents. Uh
so our documents will be
the document that we have here. Our
embedding
will be the embedding model that we have
and then the collection name.
collection name let's say will be the
sample. Okay. Now we'll convert our
vector store into a retriever. So the
retriever
equals to vector store dot as retriever.
I just need to pass the
search keyword arguments. Uh search
quirks equals to uh let me pass a dict
here. So equal to two. So I'm so I'm
basically getting a top two result here.
So query equals to let's ask about uh
embedding. So what does embedding do
and then uh based on this I'm going to
invoke my result.
So retriever
invoke
uh and then I'm going to get query
and based on this I'm I'm just going to
print the result whatever it it throws
at me.
So now
uh let's run this. We've we have used
vector store uh as a retriever here.
Let's run this and then see what
happens.
So this all of these are being com
converted to embeddings
and then a chromb uh
database is been created
and along with that it is being
converted to uh retrievers uh it is
being converted as a retriever and then
giving me the results.
If you see here what I can see is
whenever I
I ask about embedding it gives me this
result at first and then the embedding
result.
Do I have a embedding result? No. I have
this chroma uh and then also I have this
langen result here. So I have these
three documents provided to me for this
particular question here or let me
change my question and then ask what is
comma used for and I just need one
result here. Let me venture it to one
and run this.
So again as you can see my search
directly redirects me to this chroma is
a vector database optimizer lm search.
It also provides me some more answer uh
few more answers here related to
embeddings and then the openi but but my
most relevant uh result is
the particular document here or this
particular document here for the query
here. Okay. So this was another type of
retriever that we've used. In this
video, we're going to bring together
everything we've learned so far, from
document loaders and text splitters to
embeddings, vector stores, and
retrievers, and use them to build
something real. Imagine being able to
ask questions directly about the content
of any YouTube video without manually
watching or scrubbing through it. That's
exactly what we're going to do. We'll
start by pulling the transcript of a
YouTube video using the YouTube
transcript API. This gives us all the
spoken text which then becomes the
foundation of our knowledge base. Now
we'll use a recursive character text
reader to break that long trans
transcript into manageable chunks so our
system can process and retrieve
information efficiently. Then comes the
magic. We'll convert those chunks into
embeddings using a hogging face model
and store them in a fires vector
database. This allows our system to
search and retrieve context content
that's semantically related to any
questions we ask. Once that's done,
we'll turn fires into a retriever, which
will find the most relevant parts of the
transcript related to our query.
Finally, we'll use a Gemini flash model,
Google's powerful language model to
analyze that content and generate an
accurate transcriptbased answer to our
question. In short, we're transforming a
YouTube video into an intelligent,
searchable knowledge source. By the end
of this video, you'll understand how all
the individual engine components like
loaders, splitters, embeddings,
retrievers, and models come together to
create a fully functional question
answering pipeline powered by a real
video content. So, let's go to the
implementation.
So, I'm going to create a new folder
again.
Let's call this
the rack systems
and inside this I'm going to create a
new file
YouTube_rank.py.
Okay, first of all, I'm going to uh
pull up YouTube transcript API
import
YouTube transcript
API
and transcripts
disabled.
We'll need to install this library here
because I once I try to run this.
So we'll need to install YouTube
transcript API. So let me just do pip
install YouTube
transcript
API.
Okay, then I'm going to
import
recursive character text here.
After this
uh embedding from our hugging face
models
then we'll import our model.
I'll also import import the prompt
template.
Then I'll import the vector database.
And then finally to load our credential
I'll import load.b.
So step one is
indexing or document induction.
documentation.
So I'll take a video ID and for video ID
I'm going to take my own YouTube video
here somewhere. Uh let me also load this
credential and let's go to the video ID
part. Okay. So
so here is uh our video on on like
[snorts] building a rag pipeline which
we did uh couple of days earlier in uh
video number 17. So I'm just going to
copy this particular video ID from from
the URL and then paste it in my code.
Okay. So this is going to be my video
ID.
Then what I'm going to do is I'm going
to try uh
and then I'm going to initialize an
object of YouTube API or YouTube
transcript API
and then I'm going to
pull my transcript here. So y ai dot
fetch
my video ID will be my video ID and then
uh the language of the video is English.
I want transcript in English.
Then I'm going to flatten
the transcript
to plain text.
So for this what we're going to do is
transcript equals to
dot join
chunk [snorts]
dot text for
chunk
in transcript list.
We'll also put an exceptions here. So if
transcripts are
disabled then we'll say
no captions available
for the video.
Okay. So this is step one. We'll move to
step two.
In step two we'll do indexing uh which
means our text splitting.
So for that we already have our splitter
here. So let me initialize the object of
the splitter. So splitter equals to
recursive character text splitter. I'm
going to define a chunk size of maybe
1,000 and then a chunk overlap. Okay,
we'll not do 1,000. 1,000 is too long.
So we'll do a chunk size of 300 and then
a chunk overlap of 50.
And then I'm going to use the splitter
to split my text here or split my
transcript. So create documents
transcript
if you want to see how many chunks we've
broken it down into. So let's see if we
can indeed uh extract the captions of
transcript from our video or not. So let
me run this.
Okay, so we have 33 chunks. We can uh we
could extract the extract the transcript
from our video. So let's go to step
three. The step three is embedding
generation
and storing in vector database.
Okay. So embedding model
will be used from hugging base
embeddings and then the model name is
uh sentence
transformers all
in LM L6
we do uh the model I found it from our
hogging face library. Uh let's also
initialize our vector store. So vector
store is done using
files. So from documents
chunks and then embedding equals to
embedding model. Okay. Next is retrieval
retriever. Uh so I'm going to create my
retriever from
my vector store as
uh as a retriever
and then the search type is going to be
based on similarity
and then uh search
keyword arguments. Uh let me do this
again.
So search keyword arguments
is equal to uh the top let's say results
here.
Okay. Let me design my prompt using my
prompt template.
So in case of prompt template let me
define my template here.
So let's
provide you are a helpful
assistant
answer only from the
provided
transcript
context. If the context is
insufficient
just answer I don't know and then uh
let me add a line break and then mention
context
is equal to context
and uh question
another line break question is equal to
question.
Okay. So this will be my template here.
Uh
I don't think I need to put an string.
So let me remove this. So input
variables are context and questions.
and text and question.
Okay, so this is done.
Now let me deise my question here. Uh
what is the video
talking about
and then we will also join a content
context text. So context text is going
to be joined from uh let's do this / m
[clears throat]
dot join
dot dot
page content for
for in
uh retrieved docs and then
retrieve docs is going to The
retrieve doc is going to be retriever
dot
invoke question.
Okay. So after that I'm going to create
my final prompt.
So final
prompt equals to prompt dot invoke. I
need to pass two variables. One is the
context which I can provide from context
text
and then the other is the question which
I can provide from my question here.
And finally I can invoke my model. Uh so
model dot invoke
final prompt
I've defined my model right okay I've
not defined my model so let me also
define my model here so model equal to
chat global generative AI
model equals to Gemini 2.5
/
answer dot content from here so let Let
me run this and then see if we can
actually get the answer from our
generated transcript or not.
Let me remove this print statement for
now and then run it.
Uh I missed a comma. Let's see. Yeah, I
should have puted a comma here
at the end of the text.
Let me run it again.
It will take some time for transcript
generation as well as um the embedding
model loading and also uh it will take
some time uh for the retriever to work
here. So at first the model has been
downloaded.
I think I've already used this model
earlier. I might have changed something
here. Let me [clears throat] check.
Okay. So,
yeah. Okay. So, model was uh downloaded.
So, the video is talking about using
embeddings to make a model retrieve and
reason over real data. It will build a
simple table generation. Okay, it works.
Uh in fact, we're talking about the PC
model in that particular video. Uh and
then it works. So, I'll pass something
that is not present in the context.
So I'll pass this question here. Uh my
question will be
so does the video or like
what does
what does the creator say
creator say about
Brazil in the
world cup football?
I don't think the context for this is
present in the video. So the model
should say that it simply doesn't know
about it. Let me run this again.
It would be efficient uh to just uh
store this uh [snorts]
transcript locally once it is
downloaded. that every time um the
transcript fetching part is not done as
well as the vector store can also be
stored. So as you can see uh for this
question what does the creator say about
Brazil in the World Cup football? It
simply says I don't know because that
particular context is not present in our
transcript. So this is it. So we have
successfully implemented a YouTube
transcript uh knowledge base and then
from that created a retrieval augmented
generation system uh answering our
particular question. So I hope uh it
runs on your end too. In the earlier
videos we've used language models in a
very traditional way at chatbased
systems. You'd give them a prompt, they
would generate a response and that was
it. But now things are evolving. The new
wave of this agentic AI isn't just about
generating text. It's about actions.
Models are no longer limited to just
answering questions. They can now call
functions, interact with external
systems, and even perform real
operations on the behalf of user. This
is where tools in Langen comes into
play. Tools act as a bridge between the
reasoning power of an LLM and the
functional capabilities of your code.
Instead of the model telling you what to
do, it can now do it by invoking the
right tool. Imagine this. You ask an AI,
what is the product of five and 9? So
instead of reasoning through the
multiplication itself, it recognizes
that there's already a defined tool that
can perform this exact task and call it
automatically. This shift is massive
because it transforms LLM from being
just text generators into decision-m
agents. They can analyze context, plan
what needs to be done, and then use the
appropriate functions to get accurate
results all autonomously. In this video,
we'll explore how to define and register
tools in Langen using the tool decorator
which turns ordinary Python function
into callable AI tools. And by the end
of this video, you'll understand how
this small but powerful addition lays
the foundation for building LLM power
agents that can think,
reason, and act, not just chat. So,
let's go to the implementation.
So again I'm going to create a new
folder
folder 14 and then name it tools.
Inside this I'm going to create a new
file
tools.py.
So here uh first of all from langen core
I'm going to import
tools.
So let's create a function here. So our
function will simply be a diff multiply
function
uh
def multiply a comma b
and then uh this will result
into a integer or I'll also define it in
this way. So, a is supposed to be an
integer. B is also supposed to be an
integer. And then the product or
whatever this does uh should return an
integer here. So, a into b also make
sure to include the dock string that
actually defines what this function does
because that is important here. So let
me write multiply to numbers
and then I incorporate this using the
add tool decorator. Okay. Now this is
not just a normal function but it is a
langen
tool function here. So now what I'm
going to do is I'm going to invoke this
multiply function.
multiply dot invoke
and then simply pass a = 3
and then b = 5.
So now
uh also this should be passed within a
within a curly brackets. So let me
provide it in that way
and then what I can do is I can print
out the result. Now you can see that
this particular tool will be invoked or
or the tool will invoke the following uh
values and then return our
multiplication result here. So let me
run this
and it's pretty easy. 5 into 4 is equal
to 20. So this is how you do it. You can
also see uh the function name here. You
can also see what the dock string in the
function says
dot do string uh or
and then you can also see what arguments
you need to pass inside this particular
tool. So let me run this again and then
you can see that the function name is
multiply. You can also see this dock
string uh for this second line here. And
then the argument you need to pass is
uh
a val a variable a which needs to be an
integer and variable b which also needs
to be an integer.
What you can also do is you can also see
uh the argument schema in terms of JSON.
So for that model
JSON schema
run this again
and then you can see everything
that we have talked about here in the
form of our JSON schema. So this is just
a basic introduction of using tool in
lang. In the further videos, we'll
explore how we can integrate this with
LLM2. In the last video, we saw how the
tool decorator helps turn a simple
Python function into a callable AI tool,
something our language model can
recognize and use when reasoning. But
what if we want more structure and
control? What if we want to clearly
define the input parameters, their
types, and even attach risk rich uh
detail for each, making the tool more
transparent and reliable for the model
to use. That's where the structure tool
comes in. Structure tool takes the
concept of tools a step further. It lets
it
wrap a function with a parentic model
that defines exactly what inputs the
tool expects. That means every parameter
has a type, a detail, and validation
rules, ensuring the LLM knows precisely
how to use the tool. Think of it like
giving the model a manual. Instead of
saying, "Here's a function. Good luck
figuring it out. You're saying here's a
tool that multiplies two numbers. It
require exactly two inteious A and B and
here's what they represent. This
structured approach becomes increas
incredibly useful when you're working
with multiple tools or building complex
complex agent agents. It reduces
ambiguity and helps the LM call your
tools correctly every time. So in this
video we'll explore how to define a
structure tool using structure tool from
function. How to specify input schemas
using pyic and why this approach is
considered the more scalable and
professional way to build tools for
lenses. So let's go to the
implementation.
So here I'm going to create a new file
again
and then call it structured
tool.py.
Pi
here I will have langen
from langen dot tools I'm going to
import structure tool
and then from pyic
we're going to import base model
and a field
okay so
I'll first define my parent class here
so [snorts] multiply input
This will import base model here and
then I'll have two parameters a it will
be an integer. I can write a field for
this one and then details of this
particular field. So description
equals to the first
number to multiply.
Uh I'll also say this is a required
value. So required equals to true. And
then similarly I'll have the second
number here.
Uh let me call this second number to
multiply
and then write B here.
Now
what I can do is I can define the
multiply function. Multiply function. A
will accept an integer. B will also
accept an integer.
And then we'll also return the integer.
So I will return A into B.
I'll define my multiply tool using the
structure tool function dot from
function.
Uh and inside this I'm going to write my
function name which is multiply
function. Then I'm going to provide a
name to this which is multiply.
Then I'm going to write a dock string
for this. So multiplies
to numbers and then provide the argument
schema and then argument schema is based
on the P multiply that we have which is
multiply input. Okay. Now after this
what I can do is I can invoke this
structure tool multiply
tool dot invoke then I can pass in the
two values. Let's call a = 5 and b can
be passed as four and from this I can
print out my result
and then run this again.
So in our previous video what we had
done is we had uh used the tool
decorator to define the the tool
function but in this case we've defined
a py class for validation too and then
uh we have used structure tool to invoke
this particular function here which
follows the validation rules provided
provided in the spiral class. So I hope
you understood the concept and then the
major uh difference between tool and
structured tool in line chain. So far
we've talked about how to create
individual tools simple functions that
can that an AI model can understand,
interpret and use. But in real world
application we rarely rely on just one
tool. Think about it an AI assistant
might need to add numbers, search the
web, analyze text or fetch data from a
database. Each of these capabilities is
a separate tool.
And when you have several of them, it
makes sense to group them inside a
toolkit. So a toolkit in langen is
simply a collection of related tools
bundled together, usually around a
specific purpose. For example, you might
have a Mac toolkit for arithmetic
operation or database toolkit for
interacting with SQL or MongoDB or a web
toolkit for scraping and browsing data.
Toolkits make your code modular and
organized. Instead of handling each tool
individually, you can register, manage
or pass around entire group of tools as
a single unit, especially when
integrating them with agents. It's like
giving your AI or toolbox instead of
just one range. Now it can pick the
right tool for the right job. In this
video, we'll explore how to define a
simple toolkit in Langen, how to group
multiple tools inside it, and how this
concept becomes the foundation for
building intelligent multiskilled AI
agents that can reason and act
automatically or dynamically. So, let's
move to the code.
So, I'm going to create a new file here
called
toolkit.py.
And inside this I'm again going to
import tools
langen_core
dot tools import
tool and then in a decorator what I'm
going to do is I'm going to create two
tools here. So one will be for addition
uh which will contain two parameter and
integer b will also be an integer
and then it will also return an integer
here. So return a + b. I want to include
something in the dock string to here and
and this will say add two numbers.
Similarly I have I'll have another tool
uh for multiply and then it will also
take in two numbers integer b is also
integer and then it will reply or return
an integer itself. dock string says
multiply to numbers
and then it will do return a into b.
Okay. So I have multiple tools
implementation here. So what I'm going
to do is I'm going to define a toolkit
here called Mac toolkit
and then uh inside this
I'll have a get tools function
uh which will contain a self keyword and
it will return add and then
[clears throat] multiplication
here
a tool here.
Okay.
Then what I'm going to do is I'm going
to create a toolkit object of this map
toolkit class.
And then inside this I'm going to get my
two different uh tools using this
toolkit. So get tools
and then I'm I'm going to print each and
every tools here. So ptl in tools
let's print out the name of the tool. So
PL dot name
and then what we'll do is
we'll also print out the detail of this
tool.
So TL
and then the detail will be TL dot
description.
So if you run this, you'll see that we
have bound these two different tools.
Okay, something's wrong here. Let me
check what I've
what I've done. Uh
so I've defined a class.
I've also defined an object
or the matt class. Okay, the object
definition is wrong here. So, let me run
this again. And then using
[clears throat] this toolkit class, uh I
can get a list of all available tools
that I have. So, I have two tools here.
One is for add and then the other is for
multiply. So if I need to add one more
tool here, let's add one more tool
uh called sub which will take in a and b
both are inteious for now. It will also
return an integer and I'll include a
dock string subtract two numbers and
inside this I'll return a minus b
and then I'll also include this tool
here for sub. Then again the toolkit
wraps all of my tools and then uh it
gives me a choice
choice choice to choose one among all of
these tools here. So this is how you
implement toolkit. So I'm ending this
video right now. I hope you understood
the concept. We'll further work on tools
and then integrate it with LLMs in the
upcoming video. Up until now we've seen
how tools can be created and grouped
into toolkits. But what if we want our
AI model to actively use those tools
during conversation? That's where tool
binding comes in. So toolbinding is the
process of connecting your AI model
directly with one or more tools. So the
model can recognize when a tool should
be used and even suggest calling it with
the right parameters.
Essentially the model becomes aware of
the tools at
essentially the model becomes aware of
the tools it has. So this is a critical
step towards agent AI because now the AI
isn't just generating text it can reason
about actions and leverage external
functionality to get things done. So for
example, instead of manually calculating
a sum or quering a database, the AI can
now identify the need, prepare the
inputs and suggest invoking the relevant
tool. It's important to note that in
this setup, the model itself doesn't
execute the tool. It only proposes a
tool call complete with arguments. The
actual extension is handled
programmatically giving uh developers
control over safety, validation, and
workflow. So in this video we'll focus
on binding a tool to our LM seeing how
the model suggest tool usage and then
executing the tools action based on the
model suggestion. This is going to be a
crucial step toward creating intelligent
agent that can work or that can do task
beyond just generating text. So let's go
for the implementation.
So here I'm going to create a new file
called a toolbinding.py.
Let's import tool. So from langen
core
lang core dot tools import tool. Okay.
What else do I need? Uh we'll import our
model. So from langen Google geni import
uh chat Google generative AI. And then
I'm going to load my credential from env
import
load env. So let me first load my
credential. Let me then load my model.
My model name is uh Gemini
2.5/model.
Let me create a tool here. So, so my
first tool
here is add
which will take in two numbers a and b
both will be integers and it will also
return an integer value. I'll add a
docking to this. So add two numbers
and then uh I will return
a + b.
Okay. So this is done. Uh the tool
import I think is wrong. So it should be
small to do tool. Uh
what else? Okay, we just one tool. We'll
try to bind this with lm. Okay, so lmm
with tools
equals to uh model dot
bind tools. And then I'm going to bind
my uh tool into this. Uh so
we can see uh invoking a query with this
particular uh LLM which contains tools
here. So let's invoke this uh and then
uh we'll say can you
add three with 14
and then basically see the
result here or uh what I can do is we'll
first print out the result and then
we'll see it accordingly.
So if we print out the result then the
model will not itself start doing a
computation but it will look at this
particular query and then uh and then
the dock string and then based on the
similar doc string with the query it
will understand that now this tool needs
to be called. Okay let's run this and
then you'll get a clear picture of what
I'm trying to say here. So if we run
this
so as you can see uh the content is
actually not an exact answer but it is a
tool call here or a function call. So it
says function call and then and then the
function name here is add. The arguments
are a and b and then and then three and
14 are passed to a and b here. And then
uh what the model is suggesting is it is
suggesting me to call this particular
function or call this tool to perform
the task that is mentioned in the query
instead of the model doing the
computation itself here. So this is how
we bind the tools and then the model
like I said earlier the model does not
itself call the tool but it's
but it actually provides you the
suggestion uh suggestion to the user
that this particular tool uh can be
called to perform this particular task
that is asked to do here. Up until the
previous video, we explored tool binding
where the AI could suggest using a tool,
but it didn't actually execute it.
That's a big step towards agentic
behavior, but it still required a human
or programmatic step to run the tool. In
this video, we're taking it one step
further. We're going to enforce the
model to actually perform the tool
operation as part of the workflow. This
means the AI isn't just suggesting. It
can now actively interact with tools,
gather results and continue reasoning
based on those results. The process
works like a conversation flow. First,
the user provides a query. The model
aware of the tools,
chooses which tool to call and then
prepares the arguments. Next, we execute
the suggested tool call
programmatically. And finally, the model
gets the tools output and integrates it
into its response, allowing it to
continue the dialogue seamlessly with
real results. This approach is critical
for building truly intelligent agents,
one that just doesn't answer questions,
but can perform task or compute results
or fetch data dynamically. So, in this
video, our focus will be on implementing
this inforce tool execution workflow.
The model identifies the tool, we run
it, and then the AI incorporates the
output back into the conversation,
making the interaction fully functional
and autonomous. So let's dive into the
code.
So here I'm going to create a new file
again
call it final tool
calling.py.
Let me import few libraries here. So
from langen core dot tools I'll import
tool
then from langen core dot messages I'll
import human message
then I'll import my model
and after that I will uh import
okay Now
I'll import the credential. Then I'll
define my model
chat Google
generative AI. So the model name is
Gemini 2.5.
And then I'm going to create a tool for
basically addition like we did in the
earlier video. So def add a is going to
be an integer. B is also going to be an
integer
and I will return the result as integer.
I'll add a dock string add two
numbers
and then based on this I'm going to
return a + b.
Now I'll bind this tool. So, llm with
tools equals to llm llm
dot
bind tools and I'm going to bind my add
function tool here. My user query is
going to be in u uh add
10 and 9.
Okay. And then I'm going to convert this
into human message.
So my query is a human message.
So we'll append this in our message
list. Our query now
and then we'll uh print our
result. basically call
uh call the LLM using our
messages and then print out the result.
We'll see the result what happens here.
Okay. Okay. So if you run this,
you'll see that it does suggest a uh
function call here uh for add to
basically solve my query. So what I'm
going to do here is
uh I'm going to append this to my my
messages. So messages dot append it will
go inside here also I want to print
something here. So let me print out
result dot tool calls. I think we have
tool calls here. Yeah we have tool calls
here. So tool call and then we'll invoke
our first tool call and see what it
gives me. So let me run this again.
So if we print tool call then we can see
that this particular tool is reference
or the add tool is reference. My values
are correctly gone to a and bp.
So now what I'm going to do is I'm again
going to remove this and then I'm going
to invoke the tool add dot invoke
result dot tool call zero.
Then after this I'm going to append
append the tool result also inside my
messages and then the final result will
be the llm call again dot invoke and
then I'll print my final
result here. So print final result.
So let me clear this and then run this
again and see what happens.
Okay, as you can see the sum of 10 and 9
is 19. The content is
generated. If you only want to see the
content then you can see that the
content is shown.
Okay. So what we did here is we did in
fact invoke the invoke the tool but we
did it by ourselves. So basically we
took the model suggestion of invoking
the add tool. We invoke the add tool and
then basically uh regenerated our
response based on our query year. Uh
that's not actually uh what we want but
I actually wanted to show you how to
call the tool here. In the upcoming
video we will fix this and then we will
let the model uh suggest the tool as
well as call the tool automatically
here. So I think I uh you understood the
concept. In our previous videos we
explored how an LLM can suggest tools
and how we can execute those tools
manually. But what if we want the model
to actually carry out a multi-step
workflow, handle immediate results and
inject critical information into
subsequent tool calls automatically. And
that's where the concept of injected
tools comes in and it's what we're
exploring in this video. So imagine the
LLM as a helpful assistant with access
to a set of tools like a calculator or a
currency converter. When you ask it a
question, it might realize that to
answer fully, it needs to use more than
one tool in a sequence. For example, if
you ask like what is the conversion
factor between USD and NPR and based on
that can you convert
10 USD to NPR? The assistant first needs
to fetch the conversion rate and then
apply that rate to the value you want to
convert. The magic happens through
injected tools arguments.
Uh this tool
allows the LLM to recognize that some
tools require additional input like the
conversion rate for this case which
might not be explicitly provided by the
user but is generated by a previous tool
call. Essentially it allows the LLM to
remember and pass context between tool
executions automatically. And here's how
the workflow unfolds. First of all we
start with the LM call. So we send the
users query to the LLM bound with tools.
The LLM analyzes the query and chooses
which tools need to be executed. For
instance, it first calls the get
conversion factor tool and then plans to
call the convert tool. The second is a
multi-turn tool execution loop. Here the
code then executes each suggested tool.
When the LM calls convert, it might not
know the exact conversion rate yet. This
is where the injected argument comes
into play.
uh the program automatically injects the
conversion rate obtained from the
previous tool call ensuring the tool has
all the information it needs and the
final is the feedback or feeding results
back to the LLM. So after each tool
execution the result is packaged into a
tool message and then sent back to the
LLM. The LLM then uses these output to
uh generate the upcoming step or
finalize its response. The loop
continues until LLM produces a final
human readable answer in the content
field. So this approach is powerful
because it transforms the LLM from a
simple chat model into a true agentic
system. It can now plan, execute and
chain multiple operations together while
intelligently handling uh dependencies
between them. So in this video we will
walk through a live implementation of
this workflow. We'll see the LLM first
retrieve the conversion rate and then
use it to convert a currency amount and
finally produce a complete readable
answer for the user all automatically.
So this pattern is not only limited to
uh
currency conversion. It's a blueprint
for like building multi-step AI agents
that can handle complex task with
multiple tools and dependencies. Now
let's move on to the implementation.
So I'm going to create a new file here.
Call it test
multiple
uh tool call.py.
Okay. So inside this first of all the
imports
first import tool.
After that import
tool message as well as event message
then load the model
then env
here from inside lang line to lend code
dot tools I'm also going to in add
injected tool ar
and then uh last one from typing I'm
going to import annotated
okay so first of all load the
credentials
then I'm going to define a tool here
so this tool will be get conversion rate
it will contain two strings So one is
base currency.
It is it will be a string and then the
other is target currency.
It also be a string
and then it will basically return our
float value.
So write something in the dock string.
Return the realtime currency
conversion
uh conversion rate from base currency to
target
to target currency
using some API.
And here I'm actually not going to call
the API but I'm directly going to use
this uh static conversion rate. Let's
say 140.3.
Okay. And then I'm going to return this
conversion rate from this tool.
Next I'm going to create another another
tool here
uh that is going to be named as convert.
Here I will have a base currency value
which will be in float
and then I'll have a conversion rate
conversion rate uh which will also be in
float but it will be annotated and then
float and injected tool arc. Basically
first of all uh the value will come from
the other tool to this particular tool
here and then it will also return a
float value.
to write a dock string
converts a base currency
value into
target currency value
using
a
conversion rate. Okay. So
this is going to return
base currency value into
conversion rate. Now we will set up llm.
Uh so lm equal to chat Google generative
AI.
The model is going to be Gemini
2.5/model.
Then what I'm going to do is I'm going
to bind these tools. So lm
with
with tools equals to lm dot
by tools.
The first tool is get conversion rate
and then this and then the second tool
is convert. Now I'm going to pass in my
query. So my query is going to be what
is the
conversion factor between
USD and NPR and based on it
then you convert
10 USD to NPR.
Okay. So this is going to be my query
here.
Now what I'm going to do is I'm going to
include this query as a human message.
Human message
with the content is going to be query
and I'm going to wrap up wrap this up in
a list itself.
Okay. So let me invoke the tool and then
get the AI message from this. So, lmm
with tools dot invoke
uh messages and then whatever I get I'm
going to append it in the messages
itself. So, append AI message.
Okay. Now in step two we go for
multi-turn toolation loop. So while uh
AI AI message
dot content
equals to an empty string and AI dot AI
message
uh dot tool calls
tool calls.
So if something is present in the tool
calls what I'm going to do is
I am going to create a list of tool
message
tool messages
and then based on this I'm going to
execute all suggested tool. So for tool
call in AI dot sorry AI message dot tool
calls uh [snorts]
I'll first call in the tool name which
is going to come from tool call
name.
I will also try to get its ID. So tool
id equals to tool call
id and then I'll get the arguments. So
tool arcs equals to tool call
uh
tool call and then ars here. Okay. So
let me print
every tool that I've got. So executing
executing tool
and
here will be my tool name.
Tool name will be arcs
and then here will be my tools.
So this will be made as an f string
here.
Okay, this will be
or I can just do this and then this is
going to be my string.
So now if the tool name is equals to
get
conversion rate
then I'll invoke that function here dot
invoke
or invoke that tool and then I'll get
the tool output
in the form of a string.
and then that will be my conversion
rate.
So else uh if the tool name
is equals to convert.
So if the tool name is directly convert
then what we need to do is we need to uh
first inject the conversion rate from
the previous result step. So if uh
conversion
rate
not in tool arcs
ensure the conversion rate is actually
uh
defined from the get conversion factor
runs. So if
uh
conversion rate
not in locals
or
our conversion rate is none.
Then what we need to do is we need to
raise a value error
uh saying that
conversion rate is required
but it is not found.
else what we need to do we need to get
the conversion rate from tool arcs
conversion rate equal to conversion rate
and then based on this I need to execute
my tools so converted value equals to
convert
dot invoke
to larks
and then to log output
can be converted to a string value using
str function.
Okay, if none of this is possible,
we'll do tool output equals to
unknown
unknown tool and then we will say tool
name
Okay, most of our work is done. Uh what
we need to do is now we need to since
every condition returns a tool message
to us. So what we're going to do is
we're going to create a tool message. It
will be wrapped inside this tool message
here. The content
is going to be tool output
tool output. And then the tool called
id
ID will come from the tool ID
and I'm going to append uh tool messages
dot append
tool message
tool msg
and then and then now
I'm going to append everything to our to
our messages here. So still I means I I
am supposed to be inside this loop here.
Uh okay. I think I think I did some
mistake here. So this has to go inside
the Y loop
inside uh inside the Y loop. And then
now what I'm going to do is I'm going to
messages extend.
You can use anything to messages.
Then I'll again prompt for AI message
and then invoke the model
with with a updated
messages and then I'm again going to
append it
in my AI message. Okay. So finally
we have we already should have obtained
our result inside this particular part
here. So what I'm going to do is I'm
going to take out the last result. So
messages minus one
minus one and then I'm going to print
the final result dot content. So let's
see my query is what is the conversion
factor between USD and NPR and based on
it can you convert it from 10 USD to
NPR. So if you run this
let's see what happens.
So the model is called
the model is initialized.
uh so okay uh the conversion rate
between USD and NPR is [clears throat]
140.3 this is done using this get
conversion rate tool and then from that
after that we call this convert tool uh
with the base currency value of 10 and
then the exchange rate or conversion
rate of uh 140.3 and then 10 USD is
equivalent to 1403 NPR so this is the
result so here we've not only uh like
integrated multiple LLMs into it into
sorry multiple tools into an LLM but
we've also uh used this concept of
injected tool where the result of the
previous tool will be the input for the
upcoming tool. So I hope you understand
the concept of injected tool uh and then
you had fun implementing this particular
code here. If you have any problem, do
comment down below and I'll try to help
you out. And in the
next few videos, we're going to see AI
agents demo using Langen. So, we are
almost at the end of this Langen series
here. Hello everyone and welcome back to
the channel. So far, we've done our
Langen series. If you haven't watched
that, I'll put a playlist link down
below and then you can check it out. And
in that language series, we've learned
how to build LLM powered applications
step by step. We have explored chains,
agents, retrievers, and even tool
execution.
We saw how Langen helps us connect large
language models with external data
sources, APIs, and other tools. But as
we started building more complex
systems, you might have noticed
something missing. Langen is actually
great for creating individual component.
But when you try to build multi-step
workflows or systems where an LLM needs
to make choices dynamically, things
start getting a bit messy. You have to
manage the controls for manually, handle
intermediate steps, and sometimes even
patch the logic together with eels
conditions. It works, but it's not
scalable and it's definitely not
elegant. So that's exactly where Lang
graph comes in. Lang graph takes
everything we've learned from Langen and
gives it a structure. It lets us
represent our LLM workflows as a graph
with nodes for actions and edges for
choices parts. In simple terms, it's
like giving our agent a map to follow.
So it knows what to do, when to do it,
and how to handle intermediate results.
With Langraph, we can build stateful,
reliable and controllable AI systems
where every step is transparent and the
workflow is easy to debug, modify or
extend.
So if Langen was all about learning the
tools, Langraph is about learning how to
orchestrate them intelligently. And
that's exactly what we're going to do
next. We're starting a brand new series
on Lang Graph where we'll go from the
basics all the way up to building
advanced multi-step aentic workflows. So
get ready because if you love what
Langen could do, Langraph is going to
take things to a whole new level. So
let's get started. In the last video, we
talked about why we're moving from
Langen to Langraph. And today we're
taking our very first step into it.
So imagine you have a chatbot that takes
some input, let's say a name, and then
it needs to process that information
step by step
before giving a response. So in line
chain, we probably just call a function
or a chain directly. But in langraph, we
think differently. We treat our program
as a graph, a flow of states moving
through connected nodes. Each node
represents a specific action or step and
the state carries data as it moves
through those steps.
In the example that we'll be doing, our
state is just a collection that holds a
single key, which is messages. Think of
it as a small memory that keeps track of
whatever our chatbot knows at the
moment. Then we we will define a node. A
simple function that takes this state,
modifies it by adding a friendly
greeting and then returns the updated
state. That's all a node does. It only
transforms data. Next, the graph
connects the dots. So we say when the
process starts, go to this node, execute
it and then end the process. It's like
drawing a mini flowchart. Start then do
something and then end. So when we run
it, Lang graph automatically handles the
execution. It passes the state through
the node, updates it, and gives us the
final result. The output you see isn't
just a printed message. It's the outcome
of data flowing through structured
logical graph. And that's the beauty of
Lang Graph. You're not just writing code
anymore. You're designing a process. A
process that's visual, controllable, and
scalable. So in the upcoming videos
we'll slowly build on this foundation
adding more nodes and more edges and
eventually intelligent choice making to
our graph. So stick around because this
simple example is just the beginning of
how langraph turns logic into flow. So
let's go to the example now.
So I'll be creating a new folder here
and then I'll call it lang graph
and then inside this uh I'll create my
first file. So basic [snorts] uh
graph dot ip nb will this we'll do this
in the notebook format here.
Okay. So first of all I need to install
lang graph here. So lang graph
let me run this
and then once line graph line graph is
installed we're ready to build our first
graph here. So from typing
import typed dict
uh and dicict dict we'll run this and
then select the interpreter here python
environments and then our recommended
interpreter is this one. So as you can
see it can run the interpreter. Okay I
would also like to
uh import from langraph.graph graph
import
state graph start and then end.
Okay, this is going to be our first
import here. Okay, our first cell is
running. Now I'd like to create a class
called my message state which will uh
have a type text as inside it. So inside
this I'll have a simple message
that will be in the string format.
Then after this
what I'm going to do is I'm now going to
define my graph directly here. So my
graph will be from state graph
and then inside this I'm going to pass
my class that I've just created.
So I'll add nodes to this graph. The
first node will be uh we need to provide
a string name to this. Let's say initial
and then provide a function here. So
greeting node. Okay. So this particular
function needs to be implemented on top
of this. Let's write down this function
greeting node. This is always going to
have this parameter
of this class that I've just created.
And then it is going to return the same
class that
we have now. Okay. What does this do?
This is a simple node
that adds a greeting
message
to the state.
Okay. So let me write state
or basically update the state here. So
up until now this is all empty. Now I'm
updating this. So
I'm writing hello
uh plus sorry
hello plus state message whatever we
have in message
uh plus
uh and then maybe give full stop and
welcome to lang graph. Okay, so this is
going to be my function and then I've
already added it as a node in node in my
graph here. So we only have one node for
now. Now we're going to add ages to this
node. So graph dot add age. It is going
to at first we'll start
uh the first flow is going to be or the
first age is going to be from start to
this particular node here which is
initial
and then after this I'm I'm going to add
another age from initial
to end. So we'll only have one one
single node for this particular code
here. So I'm going to compile this. So
to compile we only need to do particle
to graph.compile
and then run this.
Okay. So we have some error. I think
it's some spelling mistake here. So this
should have been initial. Let me run
this again.
Initial right. Uh okay this is wrong. So
initial.
Okay. Our uh graph is compiled. If you
want to print this, you can also
directly run bot. And then you can see
that we have one single node. We
we begin with the start part then the
then the process flows through the
initial node and then it goes to the
end. So this is our basic graph workflow
in lang
graph that we've created. So now
in order to uh invoke this graph what we
need to do is we simply need to invoke
this bot
object that we have and then
we need to pass something here. We'll
pass messages and then pass our channel
name learning hub here
uh and then we'll print our response. So
what happens here is at first the
initial state in this particular uh
variable contains this string called
learning hub. Then it moves to greeting
node and then along with that learning
hub uh the state is now updated to this
complete message here and then basically
what we get in the end is is that
particular complete uh content that we
have processed. So if we run this
uh okay something is wrong here. So
message or messages let's see message
state message let me just
see if I've done any any kind of
spelling mistakes here.
Okay. So this must message. Okay. So if
you see the response
uh then you can see. Okay. Why is it
only showing just one response here? I
have invoke B right.
Ah something that I've not done here is
return
state
because it should return the updated
state here. Let me run this again.
And then if I run this, so it adds some
new text as per the process function or
the greeting node function here. And
then uh it gives me the response. So
this is a very simple implementation of
our graph flow or our uh workflow using
our graph here or using lang graph. So I
hope you understood the concept of lang
graph and then uh how is how it varies
from langen. In the last two videos we
explored the basics of langraph how
nodes represent logical units of work
and how the flow between them creates an
AI reasoning pipeline. But until now our
examples worked with simple single
message states. In this video, we're
taking the next big step, moving towards
structured states that carry multiple
pieces of information together, just
like an agent's memory. So, think of
state as a data package. It holds
everything your graph needs to process,
pass, and transform information at each
step.
We'll start with the code and then I'll
try to explain the code accordingly. So,
let me create a new file here.
complex graph [snorts] ipv
first of all from typing
import type dict
and list
let me run this and then select the
kernel so it's running uh from
line graph dot graph import
state
graph comma start,
end. Okay, so these are the inputs that
we'll be needing.
So the cell is running. Now I'm going to
create a class uh that contains our
state here. So class agent state
and then the type will be typed date.
Inside this I'll have a name.
I'll have age.
I'll also have skills
and then a result.
So
unlike our previous video, instead of
just one text, our our now stage
contains multiple attributes like a
person's name, age, and skills.
After this, we'll be creating our nodes
here. So for the first node, we're going
to create a function.
The parameter will be our agent state.
And I'll write a doc string for this.
This is the first node.
And then here I'm going to update my
state here. So the result
variable in the state will be updated.
state name
welcome to the system
then I will return the complete state
here okay so this is going to be my
first node similarly I'm going to type
in the second node now def second node
again it will have a parameter of state
the node will always have the parameter
of state and then it will return the
entire state here
write a doc string this is a second node
and after this I'm going to update uh
update the state result again here but
first of all I'm going to uh bring out
my skills and then convert them from a
list to a string
dot join state
skills this
and finally I'm going to update my
result.
So state
result
plus
you have skills in
state
skills
and then again I'm going to uh or let me
just do skills here not state skills
because I've already extracted my state
skills from this. So again I'm going to
pass out the entire
state here.
Okay.
So each node uh in this particular graph
that you see here or the or the graph
that I'm going to build uh reads these
structured states and then does it task
and then passes it forward uh enriching
its uh step by step.
So the first node uh here acts like an
initial greeter. It welcomes the person
into the system and then the second node
uh that I have uh adds more context like
like uh like the skills that they have
and then
uh I'll also have a third node here.
So
uh what's that? Let me write down third
node will contain an state agent state
again and then it will return agent
state for now.
So let me write down this is the third
node.
Uh and then here what I'm going to do is
I'm going to update my state
uh equals to state result plus let me
add something more here. to string. So
in string uh you are
uh state age
years old
and then end string here
and then again I'm going to return
the
So the third node uh completes the flow
by including the age creating a full
personalized summary year. uh so the
data flows from one node to another uh
gradually building the final message
just like a well organized thought
process inside an AI agent. So so now
we'll build that particular graph. So
let me run this. It's running. We'll
define a state graph uh state graph and
then inside it I'll have agent state
agent state
then I'm going to add my first node
first node will be
uh
first node
uh I'll I will now define the function
with that first node Then I'll also have
a second node similarly. And then the
third node.
Okay. Copy this down. This is going to
be my second node.
And then finally I'll have my third
node.
So I'm going to add ages now. So first
of all we will go from start to our
first node. Then
we'll go from first node to second node.
Then we'll go from graph.add_age
second node to third node. And then
finally after third node we'll go to
end. Let me compile this. So gra uh let
me write in the object name bot equals
to graph dot
compile
and then print out the part here or the
graph here. So as we can see
it goes from start to first node to
second node to third node and then
finally to the end.
So the three simple notes here uh
highlights the true power of lang graph.
Uh it helps you modularize reasoning uh
breaking large task into small reusable
logical units uh that form a clear and
and more uh traceable part of uh
execution. So instead of handling
everything in one complex step, Langraph
lets you think and build in stages
exactly like how agents reason in real
life.
So we'll complete this and then see the
result here. So let me build my input
state.
I'll type out the name here. Name can be
Alice.
Uh age and skills can be this one here.
So I don't need to pass in the result
yet because that will be formed from
within the nodes. And then I'm I'm only
going to pass until this uh skills here.
So let me say data science. Okay. Then
finally what I can do is I I can invoke
the bot using this input state and then
finally print out my response. So my
response will be printed. Uh first of
all what will happen is it will go to
this particular node here. So it will
take a name and then it will say welcome
to the system with this entire message.
It'll add you have following skills and
then after that uh along with this
entire
message it will add the age of the
person and then we'll print the
response. So let's see
skills. Okay, what is it? Skills. What
else here?
Okay, here I've done some mistake. Added
an extra space. So let me run this
again.
And then as you can see uh the result is
Alice welcome to the system. You have
skills in Python, ML and data science
and you are 30 years old. So based on
the flow of data in between these nodes
the result had been updated or basically
some extra strings have been have been
appended in each of these results here.
So I hope you understood the code and
then understood the concept of this
multiple node that we have been
implementing here. Um so far uh we built
a linear graph that processes structured
input and composes a final result step
by step. Right? But this is just a
beginning in langraph. In the upcoming
video, we'll take this concept further
by introducing branching logic where you
uh where your graph can make choices
dynamically uh based on the state. In
the last video, we learned how data can
move from one node to another with each
node performing its task and enriching
the overall result. But what if your
workflow needs to take different paths
depending on certain conditions? That's
where things start to get interesting
because in real world AI systems, not
every query or task follows a single
linear route. Sometimes your graph needs
to choose where to go next and Lang
graph allows that using conditional
edges. So in today's video we're going
to see how to introduce
choice making inside your graph. How it
can intelligently choose between
multiple nodes based on logic you
define.
First of all we'll write the code and
then I'll explain you the code here.
So let me create a new file
3 conditional
graph ipy nbv
I'll have some imports first of all from
typing I'm going to import typic
then from
lang graph dot graph. I'll import
state graph
start and end.
Let me run this and then select the
default interpreter.
After that we're going to write our
agent state. So agent state is going to
be of type dict
and I'll have number one which will be
an integer
number two which will also be an integer
operation which will be a float sorry
string and then I'll have a result which
will also be an integer. Okay. So this
will be our agent state here. Let me run
this too. Now I'm going to create few
nodes here. First of all, the first node
will be adder. It will contain state
agent state
and it will also result and or return an
agent state here. Now here what I'm
going to do is I'm going to write a dock
string. So this node adds two numbers.
After this I'm going to update my
uh final
final number or I can sorry it's not
final number it is result. So state
result equals to state
number one plus state number two
and then I'm going to return a state
here.
Okay.
Then I'm going to define another node
here which is called subtract
and state
age and state.
Uh the node will basically subtract two
numbers and then what I'm doing here is
doing number one minus number two and
then returning the state. Now I will
define a router node here which chooses
where should I go next. So
decide
next node it will also contain an agent
state. It will be of type agent state
and then it is simply going to return a
string here.
So this node will will decide or will
select
the next
node of the graph.
Now I'm going to check
one uh state called operation here. So
if state operation
double equals to add
then what I'm going to do is I'm going
to return add operation and then
else if
state
operation is subtract then I will return
subtract up. Okay so this will be my
router node here. So let me run this.
Then after this I'm going to create my
final graph. My final graph will be
based on state graph and then it will
contain this agent state object.
So agent state sorry
agent state class.
Let me add nodes here. So the first node
will be add node and then and then the
function for this node is adder. Second
node is subtract node and then uh and
then the function for that is uh
subtract. Then I'm also going to add a
third node here. So grab dot add node.
The third node will be named router. And
then what I'm going to do here is I'm
going to pass
uh pass the state as it is here. So it
will also be called a pass through
function. So whatever we get will just
be passed uh passed to that router stage
here. So now I'm going to go and add
ages. So first age is going to be start
and then it will go to router. Then I'm
going to add conditional age here. So
add conditional age.
Uh
what will this be based on? So this will
be based on the router node. And then
what will happen here is its function is
this one
decide next node.
So
so the function is here. And then the
two options from this node is
uh
the two options from this node is one is
the add up which will take us to add un
node and then the other is
uh and then the other is the subtract up
which will take us to subtract node. So
basically we're referring to add up and
subtract up from this particular node
here. And then if we get add up we'll go
to add node which means our adder uh
function and and then if we get subtract
up we'll go to our subtract node which
is our subtract function. Okay.
And then what we also need to do is
since u both of these add and subtract
will show the result and then go to the
end of the graph. So what we need to do
is we need to add two different ages
here from add to end and then subtract
to end.
Now since this is done I'm going to
compile my graph.
Let me compile my graph and then if I
print out
uh my graph here we'll see that first of
all we'll start from here. Then we go to
the router node. The router node fixes
where where we should go next based on
the agent uh
agent state u called operation here. If
we get plus then we go to add node. If
we if we get minus we go to subtract
node from either of these add or
subtract node we go to the end here. So
this is how we can introduce conditional
graphs in our line chain. Now I'm going
to declare my input state here. So input
state
is going to be uh the number one will be
10. Let's say uh the number number one
is going to be
10. Okay, I'll keep it 10. Number two is
going to be four. I don't need this
result here. The result will be operated
on its own. And then basically operation
is minus. Now if I invoke
if I invoke my boat
using input state
then and then if I print my final
response then I can see that the result
is set and then since the since the
operation is subtract so the result is
six. If I change this operation from
plus
now the result goes to 14. So either of
these two nodes are chosen based on the
uh operation uh state uh present in our
agent state here.
Okay to summarize now we start with an
agent state that carries our data two
numbers the operation we want to to do
and either addition or subtraction. Next
we define two nodes, one that adds, one
that subtracts. But the real magic is
inside this router
uh function here that we have. So this
function acts like our choice maker. It
inspects the current state and then
returns which node the graph should go
next. So if the operation is positive
then we go to add node. If the operation
is negative then we go to the subtract
node. So lang graph then uses these uh
conditional ages to to make it run. So
these ages dynamically connect the flow
uh at runtime based on the conditions uh
based on the condition functions output.
So uh instead of a single linear chain
what we have here is it
it is a conditional chain and it behaves
like an intelligent uh tree. So capable
of branching out and adapping to the
data it receives. So this concept
conditional routing is at the heart of
making agent test system flexible. So in
the upcoming videos we'll take this
concept further including memory as well
as message passing inside langraph so
agents can remember and react
contextually and that is where lang
graph toy
uh starts to feel alive. In the previous
video, we learned how to create
conditional routes in Lang graph where
our graph intelligently decides which
node to visit next based on user input
or data state. But what if we want to
chain multiple conditional flows
together like a series of choices where
one operation leads to another? That's
exactly what we're going to explore
today. So, in this video, we're taking
things one step further and see how
Langraph can handle multiple branching
routes, letting you
create more complex and intelligent
workflows that don't just stop after one
choice. So, we'll first write down the
code and then I'll explain you the code
here.
Okay. So let me create a new file here
for complex
complex conditional
ipv.
So first of all I'll have my code I'll
have my imports here.
So the import are going to be from line
graph
dot graph import
state graph
start and end
and from typing I will have type dict
type dict
I can choose the interpreter and then
while this runs uh I'll create my agent
state so agent state this will be have a
parameter of type date.
So type date I'll have number one here,
number one as integer,
number two also as an integer
and I'll have number three as an
integer.
Number four also as an integer.
I will have operation one
as a string,
operation two also as a string. And then
I'll save the
result of operation between number one
and number two in result one which is
going to be an integer. And similarly
operation two for number three and
number four is going to be saved in
result two which is also an integer.
Okay. So let me run this.
So while this is running now we'll
create our functions here. So first of
all we'll have added sorry adder where
we'll pass the
parameter as agent state. it will again
return an agent state.
So I'll update state
uh result one equals to
state uh number one
plus state
number two
and then I will return the state here.
So let me copy this and then create
another
uh subtraction operation here.
So I will have uh subtractor two or su
okay I'll write the full function name
subtractor two where I'm going to
subtract these two number one and number
two here similarly I'll have adder two
and subtractor two. So let me just copy
this down and then copy it here. So here
I'll have adder two. The result will be
stored in result two. The numbers are
number three and
number four. Similarly in subtractor two
we are going to store the result in
result two where the numbers are number
three and number four.
Okay. We also need to create two router
functions. So let's do that.
choose
next node one.
It will have an state of type agent
state
and then it will return a string.
So if state
uh operation 1
is equal to plus
then what we want to do is we want to
return
add one
else if uh state
operation 2
is equal to minus
then we want to return sub one.
Okay, I'll also create a router for our
number three and number four part here.
So choose next node two.
Uh okay sorry this has to be operation
one and then here we will have operation
two
add two oper operation two sub two. Okay
so these are our uh router nodes. Now
we'll go on to create our graph here. So
graph equals to state graph
state graph. I'm not sure why this
position is not being shown but anyway
agent state I can just type this uh
first of all I'm going to add my first
node here
so the first node is adder one and then
I'll give it give a name adder one here
similarly I have my second node
add node this is subtractor one
let me write down the function name here
which is
okay what do you name it? Okay,
subtractor one
subtractor one.
Similarly, I have adder two and
subtractor two which I'll just create
here by copying this
adder two and then subtractor two.
Now I'm going to add a node for router
one and router two which will basically
uh act as a pass through function here.
So router one uh will pass lambda state
and then basically pass the state here
and then similarly in router two or this
should have been a comma.
Okay. And then in
router two we'll do the same thing.
So router to lambda state state. Okay.
Uh now what we'll do is we'll start
adding age. So the first age is going to
be from start to router one. Then
I'll add a conditional age here. So add
conditional ages. Now the detail of this
conditional is just from router one
it will go to the choose next node one
choose next node one
and from here we we have two options we
can either go to uh okay what is the
function name let's see
so add one and sub one right so we can
either go to add one
so if add one occurs then we go to adder
one and if uh sub one occurs we go to
subtractor one.
Now I'll add one more age here. So add
age from adder one to router two and
then subtractor one to router two.
Whichever works uh whichever works in
the following case here. Adder one and
subtractor one both should go to router
two here. Now I'm going to add another
conditional age for router two here.
Uh so the name is router two
comma
and after this uh the function is choose
next node two and then within this I'll
have one more
[music]
uh
I'll have add two add two which will go
to add two and sub sub two which will go
to subtract
Now after this I'm going to uh add two
edges either from addit two to end and
subtract two to end again. Now this is
my final graph. Let me compile this
graph dot compile and then if I print
the bot then we will see our conditional
graph here. So, so from start we'll go
to router one and then either to add or
sub add one or sub one uh depending upon
the operation and then
and then we'll go to router two and then
add two to sub two depending upon the
operation that we have. So now what
we're going to do is we are going to
finally uh run this using our input
state. Let me provide the input state. I
will just going I'm just going to copy
this here. So number one is one, number
two is two, the operation is plus. So
basically we need to see the result
three in result one. And then in case of
number four and number seven, we need to
see the subtraction operation here. So
now what I'm going to do is result equal
to B dot invoke and then provide the
input state here. And finally we print
out the result
and then we can see our result here.
So uh what did we do here? We started by
defining an agent state that holds two
two separate mathematical operation. So
each operation has its own pair of
numbers and operators. One for each step
and then one for the second. Now we
define four simple nodes here. Two
adders and two subtractors. So each of
these node does the operation and store
the result back into the state. But the
interesting part here is the two routers
that we have. So uh router one and
router two. So these so these routers
act like our uh checkpoints where
choices are made. The first router looks
at operation one and then chooses
whether to go to the first order or the
first optractor and then uh similarly
for the second router again. So lang
graph hence handles this beautifully
with uh multiple conditional ages uh
joined together allowing our graph to
branch and rebranch as per needed just
like how complex reasoning systems work
in the real world. It's a simple simple
yet powerful demonstration of how Lang
graph gives you total control over your
workflows direction without any uh
hardcoded if else change in your logic.
So now uh instead of having a single
choice your system can make multiple
sequential choices each one based on the
operative state of the graph. So this
opens the door to designing more
intelligent uh multi-step workflows like
multi-turn reasoning agents, multi-stage
data processor or even uh
uh complex uh system that can react to
evolving uh inputs. So far in this line
graph series, we built conditional and
multi-step workflows, graphs that decide
where to go based on the data in the
state. In this video, we're going to
make things a bit more fun and
interactive. We'll design a mini number
guessing game using Langraph where the
system makes guesses, checks its
progress, and keeps looping
intelligently until it it gets the right
answer or runs out of it atems. This
example shows how lag graph can handle
iterations and choice loops. Something
that's incredibly important when you
want your agent to keep reasoning,
refining, and trying again just like
humans do when solving a problem. Okay,
so without any delay, let's first go
into the code.
[snorts]
So here I'm going to create a new file
to call it number game.
Number game
do ipy nb.
So from line graph
I'm going to import
state graph start
and end
from typing
I'll import type
and list
and we'll also import random here.
So let me select the virtual
environment.
I'm going to create create a class
called agent state
agent state.
So here I'll have three variables here.
The first one will be name which is
going to be a string. The second is
going to be a number which is going to
be list of integer.
uh this is basically going to contain
the guesses that the model makes and a
counter which will also be an integer.
So
at first we'll have a greeting node
which will contain
agent state as its parameter
and it will again return an agent state.
Let me write down a drop string here. So
let me say [snorts] greeting
node
which says hi to a person.
So here what I'm going to do is I'm
going to update state name
to
hi there.
I have a state name that I'll pass
in the input here. So, state name.
we will start
the G.
And what I'm going to do is I'm going to
set state
counter to zero
and then return state.
I'll have another state called uh or
another function called random state.
This will contain agent state.
age and state.
So this is going to generate
number
it will be random between
between 0 and 10. [clears throat]
So state
number will be appended
with the random number guessed by this
model here 0 to 10.
Then I'm going to update the state
counter.
State counter will be updated
by one. And then I'm going to return
state here.
The third function will be our router
function
which will say should continue
contain an agent state
that will return a string.
So this will be a function
to decide
what to do next.
What to do next
here? If
state
counter
is less than five.
I'm going to enter
loop
where this will be my state counter
value.
And I will return
loop here.
And in the else part,
I'm going to exit the loop and I'll
return exit here.
Now let me create a graph using the
graph
using the state graph
state graph and inside this I'm going to
pass my agent state
I'll add a node graph dot add node.
So first of all we'll have a greet node
using greeting node function.
Then I'll have random node
using
random state function.
I'll also start adding the ages. So
first of all the first age will go from
start to grid.
The second will go from grid to random.
And then here we're going to have
conditional ages.
So add conditional ages.
It will start from
random.
The function that it should check is
should continue.
And we'll have two different cases here.
The first case if it's loop
uh then it should go to random
or if else if it is exit then it should
go to
end.
Now I'm going to have one more uh edge
here.
If the counter is greater than five then
I need to end this loop. So one age will
be from uh random to end. So let me
initialize my graph or compile my graph.
And if I print my graph
uh okay what's wrong? Let's see. So add
age edge. Okay. Add edg edge.
Okay, as you can see, so once uh once
the model greets the user, then it will
enter inside this random node. Uh then a
loop will be carried out uh if it stays
within a certain condition or else it
will exit. Now
what I'm going to do is I'm going to
take my input input state here.
So input state
equals to name
shake
and
and the number list will will now be
empty and with this I'm going to invoke
my board
with my input data and I'm going to
print my response.
So if we run this in loop one, loop 2,
loop three, loop four, loop five because
we've entered uh a condition for the
value less than five. So in loop one it
guess 10. In loop 2 it guessed 8, 7, 2
and 3 and respectively. So this is how
we initialize loop in case of langraph.
So we'll go to our our very beginning of
the code and then we'll see what is
happening here. So we start by uh
defining this agent state that holds
everything about this game. So it holds
the player names, the target number, the
last guesses, the total attempts and the
range of the and the range of the number
you can choose from. Uh then we have
three things. We have the setup node. So
this basically initializes the state uh
resetting uh attempts as well as the
guesses. Then we have the guest node
here and then we have the uh
decision node or the router node that we
have here. So uh this is where the power
of the line graph shines. So instead of
using a while loop, we simply create a
conditional edge that loops back to the
same node until a certain condition is
met. So the condition is placed here in
this particular part of the code.
So each loop uh iteration updates the
state meaning uh lang graph remembers
the previous attempts making the
upcoming choices context of it. So
essentially uh we've not completely
implemented the game here but we've
turned this simple uh guessing game into
an uh autonomous uh iterative reasoning
process uh which holds the same uh
mechanism behind how our agentic systems
plans act [clears throat] and retries
while solving complex task. And I'm
going to end this video for now. But
what I want you to do is I also want you
to add a target number here. And then uh
basically in this particular uh router
function here, you basically check uh if
the model has correctly guessed the
number or not. If the model has
correctly guessed the number then then
the model has won the game or if uh if
the model has correctly uh or if the
model uh could not guess the number
within the given number of attempts
basically the model has lost the game.
So I think you can implement that by
yourself. In the last video, we explored
the basic structure of Lang graph where
we learned how nodes and edges from the
foundation of state-driven AI workflow.
We built a simple example that took an
input message, processed it through a
node, and returned a response. Now,
we'll take that idea one step further.
In this video, we'll connect our Lang
graph with a real AI model, Google's
Gemini 2.5 Flash and see how a complete
interaction look can be created between
a user and an intelligent agent. Here
we'll represent our state as a
collection of messages. Each message
captured in the conversation between a
human and the AI. The power of Langraph
truly begins to shine here. Instead of
hard coding a linear sequence of actions
like we often did in Langchen, Langraph
lets us visualize and manage the flow of
thought where each step is an
independent node that can evolve,
connect or branch dynamically. And by
the end of this video, you'll understand
how a line graph based chatbot can
process user inputs, generate
intelligent responses and maintain
conversation flow all through a graphic
structure. This marks our first step
towards building intelligent agent
multi-step conversational AI system
using lang. So let's get started and see
how easy is it to make your AI think in
graphs. So we'll proceed to the code.
I'm going to create a new file here
and I'll name it AI agent
agents do ipy lv
first of all start with the import from
typing I'll import typic
and list
then from lang graph
dot graph I'll import state graph
start
and end.
Then from langen_core
dot messages
I'm going to import human message.
Then the model
import chat Google generative AI then
from env I'm going to import load env
load the credentials first and then I'm
also going to load the model here. So
the model is
&gt;&gt; as I said earlier it is Gemini
2.5/model.
Okay. So let me run this. I'll select
the interpreter and then create an agent
state here.
So the agent state will be of type
picked uh
I'll [snorts] have one single messages
which will be a list of
human message.
Then I'll have a function called process
where I'll pass in the state agent state
and then this will again return agent
state here.
the
response will be uh invoked from the
model here. So whatever is present in
the state will be passed to the model
and then I'll get the
response here.
What I'm also going to do is I'm going
to append the same state messages
dot append
response
and I'll also print this. So AI
uh AI has passed this response content.
Let me also add a line break here. And
finally let me return
state here.
Now [snorts]
&gt;&gt; after this I'll pass state graph
uh and inside this I'll pass agent
state.
Let me add a new edge uh sorry new node
first.
So the node name will be process and the
function name is process.
Let me add two different ages here. So
add age start to process.
Another age will be from process to end.
Process to end.
Now compile the graph
and then we can see the workflow of our
graph here.
Okay, run this agent state and then we
can see our graph here which goes from
start to process and then to end. So
basically in the process part what
happens is whatever uh message is being
typed by the user is sent to the is sent
to the model or the AI model and from
there the AI model generates a
response. Okay. Now uh
what we'll have here is a user input.
Let me ask for the user message from the
user input.
enter your
message
and
and I'll run a loop
until the user types exit.
So not equals to exit.
And I'm going to print
print the human message
[snorts]
then my input state is going to be
the messages.
uh it key will be messages.
It will be a list of
or I can do it like this.
I can code it under human message.
invoke
the bot
using our input state
and then
basically uh ask for another
user input again. So if I run this,
I can pass any message here. Hello
and then the model has replied me back
ask what is your
what is your name
and then AI uh Apex bag as well. So my
uh each time I enter uh enter some text
that will be taken to the model it will
be invoked within our model and then our
model will generate some response here.
So this is how we integrate a real world
AI into our langraph workflow. In our
previous video, we saw how to build a
simple conversational agent using
Langraph, one that could interact with a
real AI model and respond dynamically to
human input. But what happens when we
want to want our AI to do more than just
talk? What if it could reason, maybe
perform calculations, or use specialized
tools to find answers? That is exactly
what we will do or what we'll explore in
this video. We're introducing the
concept of tool nodes in Langraph. A
powerful way to let your AI agent call
external functions like the functions
you've created
and many more. Instead of relying on the
model to guess or holutionate results,
we give it direct access to real tools
and langraph handles the flow between
model calls and tool execution
seamlessly. Here we also use conditional
ages, the graph that chooses whether the
model should stop or continue based on
whether there are tool calls remaining.
This introduces intelligent flow
control, something that was often more
complex to manage in Langen. By
combining the reasoning power of
language model with the precision of
actual code functions, we're moving
closer to a truly capable AI system,
ones that can both understand and act.
And by the end of this video, you'll see
how the model calls the right tools,
performs the operation, and keeps
reasoning step by step just like an
autonomous agent. And this is where
Langra begins to bridge the gap between
intelligent thinking and practical
agent. So without any delay, let's dive
in and how and see how your AI can now
think, plan, and calculate all on its
own. So now let's jump to the code.
I'm going to create a new file here
6 tool calls ipv
first of all we'll write the imports
from lang code messages we'll have
uh dot messages
h import
I'll have a base message
I'll and I'll have a system message
now about the model this will be my
model and then from langen core
dot tools
I'll import
tool
then from lang graph
dot graph
dot messages
I'll import
add messages
then finally from langra
dot pre-built
import
download.
So this should be message not messages.
Now everything is imported. Let's run
this.
And this is running. First of all, we'll
define
our model here or first of all we'll do
the credential. then define our model as
Gemini 2.5 flash and then run this. In
the next node, I'm going to define my
agent state.
My agent state will be of type type dict
and then inside this I'll have messages
which will be an annotated
sequence of base message and then
in between I'll write add
messages function. Okay. Now I'm going
to create a few tools. uh in this
particular case we're going to write
four different functions or four
different tools for addition,
subtraction, multiplication as well as
division. So now we'll create the
functions for them. So add the rate tool
def
integer b integer
this will also return an integer here.
Let me
write down the dock string.
This will
add two numbers
then return a + b. Similarly, we'll do
the same for subtraction but I'm I'm
just going to copy this code multiple
times. Subtraction multiplication as
well as division.
Let me change the function name.
This will be sub the two numbers. So
subtract the two numbers. Here we'll
have a minus b.
This will multiply the two numbers
multiply
we'll have a into b and then this will
divide the two numbers
divide and then a / b. Okay. So these
will be my tools uh uh for this
particular
system. Now I'm going to
list all my tools under this my tools
list. So add s mult and div.
Now after this what I'm going to do is
I'm going to bind my tools
with the lm. So okay we have our model.
So model dotbind
tools.
Let's say my tools.
Okay.
Now we'll create our nodes here. So
write the function for those nodes. So
model call will be our first node here
which will have have a type agent state
and it will also return
agent state.
So the system prompt
is going to be
a system message
uh and its content will be you are an AI
assistant
please answer my query to the best of
your ability.
Okay.
Now I'll pro I will provide the inputs.
So inputs will be our system prompt
plus
the state passages that we have.
Then I'll invoke the model with this
inputs
uh
inputs and then finally I'm going to
return
uh messages.
So this is going to be state messages
plus
response.
Okay,
I will have another router node here
which would say should
continue.
It will contain
agent state as its parameter
and we'll have message equals to state
messages
I will pick up the last message from
here.
So message minus one
and then and then if the last message
does not contain any tool calls
last message does not contain any tool
calls
then I will
take uh the flow to end
or the conversation end or else
I will continue with the conversation.
so as to allow the model to call those
tools here. Okay. So these are my nodes.
Let me run this. Create a graph from
state graph.
It will say graph dot add node
model
call will
will be represented by model call
function.
Then
here I'll have a tool node object from
class tool node and then
it will hold all of my tools.
from my tools list
and I'm going to add a node
called tools
using
two load function. Okay. So now I'm
going to add edges. So graph dot add age
edge. The first will be from start to
model call.
The second will be a conditional edge.
It will go from model call
to the function should continue.
And if if the result from should
continue is end then it will end
or else if it is continue
then it will call
tools.
So I'll have one more uh edge here.
Graph dot add edge tools to model call.
And I should have one more I guess.
graph dot add age edge
uh model call
to end.
Let me compile this
and then see the graph.
So if you visualize the graph you can
see that we start from here we go to
model call. If it is a tool call then we
go to continue and then call the tool
and again we go to the model call. Or
else if there are no more tool calls
tool calls involved then we go to the
end.
I'm going to create a function here
called print stream.
This will contain a stream object.
So for s in stream
I will pull up the last message.
So
if is instance
message message
then I will print the message
or else
message.
Print.
Okay. Now uh this is going to show the
stream stream of operation that will
happen while our tool is being called.
So now uh we'll define our input
input. So input will be messages
and then it will be from our
user
and the message will be add 40 and 12
and then
calculate the product
with six.
also
tell me a joke at the end.
Okay, we will not do 40 and 12. we'll do
uh maybe
two and four and then uh and and then
the product with six and then [snorts]
at at last I'm also asking the model for
a joke. So what should happen here is
this particular part should be done
using our tools and then for this
particular part the model uh should call
the LLM and then the LLM should a should
be able to generate the joke without
using any tools because there are no
tools present to uh generate the joke.
Now at last what I'm going to do here is
print stream
print stream. Uh
so bot dot streamm
stream. Now inside this I'm going to
pass input
and then stream load.
Sorry there should be a comma here. So
stream node
equals to values.
Okay. Now this p brings us to the end of
our program. If we run this,
we should see a stream of operation that
has been happening uh in order to
uh fulfill our current operation. But I
guess we have an error. Let's see what
the error is. Okay, I've removed this uh
last part. So, so let me run it again.
Okay, I think I might have found the
error here. So what I'm going to do here
is node name comma node value equals to
next iteration iterator s dot items
and inside this I'm going to check if
messages
is present in
node values
And uh if it is present
uh then what I'm going to do is
so message equals to message one and
then
node value equal to node messages minus
one sorry Okay,
sorry [snorts] I missed this
uh node value.
So misses so misses minus one. Let me
run this again.
What happened here? Okay, extra bracket.
So as you can see
uh our query is taken and then these
four first two numbers are taken as two
and four here and then pass to the add
uh add tool. The result is six. Then uh
both of these six and then and then the
following six is taken as a and b and
then the multiplication tool is called
and then the product is calculated and
at last the joke is also uh generated.
So this is how we can implement our own
tools uh in case of uh llm in langraph.
In the last video, we explored how
Langraph lets your AI not just think but
act. We saw how to connect tools like
addition, subtraction, multiplication,
division so that the model could reason,
calculate, and decide its own next
steps. Now, we're taking that idea even
further. In this video, we're building
something more practical, a document
editing agent powered by Langraph. We'll
call it an email drafter. Your
intelligent writing assistant that can
update, modify, and then save document
automatically using AI and tools. But
before we jump into the code, we'll try
to understand what's happening
conceptually here. Our agent can combine
LLM reasoning using Gemini tool nodes
for doing real world actions and
conditional graph control to keep
interacting until the document is saved.
We'll define two tools here. Update and
save. Update to modify the document
context content dynamically and save to
store the final version into a txt file.
The system prompt that we're going to
provide will ensure the AI always knows
what the current document state is and
the langraph manages when to loop or
exit depending on whether the user has
saved the document or not. This
structure will make your AI truly
stateful, capable of handling multiple
user interactions over time and goal
oriented. Something that's much harder
to achieve in plain lang. By the end of
this video, you'll see how Langraph
turns your language model into a
functional writing agent. One that just
doesn't respond, but works with you to
create and save real content. So, we'll
dive in and bring in Raptor to life. So
I'm going to create a new file here.
Call it email
crafter
ipy nv.
Write down the imports.
Okay, we'll load the credential first
and then we'll also uh define our Linear
run this
and then we'll have a global variable
a global variable to store
document content.
So we'll write document content equals
to an empty string.
&gt;&gt; Now we'll have our agency class.
Any
other messages will be of type annotated
sequence uh paste message and then add
messages.
Now as I said I'm going to define two
different tools here. So the first tool
will be update.
Uh we'll have a cont a string content
inside this and then we'll also return a
string content here.
So I'll write down the dock string.
Sorry for this.
So this is going to update the document
with the provided
content.
Okay. So I'll access the global document
content here. Then uh
what I'm going to do is
document content
equal to the content that I get here.
And then I'm going to return a string
here.
Uh which says document has
been updated
successfully.
The current
content is
document content.
Okay. So this is my first function or my
first tool function that I have.
Next, I'm going to create another tool
called save where I'll provide a file
name in string format and then I will
return a string here.
Write down the doc string. Save the
current
document to
to text file and finish the process.
The arguments that I'll be passing here
is the file name. So which will indicate
the name of the
text file.
So again I'm going to access the global
document content.
Then if there is no file name
uh created then um then I will provide a
file name txt file here.
Then I'll try to open this file
as file.
uh
I'll try to write the document content
into this file
and then print document saved
successfully as file name
and then also return the same. If we
have an exception
then I'm going to uh return an
error message here.
Okay. So these are my two tools that
I'll be using.
Finally I'll create a list of my
available tools. So the first one is
update
and then the second one is save.
I am going to bind my tools.
I bind the lm using my tools.
So my tools and then I'm going to create
a tool tool node object
using my tools here.
Okay. Now I need to define my nodes. So
our first node is going to be our agent.
It will contain agent state
type. It will also return
and agent state.
So what I'm going to do here is I'm I'm
going to provide a system prompt. Let me
just copy this prompt from somewhere.
System. Okay. I'll just copy this entire
thing from somewhere so that uh you can
pause the video and then see what the
actual prompt is.
Okay. So, the prompt is uh you are
you are an email drafter,
a helpful writing assistant. You're
going to help uh help the user update
and modify document. If the user want to
update or modify content, use the update
tool. And then if the user want to save
and finish, use the save tool. Okay. So,
this is done. the current document uh
content is also passed here. After this,
what I'm going to do is
I'm going to
check
if
user message is not provided.
And based on this, I am ready to help
you
update a document.
what would you like to create? Okay,
this is the first message that the user
will be seeing and then I am going to
encode this inside
human message
or else if there is already a
already some content that has been
generated then the user input is going
to be
so this should be an input
Uh, what would you sorry
what
would you like
to do
with this document?
Then I will print
user
and then print the user input.
then put it inside the
user message again. Okay, now these two
states are done.
What I'm going to do is I'm going to
combine combine the system prompt as
well as the user message that we have.
So this will go inside all messages. It
will have
system prompt plus list of state
messages plus user message. Now I'm
going to invoke the model response equal
to model dot invoke
uh invoke using all messages.
Sorry, it's model year.
Okay. Then I'm going to print out AI
response here
or just write down AI
uh and then print out
response.content. content.
Now, now I'm going to check uh if there
is any tool calls uh provided in this
particular
response. So, if has
TTR response
to calls
and response
dot
Sorry for the autocomplete part.
response.tool
calls
then I will print which tool is being
used uh right now.
So
invoking tool
please be careful with the brackets
here.
TC
uh tool name
for TC in response. call
and then I'm going to return
the messages
uh which will be the list
of state messages
plus
user message as well as the response
again. I I think I missed a bracket
here. Okay, this one extra bracket
extra. Okay, so this is done. So this is
for for our agent
uh node here. Now I'm going to create a
router node.
So router node will be shared
continue.
It will contain agent state
and then it will return a string.
So the doc string for this determine if
we should continue or end the
conversation.
Okay. So first of all access our state
messages
and then
if there is nothing then we should
continue
or else if there is something inside
this
then we reverse the list list
And then check uh check the instance uh
and see if it is save or like uh
if the tool call name is
present is in the form of save. So is
instance
message
it is message right?
Okay. some message
uh and then
to message
the spelling is wrong here. So for each
instance message and then two message
and saved
in message dot content dot lower
this is not complete
and
document in
message.contain.l
then we return end or else we return
continue.
Okay. So our router node is also fixed.
So what we're basically saying is if we
have if we don't have anything then we
should continue.
If we have save then we should end the
flow or else if we don't have save or
like if we have update then again we
should continue the flow. Okay. So this
is done. Save this.
Call our state graph.
Uh add a node.
This will point to our agent. Add
another node.
I'll have tools.
Now I'll start adding ages.
So from start we go to agent and then
from agent we go to tools. Then after
this I'm going to add conditional ages.
Here
we'll start from tools.
We'll go to should continue function
and then from should continue function
we have two options.
If we have continue we go to agent. If
we have end then we end our flow.
So let me compile this graph
graph.compile and then print out our
state graph here. So as you can see we
start from here we go to agent
we go to tools. If we have to keep on
updating our document then we continue
this particular loop here from from
agent to tools and then tools to agent
and then once and then once we call the
save tool then we go to the end of the
conversation.
Okay. Now I'm going to create some
function for prints.
So print messages it will contain our
messages inside this
function made to print the messages
in more
readable format.
Okay. So if not messages we we just
return
uh if nothing is there we just uh return
empty. But if we have messages and then
we especially focus on the last three
messages that we have
then what I'm going to do is I'm going
to check if
is instance uh is
instance message is a tool message
then I'm going to print out uh tool
message result content.
Now to run the document agent,
I'll create a new function.
So at first
the state is going to be
messages and then empty message.
Then let me uh run this. So first step
in port dot stream state
uh if messages in step
then print messages step messages
and then it's done and then at last we
can print out conversation ended.
Okay, so let me run this
and then finally call run
document agent.
Now if we run this so
what it asks me is I'm ready to help you
update a document. What would you like
to create? So write me an
write me a leave requesting
email for 22nd October
October uh October sorry for the
spelling
October 2025 Five.
Me an email.
Uh, okay. The AI says, "What should the
email say?" Uh,
it should be about
leave application.
So the AI agent does write me an email.
Now I want to uh replace the state start
date replace start date
uh to 22nd
October and uh end date to 23rd October
2025.
Let me save this file directly.
Uh or I can say the email should include
my name as Abby
and then it does uh replace my name here
as Abby. So
everything is done. Uh
what I need to do is I need to save this
file. So if I say save the file now,
save it.
So uh it asks me for a file name. So I
can say
uh
email draft
and then if we see uh somewhere in our
folder location then we have then we see
that we have an email draft file uh
where the AI generated content is saved
here. So this is uh
all about the email drafter application.
I've made a few changes uh here in this
particular part here as well as
in this particular uh cell here. So
please go through the changes and then
uh I hope it runs on your end as well.
So in this video we've seen uh how we
can use lang graph as well as our real
world AI model uh to
to invoke the tools that we have and and
then uh do something meaningful for us.
In this video we'll see how we can build
our rag agent using langra. So let's
start our video by creating a new file
8_rag
agent ipy nbv.
Well for this rag agent I'm going to use
a file uh I have a pdf file uh that I'll
be using. So let me paste that file here
first.
So this is a stock PDF file. uh I'll be
using
using the data from this particular file
uh for this rag agent here. Okay. So now
first of all start with imports.
import type dictotated
and sequence.
Now from langen code I'm going to import
something called human messages tool
messages system message and base
message. So import human message. Uh I
don't need AI message. I I'll need a
tool message
and a base message and a system message
from operator uh
import
add as add messages.
Uh then I'll open the model or import
the model import chat Google generative
AI as well as
the Google uh Google generative AI
embeddings.
Then from langen community
I'll import pipdf loader
to open the pdf file.
I PDF loader.
Then we'll import recursive text sorry
recursive character text splitter line
chain.ext splitter import
recursive character text splitter.
Then for the vector database I'm going
to use chroma
import chroma.
And finally for the tool part from lang
core
I'm going to import tool
load the credential first
then I'll load my model here
the model will not be Gemini pro uh I'll
remove the temperature for now
but it is going to be Gemini 2.5
flash model and for the embedding model
I'll be using
uh
so model equals to
model/
Gemini
embedding
001. Okay, let me run this
first cell
and the cell is running.
Okay, there is some uh error here. Let
me see what the error is. So, Gemini 2.5
flash should be correct.
Okay, it should not be model name but it
should be model
uh rerun it again.
Okay, the model is imported. I'm going
to copy this uh not put it here but put
the PDF file inside this line graph
folder so that it can be easily accessed
by the
notebook file present in this folder. So
PDF
path equals to stock
PDF. So this is going to be my PDF path.
And then we'll try to load the file
first.
So for that what I'm going to do is I'm
going to check if the file exists or
not.
path.exist PDF path. But like if it does
exist then I'm going to do PDF loader
equals to pi PDF loader PDF path.
Okay, this is also running.
Now I'm going to load the pages from the
PDF path. So load the pages
from the PDF. Uh
so I'll put it inside uh try block PDF
loader.load.
Uh let's also print something. So
a PDF has been loaded and
has
a certain number of pages. It if it
cannot be loaded then we will load an
exception and then print out our
exception here. an error occurred while
loading PDF
and then
simply raise an exception.
So our PDF has been loaded and then it
contains nine pages.
Now what we're going to do is you we are
going to use our splitter here. So text
splitter
equals to recursive character text
splitter
uh the chunk size is going to be 1,000
and then the chunk overlap is going to
be 200. That's okay. I'll keep that for
now. But you can change the values
according to your need. Uh if you want
to make
make smaller chunks more smaller chunks
then you please reduce the chunk size.
uh
and then uh if you want to uh overlap
some information between chunks then you
uh increase the value of this chunk
overlap.
So now let me split the pages. So pages
split equals to textsplitter dotsplit
document pages.
So if we want to see total number of
chunks then we can print it print it out
and then we can see that we have now uh
divided it into 24 chunks. If we reduce
this chunk size to maybe 500 then we
should see that the chunk uh that our
chunks are or the total chunks number
has should be increased here. So if we
keep it to 500 then we go to 60 chunks
but right now I'll keep it to 1,000 and
then we have 24 chunks. Okay. Now
[snorts] we go to our vector database
part.
So for vector database I'm going to give
a collection name. So collection name is
going to be stock market.
So we'll try to create a vector store
from this one. So vector store equals to
Roma dot from documents.
So document equal to page split.
For embeddings, I'm going to use my
embedding model. And then for collection
name, I'm going to use my collection
name.
And then finally, uh we're going to see
our success message here. But if we run
into any kind of exception, then we'll
print an exception. and then raise some
error.
So if we run this,
it will take some time uh to be
vectorzed but our vector store has been
successfully created means the try block
has been executed successfully.
Okay. [snorts] Now we'll define our
retriever path. So, so for retriever
we'll do uh vector store dot as
retriever
and we'll define our search type. Search
type will be based on similarity and
then the search keyword arguments will
be uh we'll keep it with three for now.
So the retriever part is also executed.
Now we define our tool.
This tool will be our retriever tool
which will [snorts] contain an query of
perform string and then it will again
return an string here
provide a dock string for this. So this
tool
searches and returns
information from the stock
market performance
performance report
document. Okay,
we're going to invoke our retriever
using our query. So retriever.
So retriever dot invoke
query.
So if nothing is found uh
with with uh similarity to query in our
document then we print out or we return
no relevant information found or if
something is found then we are going to
embed this in a list.
for I do in enumerate documents. So I'm
going to append everything inside this
uh results list here. And then what I'm
going to do is I'm going to uh convert
this list in string form.
Now we only have one tool which I will
wrap inside this list.
Then I'm going to bind my llm uh using
this
tool uh with the help of the function
bind tool
and then run this.
Now we'll have our class agent state.
It'll be of type dict uh type dict
I'll have messages which will be an
annotated sequence of base message. And
then here what we want to do is we want
to keep adding our
messages. So I'm going to use this add
messages function.
And then I will define a router function
should continue state
agent state. uh this will
return the tool calls if it has. So
what it is going to do it check
if the last message
contains
tool calls.
So for that I'm going to see my last
message
and then I'm going to return
the result
uh tool calls and len uh l tool calls is
greater than zero. Okay. So this is also
done.
Now what I'm going to do is I'm going to
write a system prompt which I'm going to
copy uh from my source. So this is my
system prompt. You are an intelligent AI
assistant who answers uh questions about
the stock market performance in 2024
based on the PDF document loaded in into
the knowledge base. Use the retriever
tool available to answer questions about
the stock of data. You can make multiple
calls if needed. If you need to look up
some information before asking a
follow-up question, you're allowed to do
that. Please always site the specific
parts of the document you use in our
answers.
So now what I'm going to do next is run
this and then I'm going to create a tool
state
and then here I'm going to do our
tool dot name
uh our tool for our tool in tool.
So it should be tool right? Uh yeah in
tool
run this again. So this is also running.
Now we'll define our llm agent.
So def call [snorts] llm
it will contain agent state.
It will also return agent state
object.
So function to call the llm with the
current state. So here what I'm going to
do is I'm going to make a list of my
messages. So, list of state
messages.
Then I'm going to convert this into a
system message or I'm going to append
append my uh system prompt as well as
the
messages that we have. And finally,
we're going to uh invoke our LLM using
the combined method that we have.
Okay, so this is done.
Uh I don't think I need to mention this
messages.
So I can just do message here.
And then what I'm going to do is I'm
going to return message
uh messages
list of [snorts] message.
Okay,
this is also running.
[snorts]
Now we create one more function for our
people agent. So def take action
state agent state.
It will also return agent state object.
So what this function will do is this
function to execute
tools from the
llm's response.
So for this what I'm going to do is I'm
going to check uh if if we contain any
tool calls from the lm. So tool calls
equals to system message minus one for
tool calls. If there is then we'll
append everything inside results. So now
we'll look through our tool calls.
So we try to understand what tool is
being called here. So print
f executing tool name with certain
input.
Uh
so if not
t name
in date
uh
I'll do tool not found
or I can just keep keep it inside. the
result uh but it's okay uh I can keep it
like this for now but uh I don't want to
directly append this so what I'm going
to do here is I'm going to do a result
equals to
uh tool not found
else
uh I'm going to append this inside the
results using the tool name
so D name
D name dot
dot
invoke
D ars
dot get
then print
uh tool result here.
Then what I'm going to do is I'm going
to append the tool message that we get
from the above loop.
So here results dot append
bool message.
So tool call id equals to t
id
uh
what else? name equals to D name and
uh content
content equals to str
result.
This should be inside the for loop.
Sorry.
And finally I'm going to print tools
execution
tools executed and then return
the agent state response.
Okay. Now we build our graph using state
graph. State graph agent state.
add our first node graph dot add node uh
which will be our
add node which is be lm represented by
all lm function.
Then the second node will be
uh add
node. The second node will be retriever
agent
which will be represented by take action
function
and then the first edge will be craft
craft dot add edge
and edge it will go from start.
I've not imported start I guess. So let
me also input start.
So the first node will be from start to
lm.
Then what I'll do is I'll add a
conditional edge.
Add conditional age. This will go from
llm
to
should continue.
And then we'll have two different cases
here. For true,
we'll go to retriever agent. And then if
false, we'll go to end.
I'll add one more age here. From
retriever agent to LLM and then the
entry point will be from LL
from LM itself. Okay. What's wrong with
should continue? Let's see.
Okay. [snorts] The spelling is wrong
here. showed underscore continue.
Uh something's wrong here. Now
function is not callable. Okay, let's
rerun everything from the beginning.
So this is running. Tool collection is
running. This is also running.
running running running
the tool call is also running.
Now the graph is also built. Let me
compile the graph. Call it rag agent
equal to graph dot compile
and then if we bring out our rag agent
then we should see our flow here. So, so
from start we go to LLM. If it has tool
calls then we go to retriever agent then
we go back to LLM. If it has multiple
tool calls we again reciprocate through
these uh retriever agents and then once
our LLM call is complete then we go to
false and then end the conversation.
So finally we'll define a function for
running our agent. So def running agent.
Uh so we'll
use a continuous loop here and then
we'll ask user for their question
input. What sorry
what is your
question?
So if the user input is exit or quit
then we exit the agent
or else we're going to append
append uh append the user given text
into a human message.
Then we will invoke our rag agent.
Then we'll print the
uh agent response
&gt;&gt; here. Okay. So this is also done. And
finally we'll call this running agent
function
since uh it was a long code. I hope we
don't have any errors here. So we can
run the application.
Okay. So, so initially uh this is being
run now.
Uh okay, what do we ask here? So, we'll
ask what is the document about?
Okay, we have an error. Let's see what
the error is.
Uh
I think I found my error here. So this
should be inside a list. The system
message should be inside a list here
because uh if we don't keep it inside
the list then this is not considered as
a base message. Let me uh rerun
everything again.
Okay. So what is the document about?
We still have an error. Let's let me
check again.
Uh for this one, I'm also going to uh
edit my
uh retriever agent function.
So let me rerun this again and then see
if I get an error.
So what is the
what is the document about?
Let's see if we get any error. Okay, so
the query is gone. Uh and then we have
two different sources uh from where
we're getting result and then the agent
responses. The document is about stock
market performance in 2024. Uh
specifically focusing on something
something something. Okay. Now we'll
open this uh stock market document
uh and then see uh what this document
actually contains
what happened here.
Okay. So let me rerun this again and
then and then check. Meanwhile, I'm
going to open this document.
&gt;&gt; [snorts]
&gt;&gt; Okay. So this is our stock market
document. Uh it contains some data and
it will be asking question based on this
here.
So we'll rerun everything again and then
see if we can continuously talk to it or
not because I think we're getting some
error there. Uh [snorts] we'll also fix
that.
Tell me about
about the EV leaders. Uh,
who is the electric
vehicle
leaders in the market?
It should find that the EV leader
vehicles are this company here.
Uh
so the agent response is correctly given
and then we'll we'll ask uh
we'll ask how much did they gain. So how
much
did
Tesla gain in 2024?
So from late 2023 to 2024 2024 gains
occurring in there. So it contains a lot
of content here. So it does give me uh
give me the share price and then and
then the growth as well as the sources
as I asked earlier. So yeah, let me go
to something else and then ask it.
Uh [snorts]
what was S NP's
gain?
So it should give me that S&amp;P's gain was
25%.
Okay. So this is working quite well.
So this is how we uh implement rag
using our langraph. Uh okay, I'll ask
it. uh something that is not uh present
in the document and then we'll see what
it tells us. So what is
why does
why does the sun rises in the east
and sets in the west.
So it should not have any response for
this. So it says I cannot answer this
question using the provided document as
it was stock market performance and not
astronomy. So this is working quite
well. Uh I'm going to end the video
here. So I hope it uh implements on your
end too. Be careful about the content of
this particular function here because
there are too many brackets being
involved here. So might make some error.
If you have any questions feel free to
comment down below and I'll try to help
you out. and I'll see you in the next
video again.