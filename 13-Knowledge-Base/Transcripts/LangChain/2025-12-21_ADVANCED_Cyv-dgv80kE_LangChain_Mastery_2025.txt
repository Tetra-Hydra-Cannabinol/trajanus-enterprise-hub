Title: LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]
Channel: James Briggs
Video ID: Cyv-dgv80kE
URL: https://www.youtube.com/watch?v=Cyv-dgv80kE
Duration: 4:46:46
Level: ADVANCED
Application: LangChain
Topics: LangChain v0.3, LLM Basics, LCEL, Prompts, Chat Memory, Agents, Agent Executor, Streaming, Async, LangSmith, Capstone Project
Ingested: 2025-12-21
Source: Playwright Browser Extraction
==============================================================================

### Result
"0:00 welcome to the AI Engineers guide for the L chain this is a four course that\n0:05 will take you from the assumption that you know nothing about Lang chain to\n0:11 being able to proficiently use the framework either you know within line\n0:17 chain within line graph or even elsewhere uh from the fundamentals that\n0:23 you will learn in this course now this course will be broken up into multiple\n0:28 chapters we're going to start by talking a little bit about what line chain is and when we should really be using it\n0:35 and when maybe we don't want to use it we'll talk about the pros and cons and also about the the why the line chain\n0:41 ecosystem not just about the line chain framework itself from there we'll\n0:46 introduce Lang chain we'll just have a look at a few examples before diving into essentially the basics of the\n0:53 framework now I will just note that all this is for Lang chain 0.3 so that is latest current version\n1:01 although that being said we will cover a little bit of where line chain comes from as well so we'll be looking at pre\n1:09 0.3 uh version methods for doing things so that we can understand okay that's\n1:15 the old way doing things how do we do it now now that we're in version 0.3 and also how do we dive a little deeper into\n1:22 those methods as well and kind of customize those from there we'll be diving into what I believe is the S of\n1:30 future of AI I mean it's it's it's a now and the short term potentially even\n1:36 further into the future and that is Agents we'll be spending a lot of time on agents so we'll be starting with a\n1:43 simple introduction to agents so that is how can we build an agent that's simple\n1:51 what are the main components of Agents what do they look like and then we'll be diving much deeper into them and we'll\n1:57 be building out our own Agent X computer which kind of like the framework around\n2:03 the AI components of an agent we're building our own and once we've done our\n2:08 Deep dive on agents we'll be diving into Lang chain expression language which\n2:14 we'll be using throughout this course so line chain expression language is the recommended way of using line chain and\n2:21 the expression language or L cell takes kind like a break from standard python\n2:27 syntax so there's a bit of weirdness in there and yes we'll be using it throughout the course but we're leaving\n2:35 the ELO chapter until this you know kind of later on in the course because we really want to dive into the\n2:41 fundamentals of Elsa by that point but the idea is that by this point you already have a good grasp of at least\n2:47 how to use the basics of lell before we really dig in that point then we'll be\n2:53 digging in streaming which is an essential ux feature of AI applications\n2:58 in general streaming it can just improve the user experience massively and it's not just about\n3:05 streaming tokens you know that that interface where you have word by word the AI is generating text on the screen\n3:12 streaming is more than just that it is also the ability if you've seen the\n3:17 interface of perplexity where as the agent is thinking you're getting an update of what the agent is thinking\n3:24 about what tools it is using and how it is using those tools that's also another essential feature that we need to have a\n3:31 good understanding of streaming to build so we'll also be taking a look at all of that then we'll finally we'll be topping\n3:38 it off with a Capstone project where we will be building our own AI agent\n3:45 application that is going to incorporate all of these features we're going to have an agent that can use tools web\n3:51 search we'll be using streaming and we'll see all of this in you know a nice\n3:57 interface that we can that we can work with so that's an overview the course of course it's very high level what I've\n4:03 just gone through there's a ton of stuff in here and truly this course can take you from you know wherever you are with\n4:09 L chain at the moment whether you're a beginner or you've used it a bit or even inter mediate and you're probably going\n4:16 to learn a fair bit from it so without any further Ado let's dive into the\n4:22 first chapter okay so the first chapter of the course we're going to focus on\n4:28 when should we actually use Lang chain and when should we use something else now through this chapter we're not\n4:35 really going to focus too much on the code we you know every other chapter is very code focused but this one is a\n4:42 little more just theoretical what is line chain where's fit in when should I use it when should I not so I want to\n4:48 just start by Framing this line chain is one of if not the most popular open\n4:56 source framework within the python ecosystem at least for AI it works\n5:01 pretty well for a lot of things and also works terribly for a lot of things as well to be completely honest there are\n5:07 massive Pros massive cons to using Lang chain here we're just going to discuss a few of those and see how Lang chain\n5:14 maybe Compares a little bit against other Frameworks so the very first\n5:19 question we should be asking ourselves is do we even need a framework a is a\n5:24 framework actually needed when we can just hit an API you have the open AI API\n5:30 other apis mral so on and we can get a response from an llm in five lines of\n5:36 code on average for those is incredibly incredibly simple however that can\n5:42 change very quickly when we start talking about agents or retrieval augmented generation research assistance\n5:50 all this sort of stuff those use cases those methods can\n5:56 suddenly get quite complicated when we outside of Frameworks and that's not necessarily a\n6:03 bad thing right it can be incredibly useful to be able to uh just understand\n6:10 everything that is going on and build it yourself but the problem is that to do\n6:16 that you need time like you need to learn all the intricacies of building these things the intricacies of these\n6:22 methods and themselves like what you know how do they even work and that kind of runs in the opposite direction of\n6:28 what we see with AI at the moment which is AI is being integrated into the world at an incredibly fast rate and because\n6:37 of this most Engineers coming into the space are not from a machine learning or\n6:43 AI background most people don't necessarily have any experience with these systems a lot of Engineers coming\n6:50 in that could be devops Engineers generic backend python Engineers even you front end Engineers coming in and\n6:57 building all these things which is is great but they don't necessarily have the experience and that you know that\n7:02 might be you as well and that's not a bad thing because the idea is that obviously you're going to learn and\n7:07 you're going to pick up a lot of these things and in this scenario there's quite a good argument for using the\n7:14 framework because a framework means that you can get started faster and a\n7:19 framework like Lang chain it abstracts away a lot of stuff and that's a that's\n7:24 a big complaint that a lot of people will have with L chain but that abstract\n7:29 in away of many things is also what made sing chain popular because it means that\n7:34 you can come in not really knowing okay what you know rag is for example and you can Implement a rag pipeline get the\n7:41 benefits of it without really needing to understand it and yes there's an argument against that as well just\n7:47 implementing something without really understanding it but as we'll see throughout the course it is possible to\n7:54 work with line chain in a way as we will in this course where you kind of\n8:00 implement these things in an abstract way and then break them apart and start understanding the intricacies at least a\n8:07 little bit so that can actually be pretty good\n8:12 however again circling back to what we said at the start if the idea or your\n8:18 application is just a very simple you know you need to generate some text based on some basic input maybe you\n8:24 should just use an API that's completely valid as well now we just said okay okay\n8:30 a lot of people coming to L chain might not be from an AI background so another question for a lot\n8:36 of these Engineers might be okay if I want to learn about you know rag agents\n8:41 all these things should I skip line chain and just try and build it from scratch myself well line chain can help\n8:49 a lot with that Learning Journey so you can start very abstract and as you gradually begin to\n8:57 understand the framework better you can strip away more and more of those abstractions and get more into the\n9:03 details and in my opinion this gradual shift towards more explicit code with\n9:11 less abstraction is a really nice feature and it's also what we focus on\n9:17 right throughout this course that's what we're going to be doing going sing abstract stripping away the abstractions\n9:22 and getting more explicit with what we're building so for example building an agent in L chain there's in very\n9:30 simple and Incredibly abstract crate tools agent method that we can use and\n9:37 like it creates a tool agent for you it's it doesn't tell you anything so you can you can use that right and we will\n9:46 use that initially in the course but then you can actually go from that to\n9:51 defining your full agent execution logic which is basically a tools call to open\n9:58 AI you going to be getting that tool information back but then You' got to figure out okay how am I going to\n10:03 execute that how am I going to Sol this information and then how am I going to iterate through this so we're going to\n10:08 be seeing that stripping way abstractions as we work through as we build agents as we do as we bu like our\n10:16 streaming use case among many other things even chat memory we'll see there as well so line chain can act as the\n10:24 onramp to your AI learning experience then what you might find and I do think\n10:31 this is quite true for most people is that if you if you're really serious\n10:36 about AI engineering and that's what you want to do like that's your focus right which isn't for everyone for certain a\n10:43 lot of people just want to understand a bit of AI and they want to continue doing what they're doing and just integrate AI here and there and maybe\n10:50 those you know if that's your focus you might sick with ly chain you know there's not necessarily a reason to move\n10:55 on but in the other scenario where you're thinking okay I want to get\n11:00 really good at this I want to just learn as much as I can and I'm going to\n11:06 dedicate basically my you know my short-term future of my career on becoming AI\n11:13 engineer then line chain might be the on-ramp it might be your initial learning curve but then after you've\n11:20 become competent with line chain you might actually find that you want to move on to other Frameworks and that\n11:25 doesn't necessarily mean that you're going to have wasted your time with L chain because one L chain is a thing helping\n11:32 you learn and two one of the main Frameworks that I recommend a lot of people to move on to is actually line\n11:38 graph which is still within the L chain ecosystem and it still uses a lot of L chain objects and\n11:45 methods and of course Concepts as well so even if you do move on from line\n11:50 chain you may move on to something like L graph which you can no line chain for\n11:56 anyway and let's say you do move on to another framework in set said in that scenario the concepts that you learn\n12:02 from Lang chain are still pretty important so to just finish up this chapter I just want to summarize on that\n12:09 question of should you be using Lang chain what's important to remember is that Lang chain does abstract a lot now\n12:16 this abstraction of L chain is both a strength and a weakness with more\n12:23 experience those abstractions can feel like a limitation and\n12:29 that is why we sort of go with the idea that L chain is really good to get\n12:35 started with but as a project grows in complexity or the engineers get more experience they might move on something\n12:41 like Lang graph which in any case is going to be using Lang chain to some degree so in either one of those\n12:48 scenarios L chain is going to be a core tool in an AI engineered toolkit so\n12:56 it's worth learning in our opinion but of course it comes with its you know it comes with its weaknesses and it's just\n13:03 good to be aware of that it's not a perfect framework but for the most part you will learn a lot from it and you\n13:09 will be able to build a lot with it so with all of that we'll move on to our\n13:16 first of Hands-On chapter with Lang chain where we'll just introduce Lang\n13:21 chain some of the essential Concepts I'm not going to Dag too much into the syntax but we're just going to understand a little bit of what we can\n13:27 do with it okay so moving on to our next next chapter getting started with a line chain in this chapter we're going to be\n13:33 introducing a line Chain by building a simple LM powered assistant that will do\n13:39 various things for us it will multimodal generating some text generating images\n13:45 generate some stret shed outputs it will do a few things now to get started we will go over to the course repo all of\n13:53 the code all the chapters are in here there are two ways of running this either locally or in Google collab we\n14:00 would recommend running in Google collab because it's just a lot simpler with environments but you can also run it\n14:06 locally and actually for the cap Zone we will be running it locally there's no\n14:12 way of us doing that in collab so if you would like to run everything locally\n14:17 I'll show you how quickly now if you would like to run in collab which I would recommend at least for the the\n14:23 first notebook chapters just skip ahead there will be chapter points\n14:29 in the timeline of the video so for running running it locally we just come\n14:34 down to here so this actually tells you everything that you need so you will\n14:40 need to install uvie all right so this is the package manager that we recommended by the python and package\n14:47 management Library you don't need to use uvie it's it's up to you uvie is is very\n14:53 simple it works really well so I would recommend that so you would install it with this command here this is on Mac so\n15:01 it will be different otherwise if you are on Windows or otherwise you can uh\n15:07 look at the installation guide there and it'll tell you what to do and so before we actually do this what I will do is go\n15:13 ahead and just clone this REO so we'll come into here I'm going to\n15:19 create like a temp directory for me because I already have the line chain course in there and what I'm going to do\n15:26 is just get loan line chain course okay so you will also need to install git if\n15:31 you don't have that okay so we have that then what we'll do is copy this okay so\n15:38 this will install python 3.2.7 for us with this command then this will create\n15:45 a new VM within that or using python 3.2.7 that we've installed and then UV\n15:53 sync we actually be looking at the Pi Project at TL file that's like the uh\n15:59 the package installation for the repo and using that to install everything\n16:04 that we need now we should actually make sure that we are within the line chain course directory and then yes we can run\n16:11 those three and there we go so everything should install with that now if you are\n16:20 in cursor you can just do cursor dot or we can run code do if mvs code I'll just\n16:28 be running this this and then I've opened up the course now within that course you have your notebooks and then\n16:35 you just run through these making sure you select your kernel pth environment and making sure you're using the correct\n16:41 VN from here so that should pop up already as this VM bin Python and you'll\n16:47 click that and then you can run through when you are running locally don't run these you don't need to you've already\n16:54 installed everything so you don't this specifically is for collab so that is\n16:59 running things locally now let's have a look at running things in collab so for\n17:06 running everything in collab we have our notebooks in here we click through and then we have each of the chapters\n17:12 through here so starting with the first chapter the introduction which is where we are\n17:18 now so what you can do to open this in collab is either just click this collab\n17:24 button here or if you really want to for example Maybe this it is not loading for\n17:31 you what you can do is you can copy the URL at the top here you can go over to\n17:36 collab you can go to open GitHub and then just paste that in\n17:43 there and press enter and there we go we have our\n17:48 notebook okay so we're in now uh what we will do first is just install the\n17:53 prerequisites so we have line chain just a load of line chain packages here line\n17:59 chain core line chain open a because we're using open Ai and line chain Community which is needed for running\n18:06 what we're running okay so that has installed everything for us so we can\n18:12 move on to our first step which is initializing our LM so we're going to be\n18:19 using GT40 mini which is side of small but fast but also cheaper model uh that\n18:26 is also very good for open AI so what we need to do here is get an API key okay\n18:33 so for getting that API key we're going to go to open's website and you can see\n18:39 here that we're opening platform. open.com and then we're going to go into settings organization API\n18:45 keys so you can copy that I'll just click it from here okay so I'm going to\n18:50 go ahead and create a new secret key actually just in case you're kind of\n18:56 looking for where this is It's settings organization API Keys again okay create\n19:01 a new API key I'm going to call it Line train\n19:06 course I'll just put it on the semantic router that's just my organization you you put it wherever you want it to be\n19:14 and then you would copy your API key you can see mine here I'm obviously going to\n19:19 reval that before you see this but you can try and use it if you really like so I'm going to copy that and I'm going to\n19:25 place it into this little box here you could also just and place it put your uh\n19:31 full API key in here it's up to you but this little box just makes things easier\n19:36 now that what we've basically done there is just passed in our API key we're setting our opening model GT40 mini and\n19:45 what we're going to be doing now is essentially just connecting and setting up our llm parameters with L chain so we\n19:53 run that we say okay we're using a GT4 mini and we're also setting ourselves up\n19:59 to use two different LMS here or two of the same LM with slightly different\n20:05 settings so the first of those is an LM with a temperature setting of zero the\n20:10 temperature setting basically controls almost the randomness of the output of\n20:17 your llm and the way that it works is when an LM is predicting the sort of\n20:24 next token or next word in sequence know provide a probability actually for all\n20:29 of the tokens within the lm's knowledge base or what the LM has been trained on\n20:35 so what we do when we set temperature of zero is we say you are going to give us\n20:41 the token with highest probability according to you okay whereas when we\n20:47 set a temperature of 0.9 what we're saying is okay there's actually an increased probability of you giving us a\n20:55 token that according to your generated output is not the token with the highest\n21:01 probability according to the lmm but what that tends to do is give us more sort of creative outputs so that's what\n21:08 the temperature does so we are creating a normal llm and then a more creative\n21:13 llm with this so what are we going to be building we're going to be taking a\n21:19 article draft so like a draft article uh from the aelio learning page and we're\n21:27 going to be using line chain to generate various sces that we might um find helpful as we're you know we have this\n21:34 article draft and we're editing it and just kind of like finalizing it so what are those going to be you can see them\n21:40 here we have the title for the article the description and SEO friendly\n21:45 description specifically third one we're going to be getting the LM to Providers\n21:50 advice on existing paragraph and essentially writing a new paragraph for us from the existing paragraph and what\n21:57 it's going to do this is the structured output part is going to write a new version of that paragraph for us and\n22:03 it's going to give us advice on where we can improve our writing then we're going to generate a thumbnail hero image for\n22:10 our article so nice image that you would put at the top so here we're just going\n22:16 to input our article you can you can put something else in here if you like essentially this is just a big article\n22:24 that's written a little while back on agents and and now we can go ahead and\n22:29 start preparing out our prompts which are essentially the instructions for our llm so line chain comes with a lot of\n22:38 different uh like utilities for prompts and we're going to dive into them in a lot more detail but I do want to just\n22:44 give you uh the Essentials now just so you can understand what we're looking at at least conceptually so prompts for\n22:51 chat agents are at a minimum broken up into three components those are the\n22:57 system pront this provides instructions to our LM on how it should behave what its objective is and how it should go\n23:03 about achieving that objective generally system prompts are going to be a bit\n23:08 longer than what we have here depending on the use case then we have our user prompts so these are user written\n23:15 messages usually sometimes we might want to pre-populate those if we want to encourage a particular type of um\n23:22 conversational patterns from our agent but for the most part yes these are going to be using generated then we have\n23:29 our AI promps so these are of course AI generated and again in some cases we\n23:36 might want to generate those ourselves beforehand or within a conversation if we have a particular reason for doing so\n23:43 but for the most part you can assume that these are actually user and AI generated now the line chain provides us\n23:51 with templates for each one of these prompt types let's go ahead and have a\n23:57 look at what these look like within line chain so to begin we are looking at this\n24:03 one so we have our system message prom template and human message which the the\n24:09 user that we saw before so we have these two system prom keeping it quite simple here you are AI system that helps\n24:16 generate article titles right so so our first component where we want to generate is article title so we're\n24:22 telling the AI that's what we want it to do and then here right so here we're\n24:29 actually providing kind of like a template for a user input\n24:36 so yes as I mentioned user input can be\n24:41 um it can be fully generated by user it might be kind of not generated by user\n24:47 it might be setting up a conversation beforehand which a user would later use or in this scenario we're actually\n24:54 creating a template and the what the user will providers will actually just be inserted\n25:00 here inside article and that's why we have this import variables so what this\n25:07 is going to do is okay we have all of these instructions around here they're all going to be provided to openai as if\n25:14 it is the user saying this but it will actually just be this here that user\n25:20 will be providing okay and we might want to also format this a little nicer it kind of depends this will work as it is\n25:27 but we can also put you know something like this to make it a little bit clearer to the llm okay what is the\n25:34 article where the prompts so we have that and you can see in this scenario\n25:41 there's not that much difference between what the system prompt and user prom is doing and this is It's a particular scenario it varies when you get into the\n25:49 more conversational stuff as we will do later uh you'll see that the user prompt\n25:54 is generally more fully user generated or mostly user generated and much of\n26:01 these types of instructions we might actually be putting into the system prompt it varies and we'll see\n26:07 throughout of course many different ways of using these different types of PRS in various different\n26:13 places then you'll see here so I just want to show you how this is working we\n26:19 can use this format method on our user prompt here to actually insert something\n26:25 within the uh article input here so we're going to go us prompt format and\n26:31 then we pass in something for article okay and we can also maybe format this a little nicer but I'll just show you this\n26:37 for now so we have our human message and then inside the content this is the the text that we had right you can see that\n26:43 we have all this right and this is what we wrote before we wrote all this except from this part we didn't write this\n26:50 instead of this we had article right so let's format this a little nicer so we\n26:57 can see okay so this is exactly what we wrote up here exactly the same except\n27:02 from now we have test string instead of article so later when we insert our\n27:07 article it's going to go inside there allly doing it's like it's an it's an F string in Python okay and this is again\n27:15 this is one of those things where people might complain about Lang chain you know this sort of thing can be you it seems\n27:21 excessive because you could just do this with an nring but there are as we'll see later particularly when you're streaming\n27:27 just really helpful features that come with using line chains kind of built-in\n27:34 uh prompt templates or at least uh message objects that we'll see so\n27:41 you we need to uh keep that in mind again as soon as you get more complicated line chain can be a bit more\n27:47 useful so chat prom template uh this is basically just going to take what we\n27:53 have here our system promt user prompts you could also include some AI prompts in there and what it's going to do is\n27:59 merge both of those and then when we do format what it's going to do is put both\n28:06 of those together into a chat history okay so let's see what that looks like first uh in a more messy way okay so you\n28:15 can see we have just the content right so it doesn't include the whole you know\n28:21 before we had human message we're not include we're not seeing anything like that here instead we're just seeing the\n28:26 string so now let's switch back to print and we can see that what we have\n28:33 is our system message here it's just prefixed with this system and then we have human and it's prefixed by human\n28:39 and then it continues right so that's that's all it's doing it's just kind of merging those in some sort of chat lug we could also put in like AI messages\n28:46 and they would appear in there as well okay so we have that now that is our\n28:51 prompt template let's put that together with an LM to create what would be in\n28:57 past line ch be called an llm chain uh now we wouldn't necessarily call it an llm chain because we're not using the\n29:03 llm chain abstraction it's not super important if that doesn't make sense we we'll go into it in more detail later\n29:10 particularly in the in the ELO chapter so what this chain will do you\n29:17 think L chain is just chains where're chaining together these multiple components it will perform the STS\n29:24 prompt formatting so that's what I just showed you LM generation so sending our\n29:31 prom to open AI getting a response and getting that output so you can also add\n29:37 another set here if you want to format that in a particular way we're going to be outputting that in a particular\n29:42 format so that we can feed it into the next set more easily but there are also things called output passes which pass\n29:49 your output in a more dynamic or complicated way depending on what you're doing so this is our first look at Elsa\n29:58 don't want us to focus too much on the syntax here because we will be doing that later but I do want you to just\n30:04 understand what is actually happening here and logically what are we writing\n30:11 so all we really need to know right now is we Define our inputs with the first\n30:17 dictionary segment here right so this is a you know our inputs which we have\n30:23 defined already okay so if we come up to our\n30:29 user prompt here we said the input variable is our article right and we might have also added input variables to\n30:34 the system prompt here as well in that case you know let's say we had your AI assistant\n30:42 called name right that helps generate article\n30:47 titles in this scenario we might have an input variables name here right and then\n30:55 what we would have to do down here is we would also have to pass that\n31:01 in right so also we would have article but we would also have name so basically\n31:09 we just need to make sure that in here we're including the variables that we have Define as input variables for our\n31:16 our first prompts okay so we can actually go ahead and let's add that uh so we can see it's in action so we'll\n31:23 run this again and just include that or or reinitialize is our first prompt so\n31:29 we see that and if we just have a look at what that means for this format\n31:35 function here it means we'll also need to pass in a name okay and call it Joe\n31:40 okay so Joe the AI right so you are an AI system called Joe now okay so we have\n31:46 Joe our AI that is going to be fed in through these input variables then we have this pipe operator the pipe\n31:53 operator is basically saying whatever is to the left of the pipe operator which\n31:58 in this case would be this is going to go into whatever is on the right of the pipe operator it's that's simple again\n32:05 we'll we'll dive into this and kind of break it apart in the Elso chapter but for now that's all we need to know so\n32:11 this is going to go into our first prompt that is going to format everything it's going to add the name\n32:17 and the article that we provided into our first prompt then it's going to Output that right going to Output that\n32:23 we have our P operate here so the output of this is going to go into the input of our Next Step it's our creative\n32:30 LM then that is going to generate some tokens it's going to generate our output\n32:36 that output is going to be an AI message and as you saw before if I take this bit\n32:44 out within those message objects we have this content field okay so we are\n32:50 actually going to extract the content field out from our AI message to just\n32:56 get the content and that is what we do here so we get the AI message out from ilm and then we're extracting the\n33:02 content from that AI message object and we're going to passing it into a dictionary that just contains article\n33:07 title like so okay we don't need to do that we can just get the AI message directly I just want to show you how we\n33:15 are using this sort of chain in Elso so once we have set up our chain we then\n33:23 call it or execute it using the invoke method into that we will need to pass in\n33:28 those variables so we have our article already but we also gave our AI a name now so let's add that and we'll ruin\n33:36 this okay so Joe has generated us a article title unlocking the future the\n33:43 rise of neuros symbolic AI agents cool much better name than what I gave the article which was AI agents are neuros\n33:52 symbolic systems no I don't think I did too bad okay so we have that\n33:58 now let's continue and what we're going to be doing is building more of these\n34:03 types of LM chain pipelines where we're feeding in some prompts we're generating\n34:10 something getting something and and doing something with it so as mentioned we have the title we're\n34:16 now moving on to the description so to generate description so we have our human message prompt template so this is\n34:22 actually going to go into a similar format as before we also\n34:28 want to redefine this because I think I'm using the same system message there\n34:33 so let's let's go ahead and do modify that or what we could also do is let's\n34:40 just remove the name now because I've showing that so what we could do is\n34:46 you're an AI system that helps build good articles right build good\n34:53 articles and we could just use this as our you know generic system prompt now\n34:58 so let's say that's our new system prompt now we have our user prompt your task creating description for the\n35:04 article the article is here fure examine article here is the article title okay so we need the article title now as well\n35:10 in our input variables and then we're going to Output an AO friendly article description and we're just saying you\n35:17 just to be certain here do not output anything other than the description so you know sometimes an LM might say hey\n35:23 look this is what I've generated for you the reason I think this is good is because so on and so on so on right if you're programmatically taking some\n35:31 output from an LM you don't want all of that fluff around what the LM is generated you just want exactly what\n35:37 you've asked it for okay because otherwise you need to pass out with code and it can get messy and also just far\n35:44 less reliable so we're just saying do iput anything else then we're putting all these together so system prompt and\n35:50 the second user prompt this one here putting those together into a new chat\n35:55 prompt template and then we're going to to feed all that in to another LOL chain\n36:00 as we have here to well to generate our our description so let's go ahead we\n36:06 invoke that as before we're just make sure we add in the article title that we got from before and let's see what we\n36:13 get okay so we have this explore the transformative potential of neuros symbolic Ai ageny and a little bit long\n36:21 to be honest but yeah you can see what it's doing here right and of course we could then go in we see this is kind of\n36:26 too long right a yeah SEO friendly description not not really so we can\n36:33 modify this output the SEO friendly description um make sure we don't exceed\n36:41 let me put that on a new line make sure we don't exceed say 200 characters or\n36:46 maybe it's even less to se I don't I don't have a clue I'll just say 120 characters I do not outly anything other\n36:53 than the description right so we could just you know go back modify our prompting see what that generates again\n36:59 okay so much shorter probably too short now but that's fine cool so we have that we have a summary process that and\n37:06 that's now you know in this dictionary form that we have here cool now the\n37:12 third step we want to consume that first article variable with our full article\n37:18 and we're going to generate a few different output Fields so for this\n37:24 we're going to be using the structured output feature so let's scroll down\n37:29 we'll see what that is what that looks like so structured output is essentially\n37:36 we're forcing their lmic like it has to Output a dictionary with these you know\n37:41 particular Fields okay and we can modify this quite a bit but in this scenario\n37:47 what I want to do is I want there to be an original paragraph right so I just want it to regenerate the original\n37:53 paragraph cuz I'm lazy and I don't want to extract it out then I want to get the new edited paragraph This is the LM\n38:00 generated improved paragraph and then we want to get some feedback because we we don't want to just automate ourselves we\n38:07 want to augment ourselves and get better with AI rather than just being like I\n38:13 you do you do this so that's what we do here and you can see that here we're using this pantic object and what pantic\n38:21 allows us to do is Define these particular fields and it also allows us to assign these descriptions to a field\n38:27 and and line chain is actually going to go ahead read all of this right even reads so for example we could put\n38:33 integer here and we could actually get a numeric score for our paragraph right we\n38:39 can try that right so let's uh let's let's just try that quickly I'll show you so numeric numeric score in fact\n38:48 let's even just ignore let's not put anything here so I'm going to put constructive feedback on the original\n38:53 paragraph but I just put into here so let's see what happens okay so we have that and what I'm going to do is I'm\n38:59 going to get our creative llm I'm going to use this with structured output method and that's actually going to\n39:05 modify that llm class create a new llm class that forces that llm to use this\n39:10 structure for the output right so passing in paragraph into here using this we're creating this new structure\n39:17 LM so let's run that and see what happens okay so we're going to modify\n39:23 our chain accordingly maybe what I can do let's also just remove this bit for now\n39:30 so we can just see what the strictured llm outputs directly and let's\n39:36 see okay so now you can see that we actually have that paragraph object\n39:42 right the one we defined up here which is kind of cool and then in there we have the original paragraph right so\n39:48 this is where this is coming from I definitely remember writing something\n39:53 that looks a lot like that so I think that is correct we have the edited par so this is okay what thinks it's better\n40:00 and then interestingly the feedback is three which is weird right because uh\n40:06 here we said the constructive feedback on the original paragraph but what we're doing when we use this with structured\n40:12 output but what Lang chain is doing is is essentially performing a tool core to open Ai and what a tool core can do is\n40:20 force a particular structure in the output of an LM so when we say feedback\n40:26 has to be an integer no matter what we put here it's going to give us an integer because how do you provide\n40:31 constructive feedback with an integer it doesn't really make sense but because we've set that limitation that\n40:38 restriction here that is what it does it just gives us the uh a numeric value so\n40:44 I'm going to shift that to string and then let's rerun this see what we get okay we should now see that we actually\n40:50 do get constructive feedback all right so yeah you can see it's quite quite long so the original paragraph\n40:57 effectively communicates the limitations of neuro AI systems in performing certain tests however it could benefit\n41:03 from slightly improved Clarity and conciseness for example the phrase was becoming clear can be made more direct\n41:09 by changing it to became evident yeah true thank you very much so yeah now we\n41:17 actually get that that feedback which is pretty nice now let's add in this final\n41:22 setep to our chain okay and it's just going to pull out our paragraph object here and\n41:29 extracting into a dictionary we don't necessarily need to do this honestly I actually kind of prefer it within this paragraph object but just so we can see\n41:38 how we would pass things on the other side of the chain okay so now we can see\n41:44 we've extracted that out cool so we have all of that interesting feedback again\n41:52 but let's leave it there for the text part of this now let's have a look at at\n41:57 the sort of multimodal features that we can work with so this is you know maybe one of those things that kind of seems a\n42:04 bit more abstracted a little bit complicated where it maybe could be improved but you know we're not going to\n42:10 really be focusing too much on the M time modal stuff sub be focusing on language but I did want to just show you\n42:16 very quickly so we want this article to look better okay we want to\n42:22 generate a prompt based on the article it's self that we can then pass to DAR\n42:30 the the image generation model from open AI that will then generate an image like like a thumbnail image for us okay so\n42:39 the first step of that is we're actually going to get an LM to generate that right so we have our prompt that we're\n42:44 going to use for that so I'm say generate a prompt with less than 500 characters to uh generate an image based\n42:52 on the following article okay so that's our prompt yeah super simple uh using\n42:57 the generic prompt template here you can use that you can use user uh prompt template it's up to you this is just\n43:03 like the generic prom template then what we're going to be doing is based on what\n43:10 this outputs we're then going to feed that in to this generate and display image function via the image prompt\n43:17 parameter that is going to use the darly API rapper from line chain it's going to\n43:23 run that image prompt and we're going to get a a eurl out from that essentially and then we're going to read that using\n43:29 SK image here right so we're just going to read that image URL going to get the image data and then we're just going to\n43:35 display it okay so pretty straightforward now again this is a lell\n43:43 thing here that we're doing we have this runable Lambda thing when we're running\n43:49 functions within lell we need to wrap them within this runable Lambda I you\n43:54 know I don't want to go too much into what this is doing here because we do cover in the L cell chapter but it's\n44:01 just you know all you really need to know is we have a custom function wrap in runable Lambda and then what we get\n44:07 from that we can use within this here right the the L Sal syntax so what are\n44:14 we doing here let's figure this out we are taking our original that image prom that we defined just up here right input\n44:21 variable to that is article okay we have our article d being input here feeding\n44:28 that into our prompt from there we get our message that we then feed into our\n44:33 llm from the LM it's going to generate us a like an image prompt like a prompt\n44:39 for generating our image for this article we can even Let's uh let's print that out so that we can see what it\n44:46 generates because I'm also kind of curious okay so we'll just run that and\n44:52 then let's see it will feed in that content into our room reable which is\n44:58 basically this function here and we'll see what it generates okay don't expect\n45:03 anything amazing from darly it's not it's not the best to be honest but we at\n45:08 least we see how to use it okay so we can see the prom that was used here\n45:14 create an image that visually represents the concept of neuros symbolic agents depict a futuristic interface where\n45:20 large D interacts with traditional code symbolizing integration of oh my gosh uh\n45:27 something computation include elements like a brain to represent neur networks gears or circuits or symbolic logic and\n45:35 web of connections illustrating vast use cases of AI agents oh my gosh look at\n45:41 all that big prompt then we get this so you know dar's interesting I would say we\n45:48 could even take this let's just see what that comes up with in something like mid\n45:53 Journey you can see these way cooler images that we get from just another image generation model far better but\n46:00 pretty cool honestly so in terms of Generation image the phrasing the The\n46:05 Prompt itself is actually pretty good the image you know could be better but\n46:11 that's it right so with all of that we've seen a little introduction to what\n46:16 we might build in with lighing chain so that's it for our introduction chapter as I mentioned we don't want to go too\n46:22 much into what each of these things is doing just really want to focus on okay\n46:29 this is kind of how we're building something with line chain this is the overall flow uh but we don't really want\n46:37 to be focusing too much on okay what exactly lell is doing or what exactly uh\n46:42 you know this prompt thing is that we're setting up we're going to be focusing\n46:48 much more on all of those things and much more in the upcoming chapters so\n46:53 for now we've just seen a little bit of what we can build before diving in in more detail okay so now we're going to\n47:00 take a look at AI observability using lsmith now lsmith is another piece of\n47:08 the broader Lang chain ecosystem its focus is on allowing us to see what our\n47:15 llms agents Etc are actually doing and it's something that we would definitely recommend using if you are going to be\n47:22 using line chain Lang graph now let's take a look at how we would set L Smith up which is incredibly simple so I'm\n47:29 going to open this in collab and I'm just going to install the prerequisites here you'll see these are all the same\n47:36 as before but we now have the Lin Smith Library here as well now we are going to be using Lin Smith throughout the course\n47:43 so in all the following chapters we're going to be importing limith and that will be tracking everything we're doing\n47:49 but you don't need Lin Smith to go through the course it's an an optional dependency but as mentioned I would\n47:55 recommend it so we'll come down to here and first thing that we will need is the line chain API key now we do need an API\n48:03 key but that does come with a reasonable free tier so we can see here they have\n48:09 each of the plans and this is the one that we are by default on so it's free\n48:16 for one user up to 5,000 traces per month if you're building out an\n48:21 application I think it's fairly easy to go beyond that but it really depends on what you're building so it's a good\n48:28 place to start with and then of course you can upgrade as required so we would go to smith. L\n48:36 chain.com and you can see here that this will log me in automatically I have all\n48:41 of these tracing projects these are all from me running the various chapters of the course yours if you do use l Smith\n48:48 throughout course your L Smith dashboard will end up looking something like this now what we need is an API key so we go\n48:56 over to settings we have API keys and we're just going to create an API key because we're\n49:03 just going through some personal learning right now I would go with personal access token we can give a name or description if you want okay and\n49:10 we'll just copy that and then we come over to our notebook and we enter our API key there and that is all we\n49:16 actually need to do that's absolutely everything supposed the one thing to be aware of is that you should set your L\n49:22 chain project to whatever project you're working within so of course Within within the course we have individual\n49:28 project names for each chapter but for your own projects of course you should make sure this is something that you\n49:34 recognize and is useful to you so L Smith actually does a lot without needing to do anything so we can\n49:40 actually go through let's just initialize our LM and start invoking it and seeing what L Smith returns to us so\n49:48 we'll need our open API key enter it here and then let's just invoke\n49:55 hello okay so nothing has changed on this end right so us running the code there's nothing different here however\n50:02 now if we go to Lang Smith I'm going to go back to my dashboard okay and you can\n50:08 see that the the order of these projects just changed a little bit and that's because the most recently used project I\n50:15 this one at the top Lang chain course Lang Smith openai which is the current chapter we're in that was just triggered\n50:21 so I can go into here and I can see oh look at this so we actually have something in the Lang Smith UI and we\n50:28 didn't all we did was enter our L train apid that's all we did and we set some environment variables and that's it so\n50:34 we can actually click through to this and it will give us more information so you can see what was the\n50:39 input what was the output and some other metadata here you see you know there's not that much in\n50:46 here however when we do the same for agents we'll get a lot more information\n50:52 so I can even show you a quick example from the future chapters if we come through to agents\n51:00 intro here for example and we just take a look at one of\n51:05 these okay so we have this input and output but then on the left here we get all this information and the reason we\n51:12 get all this information because agents are they're performing multiple LM calls etc etc so there's a lot more going on\n51:20 so we can see okay what was the first LM call and then we get these tool use traces we get another LM another rmm\n51:27 call another tool use and another LM call so you can see all this information which is incredibly useful and\n51:33 Incredibly easy to do because all I did when setting this up in that agent chapter was simply set the API key and\n51:40 the environment variables as we have done just now so you get a lot out of\n51:46 very little effort with Lang Smith which is great so let's return to our Lang Smith project here and let's invoke some\n51:53 more now I've already shown you you know we're going to see a lot of things just by default but we can also add other\n52:00 things that Lang Smith wouldn't typically Trace so to do that we will\n52:05 just import a traceable decorator from Lang Smith and then let's make these\n52:12 just random functions traceable within limith okay so we'll run those we have\n52:19 three here so we're going to generate a random number we're going to modify how\n52:24 long a function takes and also generate a random number and then in this one\n52:30 we're going to either return this no error or we're going to raise an error\n52:36 so we're going to see how limith handles these different scenarios so let's just\n52:41 iterate through and run those a few times so we're just going to run each one of those 10\n52:47 times okay so let's see what happens so they're running let's go over to our Lin\n52:54 sth UI and see what is happening over here so we can see that everything is updating we adding that information\n52:59 through and we can see if we go into a couple of these we can see a little more information so have the input and the\n53:05 output took three seconds see random error here in this\n53:12 scenario random error passed without any issues let me just refresh the page\n53:18 quickly okay so now we have the rest of that information and we can see that occasionally if there is an error from\n53:24 our random error function it is is signified with this and we can see the\n53:30 traceback as well that was returned there which is useful okay so we can see if an error has been raised we have to\n53:35 see what that error is we can see the various latencies of these functions so\n53:42 you can see that varying throughout here we see all the inputs to each one\n53:48 of our functions and then of course the outputs so we can see a lot in there\n53:53 which is pretty good now another thing that we can do do is we can actually filter so if we come to here we can add\n54:01 a filter let's filter for errors that would be value error and\n54:06 then we just get all of the cases where one of our functions has returned or\n54:12 raise an error or value error specifically okay so that's useful and then yeah there's there's various other\n54:19 filters that we can add there so we could add a name for example if we want\n54:24 to look for the generate string delay function only we could also do\n54:30 that okay and then we can see the varying latencies of that function as well cool so we have\n54:37 that now one final thing that we might want to do is maybe we want to make\n54:43 those function names a bit more descriptive or easy to search for for example and we can do that by saying the\n54:50 name of the traceable decorator like so so let's run that we'll run this a few\n54:56 times and then let's jump over to limith again going to limith project okay and you can\n55:02 see those coming through as well so then we could also search for those based on that new name so what was it chitchat\n55:09 maker like so and then we can see all that information being streamed through\n55:16 to limith so that is our introduction to limith there is really not all that much\n55:23 to go through here it's very easy to sell up and as we scen it gives us a lot of observability into what we are building\n55:31 and we will be using this throughout the course we don't rely on it too much it's a completely optional dependency so you\n55:37 don't want to use l space you don't need to but it's there and I would recommend doing so so that's it for this chapter\n55:43 we'll move on to the next one now we're going to move on to the chapter on\n55:48 prompts in Lang chain now prompts they seem like a simple concept and they are\n55:53 a simple concept but there's actually quite a lot to them when you start diving into them and they truly have\n56:00 been a very fundamental part of what has propelled us forwards from pre llm times\n56:07 to the current llm times you have to think until llms became widespread the\n56:14 way to fine-tune a AI model or ml model\n56:19 back then was to get loads of data for your particular use case spend a load of\n56:26 training your specific Transformer or part of the Transformer to essentially\n56:31 adapt it for that particular task that could take a long time depending on the\n56:38 the task it could take you you know months or in some times if it was a\n56:43 simpler task it might take probably days potentially weeks now the interesting\n56:48 thing with L LMS is that rather than needing to go through this whole\n56:54 fine-tuning process to to modify a model for one task over\n57:00 another task rather than doing that we just prompt it differently we literally tell the model hey I want you to do this\n57:07 in this particular way and that is a you know that's a paradigm shift in what you're doing it's so much faster it's\n57:14 going to take you you know a couple of minutes rather than days weeks or months and LMS are incredibly powerful when it\n57:22 comes to just generalizing to you know across these many different tasks so prompts which control those instructions\n57:31 are a fundamental part of that now line chain naturally has many functionalities\n57:37 around prompts and we can build very Dynamic prompting pipelines that modify\n57:43 the structure and content of what we're actually feeding into our llm depending on different variables different inputs\n57:49 and we'll see that in this chapter so we're going to work through prompting\n57:54 within the scope of of a rag example so let's start by just dissecting the\n58:01 various parts of a prompt that we might expect to see for a use case like rag so\n58:07 our typical prompt for rag or retrieval augmented generation will include rules\n58:14 for the LM and this is this you will see in most prompts if not all this part of\n58:22 the promt sets up the behavior of the llm that is how it should be responding\n58:28 to user queries what sort of Personality it should be taking on what it should be\n58:33 focusing on when it is responding any particular rules or boundaries that we want to set and really what we're trying\n58:41 to do here is just to Simply provide as much information as possible to the llm\n58:48 about well what we're doing we just want to give the llm context as to the the\n58:56 place that it finds itself in because an LM has no idea where it is it's just it's a it takes in some information and\n59:04 spits out information if the only information it receives is from the user you know user query it has you know\n59:10 doesn't know the context what is the application that it is within what is its objective what is its aim what are\n59:17 the boundaries all of this we need to just assume the llm has absolutely no\n59:24 idea about because it it truly does not so as much context as we can provide but\n59:32 it's important that we don't overdo it it's uh we see this all the time people will over prompt an llm you want to be\n59:39 concise you don't want fluff and in general every single part of your prompt\n59:45 the more concise and less fluffy you can make it the better now those rules or\n59:50 instructions are typically in the system prompt of your llm now the second one is\n59:55 context which is rag specific the context refers to some sort of external\n1:00:00 information that you are feeding into your llm we may have received this information from like web search\n1:00:08 database query or quite often in this case of rag it's a vector database this\n1:00:15 external information that we provide is essentially the r retrieval augmentation\n1:00:22 of rag we are augmenting the knowledge of our llm which the the knowledge of\n1:00:29 our LM is contained within the llm model weights we're augmenting that knowledge with some external knowledge that's what\n1:00:36 we're doing here now for chat LMS this context is typically placed within a\n1:00:44 conversational context within the user or assistant messages uh and with\n1:00:52 more recent models it can also be placed within uh tool messages as well then we\n1:00:58 have the question this pretty straightforward this is the query from the user this is or is this usually a\n1:01:06 user message of course there might be some additional formatting around this you might add a\n1:01:12 little bit of extra context or you might add some additional instructions if you\n1:01:17 find that you L them sometimes VAR off the rules that you've set within the system prompt you might you know append\n1:01:24 or prefix something something here but for the most part it's probably just going to be the user's input and finally\n1:01:30 uh so these are all the inputs for our prompt here is going to be the output\n1:01:36 that we get so the answer from the assistant again I mean that's not even specific to rag it's just what you would\n1:01:43 expect in a in a chat llm or any LM and of course that would be an assistant\n1:01:49 message so putting all of that together in an actual prompt you can see everything we have here so we have the\n1:01:55 uh rules for our prompt here the instructions we're just saying okay answer the question based on the context\n1:02:01 below if you cannot answer the question using the information answer with I don't know then we have some context\n1:02:08 here okay in this scenario that context that we're feeding in here because it's\n1:02:14 the first message we might putting that into the system prompt but that may also be turned around okay if you if you for\n1:02:21 example have an agent you might have your question up here before the context\n1:02:27 and then that would be coming from a user message and then this context would follow the question and be recognized as\n1:02:34 a tool message it would be fed in that way as well kind of depends on on what\n1:02:40 sort of structure you're going for there but you can do either you can feed it into the system message if it's less\n1:02:45 conversational whereas if it's more conversational you might feed it in as a tool message okay and then we have a\n1:02:51 user query which is here and then we'd have the AI answer okay and obviously\n1:02:56 that would be generated here okay so let's switch across to the code we're in\n1:03:01 the L chain course repo notebooks 03 prompts and I'm just going to open this in collab okay let scroll down and we'll\n1:03:09 start just by installing the prerequisites okay so we just have the various libraries again as I mentioned\n1:03:16 before Lang Smith is optional you don't need to install it but if you would like to see your tracers and everything in\n1:03:22 Lang Smith then I would recommend doing that and if you are using L Smith you will need to enter your API key here\n1:03:28 again if you're not using Lang Smith you don't need to enter anything here you just skip that cell okay cool and let's\n1:03:36 jump into the basic prompting then so we're going to start with this prompt\n1:03:42 answer used query based on the question below so we're just structuring what we just saw uh in\n1:03:47 code and we're going to be using the chat problem template because generally\n1:03:53 speaking we're using chat llms in most most cases nowadays so we have our chat\n1:04:00 promp template and that is going to contain a list of messages system message to begin with which is just\n1:04:06 going to contain this and we're feeding in the the context within that there and\n1:04:12 we have our user query here okay so we'll run this and if we take a look\n1:04:21 uh here we haven't specified what our input variables are okay but we can see\n1:04:28 that we have query and we have context up here right so we can see that okay\n1:04:33 these are the input variables we just haven't explicitly defined them here so\n1:04:39 let's just confirm with this that line chain did pick those up and we can see that it did\n1:04:45 so it has context and query as our input variables for the prompt template that\n1:04:50 we just defined okay so we can also see the structure of our temp plates let's\n1:04:57 have a look okay so we can see that within messages here we have a system message\n1:05:03 prompt template the way that we Define this you can see here that we have from messages and this will consume various\n1:05:10 uh different structures so you can see here that it has a from messages it is a\n1:05:18 sequence of message like representation so we could pass in a system prompt\n1:05:24 template object and then a user prompt template object or we can just use a\n1:05:31 tupol like this and this actually defines okay this system this is a user and you could also do assistant or tool\n1:05:39 messages and stuff here as well using the same structure and then we can look in here\n1:05:44 and of course that is being translated into the system message prom template and human message prompt template okay\n1:05:53 we have our input variables in there and there and we have the template too okay\n1:05:59 now let's uh continue we'll see here what I what I just said so we're\n1:06:05 importing our system message prompt template and human message prompt template and you can see we're using the\n1:06:10 same from messages method here right and you can see it's so sequence of message\n1:06:16 like representation it's just you know what that actually means it can vary right so\n1:06:22 here we have system message prom template from template here from template query you know there's various\n1:06:28 ways that you might want to do this it just depends on how explicit you want to\n1:06:33 be generally speaking I think for myself I would prefer that we\n1:06:40 stick with the objects themselves and be explicit but it is definitely a little\n1:06:45 harder to pass when you're when you're reading this so I understand why you might also prefer this it's definitely\n1:06:52 cleaner and it is it does look simpler so it just depends I suppose on\n1:07:00 preference okay so we can see again that this is exactly the same okay with chat\n1:07:06 prom template and it contains this and this okay you probably want to see the\n1:07:12 exact output so it was messages okay exactly the same as what I\n1:07:19 output before cool so we have all that let's see how we would invoke our l L with\n1:07:26 these we're going to be using 40 mini again we do need our open API key so\n1:07:32 enter that and we'll just initialize our LM we\n1:07:38 are going with a low temperature here so less Randomness or less\n1:07:43 creativity and you in in many cases this is actually what I would be doing the\n1:07:48 reason in this scenario that we're going with a low temperature is we're doing\n1:07:54 Rag and if you you remember before if we scroll up a little bit here our template says answer the user's query based on\n1:08:00 the context below if you cannot answer the question using the provided answer information answer with I don't know\n1:08:08 right so just from reading that we know that we want our llm to be as truthful\n1:08:15 and accurate as possible so a more creative llm is going to struggle with\n1:08:20 that and is more likely to hallucinate whereas a low creativity or\n1:08:26 low temperature llm will probably stick with the rules a little better so again it depends on your use case you know if\n1:08:33 you're creative writing you might want to go with a higher temperature there but for things like rag where the\n1:08:40 information being output should be accurate and truthful it's important I\n1:08:46 think that we keep temperature low okay I talk about that a little bit here so\n1:08:52 um of course lower temperature of zero makes the LM output more deterministic which in theory should lead to less\n1:08:59 hallucination okay so we're going to go with L cell again here this is for those\n1:09:04 of you that use LINE chain pass this is equivalent to an llm chain object so our\n1:09:09 prompt template is being fed into our LM okay and from now we have this pipeline\n1:09:18 now let's see how we would use that pipeline so going to get some uh create\n1:09:23 some context here so so this just some Contex around orelio\n1:09:30 AI mention that we built sematic routers SM junkers there AI\n1:09:37 platform and development services we mentioned I think we\n1:09:42 specifically outline this later on in the example so the Align chain experts little piece of information now most LMS\n1:09:49 would have not been trained on the recent internet so the fact that this\n1:09:54 came in September M 2024 is relatively recent so a lot of LMS out of the box\n1:10:00 you wouldn't expect them to know that so that is a good little bit of information\n1:10:05 to ask about so we invoke we have our query so what do we do and we have that\n1:10:11 context okay so we're feeding that into that pipeline that we defined here all right so when we invoke that that is\n1:10:18 automatically going to take query and context and actually feed it into our prompt template okay\n1:10:26 if we want to we can also be a little more explicit so you you will probably\n1:10:32 see me doing this uh throughout the course because I do like to be explicit\n1:10:37 with everything to be honest and you'll probably see me doing\n1:10:47 this okay and this is doing the same thing or you'll see it will in a moment\n1:10:54 this is doing the exact same thing again this is just a outo thing\n1:10:59 so all I'm doing in this scenario is I'm saying okay take from the dictionary\n1:11:08 query and then also take from that input dictionary the context\n1:11:16 key okay so this is doing the exact same thing uh the reason that we might want\n1:11:22 to write this is mainly for clarity to be honest just to explicit say okay\n1:11:27 these are the inputs because otherwise we don't really have them in the code other than within our original prompts\n1:11:34 up here which is not super clear so I think it's usually a good\n1:11:40 idea to just be more expc with these things and of course if you decide you're going to modify things a little\n1:11:45 bit let's say you modify this to input down the line you can still feed in the same input here you're just you know\n1:11:52 mapping it between different Keys essentially or if you would like to just modify that I don't know you need to\n1:11:58 locase it on the way in or something you can do so you have that I'll just redefine\n1:12:08 actually and we'll invoke again okay we see that it does the exact\n1:12:14 same thing okay so R AI so this is the AI message just generated by the llm\n1:12:21 okay expertise in building AI agents several open source framework router AI\n1:12:29 platform okay right so they have everything there other than the line\n1:12:34 train experts thing it didn't mention that but we will yeah we'll test it later on that okay so on to Future\n1:12:41 prompting this is a specific prompting technique now many sort of State ofthe art or also to LMS are very good at\n1:12:49 instruction following so you'll find that fuch shop prompting is less common\n1:12:54 now than it used to be at least for the sort of bigger more safy art models but\n1:13:01 when you start using smaller models not really what we can use here but let's say you're using a open source model\n1:13:08 like llama 3 or llama 2 which is much smaller you will probably need to\n1:13:15 consider things like f shot prompting although that being said with the open AI models you're at least the current\n1:13:23 open AI models this is not so important nonetheless it can be useful so the idea\n1:13:29 behind fuchia prompting is that you are providing a few examples to your llm of\n1:13:35 how it should behave before you are actually going\n1:13:41 into the main part of the conversation so let's see how that would look so we\n1:13:46 create an example prom so we have our human in AI so human input AI response\n1:13:52 so we're basically saying up okay this with this type of input you should provide this type of output that's what\n1:13:58 we're doing here and we're just going to provide some examples okay so we have our input here's query one here is the\n1:14:06 answer one right this is just I just want to show you how it works this is not what we'd actually feed into our LM\n1:14:13 then with both these examples and our example prompt we'd feed both of these\n1:14:18 into uh line chains few shot chat message prompt template okay and\n1:14:25 well you'll see what we get out of it okay so we basically get it formats everything and structures everything for\n1:14:31 us okay and using this of course it depends\n1:14:38 on let's say you see that your user is talking about a particular topic and you\n1:14:44 would like to guide your llm to talk about that particular topic and a particular way right so you could\n1:14:51 identify that the user is talking about that topic either like a keyword match or a semantic similarity match and based\n1:14:58 on that you might want to modify these examples that you feed into your few sh\n1:15:03 chat message promp template and then obviously for that could be what you do for topic a for topic B you might have\n1:15:10 another set of examples that you feed into this all all this time your example prompt is remaining the same but you're\n1:15:16 you're just modifying the examples that are going in so that they're more relevant to whatever it is your user is\n1:15:21 actually talking about so that can be useful now let's see an example of that so when we are using a tiny LM It's\n1:15:29 ability would be limited although I think we are we're probably fine here we're going to say answer the US query\n1:15:36 based on the context below always answering mark down format you know being very specific the self system\n1:15:42 prompt okay that's nice but what we've kind of said here is okay always\n1:15:48 answering mod down for I did do that but when doing so please provide headers\n1:15:54 short summary and follow bullet points then conclude okay so you see this here\n1:16:01 okay so we get this overview of already you have this and this it's actually quite good but if we come down here what\n1:16:08 I specifically want is to always follow this structure right so we have the\n1:16:14 double header for the topic summary header a couple of bullet points and\n1:16:21 then I always want to follow this pattern where it's like to conclude always it's always bold you know I want to be very specific on\n1:16:27 what I want and to be you know fully honest with GT40 mini you can actually\n1:16:34 just prompt most of this in but for the sake of the example we're going to provide a few shot um examples in our\n1:16:42 few shot prompt examples instead to get this so we're going to provide one example here second example here and\n1:16:50 you'll see we're just following that same pattern we're just setting up the pattern that the llm should use so we're\n1:16:57 going to set that up here we have our main header a little summary some\n1:17:03 subheaders bullet points subheader bullet points subheader bullet points to conclude so on and so on same with this\n1:17:09 one here okay and let's see what we\n1:17:15 got okay so this is the structure of our new F shop prompt template you can see\n1:17:24 what all this looks like let's come down and we're going to do we're basically going to insert that directly into our\n1:17:31 chat prompt template so we have for messages system\n1:17:36 prompt user prompt and then we have in there these so let me actually show you\n1:17:44 very quickly right so we just have um this few shot chat to message prompt template\n1:17:50 which will be fed into the middle here run that and then feed all this back into our pipeline okay and this will you\n1:17:57 know modify the structure so that we have that bold to conclude at the end here okay we can see nicely here so we\n1:18:04 get a bit more of that exact structure that we were getting again with GT40\n1:18:10 models and many other opening air models you don't really need to do this but you will see it in other examples we do have\n1:18:17 an example of this where we're using a llama and we're using I think llama 2 if\n1:18:23 I'm not wrong and you can see that adding this fuse shot promp template is\n1:18:29 actually a very good way of getting those smaller less capable models to follow your instructions so this is RAR\n1:18:37 when you're working those smaller lenss this can be super useful but even for so models like\n1:18:42 gp40 if you do find that you're struggling with the prompting it's just not quite following exactly what you\n1:18:48 want it to do this is a very good technique for actually getting it to\n1:18:53 follow a very straight structure or behavior okay so moving on we have Chain of Thought prompting so this is a more\n1:19:02 common prompting technique that encourages the LM to think through its\n1:19:08 reasoning or its thoughts step by step so it's Chain of Thought the idea behind\n1:19:13 this is that okay in math class when you're a kid the teachers would always\n1:19:19 push you to put down your your working out right and there was a more reasons\n1:19:25 for that one of them is to get you to think because they they know in a lot of cases actually you know you're a kid and you're in Aran you don't really care\n1:19:31 about this test and the you know they're just trying to get you to slow down a\n1:19:37 little bit and actually put down your reasoning and that kind of forc you to think oh actually I'm skipping a little\n1:19:43 bit in my head because I'm trying to just do everything up here if I write it down all of a sudden it's like oh\n1:19:48 actually I yeah I need to actually do that slightly differently you you realize okay you're probably rushing now\n1:19:55 I'm not saying an LM is rushing but it's a similar effect by an LM writing everything down they tend to actually\n1:20:01 get things right more frequently and at the same time also similar to when\n1:20:07 you're a child and a teacher is reviewing your exam work by having the LM write down its reasoning you as a as\n1:20:15 a human or engineer you can see where the llm went wrong if it did go wrong\n1:20:21 which can be very useful when you're trying to diagnose problems so with train of thought we should see uh less\n1:20:27 hallucinations and generally better performance now to implement train of thought in line chain there's no\n1:20:32 specific like line chain objects that do that instead it's it's just prompting okay so let's go down and just see how\n1:20:39 we might do that okay so be helpful assistant and answer users question you\n1:20:44 must answer the question directly without any other text or explanation okay so that's our no Chain of Thought\n1:20:51 system problems I will just note here especially with open AI again this is one of those things where you'll see it\n1:20:57 more with the smaller models most LMS are actually trained to use train thought prompting by default so we're\n1:21:03 actually specifically telling it here you must answer the question directly without any other text or explanation\n1:21:09 okay so we're actually kind of reverse prompting it to not use train of thought otherwise by default it actually will\n1:21:16 try and do that because it's been trained to that's how that's how relevant Chain of Thought is okay so I'm\n1:21:22 going to say how many key strokes I need to type in type the numbers from 1 to 500 okay we set up our like llm chain\n1:21:31 Pipeline and we're going to just invoke our query and we'll see what we get\n1:21:36 total number of key strokes needed to type the numbers from one to 500 is\n1:21:43 1,511 uh the actual an as I've written here is 1,392 without chain thought is\n1:21:50 hallucinating okay now let's go ahead and see okay with Chain of Thought apprting what does it do so be helpful\n1:21:57 assistant and answer user question to answer the question you must list systematically and in precise detail all\n1:22:05 sub problems that are needed to be solved to answer the question solve each sub problem individually you have to\n1:22:11 shout at the LM sometimes to get them to listen and in sequence finally use\n1:22:17 everything you've worked through to provide the final answer okay so we're getting it we're forcing it to kind of\n1:22:22 go through the full problem there can remove that not sure why that's there so\n1:22:27 run that again I don't know why we have context there I remove that and let's\n1:22:35 see you can see straight away that's taking a lot longer to generate output\n1:22:40 that's because it's generating so many more tokens so that's just one one drawback of this but let's see what we\n1:22:46 have so to determine how many keystrokes to tie those numbers we is breaking down\n1:22:52 several sub problems so count number of digits from 1 to 9 10 to 99 so so on and\n1:22:58 count the digits in number 500 okay interesting so that's how it's breaking it up some more digits count in the\n1:23:05 previous steps so we go through total digits and we see that's okay nine\n1:23:12 digits for those for here 180 for here\n1:23:17 1,200 and then of course three here so it gets all those sums those digits and\n1:23:25 actually comes to the right answer okay so that that is you that's the difference with with Chain of Thought\n1:23:30 versus without so without it we just get the wrong answer basically guessing with\n1:23:37 chain of thought we get the right answer just by the llm writing down its reasoning and breaking the problem down\n1:23:44 into multiple Parts which is I found that super interesting that it it does that so that's pretty cool now I will\n1:23:53 just see so as I as we mentioned before most llms nowadays are actually training\n1:23:58 to use train of thought prompting by default so let's just see if we don't mention anything right be a helpful\n1:24:03 assistant and answer the users question so we're not telling it not to think through it's reasoning and we're not\n1:24:09 telling it to think through its reasoning let's just see what it does okay so you can see again it's\n1:24:16 actually doing the exact same reasoning\n1:24:21 okay it doesn't it doesn't give us like the sub problems that the start but it is going through and it's breaking\n1:24:27 everything apart okay which is quite interesting and we get the same correct answer so the formatting here is\n1:24:33 slightly different it's probably a little cleaner actually although I think\n1:24:38 uh I don't know I here we get a lot more information so both are fine and in this\n1:24:46 scenario we actually do get the the right answer as well so you can see that that Chain of Thought prompting has\n1:24:52 actually been quite literally trained into the model and you'll see that with\n1:24:58 most well I think all save the-art lenss Okay cool so that is our our chapter on\n1:25:07 prompting again we're focusing very much on a lot of the fundamentals of\n1:25:13 prompting there and of course tying that back to the actual objects and methods\n1:25:19 within langing but for now that's it for prompting and we'll move on to the next ch chapter in this chapter we're going\n1:25:26 to be taking a look at conversational memory in line chain we're going to be\n1:25:31 taking a look at the core like chat memory components that have really been\n1:25:37 in line chain since the start but are essentially no longer in the library and\n1:25:43 we'll be seeing how we actually Implement those historic conversational\n1:25:49 memory Utilities in the new versions of Lang chain so 0.3 now as a pre-warning\n1:25:57 this chapter is fairly long but that is because conversational memory is just\n1:26:02 such a critical part of chatbots and agents conversational memory is what\n1:26:08 allows them to remember previous interactions and without it our chat\n1:26:13 boox and agents would just be responding to the most recent message without any\n1:26:18 understanding of previous interactions within a conversations so they would just not be coners ational and depending\n1:26:26 on the type of conversation we might want to go with various approaches to\n1:26:32 how we remember those interactions within a conversation now throughout\n1:26:38 this chapter we're going to be focusing on these for memory types we'll be\n1:26:43 referring to these and I'll be showing you actually how each one of these works but what we're really focusing on is\n1:26:49 rewriting these for the latest version of Lang chain using the what it's called\n1:26:54 the runable with\n1:27:00 message history so we're going to be essentially taking a look at the original implementations for each of\n1:27:07 these four original memory types and then we'll be rewriting them with the the runnable memory history class so\n1:27:14 just taking a look at each of these four very quickly conversational buffer\n1:27:19 memory is I think the simplest and most intuitive of these meor types it is\n1:27:26 literally just you have your messages they come into this object they are sold\n1:27:33 in this object as essentially a list and when you need them again it will return\n1:27:38 them to you there's nothing and nothing else to it's super simple the conversation Buffet window memory okay\n1:27:44 so new word in the middle of the window this works in pretty much the same way\n1:27:51 but those messages that it has stored is not going to return all of them for you instead it's just going to return the\n1:27:57 most recent let's say the most recent three for example okay and that is defined by a parameter K conation of\n1:28:05 summary memory rather than keeping track of the entire uh interaction memory\n1:28:11 directly what it's doing is as those interactions come in it's actually going to take them and it's going to compress\n1:28:18 them into a smaller little summary of what has been within that conversation\n1:28:24 and as every new interaction is coming in it's going to do that going to keep iterating on that summary and then that\n1:28:31 is going to be return to us when we need it and finally we have the conversational summary buffer memory so\n1:28:37 this is it's taking so the buffer part of this is actually referring to very\n1:28:43 similar thing to the buffer window memory but rather than it being a you know most K messages it's looking at the\n1:28:50 number of tokens within your memory and it's returning the most recent K\n1:28:57 tokens that's what the buffer part is there and then it's also merging that\n1:29:02 with the summary memory here so essentially what you're getting is almost like a list of the most recent\n1:29:09 messages based on the token length rather than the number of interactions plus a summary which would you know come\n1:29:16 at the the top here so you get kind of both the idea is that obviously this\n1:29:21 summary here would maintain all of your interactions in a very\n1:29:27 compressed form so you're you're losing less information and you're still maintaining you know maybe the very\n1:29:33 first interaction the user might have introduced themselves giving you their name hopefully that would be maintained\n1:29:40 within the summary and it would not be lost and then you have almost like higher resolution on the most recent um\n1:29:47 K or k tokens from your memory okay so let's jump over to the code we're going\n1:29:53 into the 04 chat memory notebook open that in collab okay now here we are let's go ahead and install the\n1:30:01 prerequisites run all we again can or cannot use align\n1:30:06 Smith it is up to you enter that and let's come down and start so first just\n1:30:13 initialize our LM using 40 mini in this example again low\n1:30:20 temperature and we're going to start with conversation buffer memory right so this is the original version of this uh\n1:30:29 memory type so let me where are we we're here so memory conversation both of\n1:30:36 memory and we're returning messages that needs to be set to true so the reason that we set return messages true it it\n1:30:44 mentions up here is if you do not do this it's going to returning your chat\n1:30:50 history as a string to an llm whereas well chat lm's nowadays would expect\n1:30:58 message objects so yeah you just want to be returning these as messages rather\n1:31:03 than as strings okay otherwise yeah you're going to get some kind of strange Behavior out from your llms if you\n1:31:10 return them strings so you do want to make sure that it's true I think by default it might not be true but this is\n1:31:16 coming this is deprecated right it does tell you here as de creation warning\n1:31:21 this is coming from older BL chain but it's a good place to start just to understand this and then we're going to rewrite this with the runnables which is\n1:31:28 the recommended way of doing so nowadays okay so adding messages to our memory\n1:31:34 we're going to write this okay so it's just a just a conversation user AI user\n1:31:39 AI so on and so on random chat main things to not here is I do provide my\n1:31:45 name we have the the model's name right towards the start of those interactions okay so I'm just going to add all of\n1:31:52 those with do it like this okay then we can just see we can load our history\n1:32:01 like so so let's just see what we have there okay so we have human message AI message human message right this is\n1:32:08 exactly what we I showed you just here it's just in that message format from line chain okay so we can do that\n1:32:16 alternatively we can actually do this so we can get our memory we initialize the conation buffer memory as we did before\n1:32:24 and we can actually add it directly the message into our memory like that so we can use this add us message add AI\n1:32:30 message so on and so on load again and it's going to give us the exact same thing again there's multiple ways to do\n1:32:37 the same thing cool so we have that to pass all of this into our LM again this\n1:32:42 is all deprecated so we're going to learn how to properly in a moment but this is how L chain was doing in the\n1:32:49 past so to pass all of this into our LM we' be using this conversation chain\n1:32:55 right again this is deprecated nowadays we would be using lell for this so I I\n1:33:02 just want to show you okay how this would all go together and then we would invoke okay what is my name again let's\n1:33:08 run that and we'll see what we get it's remembering everything remember so this\n1:33:13 conversation buffer memory it doesn't drop messages it just remembers everything right and honestly with the\n1:33:20 sort of high context Windows of many LMS might be what you do it depends on how\n1:33:25 long you expect the conversation to go on for but you could you probably in most cases would get away with this okay\n1:33:32 so what let's see what we get um I say what is my name again okay let's see\n1:33:38 what it gives me says your name is chains great thank you that works now as\n1:33:44 I mentioned all of this that I just showed you is actually deprecated that's the old way doing things let's see how\n1:33:49 we actually do this in modern or up toate L chain so we're going to be using this runable with message history to\n1:33:57 implement that we will need to use LL and for that we will need to just Define\n1:34:02 prompt templates our LM as we usually would okay so we're going to set up our system prompt which is just a helpful\n1:34:09 assist called Zeta okay we're going to put in this messages\n1:34:15 placeholder okay so that's important essentially that is where our messages\n1:34:20 are coming from our conversation Buffet for memory is going to be inserted right\n1:34:26 so it's going to be that chat history is going to be inserted after our system prompt but before our most recent query\n1:34:33 which is going to be inserted last here okay so messages placeholder item that's\n1:34:39 important and we use that throughout the course as well so we use it both for chat history and we'll see later on we\n1:34:45 also use it for the intermediate thoughts that a agent would go through as well so important to remember that\n1:34:52 little thing well link our prompt template to our LM again if we would\n1:34:58 like we could also add in the I think we only have the query here oh we would\n1:35:04 probably also want our history as well but I'm not going to do that right now\n1:35:10 okay so we have our Pipeline and we can go ahead and actually Define our runnable with message history now this\n1:35:17 class or object when we are initializing it does require a few items we can see them here okay so we' see that we have\n1:35:24 our Pipeline with history so it's basically going to be uh you can you can see here right we have that history\n1:35:30 messages key right this here has to align with what we provided as a meses\n1:35:36 placeholder in our pipeline right so we have our pipeline prompt template here\n1:35:44 and here right so that's where it's coming from it's coming from messes placeholder variable name is history\n1:35:49 right that's important that links to this then for the input messages key\n1:35:55 here we have query that again links to this okay so both important to have\n1:36:03 that the other thing that is important is obviously we're passing in that pipeline from before but then we also\n1:36:09 have this get session history basically what this is doing is it's saying okay I need to get uh the list of messages that\n1:36:16 make up my chat history that are going to be inserted into this variable so that is a function that we Define okay\n1:36:22 and with within this function what we're trying to do here is actually replicate\n1:36:28 what we have with the previous conversation buffer memory okay\n1:36:34 so that's what we're doing here so it's very simple right so we have this in\n1:36:40 memory chat message history okay so that's just the object that we're going to be returning what this will do is it\n1:36:47 will set up a session ID the session ID is essentially like a unique identifier so that eachers ation or interaction\n1:36:55 within a single conversation is being mapped to a specific conversation so you don't have overlapping let say have\n1:37:00 multiple users using the same system you want to have a unique session ID for each one of those okay and what it's\n1:37:06 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here we are going\n1:37:13 to initialize that session with an inmemory chat message history okay\n1:37:21 that's it and we return okay and all that's going to do is it's going to basically append our messages they will\n1:37:28 be appended within this chat map session ID and they're going to get returned\n1:37:33 there's nothing R there's nothing else to it to be honest so we invoke our\n1:37:39 rable let's see what we get I need to ruin\n1:37:45 this okay note that we do have this config so we have a session ID that's to\n1:37:50 again as I mentioned keep different conversations separate Okay so we've run that now let's run a few more so what is\n1:37:57 my name again let's see if it remembers your name is James how can I help you today James okay so it's what we've just\n1:38:06 done there is literally conversation buter memory but for upto-date L chain\n1:38:14 with L cell with Runner BS so you the recommended way of doing it nowadays so\n1:38:20 that's a very simple example okay really and not that much to it it gets a little\n1:38:27 more complicated as we start thinking about the different types of memory although that being said it's not\n1:38:33 massively complicated we're only rarely going to be changing the way that we're getting our interactions so let's uh\n1:38:41 let's dive into that and see how we will do something similar with the conation buff for window memory but first let's\n1:38:47 actually just understand okay what is Con station buffer window memory so as I mentioned near the start it's going to\n1:38:53 keep track of the last K messages so there's a few things to keep in mind\n1:38:58 here more messages does mean more tokens send with each request and if we have\n1:39:03 more tokens in each request it means that we're increasing the latency of our responses and also the cost so with the\n1:39:10 previous memory type we're just sending everything and because we're sending everything that is going to be\n1:39:15 increasing our cost it's going to be increasing our latency for every message especially as a conversation gets longer\n1:39:21 and longer and we don't we might not necessarily want to do that so with this conversation buffer window memory we're\n1:39:28 going to just say okay just return me the most recent messages okay so let's\n1:39:35 well let's see how that would work here we're going to return the most recent four messages okay we are again make\n1:39:41 sure we've turned messages is set to True again this is deprecated this is just the old way of doing it in a moment\n1:39:48 we'll see the updated way of doing this we'll add all of our\n1:39:54 messages okay so we have this and just see here right so we've added in all\n1:40:01 these messages there's more than four messages here and we can actually see that here so we have human message AI\n1:40:07 human AI human AI human AI right so we've got four pairs of human AI\n1:40:14 interactions there but here we don't have there's more than four pairs so four pairs will take us back all the way\n1:40:21 to here I'm researching different types of conversational uh memory okay and if\n1:40:28 we take a look here the most the first message we have is I'm researching different types of conversational memory\n1:40:33 so it's cut off these two here which will be a bit problematic when we ask you what our name is okay so let's just\n1:40:40 see going to be using conversation chain object again again just remember that is\n1:40:45 deprecated and I want to say what is my name again let's see let's see what it\n1:40:51 says uh I'm sorry I but I don't have access to your name or any personal information if you like you can tell me\n1:40:56 your name right so it doesn't actually remember uh so that's kind of like a negative of the conversation Buffet\n1:41:04 window memory of course the uh to fix that in this scenario we might just want to increase K maybe we say remember the\n1:41:12 previous eight interaction Pairs and it will actually remember so what is my\n1:41:17 name again your name is James so now it remembers we've just modified how much it is remembering but of course you know\n1:41:24 pros and cons to this it really depends on what you're trying to build so let's take a look at how we would actually\n1:41:30 implement this with the runable with message history okay so you getting a little\n1:41:38 more complicated here although it it's it's not it's not complicated but well\n1:41:44 we'll see okay so we have buffer window message history we're creating a class here this class is going to inherit from\n1:41:51 the base chat message history object from line chain okay and in all of our\n1:41:57 other message history objects can do the same thing before with the inmemory\n1:42:02 message object that was basically replicating the buffer memory so we\n1:42:08 didn't actually need to do anything we didn't need to Define our own class here\n1:42:13 so in this case we do so we follow the same pattern that line chain follows\n1:42:19 with this base chat message history and you can see a few of the functions here that are important so add messages and\n1:42:26 clear are the ones that we're going to be focusing on we also need to have messages which this object attribute\n1:42:31 here okay so we're just implementing the synchronous methods here if we want this\n1:42:38 to be async if we want to support async we would have to add a add messages um a\n1:42:44 get messages and a clay as well so let's go ahead and do that we have messages we\n1:42:49 have K again we're looking at remembering the top K messages or most recent K messages only so it's important\n1:42:56 that we have that variable we are adding messages through this class this is\n1:43:01 going to be used by line chain within our runnable so we need to make sure that we do have this method and all\n1:43:06 we're going to be doing is extending the self messages uh list here and then we're actually just going to be trimming\n1:43:13 that down so that we're not remembering anything beyond those you know most recent K\n1:43:19 messages that we have set from here and then we also have the clear method\n1:43:25 as well so we need to include that that's just going to clear the history okay so it's not this isn't complicated\n1:43:31 right it just gives us this nice default standard interface for message history\n1:43:37 and we just need to make sure we're following that pattern okay I've included the uh this print here just so we can see what's happening okay so we\n1:43:45 have that and now for that get chat history function that we defined earlier\n1:43:51 rather than using the buin method we're going to be using our own object which is a buffer window message history which\n1:43:58 will be defined just here okay so if session ID is not in the chat map as we\n1:44:05 did before we're going to be initializing our buffer window message history we're setting K up here with a\n1:44:10 default value of four and then we just return it okay and and that is it so let's run this we have our runable with\n1:44:18 message history we have all of these variables which are exactly the same as before four but then we also have these\n1:44:25 variables here with it's history Factory config and this is where if we have um\n1:44:33 new variables that we've added to our message history in this case k that we\n1:44:38 have down here we need to provide that to line train and sell it this is a new configurable field okay and we've also\n1:44:46 added it for the session ID here as well so we're just being explicit and have everything in that so we have that\n1:44:53 and we run okay now let's go ahead and invoke and see what we get okay so\n1:45:00 important here this history Factory config that is kind of being fed through\n1:45:06 into our invoke so that we can actually modify those variables from here okay so we have config configurable session ID\n1:45:14 okay we just put whatever we want in here and then we also have the number K okay so remember the previous four\n1:45:22 interaction I think in this one we're doing something slightly different I think we're remembering the four\n1:45:27 interactions rather than the previous four interaction pairs okay so my name is James uh we're going to go through\n1:45:34 I'm just going to actually clear this and now I'm going to start again and we're going to use the exact same ad\n1:45:40 user message ad AI message that we used before we're just manually inserting all that into our history so that we can\n1:45:46 then just see okay what is the result and you can see that k equal 4 is actually unlike before where we were\n1:45:53 having the uh saving the top four interaction pairs we now saving the most\n1:46:01 recent four interactions not pairs just interactions and honestly I just think\n1:46:06 that's clearer I think it's weird that the number four for K would actually save the most recent eight messages\n1:46:14 right I I think that's odd so I'm just not replicating that weirdness we could\n1:46:19 if we wanted to I just don't like it so I'm not doing that and anyway we can see\n1:46:26 from messages that we're returning just the most four recent messages okay which\n1:46:31 should be these four Okay cool so we've just using the runable we've replicated\n1:46:38 the old way of having a window memory and okay I'm going to say what is my\n1:46:43 name again as before it's not going to remember so we can come to here I'm sorry about I don't have access to\n1:46:48 personal information so on and so on if you like to tell me your name doesn't know now let's try a new one where we\n1:46:56 initialize a new session okay so we're going with ID K4 so that's going to\n1:47:01 create a new conversation there and we're going to say we're going to set K to\n1:47:07 14 okay great I'm going to manually insert the other uh messages as we did\n1:47:13 before okay and we can see all of those and see at the top here we are still maintaining that hi my name is James\n1:47:19 message now let's see if it remembers my name your name is James okay there we go\n1:47:27 cool so that is working we can also see so we just added this what is my name again let's just see if did that get\n1:47:34 added to our list of messages right what is my name again nice and then we also\n1:47:39 have the response your name is James so just by invoking this because we're using the the runable with message\n1:47:46 history it's just automatically adding all of that into our message history\n1:47:52 which is nice cool all right so that is the buffer window memory now we are going to take a\n1:47:59 look at how we might do something a little more complicated which is the the summaries okay so when you think about\n1:48:06 the summary you know what are we doing we're actually taking the messages we're using that LM call to summarize them to\n1:48:14 compress them and then we're storing them within messages so let's see how we would actually uh do that so to start\n1:48:22 with let's just see how it was done in Old Line chain so we have conversation\n1:48:28 summary memory go through that and let's just see what we get so\n1:48:35 again same interactions right I'm just invoking invoking invoking I'm not adding these\n1:48:41 directly to the messages because it actually needs to go through a um like that summarization process and if we\n1:48:49 have a look we can see it happening okay current conversation so sorry current\n1:48:55 conversation hello there my name is James AI is generating current conversation the human introduces\n1:49:01 himself as James AI greets James warmly and expresses its Readiness to chat and assist inquiring about how his day is\n1:49:08 going right so it's summarizing the the previous interactions and then we have\n1:49:14 you know after that summary we have the most recent human message and then the AI is going to generate its response\n1:49:21 okay and that continues your own Contin is going and you see that the the final summary here is going to be a lot longer\n1:49:27 okay it's different that first summary of course asking about his Day Men researching different types of\n1:49:32 conversational memory the AI responds enthusiastically explaining that conversational memory includes\n1:49:37 short-term memory longterm memory contextual memory personalized memory and then inquires if James is focused on a specific type of memory Okay cool so\n1:49:46 we get essentially the summary is just getting uh longer and longer as we go but at some point the idea is that it's\n1:49:53 not going to keep growing and it should actually be shorter than if you were saving every single interaction whilst\n1:49:59 maintaining as much all the information as possible but of course you're not\n1:50:04 going to maintain all of the information that you would with for example the the\n1:50:09 buffer memory right with the summary you are going to lose information but\n1:50:15 hopefully less information than if you're just cutting interactions so\n1:50:20 you're trying to reduce your token count whilst maintaining as much information as\n1:50:25 possible now let's go and ask what is my name again it should be able to answer\n1:50:30 because we can see in the summary here that I introduced myself as\n1:50:36 James okay respondents your name is James how is your research going okay so\n1:50:41 has that cool let's see how we'd Implement that so again as before we're\n1:50:46 going to go with that conversation summary message history we're going to\n1:50:52 be importing this system message uh we're going to be using that not for the LM that we're chatting with but for the LM that will be generating our summary\n1:51:01 so actually that is not quite correct there is create a summary not that it\n1:51:06 matters it's just the doct string so we have our messages and we also have the LM so different different attribute here\n1:51:12 to what we had before when we initialize a conversation summary message history we need to passing in our LM we have the\n1:51:19 same methods as before we have ADD messages and clear and what we're doing is as messages coming we extend with our\n1:51:27 current messages but then we're modifying those okay so we construct our like\n1:51:34 instructions to make a summary okay so that is here we have the system front uh\n1:51:40 giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring to\n1:51:45 maintain as much relevant information as possible okay then we have a human message here through that we're passing\n1:51:52 the existing summary okay and then we're passing in the new\n1:51:57 messages Okay cool so we format those invoke the\n1:52:05 llm here and then what we're doing is in the messages we're actually replacing\n1:52:11 the existing history that we had before with a new history which is just a single system summary message okay let's\n1:52:20 see what we get as before we have that get chat history exactly the same as before the only real difference is that\n1:52:26 we're passing in the llm parameter here and of course as we're passing in the LM parameter in here it does also mean that\n1:52:33 we're going to have to include that in the configurable field spec and that we're going to need to include that when\n1:52:40 we're invoking our pipeline okay so we run that pass in the\n1:52:48 LM now of course one side effect of generating summaries for everything is that way actually you know we're\n1:52:54 generating more so you are actually using quite a lot of tokens whether or\n1:52:59 not you are saving tokens or not actually depends on the length of a conversation as the conversation gets\n1:53:05 longer if you're storing everything after a little while that the token usage is actually going to increase so\n1:53:12 if in your use case you expect to have shorter conversations you would be\n1:53:17 saving money and tokens by just using this standard buffer memory\n1:53:23 whereas if you're expecting very long conversations you would be saving tokens and money by using the summary history\n1:53:31 okay so let's see what we got from there we have a summary of the conversation James introduced himself by saying hi\n1:53:37 name James a I responded War asking hi James Interac include details about token\n1:53:42 usage okay so we actually included everything here which we probably should\n1:53:48 not have done why did we do that as so in here we're including all\n1:53:56 of the out in here so we using or including\n1:54:02 all of the content from the messages so I think maybe we just do X content for X in messages that\n1:54:12 should resolve that okay there we go so we quickly fli\n1:54:20 that so yeah before we pass them in the entire mage object which obviously includes all of this information whereas\n1:54:26 actually we just want to be passing into the content so we modified that and now\n1:54:31 we're getting what we would expect okay cool and then we can keep going right so as we as we keep going\n1:54:38 the summary should get more like abstract like as we just saw here is\n1:54:43 literally just giving us the messages directly almost okay so we're getting a bit of summary there and we can keep\n1:54:50 going we're going to add just more messages to that we'll see the you as we'll get send those we'll get a\n1:54:57 response send it again get a response and we just adding all of that invoking all of that and that will be of course\n1:55:03 adding everything into our message history Okay cool so we've run that\n1:55:09 let's see what the latest summary is okay and then we have this so this is\n1:55:15 a summary that we have inside of our our chat history okay cool now finally let's see\n1:55:24 what is my name again we can just double check you know it has my name in there so it should be able to tell\n1:55:33 us okay cool so your name is James pretty interesting so let's have a quick\n1:55:39 look over at limith so the reason I want to do this is just to point out okay the\n1:55:45 different essentially token usage that we're getting with each one of these okay so we can see that we have these Runner mess history which probably uh\n1:55:53 improved in naming there but we can see okay how long is each one of these taken\n1:55:59 how many tokens are they also using come back to here we have this runable\n1:56:04 message history this is we'll go through a few of these maybe to here I think we\n1:56:11 can see here this is that first interaction where we're using the buffer memory and we can see how many tokens we\n1:56:17 used here so 112 tokens when we're asking what is my name again okay then\n1:56:23 we modified this to include I think it was like 14 interactions or something on\n1:56:29 those lines obviously increases the number of tokens that we're using right so we can could see that actually happening all in Lang which is quite\n1:56:36 nice and we can compare okay how many tokens is each one of these using now this is looking at the buffer window and\n1:56:44 then if we come down to here and look at this one so this is using our summary\n1:56:49 okay so our summary with what is my name again actually use more tokens in this scenario right which is interesting\n1:56:55 because we're trying to compress information the reason there more is because there's not there hasn't been that many interactions as the\n1:57:02 conversation length increases with the summary this total\n1:57:08 number of tokens especially if we prompt it correctly to keep that low that should remain relatively small\n1:57:15 whereas with the buffer memory that will just keep increasing and increasing as the as the conversation gets longer\n1:57:23 so useful little way of using Lang Smith there to just kind of figure out okay in\n1:57:29 terms of tokens and costs of what we're looking at for each of these memory types okay so our final memory type acts\n1:57:37 as a mix of the summary memory and the buffer memory so what it's going to do\n1:57:43 is keep the buffer up until an N number of tokens and then once a message\n1:57:50 exceeds the N number of tokens limit for the buffer it is actually going to be\n1:57:55 added into our summary so this memory has the benefit of remembering in detail\n1:58:03 the most recent interactions whilst also not having the limitation of using too\n1:58:10 many tokens as a conversation gets longer and even potentially exceeding context Windows if you try super hard so\n1:58:18 this is a very interesting approach now as before let's try the original way of\n1:58:24 implementing this then we will go ahead and use our update method for\n1:58:30 implementing this so we come down to here and we're going to do L chain memory import conversation summary\n1:58:37 buffer memory okay a few things here LM for summary we have the N number of\n1:58:44 tokens that we can keep before they get added to the summary and then return messages of course okay you can see\n1:58:51 again this is dicated we use the conversation chain and then we just passing our memory there and\n1:58:57 then we can chat okay so super straightforward first message we'll add\n1:59:03 a few more here and we have to invoke because how\n1:59:08 memory type here is using NM to create those summaries as it goes and let's see\n1:59:14 what they look like okay so we can see for the first message here we have human message and then an AI message\n1:59:22 then we come a little bit lower down again it's same thing human message is the first thing in our history here then\n1:59:29 it's a system message so this is at the point where we've exceeded that 300 token limit and the memory type here is\n1:59:36 generating those summaries so that summary comes in as a system message and we can see okay the human named James\n1:59:43 introduces himself and mentions he's researching different types of conversational memory and so on and so on right okay cool so we have that then\n1:59:52 let's come down a little bit further we can see okay so the summary there okay\n1:59:59 so that's what we that's what we have that is the implementation for the old\n2:00:05 version of this memory again we can see it's deprecated so how do we implement this for our more recent versions of\n2:00:14 Lang chain and specifically 0.3 well again we're using that runable message\n2:00:20 history and it looks a little more complicated than we were getting before but it's actually just you know it's\n2:00:27 nothing too complex we're just creating a summary as we did with the previous\n2:00:33 memory type but the decision for adding to that summary is based on in this case\n2:00:39 actually the number of messages so I didn't go with the the Lang chain version where it's a number of tokens I\n2:00:46 don't like that I prefer to go with messages so what I'm doing is saying okay let K messages\n2:00:52 okay once we exceed K messages the messages beyond that are going to be\n2:00:58 added to the memory Okay cool so let's see we first initialize our conversation\n2:01:06 summary buffer message history class with llm and K okay so these two here so\n2:01:14 LM of course to create summaries and K is just the the limit of the number of messages that we want to keep before\n2:01:19 adding them to the summary or dropping them from now messages and adding them to the summary okay so we will begin with okay\n2:01:29 do we have an existing summary so the reason we set this in none is we can't\n2:01:36 extract the summary the existing summary unless it already exists and the only\n2:01:41 way we can do that is by checking okay do we have any messages if yes we want to check if within those messages we\n2:01:48 have a system message because we're we're doing the same structure is what we have up here where the system message\n2:01:54 that first system message is actually our summary so that's what we're doing here we're checking if there is a\n2:02:00 summary message already stored within our messages okay so we're checking for that if we\n2:02:08 find it we'll just do we have this little print statement so we can see that we found something and then we just\n2:02:14 make our existing summary I should actually move this to the first instance\n2:02:21 here yeah okay so that existing summary will be set to the first\n2:02:30 message okay and this would be a system message rather than a\n2:02:35 string cool so we have that then we want to add any new messages to our history\n2:02:43 okay so we're extending the history there and then we're saying okay if the length of our history is exceeds the K\n2:02:50 value that we set we're going say okay we found that many messages we're going to be dropping the latest it's going to be the latest two\n2:02:57 messages this I will say here one thing or one problem with this is that we're\n2:03:04 not going to be saving that many tokens if we're summarizing every two messages so what I would probably do is in in an\n2:03:12 actual like production setting I would probably say let's go up to 20 messages\n2:03:19 and once we hit 20 messages let's take the previous 10 we're going to summarize them and put them into our summary\n2:03:26 alongside any you know previous summary that already existed but in in you know this is also fine as well okay so we say\n2:03:36 we found those mes we're going to drop the latest two messages okay so we pull\n2:03:41 the the oldest messages out I should say not the latest it's the\n2:03:48 oldest not the latest I want to keep the latest drop the oldest so we pull out\n2:03:54 the oldest messages and keep only the most recent messages okay then I'm\n2:04:01 saying okay if we if we don't have any old messages to summarize we don't do\n2:04:06 anything we just return okay so this in the case that this has not been triggered we would hit this but in the\n2:04:14 case this has been triggered and we do have old messages we're going to come to\n2:04:20 here okay okay so this is we can see have a system message prompt template\n2:04:26 saying giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring\n2:04:32 to maintain as much relevant information as possible so if you want to be more conservative with tokens we could modify\n2:04:39 this prompt here to say keep the summary to within the length of a single\n2:04:44 paragraph for example and then we have our human M prom template which is going to say okay here's the existing\n2:04:50 conversation summary in here on new messages now new messages here is actually the old messages but the way\n2:04:57 that we're framing it to the llm here is that we want to summarize the whole conversation right it doesn't need to\n2:05:03 have the most recent messages that we're storing within our buffer it doesn't need to know about those that's\n2:05:09 irrelevant to the summary so we just tell it that we have these Zoom mes and as far as this LM is concerned this is\n2:05:15 like the full set of interactions okay so then we would format those and invoke\n2:05:21 our LM and then we'll print out our new summary so we can see what's going on there and we would prend that new\n2:05:30 summary to our conversation history okay and and this will work so we can just\n2:05:37 prend it like this because we've already\n2:05:42 popped where was it up here if we have an existing\n2:05:47 summary we already pop that from the list it's already been pulled out of that list so it's okay for us to just we\n2:05:53 don't need to say like we don't need to do this because we've already dropped that initial system message if it\n2:06:00 existed okay and then we have the clear method as before so that's all of the\n2:06:05 logic for our conversational summary buffer memory we redefine our get chat\n2:06:14 history function with the LM and K parameters there and then we'll also\n2:06:20 want to set the configurable Fields again so that is just going to be of course session ID LM and\n2:06:28 K okay so now we can invoke the K value to begin with is going to be\n2:06:35 four okay so we can see no old messages to update summary with it's good let's\n2:06:42 invoke this a few times and let's see what we get okay so now M to summary with\n2:06:51 found six messages dropping the aest 2 and then we have new summary in the conversation James Inu himself and first\n2:06:58 is interestes in researching different types of conversational memory right so you can see there's quite a lot in here\n2:07:03 at the moment so we would definitely want to prompt the LM the summary LM to keep\n2:07:10 that short otherwise we're just getting a ton of stuff right but we can see that that is\n2:07:17 you know it's it's working it's functional so let's go back and see if we can prompt it to be a little more\n2:07:23 concise so we come to here ensuring to maintain as much relevant information as\n2:07:28 possible however we need to\n2:07:33 keep our summary concise the\n2:07:39 limit is a single short paragraph okay\n2:07:45 something like this let's try and let's see what we get with\n2:07:50 that okay so message one again nothing to update see this so new summary you can\n2:07:56 see it's a bit shorter it doesn't have all those bullet\n2:08:01 points okay so that seems better let's see so you can see the first summary is\n2:08:09 a bit shorter but then as soon as we get to the second and third summaries the\n2:08:14 second summary is actually slightly longer than the third one okay so we're going to be we're going to be losing a\n2:08:20 bit of information in this case more than we were before but we're saving a ton of tokens so that's of course a good\n2:08:28 thing and of course we could keep going and adding many interactions here and we should see that this conversation\n2:08:34 summary will be it should maintain that sort of length of around one short\n2:08:40 paragraph So that is it for this chapter on conation memory we've seen a few\n2:08:47 different memory types we've implemented their old deprecated version so we can see what they were like and then we've\n2:08:55 reimplemented them for the latest versions of Lang chain and to be honest using logic where we are getting much\n2:09:02 more into the weees and that is in some ways okay it complicates things that is\n2:09:07 true but in other ways it gives us a ton of control so we can modify those memory\n2:09:13 types as we did with that final summary buffer memory type we can modify those\n2:09:18 to our liking which is incredibly useful when you're actually building\n2:09:23 applications for the real world so that is it for this chapter we'll move on to the next one in this chapter we are\n2:09:30 going to introduce agents now agents I think are one of the most important\n2:09:37 components in the world of AI and I don't see that going away anytime soon I\n2:09:43 think the majority of AI applications the intelligent part of\n2:09:49 those will be was always an implementation of an AI agent or mle AI\n2:09:55 agents so in this chapter we are just going to introduce agents within the\n2:10:00 context of line chain we're going to keep it relatively simple we're going to go into much more depth in agents in the\n2:10:09 next chapter where we'll do a bit of a deep dive but we'll focus on just introducing the Core Concepts and of\n2:10:16 course agents within line chain here so jumping thing straight into our notebook\n2:10:24 let's run our prerequisites you'll see that we do have an additional prerequisite here which is\n2:10:30 Google search results that's because we're going to be using the sub API to allow our llm as an agent to search a\n2:10:38 web which is one of the great things about agents is that they can do all of\n2:10:44 these additional things and LM by itself obviously cannot so we come down to here\n2:10:49 we have our lsmith parameters again of course so you enter your Lang chain API\n2:10:54 if you have one and now we're going to take a look at tools which is a very\n2:11:00 essential part of Agents so tools are a way for us to augment our llms with\n2:11:07 essentially anything that we can write in code so we mentioned that that we're going to have a Google Search tool that\n2:11:14 Google Search tool it's some code that gets executed by our llm in order to\n2:11:19 search Google and get some results so a tool can be thought of as any code logic\n2:11:26 or any function in the C in the case of python any function that has been\n2:11:32 formatted in a way so that our LM can understand how to use it and then\n2:11:38 actually use it although the the LM itself is not using the tool it's more\n2:11:43 our agent execution logic which uses the tool for the llm so we're going to go\n2:11:49 ahead and actually create a few simple tools we're going to be using what is called the tool decorator from Lang\n2:11:55 chain and there are a few things to keep in mind when we're building tools so for\n2:12:02 Optimal Performance our tool needs to be just very readable and what I mean by readable is we need three main things\n2:12:10 one is a DOT string that is written in natural language and it is going to be used to explain to the Alm when and why\n2:12:18 and how it should use this tool we should also have clear parameter names\n2:12:23 those parameter names should tell the llm okay what each one of these\n2:12:28 parameters are they should be self-explanatory if they are not self-explanatory we should be including\n2:12:36 an explanation for those parameters within the doc string then finally we should have type annotations for both\n2:12:43 our parameters and also what we're returning from the tool so let's jump in\n2:12:48 and see how we would Implement all of that so we come down to here and we have line chain core tools import tool okay so\n2:12:56 these are just four incredibly simple tools we have the addition or add tool\n2:13:03 multiply the exponentiate and the subtract tools okay so a few calculator\n2:13:09 S tools now when we add this tool decorator it is turning each of these\n2:13:16 tools into what we call a structured tool object so we can see that\n2:13:21 here we can see we have this structured tool we have a name description okay and\n2:13:28 then we have this Al schema we'll see this in a moment and a function right so this function is literally just the\n2:13:35 original function it's it's a mapping to the original function so in this case it it's the add function now the\n2:13:41 description we can see is coming from our doc string and of course the name as well is just coming from the function\n2:13:47 name okay and then we can also see let's just print the name and\n2:13:53 description but then we can also see the ARs schema right we can so this thing\n2:13:58 here that we can't read at the moment to read it we're just going to look at the\n2:14:03 model Json schema method and then we can see what that contains which is all of this information so this actually\n2:14:10 contains everything includes properties so we have the X it C or title for that\n2:14:16 and it also specifies the type okay so the type that we Define is float float\n2:14:23 for open AI gets mapped to number rather than just being float and then we also\n2:14:28 see that we have this required field so this is telling how LM which parameters\n2:14:33 are required which ones are optional so we yeah in some cases you would we can\n2:14:39 even do that here let's do Z that is going to be float or none okay and we\n2:14:48 just going to say it is 0.3 all right well I'm going to remove this\n2:14:53 in a minute because it's kind of weird but let's just see what that looks like so you see that we now have X Y and\n2:15:02 Z but then in Z we have some additional information okay so it can be any of it\n2:15:08 can be a number or it can just be nothing the default value for that is 0.3 okay and then if we look here we can\n2:15:16 see that the required field does not include Z so it's just X and Y so it's\n2:15:21 describing the full function schema for us but let's remove\n2:15:27 that okay and we can see that again with our exponentiate tool similar thing okay\n2:15:33 so how how are we going to invoke our tool so the llm the underlying LM is\n2:15:41 actually going to generate a string okay so we'll look something like this this is going to be our llm output so it is\n2:15:50 it's a string that is some Json and of course to load a string into a\n2:15:56 dictionary format we just use Json loes okay so let's see that so this could be\n2:16:03 the output Fromm we load it into a dictionary and then we get an actual dictionary and then what we would do is\n2:16:11 we can take our exponentiate uh tool we access the underlying function and then\n2:16:17 we pass it the keyword arguments from our diction here\n2:16:24 okay and that will execute our tool that is the tool execution log you that line chain implements and then later on in\n2:16:31 the next chapter we'll be implementing ourselves cool so let's move on to creating an agent now we're going to be\n2:16:38 constructing a simple tool calling agent we're going to be using Lang chain expression language to do this now we\n2:16:45 will be covering Lang chain expression language or also more in a upcoming\n2:16:50 chapter but for now all we need to know is that our agent will be constructed\n2:16:57 using syntax and components that like this so we would start with our input\n2:17:03 parameters that is going to include our user query and of course the chat history because we need our agent to be\n2:17:09 conversational and remember previous interactions within the conversation these input parameters will also include\n2:17:15 a placeholder for what we call the agent scratch Pad now the agent scratch Pad is essentially where we are storing the\n2:17:22 internal thoughts or the internal dialogue of the agent as it is using tools getting observations from those\n2:17:28 tools and working through those multiple internal steps so in the case that we\n2:17:34 will see it will be using for example the addition tool getting the result using the multiply tool getting the\n2:17:40 result and then providing a final answer to us as a user so let's jump in and see\n2:17:46 what it looks like okay so we'll just start with defining our prompt so our prompt is going to include the system\n2:17:52 message there nothing we're not putting anything special in there we're going to\n2:17:57 include the chat history which is a messages placeholder then we include our\n2:18:03 human message and then we include a placeholder for the agent scratch Pad\n2:18:08 now the way that we implement this later is going to be slightly different for the scratch Pad we actually use this\n2:18:13 message's placeholder but this is how we use it with the built-in create tool agent from BL chain next we sign our LM\n2:18:21 we do need our Opening Our API key for that so we'll enter that here like so\n2:18:28 okay so come down Okay so we're going to be creating this agent we need conversation memory and we are going to\n2:18:34 use the older conversation buffer memory class rather than the newer renable with message history class that's just\n2:18:40 because we're also using this older create tool calling agent and this is\n2:18:46 this is the older way of doing things in the next chapter we are going to be using the more recent basically what we\n2:18:54 already learned on chat history we're going to be using all of that to implement our chat history but for now\n2:19:00 we're going to be using the older method uh which is deprecated just as a pre-warning but again as I mentioned at\n2:19:07 the very solid of course we're starting abstract and then we're getting into the details so we're going to initialize our\n2:19:15 agent for that we need these four things LM as we defined tools as we have\n2:19:20 defined prompt as we have defined and then the memory which is our old conversation\n2:19:26 buffer memory so with all of that we are going to go ahead and we create a tool\n2:19:32 calling agent and then we just provide it with everything okay there we go now H you'll see here I didn't pass\n2:19:40 in the the memory I'm passing it in down here instead so we're going to start with this question which is what is 10.7\n2:19:47 MTI 7.68 eight okay so given the Precision of\n2:19:55 these numbers our l a normal LM would not be able to answer that or almost\n2:20:01 definitely will not be able to answer that correctly we need a external tool to answer that accurately and we'll see\n2:20:08 that that is exactly what it's going to do so we can see that the tool agent\n2:20:15 action message here we see that it decided okay I'm going to use the multiply tool and here at parameters\n2:20:21 that I want to use for that tool okay we can see X is 10.7 and Y is\n2:20:26 7.68 you can see here that this is already a dictionary and that is because\n2:20:32 Lang chain has taken the string from our llm C and already converted it into a\n2:20:38 dictionary for us okay so that's just it's happening behind the scenes there and you can actually see if we go into\n2:20:44 the details a little bit we can see that we have these arguments and this is the original string that was coming fromn\n2:20:50 okay which has already been of of course processed by line chain so we have that\n2:20:56 now the one thing missing here is that okay we've got that the LM\n2:21:03 wants us to use multiply and we've got what the LM wants us to put into modly but where's the answer right there is no\n2:21:11 answer because the tool itself has not been executed because it can't be executed by the llm but then okay didn't\n2:21:19 we already Define our agent here yes redefined the part of our agent that is\n2:21:26 how llm has our tools and it is going to generate which tool to use but it actually doesn't include the\n2:21:33 agent execution part which is okay the agent executor is a broader thing it's\n2:21:41 it's broader logic like just code logic which acts as a scaffolding within which\n2:21:47 we have the iteration through multiple steps of our llm calls followed by the\n2:21:54 llm outputting what tools use followed by us actually executing that for the llm and then providing the output back\n2:22:02 into the llm for another decision or another step so the agent itself here is\n2:22:08 not the full agentic flow that we might expect instead for that we need to\n2:22:15 implement this agent executor class this agent executor includes our agent from\n2:22:21 before then it also includes the tools and one thing here is okay we we already\n2:22:26 passed the tools to our agent why do we need to pass them again well the tools being passed to our agent up\n2:22:32 here that is being used so that is essentially extracting out those\n2:22:38 function schemers and passing it to our LM so that our LM knows how to use the tools then we're down here we're passing\n2:22:44 the tools again to our agent executor and this is rather than looking at how to use those tools this is just\n2:22:50 looking at okay I want the functions for those tools so that I can actually execute them for the llm or for the\n2:22:57 agent okay so that's why it's happening there now we can also pass in our memory\n2:23:02 directly so you see if we scroll up a little bit here I actually had to pass\n2:23:08 in the memory like this with our agent that's just because we weren't using the agent executor now we have the agent\n2:23:14 executor it's going to handle that for us and another thing that's going to handle for us is it intermediate steps\n2:23:21 so you'll see in a moment that when we invoke the agent executor we don't include the intermediate steps and\n2:23:27 that's because it that is already handled by the agent executor now so we'll come down we'll set the both equal\n2:23:34 to true so we can see what is happening and then we can see here there's no\n2:23:39 intermediate steps anymore and we we do still pass in the chat history like this\n2:23:46 but then the addition of those new interactions to our memory is going to be handled by the\n2:23:52 executor so let me actually show that very quickly before we jump in okay so\n2:23:59 that's cently empty we're going to execute this okay we entered that new Asian\n2:24:05 execute chain let's just have a quick look at our messages again and now you can see that the agent\n2:24:11 executor automatically handled the addition of our human message and then the responding AI message for us okay\n2:24:19 which is useful now what happened so we can see that the multiply tool was\n2:24:25 invoked with these parameters and then this pink text here that we got that is\n2:24:30 the observation from the tool assist what the tool output back to us okay then this final message here it's not\n2:24:37 formatted very nicely well this final message here is coming from our llm so the green is our llm output the pink is\n2:24:45 our tool output okay so the LM after seeing this output says 10.7 MTI by 7.68\n2:24:56 is approximately 82.8 okay cool use and then we can also\n2:25:03 see the the chat history which we we already just saw great so that has been\n2:25:08 used correctly we can just also confirm that that is correct okay 82\n2:25:15 1759 recurring which is exactly what we get here okay and we the reason for all\n2:25:20 that is obviously how multiply tool is just doing this exact operation cool so let's try this with a\n2:25:29 bit of memory so I'm going to ask or I'm going to sayate to the agent hello my\n2:25:34 name is James we'll leave that as the it's not actually the first interaction because\n2:25:40 we already have these but it's an early interaction with my name in there then\n2:25:48 we're going to try and perform more tool calls within a single execution Loop and what you'll see with when it is calling\n2:25:54 these tools is that it can actually use multiple Tools in parallel so for sure I think two or three of these were used in\n2:26:00 parallel and then the final subtract had to wait for those previous results so it would have been executed afterwards and\n2:26:08 we should actually be able to see this in Langs Smith so if we go here yeah we\n2:26:13 can see that we have this initial cord and then we have add and multiply and exponentially we all use in parallel\n2:26:20 then we have another call which use subtract and then we get the response okay which is pretty cool and\n2:26:27 then the final result there is11 now when you look at whether the\n2:26:33 answer is accurate I think the order here of calculations is not quite\n2:26:39 correct so if we put the actual computation here it gets it right but\n2:26:45 otherwise if I use natural language it's like I'm doing maybe I'm phrasing it in a in a poor way\n2:26:52 okay so I suppose that is pretty important so okay if we put the computation in here we get\n2:26:59 the13 so it's something to be careful with and probably requires a little bit of prompting to promting and maybe\n2:27:07 examples in order to get that smooth so that it does do things in the way that\n2:27:12 we might expect or maybe we as humans are just bad and misus the systems one\n2:27:18 or the other okay so now we've gone through that a few times let's go and see if our agent can still recall our\n2:27:25 name okay and it remembers my name is James good so it still has that memory in there as well that's good let's move\n2:27:32 on to another quick example where we're just going to use Google search so we're going to be using the Ser\n2:27:39 API you can okay you can get the API key that you need from here so Ser ai.com\n2:27:46 usersign in and just enter that in here so you will get it up to 100 stes per\n2:27:54 month for free so just be aware of that if you overuse it I don't think they\n2:28:00 charge you cuz I don't think you enter your card details straight away but yeah just be aware of that\n2:28:07 limit now there are certain tools that line chain have already built for us so\n2:28:12 they're pre-built tools and we can just load them using the load tools function so we do that like so we have our load\n2:28:19 tools and we just pass in the Ser API tool only we could pass in more there if we want to and then we also pass in our\n2:28:26 LM now I'm going to one use that tool but I'm also going to Define my own tool\n2:28:32 which is to get the current location based on the IP address now this is we're in collab at the moment so it's\n2:28:38 actually going to get the IP address for the collab instance that I'm currently on and we'll find out where that is so\n2:28:45 that is going to get the IP address and then it's going to provide the data back to our LM this format here so we're\n2:28:51 going to latitude longitude City and Country okay we're also going to get the current day and time so now we're going\n2:28:59 to redefine our prompt I'm not going to include chat history here I just want this to be like a one shot\n2:29:06 thing I'm going to redefine our agent and agent executor using our new tools which is our set API plus to get current\n2:29:14 date time and get location from IP then I'm going to invoke our agent executor\n2:29:20 with I have a few questions what is the date and time right now how is the weather where I am and please give me\n2:29:27 degrees in celce so when it gives me that weather okay and let's see what we\n2:29:33 get okay so apparently we're in Council Bluffs in the\n2:29:39 US it is 13 fah which I think is absolutely freezing oh my gosh it is yes\n2:29:46 minus 10 so it's super cold over there and you can see that okay it did give us\n2:29:53 Fahrenheits that is because the tool that we were using provided us with Fahrenheit which is fine but it did\n2:30:00 translate that over into a estimate of Celsius fours which is pretty cool so let's actually output that so we get\n2:30:08 this which I is correct we do us approximately this and\n2:30:14 we also get an description of the conditions as well as partly cloudy with z % precipitation lucky for\n2:30:21 them and humidity of 66% okay well pretty cool so that is it\n2:30:27 for this introduction to Lang chain agents as I mentioned next chapter we're going to dive much deeper into agents\n2:30:34 and also Implement that for Lang chain version 0.3 so we'll leave this chapter here and jump into the next one in this\n2:30:41 chapter we're going to be taking a deep dive into agents with the Lang chain and\n2:30:48 we're going to be covering what an agent is we're going to talk a\n2:30:53 little bit conceptually about agents the react agent and the type of agent that\n2:30:59 we're going to be building and based on that knowledge we are actually going to build out our own agent execution logic\n2:31:07 which we refer to as the agent executor so in comparison to the previous video\n2:31:14 on agents in line chain which is more of an introduction this is far far more\n2:31:20 detailed we'll be getting into the weeds a lot more with both what agents are and\n2:31:25 also agents within Lang chain now when we talk about agents a significant part\n2:31:31 of the agent is actually relatively simple code\n2:31:37 logic that iteratively runs llm calls\n2:31:42 and processes their outputs potentially running or executing tools the exact\n2:31:49 logic for each approach to building an agent will actually vary pretty\n2:31:55 significantly but we'll focus on one of those which is the react agent now react\n2:32:02 is it's a very common pattern and although being relatively old now most\n2:32:08 of the tool agents that we see used by openai and essentially every LM company\n2:32:15 they all use a very similar pattern now the reactor agent follows a patter and like this okay so we would have our user\n2:32:23 input up here okay so our input here is a question right aside from the Apple\n2:32:29 remote what other device you can control the program Apple remote was originally designed to interact with now probably\n2:32:35 most LMS would actually be able to answer this directly now this is from the paper which was a few years back now\n2:32:42 in this scenario assuming our LM didn't already know the answer there are most steps\n2:32:48 that an llm or an agent might take in order to find out the answer okay so the\n2:32:54 first of those is we say our question here is what other device can control the program Apple remote was originally\n2:33:01 designed to interact with so the first thing is okay what was the program that the Apple remote was originally designed\n2:33:07 to interact with that's the first question we have here so what we do is I\n2:33:13 need to search Apple remote and find a program it was use for this is a reasoning step so the llm is reasoning\n2:33:19 about what it needs to do I need to search for that and find a program useful so we are taking an action this\n2:33:26 is a tool call here okay so we're going to use the search tool and our query will be apple remote and the observation\n2:33:33 is the response we get from executing that tool okay so the response here would be the Apple remote it's designed\n2:33:39 to control the front row mediate Center so now we know the programmer for was\n2:33:45 originally designed to interact with now we're going to go through another it\n2:33:50 okay so this is one iteration of our reasoning action and\n2:33:56 observation so when we're talking about react here although again this sort of\n2:34:02 pattern is very common across many agents when we're talking about react\n2:34:07 the name actually is reasoning or the first two characters of re reasoning\n2:34:13 followed by action okay so that's where the react comes from so this is one of\n2:34:19 our react agent Loops or iterations we're going to go and do another one so\n2:34:24 next step we have this information the LM is now provided with this information now we want to do a search for front row\n2:34:32 okay so we do that this is the reasoning step we per the action search front row\n2:34:38 okay tool search query front row observation this is the response front\n2:34:43 row is controlled by an apple remote or keyboard function keys all right cool so\n2:34:50 we know keyboard function keys are the other device that we were asking about up here so now we have all the\n2:34:58 information we need we can provide an answer to our user so we go through\n2:35:04 another iteration here reasoning and action our reasoning is I can now\n2:35:09 provide the answer of keyboard function keys to the user okay great so then we\n2:35:16 use the answer tool like Final Answer In more common tool agent use and the\n2:35:25 answer would be keyboard function keys which we then output to our user okay so\n2:35:32 that is the react Loop okay so looking at this how where are we actually calling\n2:35:40 an llm and what and in what way are we actually calling llm\n2:35:46 so we have our reasoning step our LM is generating the text here right so LM is\n2:35:52 generating okay what should I do then our LM is going to generate input\n2:35:59 parameters to our action step here that will th those input parameters and and\n2:36:05 the tool being used will be taken by our code logic our agent executor logic and\n2:36:10 they'll be used to execute some code in which we will get an output that output\n2:36:15 might be taken directly to our observation or our llm might take that output and then generate an observation\n2:36:22 based on that it depends on how you've implemented everything so our LM could\n2:36:29 potentially being be being used at every single step there and of course that\n2:36:35 will repeat through every iteration so we have further iterations down here so\n2:36:41 you're potentially using LM multiple times throughout this whole process which of course in terms of latency and\n2:36:47 token cost it does mean that you're going to be paying more for an agent\n2:36:53 than you are with just a sun LM but that that is of course expected because you have all of these different things going\n2:36:59 on but the idea is that what you can get out of an agent is of course much better\n2:37:05 than what you can get out of an LM alone so when we're looking at all of this all\n2:37:11 of this iterative Chain of Thought and Tool use all this needs to be controlled\n2:37:17 by what we call the agent executor okay which is our code logic which is hitting our llm processing its outputs and\n2:37:25 repeating that process until we get to our answer so breaking that part down what does it actually look like it looks\n2:37:32 kind of like this so we have our user input goes into our llm okay and then we move on to the\n2:37:39 reasoning and action steps is the action the answer if it is the answer so as we\n2:37:47 saw here where is the answer if the action is the answer so true we\n2:37:53 would just go straight to our outputs otherwise we're going to use our select tool agent executor is going to handle\n2:37:59 all this it's going to execute our tool and then from that we get our you know\n2:38:04 three reasoning action observation inputs and outputs and then we're feeding all that information back into\n2:38:11 our llm okay in which case we go back through that Loop so we could be looping\n2:38:16 for a little while until we get to that final but okay so let's go across to the\n2:38:22 code when be going into the agent executor notebook we'll open that up in\n2:38:27 coab and we'll go ahead and just install our prerequisites nothing different here is\n2:38:34 just L chain L Smith optionally as before again optionally line chain API\n2:38:40 key if you do want to use l Smith okay and then we'll come down to our first\n2:38:47 section where it's going to define a few quick tools I'm not necessarily going to\n2:38:52 go through these because we've already covered them in the agent introduction\n2:38:58 but very quickly Lang chain core tool is we're just importing this tool decorator which transforms each of our functions\n2:39:05 here into what we would call a structured tool object this thing here\n2:39:12 okay which we can see just having a quick look here and then if we want to we can extract all of the sort of key\n2:39:19 information from that structure tool using these parameters here or attributes so name description AR\n2:39:25 schemer model Json streer which give us essentially how the llm should use our\n2:39:32 function okay so I'm going to keep pushing through that now very quickly\n2:39:40 again we did cover this in the intro video so I don't want to necessar go over again into much detail but our\n2:39:48 agent EX future logic is going to need this part so we're going to be getting a\n2:39:53 string from our llm we're going to be loading that into to a dictionary object and we're going to be using that to\n2:40:00 actually execute our tool as we do here using keyword\n2:40:05 arguments okay like that okay so with the tools out of the way let's take a\n2:40:11 look at how we create our agent so when I say agent here I'm specifically\n2:40:16 talking about the part that is generating our reasoning St then generating which\n2:40:24 tool and what the input parameters to that tool will be then the rest of that\n2:40:29 is not actually covered by the agent okay the rest of that would be covered by the agent execution logic which would\n2:40:35 be taking the tool to be used the parameters executing the tool getting\n2:40:41 the response aka the observation and then iterating through that until the llm is satisfied and we have enough\n2:40:47 information to answer a question so looking at that our agent we look\n2:40:53 something like this it's pretty simple so we have our input parameters including the chat history user query we\n2:41:00 have our input parameters including the chat history us query and actually would also have any intermediate STS that have\n2:41:07 happened in here as well we have our prompt template and then we have our llm binded with tools so let's see how all\n2:41:15 this would look starting with we'll Define our promp template searching look\n2:41:20 like this we have our system message your helpful assistant when answering these question you should use on to\n2:41:27 provide after using a tool tool outp will provide in the scratch Pad below okay which we naming here if you have an\n2:41:34 answer in scratch Pad you should not use any more tools and set answer directly to the user okay so we have that as our\n2:41:41 system message we could obviously modify that based on what we're actually doing\n2:41:46 then following our system message we're going to have our chat history so any previous interactions between the user\n2:41:52 and the AI then we have our current message from the user okay we should be\n2:41:57 fed into the input field there and then following this we have our agent stretch\n2:42:03 pad or the intermediate thoughts so this is where things like the llm deciding\n2:42:09 okay this is what I need to do this is how I'm going to do it AKA The Tool call and this is the observation that's where\n2:42:15 all of that information will be going right so each of those to pass in as a\n2:42:21 message okay and the way that we look is that any tool call generation from the\n2:42:27 llm so when the llm is saying use this tool please that will be a assistant\n2:42:32 message and then the responses from our tool so the\n2:42:37 observations they will be returned as tool messages great so we'll run that to\n2:42:44 Define our prompt template we're going to Define our LM we're going to be using\n2:42:49 J2 40 mini with a temperature of zero because we want less creativity here\n2:42:55 particularly when we're doing tour calling there's just no need for us to use a high temperature here so we need\n2:43:01 to enter our open ey API key which we would get from platform open ey.com we enter this then we're going to continue\n2:43:09 and we're just going to add tools to our LM here\n2:43:15 okay these and we're going to bind them here then we have tool Choice any so\n2:43:22 tool Choice any we we'll see in a moment I'll go through this a little bit more in a second but that's going to\n2:43:28 essentially force a tool call you can also put required which is actually a bit more uh it's bit clearer but I'm\n2:43:35 using any here so I'll stick with it so these are our tools we're going through we have our inputs into the agent\n2:43:42 runable we have our prom template and then that will get fed into our llm so\n2:43:48 let's run that now we would invoke the agent part of everything here with this okay so\n2:43:55 let's see what it outputs this is important so I'm asking what is 10+ 10 obviously that should use the addition\n2:44:01 tool and we can actually see that happening so the agent message content is actually empty here this is where\n2:44:08 you'd usually get an answer but if we go and have a look we have additional keyword dos in there we have tool calls\n2:44:16 and then we have function arguments Okay so we're calling a function Arguments for that function are this okay so we\n2:44:24 can see this is string again the way that we would pass that as we do Json loads and that becomes a dictionary and\n2:44:30 then we can see which function is being called and it is the add function and that is all we need in order to actually\n2:44:37 execute our function or our our tool okay we can see it's a little more\n2:44:43 detail here now what do we do from here we're going to map the to name to the\n2:44:49 tool function and then we're just going to execute the tool function with the generated ARS I\n2:44:55 those I'll also just point out quickly that here we are getting the dictionary directly which I think is coming from\n2:45:02 somewhere else in this which is prob which is here okay so even that step\n2:45:08 here where we're passing this out we don't necessarily need to do that because I think on the L chain side\n2:45:14 they're doing it for us so we're already getting that so Json loads we don't\n2:45:20 necessarily need here okay so we're just creating this tool name to function\n2:45:25 mapping dictionary here so we're taking the well the tool names and we're just mapping those back to our tool functions\n2:45:32 and this is coming from our tools list so that tools list that we defined here\n2:45:37 okay or can even just see quickly that that will include everything or each of\n2:45:43 the tools you define there okay that's all it is now we're going to execute\n2:45:49 using our name to Tool mapping okay so this here will get us the function so it will get us this\n2:45:56 function and then to that function we're going to pass the arguments that we\n2:46:02 generated okay let's see what it looks like all right so the response so the observation\n2:46:10 is 20 now we are going to feed that back\n2:46:15 into our llm using the tool message and we're actually going to put a little bit of text around this to make it a little\n2:46:22 bit nice so we don't necessarily need to do this to be completely honest we could\n2:46:27 just return the answer directly uh I don't understand I don't even think\n2:46:33 there would really be any difference so we we could do either in some cases that\n2:46:38 could be very useful in other cases like here it doesn't really make too much difference particularly because we have\n2:46:44 this tool call ID and what this tool call ID is doing is it's being used by AI is being read by the LM so that the\n2:46:52 LM knows that the response we got here is actually mapped back to the the tool\n2:47:01 execution that it's identified here because you see that we have this ID right we have an ID here the LM is going\n2:47:08 to see the ID it's going see the ID that we pass back in here and it's going to\n2:47:13 see those two are connected so see okay this is the tool I called and this is the response I got from\n2:47:19 because of that you don't necessarily need to say which tool you used here you can it it depends on what you're\n2:47:27 doing okay so what do we get here we have okay just running everything again\n2:47:34 we've added our tool call so that's the original AI message that includes okay user add tool and then we have the tool\n2:47:40 execution tool message which is the observation we map those to the agent\n2:47:46 scratch pad and then what do we get we have an AI message but the content is empty again which is interesting because\n2:47:53 we we said to our llm up here if you have an answer to the in the scratchpad\n2:47:59 you should not use any more tools and said answer directly to the user so why why is our\n2:48:06 llm not answering well the reason for that is down here we specify tool Choice\n2:48:15 equals any which again it's the same tool Choice required which is telling\n2:48:22 the L land that it cannot actually answer directly it has to use a tool and\n2:48:28 I usually do this right I would usually put tool Choice equals any or required and for the LM to use a tool every\n2:48:36 single time so then the question is if it has to use a tool every time how does\n2:48:41 it answer our user well we'll see in a moment first I just want to show you\n2:48:49 the two options essentially that we have the second is what I would usually use but let's let's start with the first so\n2:48:56 the first option is that we set tool Choice equal to Auto and this tells the Ln that it can either use a tool or it\n2:49:03 can answer the user directly using the the final answer or using that content\n2:49:09 field so if we run that like we're specifying to choices Auto we run that\n2:49:15 let's invoke okay initially you see ah wait there's still no content that's because\n2:49:21 we didn't add anything into the agent scratch Pad here there's no information right it's all\n2:49:27 empty um actually it's empty because sorry so here you have the chat history that's empty we didn't specify the agent\n2:49:36 scratch Cad and the reason that we can do that is because we're using if you look here we're using get so essentially\n2:49:42 it's saying try and get agent scratch pad from this dictionary but if it hasn't been provided we're just going to\n2:49:48 give an empty list so that's what that's why we don't need to specify it here but\n2:49:54 that means that oh okay the the agent doesn't actually know anything here it hasn't used a tool yet so we're going to\n2:50:01 just go through our iteration again right so we're going to get our tool output we're going to use that to create\n2:50:07 the tool message and then we're going to add our tool call from the AI and the\n2:50:14 observation we're going to pass those to the agent scratch pad and this time we see we run that okay now we get the\n2:50:22 content okay so now it's not calling you see here there's no to call or anything going\n2:50:28 on we just get content so that is this is the standard\n2:50:34 way of doing or building a tool calling agent the other option which I mentioned\n2:50:40 this is what I would usually go with so number two here I would usually create a\n2:50:45 final answer tool so why would we even do that why would we\n2:50:53 create a final answer tool rather than just you know this method is actually perfectly you know it works so why would\n2:50:59 we not just use this there are a few reasons the main ones are that with\n2:51:05 option two where we're forcing tool calling this removes possibility of an\n2:51:11 agent using that content field directly and the reason at least the reason I\n2:51:17 found this good when building agents in the past is that occasionally when you do want to use a tool it's actually\n2:51:22 going to go with the content field and it can get quite annoying and and use the content field quite frequently when\n2:51:29 you actually do want it to be using one of the tools and this is particularly\n2:51:36 noticeable with smaller models with bigger models it's not as common\n2:51:41 although does so happen now the second thing that I quite like about using a\n2:51:47 tool as your final answer is that you can enforce a\n2:51:52 structured output in your answer so this is something we're stting I think the first yes the first line chain example\n2:52:01 where we were using the structured output tool of Lang chain and what that\n2:52:06 actually is the structured outputs feature of Lang chain it's actually just a tool call right so it's forcing a tool\n2:52:13 call from your LM it's just abstracted away so you don't realize that that's what it's doing but that is what it's doing\n2:52:20 so I find that structured outputs are very useful particularly when you have a\n2:52:26 lot of code around your agent so when that output needs to go Downstream into\n2:52:32 some logic that can be very useful because you can you have a reliable\n2:52:39 output format that you know is going to be output and it's also incredibly useful if you have multiple outputs or\n2:52:47 multiple fields that you need to generate for so those can be very useful\n2:52:53 now to implement this so to implement option two we need to create a final answer tool we as with our other tools\n2:53:02 we're actually going to description and you can or you cannot do this so you can\n2:53:08 you can also just return non and actually just use the generated\n2:53:14 action as the essentially what you're going to send out of your agent\n2:53:19 execution logic or you can actually just execute the tool and just pass that\n2:53:24 information directly through perhaps in some cases you might have some additional postprocessing for your final\n2:53:31 answer maybe you do some checks to make sure it hasn't said anything weird you could add that in this tool\n2:53:37 here but yeah in in this case we're just trying to pass those through directly\n2:53:43 so let's run this we've added where are we\n2:53:48 Final Answer we've added the final answer tool to our named tool mapping so our agent can now use it we redefine our\n2:53:56 agent setting tool choice to any because we're forcing the tool Choice here and let's go with what is 10 + 10 see what\n2:54:04 happens okay we get this right we can also one thing nice thing here is that\n2:54:10 we don't need to check is out up in the content field or is it in the tool course field we know it's going to be in\n2:54:15 the tool course field because we're forcing that tool use quite nice so okay we know we're using the ad tool and\n2:54:22 these are the arguments great we go or go through our process again we're going\n2:54:27 to create our tool message and then we're going to add those messages into our scratch pad or intermediate sets and\n2:54:34 then we can see again ah okay content field is empty that is expected we we're\n2:54:41 forcing tool users no way that this can be this can be or have anything inside\n2:54:46 it but then if we come down here to our to calls nice final answer arbs answer\n2:54:53 10 + 10 = 20 all right we also have this tools used where's tools used coming\n2:55:00 from okay while I mentioned before that you can add additional things or or\n2:55:06 outputs when you're using this tool use for your final answer so if you just\n2:55:11 come up here to here you can see that I asked the llm to use that Tool's use\n2:55:18 field which I defined here it's a list of strings use this to tell me what tools you used in your answer right so\n2:55:25 I'm getting the normal answer but I'm also getting this information as well which is kind of nice so that's where\n2:55:30 that is coming from see that okay so we have our actual answer here and then we\n2:55:36 just have some additional information okay and we've also defined a type here it's just a list of strings which is\n2:55:41 really nice it's giving us a lot of control over what we're outputting which is perfect that's you know when you're\n2:55:47 building with agents the biggest problem in most cases\n2:55:52 is control of your llm so here we're getting a honestly pretty unbelievable\n2:56:01 amount of control over what our LM is going to be doing which is perfect for\n2:56:06 when you're building in the real world so this is everything we need this\n2:56:13 is our answer and we would of course be passing that Downstream into whatever log\n2:56:19 our AI application would be using okay so maybe that goes directly to a front\n2:56:25 end and we're displaying this as our answer and we're maybe providing some information about okay where did this\n2:56:31 answer come from or maybe there's some additional steps Downstream where we're\n2:56:36 actually doing some more processing or Transformations but yeah we have that that's great now everything we've just\n2:56:44 done here we've been executing everything one by one and that's to help us understand what process we go through\n2:56:53 when we're building an agent executor but we're not going to want to\n2:56:59 do that all the time are we most of the time we probably want to abstract all this away and that's what we're going to\n2:57:05 do now so we're going to build essentially everything we've just taken\n2:57:11 we're going to abstra take that and Abstract it away into a custom agent\n2:57:16 executor class so let's have a quick look at what we're doing here although it's it's literally just what we we just\n2:57:23 did okay so custom maor executor we initialize it we set this m\n2:57:29 Max iterations I'll talk about this in a moment we initialize it that is going to set out chat history to just being empty\n2:57:38 okay it's a new agent there should be no chat history in this case then we actually Define our agent right so that\n2:57:44 poted logic that is going to be taking out inputs and generating what to do next AKA what tool call to do okay and\n2:57:52 we set everything as attributes of our class and then we're going to Define an\n2:57:58 invoke method this invoke method is going to take an input which just a\n2:58:03 string so it's going to be our message from the user and what it's going to do is it's\n2:58:09 going to iterate through essentially everything we just did okay until we hit\n2:58:15 the The Final Answer tool Okay so well what does that mean we have our\n2:58:21 tool call right which is we're just invoking our agent right so it's going to generate what tool to use and what\n2:58:28 parameters should go into that okay and that's a that's an AI message so we would append that to our\n2:58:36 agent stretch pad and then we're going to use the information from our tool call so the name of the tool and the ARs\n2:58:42 and also the ID we're going to use all of that information to execute our tool\n2:58:49 and then provide the observation back to our llm okay so we execute our tool here\n2:58:55 we then format the tool output into a tool message see here that I'm just\n2:59:01 using the the output directly I'm not adding that additional information there\n2:59:06 we need do need to always pass in the tool call ID so that our LM knows which\n2:59:12 output is mapped to which tool I didn't mention this before in in this video at\n2:59:17 least but that is that's important when we have multiple toour calls happening in parallel because that can happen when\n2:59:23 we have multiple toour calls happening in parallel let's say we have 10 tool calls all those responses might come\n2:59:28 back at different times so then the order of those can get messed up so we\n2:59:34 wouldn't necessarily always see that it's a AI message beginning a tool call followed\n2:59:41 by the answer to that tool call instead it might be AI message followed by like 10 different tool call responses so you\n2:59:49 need to have those IDs in there okay so then we pass our tool output back to our\n2:59:57 agent scratch pad or intermediate steps I'm sing a print in here so that we can see what's happening whilst everything\n3:00:03 is running then we increment this count number we'll talk about that in a moment\n3:00:08 so com past that we say okay if the tool name here is final answer that means we\n3:00:15 should stop okay so so once we get the final answer that means we can actually\n3:00:21 extract our final answer from the the final tool call okay and in this case\n3:00:26 I'm going to say that we're going to extract the answer from the tool call or\n3:00:33 the the observation we're going to extract the answer that was generated we're going to pass that into our chat\n3:00:40 history so we're going to have our user message is the one the user came up with followed by our answer which is just the\n3:00:47 the natur answer field and that's going to be an AI message but then we're actually going to be including all of\n3:00:53 the information so this is the the answer natural language answer and also\n3:00:59 the tools used output we're going to be feeding all of that out to some\n3:01:04 Downstream process as preferred so we have that now one thing that can happen if\n3:01:12 we're not careful is that our agent executor might may run many many times\n3:01:19 and particularly if we've done something wrong in our logic as we're building these things it can happen that maybe\n3:01:26 we've not connected the observation back up into our agent executor logic and in\n3:01:33 that case what we might see is our agent executor runs again and again and again and I mean that's fine we're going to\n3:01:38 stop it but if we don't realize straight away and we're doing a lot of llm cords\n3:01:44 that can get quite expensive quite quickly so what we can do is we can set a limit right so that's\n3:01:50 what we've done up here with this Max iterations we said okay if we go past three max iterations by default I'm\n3:01:56 going to say stop all right so that's that's why we have the count here while\n3:02:01 count is less than the max iterations we're going to keep going once we hit the number of Max iterations we stop\n3:02:08 okay so the while loop will will just stop looping okay so it just protects Us\n3:02:14 in case of that and it also potentially maybe it's Point your agent might be\n3:02:19 doing too much to answer a question so this will force it to stop and just provide an answer although if that does\n3:02:26 happen I just realize there's a bit of a fault in the logic here if that does happen we wouldn't necessarily have the\n3:02:33 answer here right so we would probably want to handle that nicely but in this\n3:02:39 scenario a very simple use case we're not going to see that happening so we\n3:02:44 initialize our custom agent executor and then we invoke\n3:02:50 it okay and let's see what happens all right there we go so that just wrapped\n3:02:56 everything into a single single invoke so everything is handled for us uh we\n3:03:03 could say okay what is 10 you know we can modify that and say 7.4 for example\n3:03:12 and that we'll go through we'll use the multiply tool instead and then we'll come back to the final answer again okay\n3:03:18 so we can see that with this custom agent executor we've built an agent and\n3:03:24 we have a lot more control over everything that is going on in here one thing that we would probably need to add\n3:03:34 in this scenario is right now I'm assuming that only one tool call will happen at once it's also why I'm asking\n3:03:39 here I'm not asking a complicated question because I don't want it to go and try and execute multiple tool Calles\n3:03:46 at once uh which which can happen so let's just try\n3:03:51 this okay so this is actually completely fine so this did just execute it one after the other so you can see that when\n3:03:59 asking this more complicated question it first did the exponentiate tool followed\n3:04:05 by the ad tool and then they actually gave us our final answer which is cool also told us we use both of those tools\n3:04:12 which it did but one thing that we should just be aware of is that from\n3:04:18 open AI open AI can actually execute multiple tool calls in parallel so by\n3:04:24 specifying that we're just using this zero here we're actually assuming that we're only ever going to be calling one\n3:04:31 tool at any one time which is not always going to be the case so you would probably need to add a little bit of exual logic there in case of scenarios\n3:04:38 if you're building an an agent that is likely to be running parallel to calls\n3:04:44 but yeah you can see here actually it's completely fine so it's running one after the other okay so with that we\n3:04:50 built our agent executor I know there's a lot to that and of course you can just\n3:04:56 use the very abstract agent executor in L chain but I think it's very good to\n3:05:01 understand what is actually going on to build our own agent executor in this case and it sets you up nicely for\n3:05:08 building more complicated or use case specific agent logic as\n3:05:14 well so that is it for this chapter in this chapter we're going to\n3:05:20 be taking a look at line chains expression language we'll be looking at the runnables the serializable and\n3:05:27 parallel of those the runable pass through and essentially how we use l\n3:05:33 cell in its full capacity now to do that well what I want to do is actually start\n3:05:40 by looking at the traditional approach to building chains in L chain so to do\n3:05:47 do that we're going to go over to the ELO chapter and open that Cur up okay so\n3:05:54 let's come down we'll do the prerequisites as before nothing measure in here the one thing that is new is Doc\n3:06:01 array because later on as you see we're going to be using this as an example of\n3:06:08 the parallel capabilities in L cell if you want to use Lim Smith you just need\n3:06:14 to add in your lime train API key okay and then let's okay so now let's dive into the\n3:06:20 traditional approach to chains in line chain so the LM chain I think is\n3:06:28 probably one of the first things introduced in line chain if I'm not wrong this take it to prompt and feeds\n3:06:33 into an l and that that's it it you can also you can add like output passing to\n3:06:40 that as well but that's optional and I don't think we're going to cover here so\n3:06:47 what that might look like is we have for example this promp template here give me a small report on topic okay so that\n3:06:54 would be our prompt template we set up as we usually do with the prom templates\n3:07:01 as we've seen before we then Define our LM need our\n3:07:07 open a key for this which as usual we would get from platform.\n3:07:13 open.com then we go ahead I'm just just showing you that you can Ino the LM there then we go ahead\n3:07:20 actually Define a output POS so we do do this I wasn't sure we did but we would\n3:07:26 then Define our LM chain like this okay so LM chain we adding our prompt adding\n3:07:32 our LM adding our alasa okay this is the traditional\n3:07:39 approach so I would then say Okay retrieve Org the generation and what's going to do it's going to give me a\n3:07:45 little report back on on rag okay t a moment but you can see that\n3:07:51 that's what we get here we can format out nicely as we\n3:07:56 usually do and we get okay look we get a nice little report however the LM chain\n3:08:02 is one it's quite restrictive right we have to have like particular parameters that have been predefined as being\n3:08:09 usable which is you know restrictive and it's also been deprecated so you know\n3:08:16 this isn't the standard way of doing this anymore but we can still use it however the preferred method to building\n3:08:23 this and building anything else really or chains in general in L chain is using El cell right and it's super simple\n3:08:30 right so we just actually take the prompt lemon Apple P that we had before and then we just chain them together\n3:08:36 with these pipe operators so the pipe operator here is saying take what is output from here and input it into here\n3:08:43 take wi's output from here and input it into here it's all it does super simple\n3:08:48 so put those together and we invoke it in the same way and we'll get the same\n3:08:54 output okay and that's what we get there is actually a slight difference on what\n3:09:00 we're getting out from there you can see here we got actually a dictionary but\n3:09:06 that is pretty much the same okay so we get that and as before we can display\n3:09:12 that in markdown with this okay so we saw just now that we have this pipe\n3:09:18 operator here it's not really\n3:09:23 standard P python syntax to use this or at least it's definitely not common it's\n3:09:29 it's it's an aberration of the intended use of python I think but anyway it\n3:09:36 does it looks cool and when you understand it I kind of get why they do\n3:09:42 because it make it does make things quite simple in comparison to what it could be otherwise so I kind get it it's\n3:09:48 a little bit weird but it's what they're doing and I'm teaching that so that's\n3:09:53 what we're going to learn so what is that pipe operator\n3:09:59 actually doing well it's as I mentioned it's\n3:10:04 taking the output from this putting it as input into into what is ever under right but how does that actually work\n3:10:12 well let's actually implement it ourselves without line chain so we're going to create this class called\n3:10:18 runnable this class when we initialize it it's going to take a function okay so this is literally a python function it's\n3:10:24 going to take that and it's going to essentially turn it into what we would\n3:10:30 call a runnable in line chain and what does that actually mean well it doesn't really mean anything it just means that\n3:10:38 when you use run the invoke method on it it's going to call that function in the\n3:10:43 way that you would have done otherwise all right so using just function you know brackets open parameters brackets\n3:10:50 closed it's going to do that but it's also going to add this method this all method now this all method in typical\n3:10:59 python syntax now this all method is essentially going to take your runnable\n3:11:06 function the one that you initialize with and it's also going to take an other function okay this other function\n3:11:13 is actually going to be a runnable I believe yes it's going to be runnable just like this and what it's going to do\n3:11:20 is it's going to run this runnable based on the output of your current runable\n3:11:28 okay that's what this or is going to do seems a bit weird maybe but I'll explain\n3:11:34 in a moment we'll see why that works so I'm going to chain a few functions\n3:11:39 together using this or method so first we're just going to turn\n3:11:45 them all into runnables Okay so these are normal functions as you can see normal python functions we then turn\n3:11:51 them into this runnable using our runnable class then look what we can do right so\n3:11:58 we we're going to create a chain that is going to be our runnable\n3:12:04 chained with another runnable chained with another runnable okay let's see what happens so we're going to invoke\n3:12:11 that chain of runnables with three so what is this going to do\n3:12:16 okay we start with five we're going to add five to three so we'll get eight\n3:12:22 then we're going to subtract five from8 to give us three again and then we're\n3:12:29 going to multiply three by five to give us 15 and we can inval that and we get\n3:12:38 15 okay pretty cool so that is interesting how does that relate to the\n3:12:44 pipe operator well that pipe operator in Python is actually a\n3:12:51 shortcut for the or method so what we just implemented is the pipe operator so\n3:12:56 we can actually run that now with the pipe operator here and we'll get the same get 15 right so that's that's what\n3:13:03 line chain is doing like under the hood that is what that pipe operator is it's\n3:13:08 just chaining together these multiple runnables as we'd call them using their own internal or operator okay which is\n3:13:17 cool I I I will give them that it's kind of a cool way of doing this creative I wouldn't have thought about it\n3:13:24 myself so yeah that is a pipe operator then we have these runnable things okay\n3:13:31 so this is a this is different to the runable I just defined here this is we Define this ourselves it's not a lang\n3:13:37 chain thing we didn't get this from Lang chain Instead This runnable Lambda\n3:13:44 object here that is actually exactly the same as what we just defined\n3:13:49 all right so what we did here this runnable this runnable Lambda is the same thing but in Lang\n3:13:58 chain okay so if we use that okay we use that to now Define three runnables from\n3:14:05 the functions that we defined earlier we can actually pair those together now using the the pipe operator you could\n3:14:12 also pair them together if you want with the or operator right\n3:14:18 so we could do what we did earlier we can invoke that okay or as we were doing\n3:14:25 originally we use pipe operator exactly the same so this runnable Lambda from line chain is just\n3:14:31 what we what we just built with the runable cool so we have that now let's\n3:14:37 try and do something a little more interesting we're going to generate a report and we're going to try and edit that report using this this\n3:14:43 functionality okay so give me a small report about topic okay we'll Z through\n3:14:48 here we're going to get our report on\n3:14:54 AI okay so we have this you can see that AI is mentioned many times in\n3:15:00 here then we're going to take a very simple function right so I'm extract\n3:15:07 fact this is basically going to take uh what is it see taking the\n3:15:14 first okay so we're actually trying to remove the introduction here I'm not sure if this actually will work as\n3:15:20 expected but it's it's fine try it anyway but then more importantly we're\n3:15:28 going to replace this word okay so we're going to replace an old word with a new word our old word going to be Ai and the\n3:15:34 word is going to be Skynet okay so we can wrap both of these functions as\n3:15:40 runable lambas okay we can add those as additional steps inside our entire chain\n3:15:46 all right so we're going to extract try and remove the introduction although I think it needs a bit more processing\n3:15:53 than just splitting here and then we're going to replace the word we need that actually to be AI run that run\n3:16:01 this okay so now we get artificial intelligence Skynet refers to the\n3:16:07 simulation of human intelligent process by machines uh we have narrow Skynet weak\n3:16:12 Skynet and strong Skynet applications of Skynet Skynet Technologies is being applied in\n3:16:18 numerous Fields including all these things scary despite potential sky that poses\n3:16:24 several challenges systems can perpetrate exist and biases it ra significant privacy\n3:16:31 concerns it can be exploited for malicious purposes okay so we have all\n3:16:37 these you know it's just a silly little example we can see also the introduction didn't work here the reason for that is\n3:16:43 because our introduction includes multiple new lines here so I would actually if I want to remove the\n3:16:50 introduction we should remove it from here I think and this is a I I would\n3:16:56 never actually recommend you do that uh because it's not it's not very flexible\n3:17:01 it's not very robust but just so I show you that that is actually working so\n3:17:07 this extract fact runnable right so now we're essentially just removing the\n3:17:14 introduction right why what do we want to do that I don't know but it's there just so you can see that we can have\n3:17:20 multiple of these runnable operations running and they can be whatever you want them to be okay it is worth knowing\n3:17:28 that the inputs to our functions here were all single arguments okay if you\n3:17:35 have function that is accepting multiple arguments you can do that the way that I would probably do it or you can do it in\n3:17:42 multiple ways one of the ways that you can do that is actually write your function to except for arguments but\n3:17:49 actually do them through a single argument so just like a single like X which would be like a dictionary or something and then just unpack them\n3:17:56 within the function and and use them as needed that's just yeah that's one way you can do it now we also have these\n3:18:02 different uh runnable objects that we can use so here we have runnable parallel and runnable pass through kind\n3:18:10 of self-explanatory to some degree so let me let just go through those so runable parallel allow you to run\n3:18:17 multiple runnable instances in parallel runnable pass through May was less\n3:18:24 self-explanatory allows us to pass a variable through to the next runnable without modifying it okay so let's see\n3:18:31 how they would work so we're going to come down here and we're going to set these two dock arrays obiously these two\n3:18:38 sources of information and we're going to need our\n3:18:43 LM to pull information from both of these sources of information in parallel which is going to look like this so we\n3:18:49 have these two sources of information Vector store a vector store B this is\n3:18:55 our dock array a and dock array B these are both going to be fed in as\n3:19:00 context into our prompt then our LM is going to use all of that to answer the\n3:19:06 question okay so to actually Implement that we have our we need an embedding\n3:19:12 model so he open our embeddings we have our vetur a a vector B they're not you\n3:19:18 know real vectors they're not full-on vectors SS here we're just passing in a very small amount of information to both\n3:19:26 so we're saying okay we're going to create an inmemory vect S using these\n3:19:31 two bits of information so when say half the information is here this would be an irrelevant piece of information then we\n3:19:37 had the relevant information which is deep seek re3 was released in December 2024 okay then we're going to have some\n3:19:45 other information in our other Vector sore again irrelevant piece here and\n3:19:50 relevant piece here okay the Deep seek V3 LM is a mixure of experts model with\n3:19:56 671 billion parameters at its largest okay so based on that we're also going\n3:20:03 to build this prompt string so we're going to pass in both of those contexts into our prompt then I'm going to ask a\n3:20:10 question we don't actually need we don't need that bit and actually we don't even\n3:20:15 need that bit what am I doing so we just need this so we have the both the contexts there and we would run them\n3:20:22 through our prompt template okay so we have our system promp template which is\n3:20:27 this and then we're just going to have okay our question is going to go into here as a user message cool so we have that and then\n3:20:36 let me make this easier to read we're going to convert both those\n3:20:41 STS to retrievers which just means we can retrieve stuff from them and we're going to use this runnable parallel to\n3:20:49 run both of these in parallel right so these are being both being run in\n3:20:55 parallel but then we're also running our question in parallel because this needs to be essentially passed through this\n3:21:02 component without us modifying anything so when we look at this here it's almost\n3:21:07 like okay the this section here would be our runable parallel and these are being\n3:21:14 running parallel but also our query is being passed through so it's almost like\n3:21:19 there's another line there which is our runable pass through okay so that's what we're doing here these running in\n3:21:25 parallel one of them is a pass through I need\n3:21:30 to run here I just realized here we're using\n3:21:35 the uh deprecated embeddings just switch it to this so L chain open\n3:21:42 AI we run that run this run that and now\n3:21:47 this is set up okay so we then put our initial so this\n3:21:56 using our runable parallel and runnable pass through that is our initial step we\n3:22:01 then have our prompt LM now pass which would being chained together with usual\n3:22:07 you know the usual type operator okay and now we're going to invoke question what architecture does\n3:22:13 the mod deep seek release in December use okay okay so for the elm to answer\n3:22:19 this question it's here to need to tell us what it needs the information about the Deep seek model that was released in\n3:22:24 December which we have specified in one half uh here and then it also needs to\n3:22:31 know what architecture that model uses which is defined in the other half over\n3:22:37 here okay so let's run this okay there we go deep SE V3 model\n3:22:44 released in December 2024 is a mix experts model with 671 billion\n3:22:49 parameters okay so mixture of experts and this many parameters pretty cool so\n3:22:55 we've put together our pipeline using elol using the pipe operator the\n3:23:01 runnables specifically we've looked at the runable parallel runable pass through and also the runable lampas so\n3:23:08 that's it for this chapter on lell and we'll move on to the next one in this\n3:23:14 chapter we're going to cover streaming and async in Lang chain now both using\n3:23:19 async code and using streaming are incredibly important components of I\n3:23:27 think almost any conversational chat interface or at least any good\n3:23:32 conversational chat interface for async if your application is not async and\n3:23:39 you're spending a load of time in your API or whatever else waiting for llm\n3:23:45 calls because a lot of those are behind apis you are waiting and your\n3:23:50 application is doing nothing because you've written synchronous code and that\n3:23:55 well there are many problems with that mainly it doesn't scale so asyn code\n3:24:00 generally performs much better and especially for AI where a lot of the\n3:24:05 time we're kind of waiting for API calls so asyn is incredibly important for that\n3:24:10 for streaming now streaming is slightly different thing so let's say I want to\n3:24:16 to tell me a story okay I'm using gbt 4 here it's a\n3:24:22 bit slower so we can achieve string we can see that token by token this text is being produced and sent to us now this\n3:24:29 is not just a visual thing this is the LM when it is generating tokens or words\n3:24:38 it is generating them one by one and and that's because these llms literally\n3:24:43 generate tokens one by one so they're looking at all of the previous tokens in order to generate the next one and then\n3:24:48 generate next one generate next one that's how they work so when we are\n3:24:54 implementing streaming we're getting that feed of tokens directly from the LM\n3:24:59 through to our you know our back end or our front end that is what we see when when we see that token by token\n3:25:06 interface right so that's one thing what one other thing that I can do that let\n3:25:12 me switch across to 40 is I can say okay we just got this story I'm going to\n3:25:18 ask are there any standard\n3:25:25 storytelling techniques to follow used above please\n3:25:32 use search okay so look we we get this very\n3:25:41 briefly there we saw that it was searching the web and the way it's not because we told it okay we told the llm\n3:25:48 to use the search tool but then the llm output some tokens to say use the search\n3:25:55 tool that is going to use a Search tool and it also would have output the token saying what that search query would have\n3:26:02 been although we didn't see it there but what the chat GPT interface is\n3:26:08 doing there so it received those tokens saying hey I'm going to use a Search tool it didn't just send us those tokens\n3:26:15 like it does with the tokens here instead it used those tokens to show us\n3:26:21 that searching the web little text box so streaming is not just the streaming\n3:26:28 of these direct tokens it's also the streaming of these intermediate steps\n3:26:34 that the llm may be thinking through which is particularly important when it\n3:26:40 comes to agents and agentic interfaces so it's also a feature thing right\n3:26:45 streaming does doesn't just look nice is also a feature then finally of course\n3:26:51 when we're looking at this okay let's say we go back to\n3:26:57 GT4 and I say okay use all of this\n3:27:04 information to generate a long story for\n3:27:10 me right and okay we are getting the first token now we know something is\n3:27:16 happening and we need start reading now imagine if we were not streaming anything here and we're just waiting\n3:27:23 right we're still waiting now we're still waiting and we wouldn't see anything we're just like oh it's just\n3:27:28 blank or maybe there's a little loading spinner so we'd still be waiting and even now we're still\n3:27:39 waiting right this is an extreme example but can you imagine just waiting\n3:27:45 for so long and not seeing anything as a user right now just now we would have\n3:27:51 got our answer if we were not streaming I mean that that would be painful as a\n3:27:56 user you you not want to wait especially in a chat interface you don't want to wait that\n3:28:01 long it's okay with okay for example deep research takes a long time to process but you know it's going to take\n3:28:08 a long time to process and it's a different user case right you're getting a report this is a chat interface and\n3:28:15 yes most messages are not going to take that long to generate we're also\n3:28:21 probably not going to be using GPT 4 depending on I don't know maybe some people still do but in some scenarios\n3:28:29 it's painful to need to wait that long okay and it's also the same for agents it's nice when you're using agents again\n3:28:36 update on okay we're using this tool it's using this tool this is how it's using them perplexity for example have a\n3:28:42 very nice example of this so okay what what's this open I founder joins morati\n3:28:49 sub let's see right so we see this is really nice it's we're using Pro search it's searching for news sharing with the\n3:28:55 results like we're getting all this information as we're waiting which is really cool and it helps to understand\n3:29:02 what is actually happening right it's not needed in all use cases but it's super nice to have those intermediate\n3:29:09 steps right so then we're not waiting and then I think this bit probably also streamed but it was just super fast so I\n3:29:15 I didn't see it but that's pretty cool so streaming is pretty important let's\n3:29:22 dive into our example okay we'll open that in cab enough we out so starting\n3:29:27 with the prerequisites same as always Lang chain optionally L Smith we'll also\n3:29:34 enter our L chain API key if you'd like to use l Smith we'll also enter our openi API key so that is platform.\n3:29:42 open.com and then as usual we can just invoke our l m right so we have that\n3:29:48 it's working now let's see how we would stream with a stream okay so whenever a\n3:29:56 method so stream is actually a method as well we could use that but it's not acing right so whenever we see a method\n3:30:02 in line chain that has a prefixed onto what would be another method that's like\n3:30:08 the async version of this so we can actually stream using async\n3:30:16 super easily using just LM a stream okay\n3:30:21 now this is just a an example in to be completely honest you probably will not\n3:30:27 be able to use this in an actual application but it's just an example and we're going to see how we would use this\n3:30:34 or how we would stream asynchronously in an application further down in this\n3:30:40 notebook so starting with this you can see here that we're getting these tokens\n3:30:45 right we're just appending it to token here we don't actually need to do that I don't think we're using this but maybe\n3:30:51 we yeah we do it here it's fine so we're just pending the tokens as they come\n3:30:56 back from our LM pending it to this we'll see what that is in a moment and\n3:31:02 then I'm just printing the token content right so the content of the\n3:31:07 token so in this case that would be l in this case it would be LP it would be Sans for so on and so on so you can see\n3:31:14 for the most part it's it's tends to be Word level but it can also be subword level as you see scent iment is one word\n3:31:23 of course so you know they get broken up in in various ways then adding this pipe character\n3:31:30 onto the end here so we can see okay where are our individual tokens then we\n3:31:36 also have flush so flush uh you can actually turn this off and it's still going to stream you're still going to\n3:31:41 see everything which going to be a bit more you can see it's kind of a it's like bit by bit when we use flush it\n3:31:49 forces the console to update what is being shown to us immediately all right\n3:31:54 so we get a much smoother um when we're looking at this it's much smoother versus when flush is\n3:32:02 not set true so yeah when you're printing that is good to do just so you can see you don't necessarily need to\n3:32:08 okay now we added all those tokens to the tokens list so we can have a look at each individual object that was returned\n3:32:15 turn to us right and this is interesting so we see that we have the AI message chunk right that's an object and then\n3:32:22 you have the content the first one's actually empty second one has that n for NLP and\n3:32:30 yeah I mean that's all we rarely need to know they're very simple objects but they're actually quite\n3:32:35 useful because uh just look at this right so we can add each one of our AI\n3:32:40 message chunks right let's see what that does it doesn't create a list it creates this right so we still just have one AI\n3:32:48 message chunk uh but it's combined the content within those AI message chunks\n3:32:55 which is kind of cool right so for example like we could remove\n3:33:01 these right and then we just see NLP so that's kind of nice little feature there\n3:33:06 I do I actually quite like that but uh you do need to just be a little bit careful because obviously you can do\n3:33:13 that the wrong way and you're going to get like a I don't know all that is some weird token salad so yeah you need to\n3:33:21 just make sure you are going to be merging those into correct order unless you I don't know unless you're doing\n3:33:28 something weird Okay cool so streaming that that was streaming from a LM let's\n3:33:34 have look at streaming with agents so we it gets a bit more complicated to be\n3:33:41 completely honest but we also need to things are going to get a bit more\n3:33:46 complicated so that we can implement this in for example an API right so is\n3:33:52 it's kind of like a necessary thing in any case so to just very quickly we're\n3:33:58 going to construct our agent executor like we did in the agent execution chapter and for that for the agent\n3:34:06 executor we're going to need tools chat prompt template llm agent and the agent H itself okay very quickly I'm not going\n3:34:13 to go through these uh in detail we just def find our tools have ADD multiply exponentiate subtract and Final Answer\n3:34:20 tool merge those into a single list of tools then we have our prompt template\n3:34:26 again same as before we just have system message we have chat history we have you query and then we have the agent scratch\n3:34:34 pad for those intermediate sets then we Define our agent using L cell L cell\n3:34:40 works quite well with both streaming and async by the way it supports both out of\n3:34:46 the box which is nice so we Define our agent then coming down here we're going\n3:34:54 to create the agan ice fter this is the same as before right so there's nothing\n3:34:59 new in here I don't think so just initialize our agent things there then\n3:35:05 it's you know We're looping through looping through yeah nothing\n3:35:11 nothing new there so we're just executing invoking our agent seeing if\n3:35:17 there's a tool call uh this is slightly we could shift this to before or after it doesn't actually matter that\n3:35:23 much so we're checking if it's final answer if not we continue X to our tools\n3:35:29 and so on Okay cool so then we can invoke\n3:35:35 that okay we go what is 10 + 10 there we go right so we have our\n3:35:42 agent executor it is working now now when we are running our agent executor\n3:35:51 with every new query if we're putting this into an API we're probably going to need to\n3:35:57 provide it with a a fresh callback Handler okay so this is the corat Handler that's going to handle taking\n3:36:04 the tokens that are being generated by a LMO agent and giving them to some other\n3:36:10 piece of code like for example the the streaming response for a API\n3:36:16 and our Corbat Handler is going to put those tokens in a queue in our case and\n3:36:22 then our for example the streaming object is going to pick them up from the queue and put them wherever they need to\n3:36:28 be so to allow us to do that with every\n3:36:33 new query or is needing to initialize everything when we actually initialize\n3:36:39 our agent we can add a configural field to our llm okay so we set the configural\n3:36:45 Fields here oh also one thing is that way we set streaming equal to true that's very manting but just so you see\n3:36:52 that there we do do that so we add some configurable fields to our LM which means we can basically pass an object in\n3:37:00 for these on every new invocation so we set our configurable\n3:37:05 field it's going to be called callbacks and we we just add a description right there's nothing more to it so this will\n3:37:12 now allow us to provide that field when we're invoking our agent okay now we\n3:37:21 need to Define our callback Handler and as I mentioned what is basically going\n3:37:26 to be happening is this callback Handler is going to be passing tokens into our a\n3:37:32 sync IO Q object and then we're going to be picking them up from the que\n3:37:37 elsewhere okay so we can call it a q callback Handler okay and that is\n3:37:42 inhering from the async Callback Handler cuz we want all this to be done asynchronously because we're we're\n3:37:47 thinking here about okay how do we Implement all this stuff within apis and actual real world code and we we do want\n3:37:55 to be doing all this in aing so let me execute that and I'll just explain a little bit of what we're looking at so\n3:38:01 we have the initialization right it's nothing nothing specific here we just\n3:38:07 what we really want to be doing is we want to be setting our Q object assigning that to the class attributes\n3:38:14 and then there's also this Final Answer scene which we're setting to fults so\n3:38:19 what we're going to be using that for is we our llm will be streaming tokens\n3:38:26 towards whilst it's using its tool calling and we might not want to display those immediately or we remember to\n3:38:32 display them in a different way so by setting this Final Answer scene to\n3:38:37 false whilst our LM is outputting those tool tokens we can handle them in a\n3:38:44 different way and then as soon as we see that it's done with the tool calls and it's on to the final answer which is\n3:38:49 actually another tool call but once we see that it's on to the final answer tool call we can set this true and then\n3:38:56 we can start processing our tokens in a you know different way essentially okay so we have that then we have this AER\n3:39:04 method this is required for any async generator object\n3:39:11 so what that is going to be doing is going to iterating through right it's a generator it's going to be going\n3:39:16 iterating through and it's going saying okay if our queue is empty right this is the que that we set up here if it's\n3:39:22 empty wait a moment right we use the Sleep Method here and this is an async\n3:39:28 Sleep Method this is super important we're using we are waiting for an asynchronous sleep all right so whilst\n3:39:35 we're whilst we're waiting for that 0.1 seconds our our code can be doing other things\n3:39:42 right that that is important if we if we use I think the standard is time dos sleep that is not asynchronous and so it\n3:39:50 will actually block the thread for that 0. one seconds so we don't want that to happen generally our Q should probably\n3:39:57 not be empty that frequently given how quickly uh tokens are going to be added to the queue so the only way that this\n3:40:05 would potentially be empty is maybe our LM stops maybe there's like a connection\n3:40:11 Interruption for it you know a brief second or something and no tokens are added so in that case we don't actually\n3:40:17 do anything we don't keep the checking the queue we just wait a moment okay and then we check again now if it was empty\n3:40:26 we wait and then we continue onto the next iteration otherwise it probably won't be empty we get whatever is from\n3:40:33 our inside our queue we get that out pull it out then we say Okay if that\n3:40:40 token is a done token we're going to return so we're going to stop this\n3:40:45 generator right we're finished otherwise if it's something else we're going to yield that token\n3:40:52 which means we're we're returning that token but then we're continuing through that loop again\n3:40:57 right so that is our generator logic then we have some other methods here\n3:41:04 these are L these are line chain specific okay we have on LM new token\n3:41:10 and we have on LM end starting with on LM new token this is basically when an\n3:41:15 LM returns a token to us line chain is going to run or execute this method okay\n3:41:22 this is the method that will be called what this is going to do is it's going to go into the keyword arguments and\n3:41:28 it's going to get the chunk object so this is coming Fromm if there is something in that chunk it's going to\n3:41:36 check for a final answer tool call First okay so we get our tool calls and we say\n3:41:43 if the name within our ch chunk right probably this will be emptying most of\n3:41:48 the tokens we return right so you remember before when we're looking at the chunks here this is what we're\n3:41:54 looking at right the content for us is actually always going to be empty and instead we're actually going to get the\n3:41:59 additional keyword objects here and inside there we're going to have our tool calling our tool calls as we s in\n3:42:06 the the previous videos right so that's what we're extracting we're extracting that information that's why we're going\n3:42:12 additional keyword ARS right and get those tool the tool call information\n3:42:18 right or it will be nonone right so if if it is nonone I don't think it ever\n3:42:24 would be none to be honest it would be strange if it's none I think that means something would be wrong okay so here\n3:42:29 we're using the wars operator so the wars operator what it's doing here is whil we're checking the if logic here\n3:42:38 whilst we do that it's also assigning whatever is inside this it's assigning\n3:42:43 over to Tool Calles and then with the if we're checking whether tool cause is\n3:42:49 something or nonone right because we're using get here so if if this get\n3:42:54 operation fails and there is no tool calls this object here will be equal to\n3:42:59 none which gets assigned to Tool calls here and then this this if none will\n3:43:05 return false and this logic will not run okay and it will just continue if this\n3:43:11 is true so if there is something returned here we're going to check if that's something returned is using the\n3:43:16 function name or tool name final answer if it is we're going to set that final answer see equal to True otherwise we're\n3:43:24 just going to add our chunk into Q okay we use put no weight here because we're\n3:43:30 we're using async otherwise if you were not using async I think you might just put weight or maybe even put put no okay\n3:43:39 you you use put if it's a synchronous code but I I don't think I've ever implemented a synchronous so it would\n3:43:46 actually just be put no weight for Asing okay and then return so we have\n3:43:52 that then we have on llm end okay so this is when line chain sees that the\n3:43:59 llm has returned or indicated that it is finished with the response line chain\n3:44:06 will call this so you you have to be aware that this\n3:44:12 will happen multiple times during an agent execution because if you think within our agent executor we're hitting\n3:44:21 the LM multiple times we have that first step where it's deciding oh I'm going to use the add tool or the multiply tool\n3:44:28 and then that response gets back towards we execute that tool and then we pass\n3:44:33 the output from that tool and all the original user query in the chat history pass that back to our LM again all right\n3:44:39 so that's another call to our LM that's going to come back it's going to finish it's going to give us something else\n3:44:45 right so there's multiple llm cods happening throughout our agent execution\n3:44:50 logic so this on LM call will actually get called at the end of every single one of those llm calls now if we get to\n3:44:59 the end of our llm call and it was just a it was a tool invocation so we had the you know it\n3:45:05 called the ad tool we don't want to put the done token into our Cube because\n3:45:13 when the done token is added to our Cube we're going to stop iterating\n3:45:19 okay instead if it was just a tool call we're going to say step end right and\n3:45:24 we'll actually get this token back so this is useful on for example the front\n3:45:29 end you could have okay I've I've used the ad tool the these are the parameters\n3:45:35 and it's the end of the step so you could have that your tool callers being used on some front end and then as soon\n3:45:42 as it sees step end it knows okay we're done with here was a response right and and it can just show you that and we're\n3:45:49 going to use that we'll see that soon but let's say we get to the final an tool we're on the final answer tool and\n3:45:55 then we get this signal that the llm has finished then we need to stop iterating\n3:46:02 otherwise our our streaming generator is just going to keep going forever right nothing's going to stop it or maybe it\n3:46:09 will time out I don't think it will though so at that point we need to send okay\n3:46:15 stop right we need to say we're done and then that will that will come back to here to our a iterator and to our asnc\n3:46:23 iterator and it will return and stop the generator okay so that's the core logic\n3:46:31 that we have inside there I know there's a lot going on there it's but we need all of this so it's important to be\n3:46:38 aware of it okay so now let's see how we might actually call our agent with all\n3:46:45 of this streaming uh in this way so we're going to initialize our queue\n3:46:52 we're going to use that to initialize a streamer okay using the the custom streamer that we just sell custom\n3:46:57 callback Handler whatever you want to call it okay then I'm going to define a function so this is an asynchronous\n3:47:04 function it has to be if if we're using async and what it's going to do is it's going to call our agent with a config\n3:47:12 here and we're going to pass it that call the the Callback which is the streamer right note here I'm not calling\n3:47:19 the agent executor I'm just calling the agent right so the uh if we come back up\n3:47:24 here we're calling this all right so that's not going to include all tool execution logic and\n3:47:30 importantly we're calling the agent with the config that uses callbacks right so\n3:47:37 this this configurable field here from our LM is actually being fed through it propagates through to our agent object\n3:47:43 as well to the runable so realizable all right so that's what we're executing here we see agent with\n3:47:49 config and we're passing in those callbacks which is just one actually okay so that sets up our agent and then\n3:47:57 we invoke it with a stram okay like we did before and we're just going to return everything so let's uh run that\n3:48:05 okay and we see all the token or the chunk objects that are being returned and this is useful to understand what\n3:48:12 we're actually doing up here right so when we're doing this chunk\n3:48:17 message additional C keyword arguments right we can see that in here so this would be the chunk message object we get\n3:48:24 the additional keyword objects go into tool calls and we get the information here so we have the ID for that tool\n3:48:30 call as we saw in the previous chapters then we have our function right so the\n3:48:37 function includes the name right so we know what tool we're calling from this first chunk but we don't know the\n3:48:43 arguments right those arguments are going to be streamed to us so we can see them begin to come through in the next\n3:48:50 chunk so the next chunk is just it's just a first token for for the ad\n3:48:55 function right and we can see these all come together over multiple steps and we\n3:49:01 actually get all of our arguments okay that's pretty cool\n3:49:07 so actually one thing I would like to show you here as well so if we just do token\n3:49:13 equals token sorry and we\n3:49:20 do tokens. pen\n3:49:26 token okay we have all of our tokens in here now right you see that they're all\n3:49:31 AI message chunks so we can actually add those together right so let's we'll go with\n3:49:37 these here and based on these we're going to get all the arguments okay so this is kind of interesting so it's one\n3:49:45 until I think like the second to last\n3:49:51 maybe right so we have these and actually we just want to add those\n3:49:56 together so I'm going to go with tokens one I'm just going to go\n3:50:04 four uh four token in we're going to go from\n3:50:10 the second onwards I'm going to TK plus\n3:50:16 token right and let's see what TK looks like at the end here\n3:50:23 TK okay so now you see I kind of merged with all those um arguments here sorry\n3:50:30 plus equal okay so run that and you can see here that it's merged those arguments it\n3:50:36 didn't get all of them so I kind of missed some at the end there but it's merging them right so we can see that that logic where it's you know before it\n3:50:43 was adding the content from various trunks it also does the same for the\n3:50:49 other parameters within your trunk object which is is I I think it's pretty cool you can see here the name wasn't\n3:50:55 included that's because we started on token one or on token zero where the name was so if we actually started from\n3:51:02 token zero and let's just let's just pull them in there right so from one\n3:51:10 onwards we're going to get a complete AI message chunk which includes the name\n3:51:16 here and all of those arguments and you you'll see also here right populate everything which is pretty\n3:51:23 cool okay so we have that now based on this we're going to want to modify our\n3:51:29 custom agent executor because we're streaming everything right so we want to add\n3:51:35 streaming inside our agent executor which we're doing here right so this is\n3:51:40 async death stream and we're sharing async for token in the a string okay so\n3:51:47 this is like the very first instance if output is none we're just going to be adding our token so the the chunk sorry\n3:51:56 to our output like the first token becomes our output otherwise we're just\n3:52:02 appending our tokens to the output okay if the token content is empty which it\n3:52:08 should be right because we're using tool cores all the time we're just going to print content okay I just added these as\n3:52:15 so we see like print everything I just want to want to be able to see that I wouldn't expect this to run because\n3:52:22 we're saying it has to use tool calling okay so within our agent if we come up\n3:52:28 to here we said tool Choice any so it's been forced to use tool calling so it should never really be returning\n3:52:34 anything inside the content field but just in case it's there right so we'll we'll see if that is actually true then\n3:52:40 we're just getting out our tool CES information okay from our trunk and we're going to say okay if there's\n3:52:46 something in there we're going to print what is in there okay and then we're going to extract our tool name if there\n3:52:51 is some if there is a tool name I'm going to show you the tool name then we're going to go to ORS and\n3:52:57 if the ORS are not empty we're going to see what we get in there okay and then\n3:53:03 from all of this we're actually going to we merge all of it into our AI message right because we're merging everything\n3:53:09 as we're going through we're merging everything into outputs as I showed you before okay cool and then we just\n3:53:14 awaiting our stream that will like kick it off okay and then we do the the standard agent execut stuff again here\n3:53:21 right so we're just pulling out tool name Tool logs tool call ID and then we're using all that to execute our tool\n3:53:27 here and then we're creating a new tool message and passing that back in and then also here I move the break for The\n3:53:35 Final Answer into the final step so that is our custom Asian executor with streaming and let's see what let's see\n3:53:42 what it does okay St for b equal true so we see all those print\n3:53:49 statements okay so you can kind of see it's a little bit messy but you can see\n3:53:55 we have tool calls that had some stuff inside it had add here and what we're\n3:54:01 printing out here is we're printing out the full AI message chunk with tool calls and then I'm just printing out\n3:54:06 okay what are we actually pulling out from from that so these are actually coming from the same thing okay and then\n3:54:12 the same here right so we're looking at full message and then we're looking okay we're getting this argument out from it\n3:54:19 okay so we can see everything that is being pulled out you know chunk by chunk\n3:54:25 or token by to token and that's it okay so we could just get everything like that however right so I'm I'm printing\n3:54:32 everything so we can see that it's streaming what if I don't print okay so we're setting the bo or by default the\n3:54:39 both is equal to false here so what happens if we invoke now plus\n3:54:47 C okay cool we got\n3:54:54 nothing so the reason we got nothing is because we're not\n3:54:59 printing but we don't if you are if you're building an an API for example\n3:55:06 you're you're pulling your tokens through you can't print them to your\n3:55:14 like like a front end or or print them as to the output of your API printing\n3:55:20 goes to your terminal right your console window it doesn't go anywhere else instead what we want to do is we\n3:55:28 actually want to get those tokens out right but if but how do we do that all\n3:55:33 right so we we printed them but another place that those tokens are is in our que all right because we set them up to\n3:55:41 go to the que so we can actually pull them out of\n3:55:47 our queue whilst our agent executor is running and then we can do whatever we\n3:55:52 want with them because our code is async so it can be doing multiple things at the same time so whilst our code is\n3:55:58 running the agent executor whilst that is happening our code can also be pulling out from our\n3:56:05 queue tokens that are in there and sending them to like an API for example\n3:56:12 right or whatever Downstream you you have so let's see what that looks like\n3:56:17 we start by just initializing our que initializing our streamer with that que then we create a task so this is\n3:56:24 basically saying okay I I want to run this but don't run it right now I'm not ready yet the reason that I say I'm not\n3:56:31 ready yet is because I also want to Define here my async Loop which is going\n3:56:37 to be printing those tokens right but this is async right so we we set this up\n3:56:43 this is like get ready to run this because it is async this is running right this is just running like it there\n3:56:50 it's already running so we get this we continue we continue this none of this is actually executed\n3:56:57 yet right only here when we await the task that we set up here only then does\n3:57:04 our agent executor run and our async object here\n3:57:11 begin getting tokens right and here again printing but I don't need to print\n3:57:16 I could I could have like a let's say where this is within an API or\n3:57:22 something let's say I'm I'm saying okay send token to\n3:57:29 XYZ token right that's sending up token somewhere or if we're maybe we're yielding this to our some sort streamer\n3:57:37 object within our API right we can do whatever we want with those tokens okay I'm just printing them because I want to\n3:57:44 see them okay but just important here is that we're not printing them within our\n3:57:50 agent executor we're printing them outside the agent executor we've got them out and we can put them wherever we\n3:57:56 want which is perfect when you're building an actual sort real world use KS we using an API or something else\n3:58:02 okay so let's run that let's see what we get look at that we get all of the\n3:58:07 information we could need and a little bit more right because now we're using the agent executor and now we can also\n3:58:15 see oh we have this step end right so I know all I I know just from looking at this right this is my first tool use so\n3:58:24 what tool is it let's have a look it's the add tool and then we have these arguments I can then pass them right\n3:58:31 Downstream then we have the next tool use which is here down here so we can\n3:58:38 then pass them in the way that we like so that's pretty cool\n3:58:45 let's I mean let's see right so we're getting those fers out can we can we do\n3:58:50 something with them before I before I print them and show them yes let's see\n3:58:55 okay so we're now modifying our our Loop here same stuff right we're still\n3:59:01 initializing our queue initializing our streamer initializing our task okay and we're still doing this aing for token\n3:59:08 streamer okay but then we're doing stuff with our tokens so I'm saying okay if if we're on\n3:59:15 stream end I'm not actually going to print stream end I'm going to print new line okay otherwise if we're getting a tool\n3:59:23 call here we're going to say if that tool call is the tool name I am going to\n3:59:29 Sprint calling tool name okay if it's the arguments I'm going to print the\n3:59:35 tool argument and I'm going to end up with nothing so that we don't go to a new line so we're actually going to be\n3:59:41 streaming everything okay so let's just see what this looks\n3:59:47 like oh my bad I just added\n3:59:52 that okay you see that so it go it goes very\n3:59:58 fast so it's kind of hard to see it I'm going to slow it down so you can see so you can see that we as soon as we get\n4:00:04 the toour name we stream that we're calling the add tool then we stream token by token the actual Arguments for\n4:00:11 that tool then for the next one again we do the same we're calling this tool name then we're streaming token by token\n4:00:18 again we're processing everything Downstream from outside of the agent\n4:00:24 executor and this is an essential thing to be able to do when we're actually implementing streaming and acing and\n4:00:32 everything else in an actual application so I know that's a lot but it's\n4:00:39 important so that is it for our chapter on streaming and Asing\n4:00:44 I hope this all been useful thanks now we're on to the final Capstone chapter\n4:00:50 we're going to be taking everything that we've learned so far and using it to build a actual chat application now the\n4:00:59 chat application is what you can see right now and we can go into this and ask some pretty interesting questions\n4:01:05 and because it's an agent because as iess is tools it will be able to answer them for us so we'll see inside our\n4:01:12 application that we can ask questions s that require tool use such as this and\n4:01:17 because of the streaming that we've implemented we can see all this information real time so we can see that sub API tool is being used these are the\n4:01:24 queries we saw all that was in parallel as well so each one of those tools were\n4:01:29 being used in parallel we modified the code a little bit to enable that and we\n4:01:35 see that we have the answer we can also see the structured output being used here so we can see our answer followed\n4:01:41 by the tools used here and then we could ask followup questions as well because this is conversational so we say how is\n4:01:48 the weather in each of those\n4:02:00 cities okay that's pretty cool so this is what we're going to be building we\n4:02:05 are of course going to be focusing on the API the back end I'm not front end engineer so I can't take you through\n4:02:11 that but the code is there so for those of you that do want to go through the front end code you can of course go and\n4:02:17 do that but we'll be focusing on how we build the API that powers all of this\n4:02:23 using of course everything that we've learned so far so let's jump into it the first thing we going to want to do is\n4:02:29 clone this repo so we'll copy this URL this is repo orelio Labs line chain\n4:02:36 course and you'll just clone your repo like so I've already done this so I'm\n4:02:42 not going to do it again instead I'll just navigate to the line chain course repo now there's a few\n4:02:50 setup things that you do need to do all of those can be found in the read me so\n4:02:56 we just open a new tab here and I'll open the read me okay so\n4:03:02 this explains everything we need we have if you were running this locally already you will have seen this or you will have\n4:03:09 already done all of this but for those of you that haven't we go through quickly now so you will need to install\n4:03:17 the UV Library so this is how we manage our pyth environment our packages we use\n4:03:24 UV on Mac you would install it like so if you're on Windows or Linux just\n4:03:31 double check how you'd install over here once you have installed this you then go\n4:03:37 to install python so UV python install then we want to to create our VM our\n4:03:45 virtual environment using that version of python so the VM\n4:03:52 here then as you can see here we need to activate that virtual environment which\n4:03:57 I did miss from here so let me quickly add that so you just run that for me I'm\n4:04:03 using fish so I just add fish onto the end there but if you're using bash or zsh I think you can you can just run\n4:04:09 that directly and then finally we need to sync I install all of our packages using UV\n4:04:17 sync and you see that will install everything for you great so we have that\n4:04:24 and we can go ahead and actually open cursor or vs code and then we should\n4:04:31 find ourselves within cursor or vs code so in here you'll find a few things that\n4:04:39 we will need so first is environment variables so we can come over to here\n4:04:45 and we have open AI API key larning chain API key and ser API API key create\n4:04:50 a copy of this and you would make this your EMV file or if you want to run it\n4:04:57 with Source you can well I like to use mac. EnV when I'm on Mac and I just add\n4:05:03 export onto the start there and then enter my API Keys now I actually already\n4:05:09 have these in this local. EMV file which over in my terminal I'll just activate with Source\n4:05:17 again like that now we'll need that when we are running our API and application\n4:05:23 later but for now let's just focus on understanding what the API actually\n4:05:29 looks like so navigating into the 09 Capstone chapter we'll find a few things\n4:05:36 what we're going to focus on is the API here and we have a couple of notebooks that help us just understand okay what\n4:05:44 are we actually doing here so let me give you a quick overview of the API\n4:05:49 first so the API we're using fast API for this we have a few functions in here\n4:05:54 the one we'll start with is this okay so this is our post Endo for invoke and\n4:06:01 this essentially sends something to our llm and begins a streaming response so\n4:06:07 we can go ahead and actually start the API and we can just see what this looks like so we'll go into chapter 09 caps\n4:06:14 there and API after setting our environment variables here and we just want to do UV\n4:06:20 run uicorn main colon app reload we don't need to reload but if we're\n4:06:26 modifying the code that can be useful okay and we can see that our API is now running on Local Host Port\n4:06:34 8000 and if we go to our browser we can actually open the dots for our API so we\n4:06:42 go to 8,000 slash dos okay we just see that we have that single invoke method\n4:06:48 it stripes the content and it gives us a small amount of information there now we could try\n4:06:55 out here so if we say say hello we can\n4:07:00 run that and we'll see that we get a response we get this okay now the thing\n4:07:09 that we're missing here is that this is actually being streamed back to us okay so this is not a just a direct response\n4:07:16 this is a stream to see that we're going to navigate over to here to this streaming test notebook and we'll run\n4:07:25 this so we are using request here we are not just doing a you know the standard\n4:07:32 post request because we want to stream the output and then print the output as we are receiving them okay so that's why\n4:07:40 this looks a little more complicated than just a typical request. post or request. getet so what we're doing here\n4:07:48 is we're starting our session which is our our post request and then we're just\n4:07:53 iterating through the content as we receive it from that request when we\n4:07:59 receive a token because sometimes this might be non we print that okay and we\n4:08:04 have that flush equals TRS we have used in the past so let's define that and\n4:08:10 then let's just ask a simple question what is 5 + 5\n4:08:16 okay and we we saw that that was it was pretty quick so it generated this response first and then it went ahead\n4:08:23 and actually continued streaming with all of this okay and we can see that there\n4:08:29 these special tokens are being provided this is to help the front end basically\n4:08:35 decide okay what should go where so here where we're showing these multiple steps\n4:08:43 of tool use and the parameters the way the front end is deciding how to display\n4:08:48 those is it's just it's being provided the single stream but it has the SE tokens has a SE has se name then it has\n4:08:57 the parameters followed by the sort of ending of the step token and it's looking at each one of these and then\n4:09:03 the one step name that it treats differently is where it will see The Final Answer step name when it sees the\n4:09:10 final answer step name rather than displaying this tool interface it instead begins streaming the tokens\n4:09:16 directly at like typical chat interface and if we look at what we actually get\n4:09:22 in our final answer it's not just the answer itself right so we have the answer here this is streamed into that\n4:09:31 typical chat output but then we also have tools used and then this is added\n4:09:36 into the little boxes that we have below the chat here so there's quite a lot\n4:09:42 going on just within this little stream now we can try with some other questions\n4:09:47 here so we can say okay tell me about the latest news in the world you can see that there's a little bit of a wait here\n4:09:53 whilst it's waiting to get the response and then yeah it's streaming a lot of stuff quite quickly okay so there's a\n4:10:00 lot coming through here okay and then we can ask other questions like okay this\n4:10:05 one here how called is in Osa right now is five mtip by five right so these two\n4:10:10 are going to be executed in parallel and then it will after it has the answers for those the agent will use the another\n4:10:18 multiply tool to multiply those two values together and all of that will get streamed okay and then as we saw earlier\n4:10:26 we have the what is the current Daye and time in these places same thing so three questions three questions here what is\n4:10:33 the current date and time in Dubai what is the current date and time in Tokyo and what is the current date and time in Berlin those three questions get\n4:10:41 executed in parallel against St I search at all and then all answers get returned\n4:10:47 within that final answer okay so that is how our API is working now let's dive a\n4:10:55 little bit into the code and understand how it is working so there are a lot of\n4:11:01 important things here there's some complexity but at the same time we've tried to make this as simple as possible\n4:11:07 as well so let's just fast API syntax here with the app post invoke so our\n4:11:13 invoke endpoint we consume some some content which just a string and then if you remember from the agent execut a\n4:11:21 deep dive which is what we've implemented here or a modified version of that we have to initialize our asyn q\n4:11:30 and our streamer which is the Q coreback Handler which I believe is exactly the same as what we defined in that earlier\n4:11:37 chapter there's no differences there so we Define that and then we return and\n4:11:43 this streaming response object right again this is a fast API thing this is so that you are streaming a response\n4:11:50 that streaming response has a few attributes here which again are fast API\n4:11:55 things or just generic API things so some headers giving instructions to the\n4:12:01 API and then the media type here which is text event stream you can also use I\n4:12:06 think it's text plane possibly as well but I believe this standard here would\n4:12:11 be to use event screen and then the more important part for us is this token\n4:12:17 generator okay so what is this token generator well it is this function that\n4:12:23 we defined up here now if you again if you remember that earlier chapter at the\n4:12:28 end of the chapter we set up a a for Loop where we were printing out\n4:12:34 different tokens in various formats so we kind of pro postprocessing them before deciding how to display them\n4:12:42 that's exactly what doing here so in this block here We're looping through\n4:12:50 every token that we're receiving from our streamer We're looping through and we're just saying okay if this is the\n4:12:57 end of a step we're going to yield this end of Step token which we we saw here\n4:13:03 okay so it's this end of end of St token there otherwise if this is a tool call\n4:13:10 so again we've got that W operator here so what we're doing is saying okay get the tool calls out from our current\n4:13:18 message if there is something there so if this is not nonone we're going to execute what inside here and what is\n4:13:25 being executed inside here is we're checking for the tool name if we have the tool name we return this okay so we\n4:13:32 have the start step token the start of Step name token the tool name or set\n4:13:39 name whichever those you want to call it and then the end of the set name token\n4:13:45 okay and then this of course comes through to the front end like that okay that's what we have there\n4:13:52 otherwise we should only be seeing the tool name returned as part of first token for every step after that it\n4:13:59 should just be tool arguments so in this case we say okay if we have those tool\n4:14:05 or function arguments we're going to just return them directly so then that is the part that would stream all of\n4:14:11 this here okay like these would be individual tokens right for example\n4:14:16 right so we might have the open curly brackets followed by query could be a token latest could be a token world\n4:14:24 could be a token news could be a token Etc okay so that is what is happening there this should not get executed but\n4:14:31 we have a we just handle that just in case so we have any issues with tokens\n4:14:37 being returned there we're just going to print as error and we're going to continue with the streaming but that\n4:14:43 should not really be happening cool so that is our like token streaming Loop\n4:14:51 now the way that we are picking up tokens from our stream object here is of\n4:14:56 course through our agent execution logic which is happening in parallel okay so\n4:15:02 all of this is asynchronous we have this async definition here so all of this is happening asynchronously so what has\n4:15:09 happened here is here we have created a task which is the agent ex you to invoke\n4:15:16 and we passing our content we passing that streamer which we're going to be pulling tokens from and we also set\n4:15:21 Theos to true we can actually remove that but that would just allow us to see\n4:15:27 additional output in our terminal window if we want it I don't think there's\n4:15:33 anything particularly interesting to look at in there but particularly if you are debugging that can be useful so we\n4:15:41 create our task here but this does not begin the task right this is a asyn iio\n4:15:47 create task but this does not begin until we await it down here so what is\n4:15:53 happening here is essentially this code here is still being run and like a we're\n4:16:00 in an asynchronous Loop here but then we await this task as soon as we await this task tokens will start being placed\n4:16:07 within our que which then get picked up by the streamer object here so then this\n4:16:13 begins receiving tokens I know asyn code is always a little bit more confusing\n4:16:21 given the strange order of things but that is essentially what is happening\n4:16:26 you can imagine all this is essentially being executed all at the same time so\n4:16:31 we have that is there anything else to go through here I don't think so it's all sort of boiler plates stuff for fast\n4:16:37 API rather than the actual AI code itself so we have that that's our\n4:16:43 streaming function now let's have a look at the agent code itself okay so agent code where would\n4:16:50 that be so we're using this agent executor invoke and we're importing this\n4:16:56 from the agent file so we can have a look in here for this now you can see\n4:17:02 straight away we're pulling in our API Keys here just yeah make sure that you\n4:17:07 do have those now all of our C okay this is what we've seen before\n4:17:13 in that agent execut to Deep dive chapter this is all practically the same\n4:17:20 so we have our LM we've set those configurable fields as we did in the\n4:17:26 earlier chapters that configurable field is for our callbacks we have our prompt this has been modified a little bit so\n4:17:35 essentially just telling it okay make sure you use the tools provided we say You must use the final\n4:17:41 answer tool to provide a final answer to the user and one thing that I added that I notice every now and again so I have explicitly\n4:17:48 said Ed to answer to users current question not pre-used questions so I\n4:17:54 found with this setup it will occasionally if I just have a little bit of small talk with the agent and\n4:18:01 beforehand I was asking questions about okay like what was the weather in this place or that place the agent will kind\n4:18:07 of hang on to those previous questions and try and use a tool again to answer and that is just something that you can\n4:18:13 more or less prompt out of it okay so we have that this is all exactly the same\n4:18:19 as before okay so we have our chat history to make this conversational we have our human message and then our\n4:18:25 agent scratchpad so that agent can think through multiple tool use messages great\n4:18:31 so we also have the article class so this is to process results from Sur\n4:18:39 API we have our Ser API function here I will talk about that a little more in a\n4:18:44 moment because this is also a little bit different to what we covered before what we covered before with C API if you\n4:18:51 remember was synchronous because we're using the Ser API client directly or the\n4:18:57 Ser API tool directly from line chain and because we want everything to be\n4:19:02 asynchronous we have had to recreate that tool in a asynchronous fashion\n4:19:09 which we'll talk about a little bit later but for now let's move on from that we see our final answer being used\n4:19:18 here so this is I think we defined the exact same thing before probably in that\n4:19:23 deep dive chapter again where we have just the answer and the tools that have been used great so we have that one thing\n4:19:31 that is a little different here is when we are defining our name to Tool\n4:19:38 function so this takes a tool name and it Maps it to a tool to function when we\n4:19:45 have synchronous tools we actually use tool Funk here okay so rather than tool\n4:19:52 cartin it would be tool Funk however we are using a synchronous\n4:19:58 tools and so this is actually tool co-routine and this is why this is why\n4:20:04 if you if you come up here I've made every single tool asynchronous now that is not really NE\n4:20:12 for a tool like final answer because there is no there's no API calls happening an API call is a very typical\n4:20:19 scenario where you do want to use async because if you make an API call with a synchronous function your code is just\n4:20:26 going to be waiting for the response from the API while the API is processing\n4:20:32 and doing whatever it's doing so that is an ideal scenario where you would want to use async because rather\n4:20:39 than your code just waiting for the response from the API it can instead go\n4:20:44 and do something else whilst it's waiting right so that's an ideal scenario where you'd use async which is\n4:20:50 why we would use it for example with a Ser API tool here but for final answer\n4:20:55 and for all of these calculator tools that we built there's actually no need\n4:21:02 to have these as async because our code is just running through its executing\n4:21:07 this code there's no waiting involved so it doesn't necessarily make sense have these a synchronous however by making\n4:21:15 them asynchronous it means that I can do tool care routine for all of them rather than saying oh if this tool is\n4:21:22 synchronous use tool. Funk whereas if this one is async use tool. cartin so\n4:21:28 just simplifies the code for us a lot more but yeah not directly necessary but\n4:21:34 it does help us write cleaner code here this is also true later on because we actually have\n4:21:40 to await our tool code which we can see over here right so we\n4:21:47 have to await those tool calls that would get Messier if we were using the\n4:21:53 like some sync tools some async tools so we have that we have our Q callback\n4:21:59 Handler this is again that's the same as before so I'm not going to go through\n4:22:04 I'm not going to go through that we covered that in the earlier Deep dive chapter we have our execute tool\n4:22:09 function here again that is a synchronous this just helps us you know clean up code a little bit this would I\n4:22:16 think in the Deep dive chapter we had this directly placed within our agent\n4:22:21 executor function and you can do that it's fine it's just a bit cleaner to kind of pull this out and we can also\n4:22:28 add more type annotations here which I like so execute tool expects us to\n4:22:33 provide an AI message which includes a tool call within it and it will return\n4:22:38 as a tool message okay agent exor this\n4:22:43 is all the same as before and we're actually not even using verose here so we could fully remove it but I I will\n4:22:49 leave it of course if you would like to use that you can just add a ifos and then log or print some stuff where you\n4:22:57 need it okay so what do we have in here we have our streaming function so this\n4:23:03 is what actually calls our agent right so we have a query this will\n4:23:10 call our agent just here and we could even make this a little clearer so for example this could be\n4:23:17 configured agent because this is this is not the response this is a configured\n4:23:23 agent so I think this is may be a lot clearer so we are configuring our agent with our callbacks okay which is just\n4:23:30 our streamer then we're iterating through the tokens are returned by our\n4:23:35 agent using a stream here okay and as we are iterating through this because we\n4:23:42 pass our streamer to the Callback here what that is going to do is every single\n4:23:48 token that our agent returns is going to get processed through our Q callback\n4:23:56 Handler here okay so this on LM new token on LMN these are going to get\n4:24:03 executed and then all of those tokens you can see here I'll pass to our Q okay\n4:24:09 then we come up here and we have this a it so that this aor method here is used\n4:24:15 by our generator over in our API is used by this token\n4:24:21 generator to pick up from the queue the tokens that have been put in the queue\n4:24:28 by these other methods here okay so it's putting tokens into the queue and\n4:24:34 pulling them out with this okay so that is just happening in\n4:24:39 parallel as well as this code is running here now the reason that we extract the\n4:24:45 tokens out here is that we want to pull out our tokens and we append them all to\n4:24:50 our outputs now those outputs that becomes a list of AI messages which are\n4:24:57 essentially the AI telling as what tool to use and what parameters to pass to each one of those tools this is very\n4:25:05 similar to what we covered in that deep dive chapter but the one thing that I have modified here is I've enabled us to\n4:25:12 use parallel tool calls so that is what we see here with this these four lines\n4:25:20 of code we're saying okay if our tool call includes an ID that means we have a\n4:25:25 new tool call or a new AI message so what we do is we append that AI message\n4:25:32 which is the AI message chunk to our outputs and then following that if we\n4:25:37 don't get an ID that means we're getting the tool arguments so following that we're just\n4:25:42 adding our AI message chunk to the most recent AI message Chunk from our outputs\n4:25:49 okay so what that will do is it it will create that list of AI messages would be\n4:25:56 like AI message one and then this will just append everything to that AI\n4:26:02 message one then we'll get our next AI message chunk this will then just append\n4:26:08 everything to that until we get a complete AI message and so on and so on\n4:26:15 okay so what we do here is here we've collected all our AI message chunk\n4:26:21 objects then finally what we do is just transform all those AI message chunk objects into actual AI message objects\n4:26:28 and then return them from our function which we then receive over here so into the tool cuse variable okay now this is\n4:26:37 very similar to The Deep dive chapter again we're going through that that count that Loop where we have a Max\n4:26:44 iterations at which point we will just stop but until then we continue\n4:26:49 iterating through and making more tool calls executing those tool calls and so on so what what is going on here let's\n4:26:57 see so we got our tool calls there's going to be a list of AI message objects then what we do with those AI\n4:27:04 message objects is we pass them to this ex cuute tool function if you remember what is that that is this function here\n4:27:13 so we pass each AI message individually to this function and that will execute\n4:27:19 the tool force and then return us that observation from the tool okay so that is what you see\n4:27:28 happening here but this is an async method so typically what you'd have to\n4:27:33 do is you'd have to do await X you tool and we could do that so we could do a\n4:27:39 okay let me let me make this a little bigger for us okay and so what we could do for example\n4:27:45 which might be a bit clearer is you could do tool OBS equals an empty list\n4:27:51 and what you can do is you can say for Tool call oops in tool calls the\n4:27:58 tool observation is we're going to append execute tool call which would\n4:28:04 have to be in a weit so we' actually put your weight in there and what this would do is actually the exact same thing as\n4:28:10 what we're doing here the difference being that we're doing this tool by Tool\n4:28:16 okay so we are we're executing async here but we're doing them sequentially\n4:28:23 whereas what we can do which is better is we can use asyn I gather so what this\n4:28:28 does is gathers all those Co routines and then we await them all at the same\n4:28:33 time to run them all asynchronously they all begin at the same time or almost\n4:28:38 exactly at the same time and we get those responses kind of in parallel but of course it's\n4:28:44 saying so it's not fully in parallel but practically in parallel cool so we have\n4:28:51 that and then that okay we get all of our tool observations from that so that's all of our tool messages and then\n4:28:57 one interesting thing here is if we let's say we have all of our AI messages\n4:29:04 of all of our tool cores and we just append all of those to our agent scratch Pad right so let's say here we're just\n4:29:11 like oh okay scratch Pad extend and then we would just have\n4:29:17 okay we' have our tool calls and then we do agent stretch PCT\n4:29:23 send tool OBS all right so what what is happening here is this would essentially\n4:29:28 give us something that looks like this so we have our AI message say I'm just\n4:29:37 going to put okay we'll just put tool call IDs in here to simplify a little bit\n4:29:42 this would be tool call ID a then we would have ai message tool call ID B\n4:29:50 then we'd have tool message let's just remove this content\n4:29:55 field I don't want that and Tool message tool call ID B right so it would look\n4:30:02 something like this so the the order is the tool message is not following the AI\n4:30:07 message which you would think okay we have this tool qual ID that's probably fine actually when we're running this if\n4:30:14 you add these to your agent scratch pad in this order what you'll see is your response\n4:30:20 just hangs like nothing nothing happens when you come through to your second uh\n4:30:25 iteration of your agent call so actually what you need to do is these need to be\n4:30:30 sorted so that they are actually in order and it doesn't actually doesn't\n4:30:36 necessarily matter which order in terms of like a or b or c or whatever you use so you could have this order we have ai\n4:30:42 message tool message AI message tool message just as long as you have your tool call IDs are both together or you\n4:30:49 could know invert this for example right so you could have this right and that that will work\n4:30:55 as well it's essentially just as long as you have your AI message followed by your tool message and both of those are\n4:31:02 sharing that tool call ID you need to make sure you have that order okay so\n4:31:08 that of course would not happen if we do this and instead what we need to do is\n4:31:14 something like this okay so I made this a lot easier to read okay so we're\n4:31:20 taking the tool call ID we are pointing it to the tool observation and we're\n4:31:26 doing that for every tool call and to Observation within like a zip of those\n4:31:31 okay then what we're saying is for each tool call within our tool calls we are\n4:31:37 extending our agent scratch pad with that tool call followed by by the tool\n4:31:43 observation message which is the tool message so this would be our this is the AI message and that is\n4:31:50 the tool messages down there okay so that is what it's happening and that is how we get this correct order which will\n4:31:58 run otherwise things will not run so that's important to be aware of okay now\n4:32:05 we're we're almost done I know there's we just been through quite a lot so we continue we incre increment our count as\n4:32:11 we were doing before then we need to check for the final answer tool okay and because we're running these tools in parallel okay\n4:32:18 because we're allowing multiple tool calls in one step we can't just look at the most recent tool and look if it is\n4:32:25 it has the name Final Answer instead we need to iterate through all of our tool calls and check if any of them have the\n4:32:30 name final answer if they do we say okay we extract that final answer call we\n4:32:35 extract the final answer as well so this is the direct text content and we say\n4:32:40 okay we have found found the final answer so this will we set to True okay which should happen every time but let's\n4:32:47 say if our agent gets stuck in a loop of calling multiple tools this might not\n4:32:53 happen before we break based on the max iterations here so we might end up\n4:32:59 breaking based on Max iterations rather than we found a final answer okay so\n4:33:04 that can happen so anyway if we find that final answer we break out of this\n4:33:09 for Loop here and then of course we do need to break out of our wow Loop which is here so we say if we found the final\n4:33:16 answer break okay cool so we have that\n4:33:22 finally after all of that so this is our you know we've executed our tool our\n4:33:27 agent steps and iterations has process we've been through those finally we come\n4:33:34 down to here where we say okay we're going to add that final output to our chat history so this is just going to be\n4:33:41 the text content right so this here get direct answer but then what we do is we\n4:33:49 return the full final answer call the full final answer call is basically this here right so this answer and tools used\n4:33:57 but of course populated so we're saying here that if we have a final answer okay\n4:34:03 if we have that we're going to return the final answer call which was generated by our llm otherwise we're\n4:34:09 going to return this one so this is in the scenario that maybe the agent got caught in a loop and just kept iterating\n4:34:16 if that happens we'll say it will come back with okay no answer found and it will just return okay we didn't use any\n4:34:23 tools which is not technically true but it's this is like a exception handling\n4:34:29 event so it ideally it shouldn't happen but it's not really a big deal if we're\n4:34:35 saying okay there were no tools use in my opinion anyway cool so we have all of\n4:34:41 that and yeah we just we initialize our agent executor and then I mean that that is\n4:34:48 our agent execution code the one last thing we want to go through is the Ser API tool which we will do in a moment\n4:34:56 okay so Ser API let's see what let's see\n4:35:01 how we build our Ser API tool okay so we'll start with the synchronous Ser API\n4:35:10 now the reason we're starting with this is that it's actually it's just a bit simpler so I'll show you this quickly\n4:35:16 before we move on to the async implementation which is what we're using within our app so we want to get our set\n4:35:23 API API key so I'll run that and we just enter it at the top\n4:35:29 there and this will R so we're going to use the sub API SDK first we're\n4:35:36 importing Google search and these are the input prameters so we have our API key we're using we say want use Google\n4:35:42 we our question is so query so Q for query we're searching for the latest\n4:35:47 news in the world it will return quite a lot of stuff you can see there's a ton of stuff in there right\n4:35:56 now what we want is contained within this organic results key so we can run\n4:36:02 that and we'll see K is talking about you various things pretty recent stuff\n4:36:09 at the moment so we can tell okay that is that is in fact working now this is\n4:36:14 quite messy so what I would like to do first is just clean that up a little bit so we Define this article base model\n4:36:21 which is pantic and we're saying okay from a set of results okay so we're\n4:36:27 going to iterate through each of these we're going to extract the title source\n4:36:32 link and the snippet so you can see title source link and snippet here\n4:36:42 okay so that's all usedful we'll run that and what we do is we go through\n4:36:48 each of the results in organic results and we just load them into our article using this class method here and then we\n4:36:55 can see okay let's have a look at what those look like it's much nicer okay we\n4:37:02 get this nicely formatted object here cool that's great now all of this what\n4:37:10 we just did here so this is using sub apis SDK which is great super easy to\n4:37:15 use the problem is that they don't offer a async SDK which is a shame but it's\n4:37:22 not that hard for us to set up ourselves so typically with a synchronous requests\n4:37:29 what we can use is the aiio HTTP Library it's well it's you can see what we're\n4:37:35 doing here so this is equivalent to requests Dot get okay that's essentially\n4:37:43 what we're doing here and the equivalent is literally this okay so this is the\n4:37:50 equivalent using requests that we are running here but we're using asyn Code\n4:37:55 so we're using AI Hep client session and then session. getet okay with this async\n4:38:03 width here and then we just await our response so this is all yeah this is what we do rather than this to make our\n4:38:11 code async so it's really simple and then the output that we get is exactly the same\n4:38:16 right so we still get this exact same output so that means of course that we can use that articles method like this\n4:38:25 in the exact same way and we get we get the same result there's no need to make this article from sub API result asnc\n4:38:34 because again like this this bit of code here is fully local it's just our python\n4:38:39 running everything so this does not need to be async okay and we can see that we\n4:38:45 get literally the exact same result there so with that we have everything\n4:38:51 that we would need to build a fully asynchronous Sur API tool which is exactly what we do here for Lang chain\n4:38:58 so we import those tools and I mean there's nothing is there anything different here no this is exactly what\n4:39:04 we we just said but I will run this because I would like to show you very quickly this okay so this is how we were\n4:39:12 initially calling our Tools in previous chapters because we we were okay mostly\n4:39:18 with using the the synchronous tools however you can see that the funk here\n4:39:26 is just empty right so if I do type just a non-type that is\n4:39:31 because well this is an async function okay it's an async tool sorry so it was\n4:39:40 defined with async here what happens when you do that is you get\n4:39:45 this Co routine object so rather than Funk which is it isn't here you get that\n4:39:52 cartine if we then modified this which would be kind of okay let's just remove\n4:39:58 all the ayns here and the await if we modify that like so and then\n4:40:06 we look at the set API structure tool we go across we see that we now get that\n4:40:12 funk okay so that is that is just the difference between an async structured\n4:40:17 tool versus a sync structured tool we of course on\n4:40:22 async okay now we have K again so important to be aware of that and of\n4:40:29 course we we run using the sub API care\n4:40:34 routine so that is that's how we build the sub API tool\n4:40:41 uh there's nothing I mean that is exactly what we did here so I don't need to I don't think we need to go through that any further so yeah I think that is\n4:40:50 basically all of our code behind this API with all of that we can then go\n4:40:55 ahead so we have our API running already let's go ahead and actually run also our\n4:41:02 front end so we're going to go to documents orelo line chain course and\n4:41:07 then we want to go to Chapters 09 Capstone app and you will need to have npm\n4:41:14 installed so to do that what do we do we can take a look at this answer for example this is probably what I would\n4:41:21 recommend okay so I would run Brew install node followed by Brew install mpm if you're on Mac of course it's\n4:41:28 different if you're on Linux or Windows once you have those you can do npm install and this will just install all\n4:41:34 of the oop sorry mpm install and this would just install all of the node\n4:41:40 packages that we need and then we can just run npm run Dev okay and now we\n4:41:48 have our app running on locost 3000 so we can come over to here open\n4:41:53 that up and we have our application can ignore this so in here we can begin just\n4:42:00 asking questions okay so we can start with quick question what is 5 +\n4:42:07 5 and you see so we have our streaming happening here it said the agent wants to use ad tool and these are the input\n4:42:14 parameters to the ad tool and then we get the streamed response so this is the\n4:42:20 final answer tool where we're outputting that answer key and value and then here we're outputting that tools used key and\n4:42:27 value which is just an array of the tools being used which just functions add so we have that then let's ask\n4:42:35 another question this time we'll trigger Ser API with tell me about the latest news in the world\n4:42:42 okay so we can see that's using C API and a query is latest world\n4:42:48 news and then it comes down here and we actually get some citations here which is kind of cool so you can also come\n4:42:55 through to here okay and it teses through to here so that's pretty cool unfortunately I\n4:43:01 just lost my chat so fine let me I can ask that\n4:43:07 question again\n4:43:16 okay we can see that to us set API there now let's continue with the next question from our notebook which is how\n4:43:23 cold is in I like right now what is five M by five what do you get when multiplying those two numbers together\n4:43:30 I'm just going to modify that to say in Celsius so that I can understand thank\n4:43:36 you okay so for this one we can see what did we get so we got current temperature\n4:43:41 in ow we got multiply 5 by five which our second question and then we also got\n4:43:49 subtract interesting that I I don't know why it did that it's kind of weird so it\n4:43:54 it decided to use oh ah okay so this is\n4:44:00 okay so then here it was okay that kind of makes sense does\n4:44:05 that make sense roughly okay so I think the the conversion for Fahrenheits Celsius is\n4:44:11 say like subtract 32 okay yes so to go from Fahrenheit to\n4:44:18 Celsius you are doing basically Fahrenheit minus 32 and then you're\n4:44:23 multiplying by this number here which the iume the AI did\n4:44:28 not oh it roughly did okay so subtracting 36 like 32 would have given us four and it gave us approximately two\n4:44:36 so if you think okay multiply by this it's practically multiplying by 0.5 five\n4:44:41 so halfing the value and that would give us roughly 2 so that's what this was\n4:44:47 doing here kind of interesting Okay cool so we've gone through we have seen how\n4:44:54 to build a fully fledged chat application using what\n4:44:59 we've learned throughout a course and we've built quite a lot if you think about this application you're getting\n4:45:07 the real time updates on what tools are being used the parameters being input to those tools and then that is all being\n4:45:13 returned in a streamed output and even in a structured output for your final\n4:45:18 answer including the answer and the tools that we use so of course you know what we built here is fairly limited but\n4:45:26 it's super easy to extend this like you could maybe something that you might want to go and do is take what we've\n4:45:33 built here like Fork this application and just go and add different tools to it and see what happens because this is\n4:45:40 very extensible you can do a lot with it but yeah that is the end of the course\n4:45:46 of course this is just the beginning of whatever it is you're wanting to learn\n4:45:52 or build with AI treat this as the beginning and just go out and find all\n4:45:58 the other cool interesting stuff that you can go and build so I hope this course has been useful\n4:46:06 informative and gives you an advantage in whatever it is is you're going out of this build so thank you very much for\n4:46:13 watching and taking the course and sticking through right to the end I know it's pretty long so I appreciate it a\n4:46:21 lot and I hope you get a lot out of it thanks bye\n0:00 welcome to the AI Engineers guide for the L chain this is a four course that\n0:05 will take you from the assumption that you know nothing about Lang chain to\n0:11 being able to proficiently use the framework either you know within line\n0:17 chain within line graph or even elsewhere uh from the fundamentals that\n0:23 you will learn in this course now this course will be broken up into multiple\n0:28 chapters we're going to start by talking a little bit about what line chain is and when we should really be using it\n0:35 and when maybe we don't want to use it we'll talk about the pros and cons and also about the the why the line chain\n0:41 ecosystem not just about the line chain framework itself from there we'll\n0:46 introduce Lang chain we'll just have a look at a few examples before diving into essentially the basics of the\n0:53 framework now I will just note that all this is for Lang chain 0.3 so that is latest current version\n1:01 although that being said we will cover a little bit of where line chain comes from as well so we'll be looking at pre\n1:09 0.3 uh version methods for doing things so that we can understand okay that's\n1:15 the old way doing things how do we do it now now that we're in version 0.3 and also how do we dive a little deeper into\n1:22 those methods as well and kind of customize those from there we'll be diving into what I believe is the S of\n1:30 future of AI I mean it's it's it's a now and the short term potentially even\n1:36 further into the future and that is Agents we'll be spending a lot of time on agents so we'll be starting with a\n1:43 simple introduction to agents so that is how can we build an agent that's simple\n1:51 what are the main components of Agents what do they look like and then we'll be diving much deeper into them and we'll\n1:57 be building out our own Agent X computer which kind of like the framework around\n2:03 the AI components of an agent we're building our own and once we've done our\n2:08 Deep dive on agents we'll be diving into Lang chain expression language which\n2:14 we'll be using throughout this course so line chain expression language is the recommended way of using line chain and\n2:21 the expression language or L cell takes kind like a break from standard python\n2:27 syntax so there's a bit of weirdness in there and yes we'll be using it throughout the course but we're leaving\n2:35 the ELO chapter until this you know kind of later on in the course because we really want to dive into the\n2:41 fundamentals of Elsa by that point but the idea is that by this point you already have a good grasp of at least\n2:47 how to use the basics of lell before we really dig in that point then we'll be\n2:53 digging in streaming which is an essential ux feature of AI applications\n2:58 in general streaming it can just improve the user experience massively and it's not just about\n3:05 streaming tokens you know that that interface where you have word by word the AI is generating text on the screen\n3:12 streaming is more than just that it is also the ability if you've seen the\n3:17 interface of perplexity where as the agent is thinking you're getting an update of what the agent is thinking\n3:24 about what tools it is using and how it is using those tools that's also another essential feature that we need to have a\n3:31 good understanding of streaming to build so we'll also be taking a look at all of that then we'll finally we'll be topping\n3:38 it off with a Capstone project where we will be building our own AI agent\n3:45 application that is going to incorporate all of these features we're going to have an agent that can use tools web\n3:51 search we'll be using streaming and we'll see all of this in you know a nice\n3:57 interface that we can that we can work with so that's an overview the course of course it's very high level what I've\n4:03 just gone through there's a ton of stuff in here and truly this course can take you from you know wherever you are with\n4:09 L chain at the moment whether you're a beginner or you've used it a bit or even inter mediate and you're probably going\n4:16 to learn a fair bit from it so without any further Ado let's dive into the\n4:22 first chapter okay so the first chapter of the course we're going to focus on\n4:28 when should we actually use Lang chain and when should we use something else now through this chapter we're not\n4:35 really going to focus too much on the code we you know every other chapter is very code focused but this one is a\n4:42 little more just theoretical what is line chain where's fit in when should I use it when should I not so I want to\n4:48 just start by Framing this line chain is one of if not the most popular open\n4:56 source framework within the python ecosystem at least for AI it works\n5:01 pretty well for a lot of things and also works terribly for a lot of things as well to be completely honest there are\n5:07 massive Pros massive cons to using Lang chain here we're just going to discuss a few of those and see how Lang chain\n5:14 maybe Compares a little bit against other Frameworks so the very first\n5:19 question we should be asking ourselves is do we even need a framework a is a\n5:24 framework actually needed when we can just hit an API you have the open AI API\n5:30 other apis mral so on and we can get a response from an llm in five lines of\n5:36 code on average for those is incredibly incredibly simple however that can\n5:42 change very quickly when we start talking about agents or retrieval augmented generation research assistance\n5:50 all this sort of stuff those use cases those methods can\n5:56 suddenly get quite complicated when we outside of Frameworks and that's not necessarily a\n6:03 bad thing right it can be incredibly useful to be able to uh just understand\n6:10 everything that is going on and build it yourself but the problem is that to do\n6:16 that you need time like you need to learn all the intricacies of building these things the intricacies of these\n6:22 methods and themselves like what you know how do they even work and that kind of runs in the opposite direction of\n6:28 what we see with AI at the moment which is AI is being integrated into the world at an incredibly fast rate and because\n6:37 of this most Engineers coming into the space are not from a machine learning or\n6:43 AI background most people don't necessarily have any experience with these systems a lot of Engineers coming\n6:50 in that could be devops Engineers generic backend python Engineers even you front end Engineers coming in and\n6:57 building all these things which is is great but they don't necessarily have the experience and that you know that\n7:02 might be you as well and that's not a bad thing because the idea is that obviously you're going to learn and\n7:07 you're going to pick up a lot of these things and in this scenario there's quite a good argument for using the\n7:14 framework because a framework means that you can get started faster and a\n7:19 framework like Lang chain it abstracts away a lot of stuff and that's a that's\n7:24 a big complaint that a lot of people will have with L chain but that abstract\n7:29 in away of many things is also what made sing chain popular because it means that\n7:34 you can come in not really knowing okay what you know rag is for example and you can Implement a rag pipeline get the\n7:41 benefits of it without really needing to understand it and yes there's an argument against that as well just\n7:47 implementing something without really understanding it but as we'll see throughout the course it is possible to\n7:54 work with line chain in a way as we will in this course where you kind of\n8:00 implement these things in an abstract way and then break them apart and start understanding the intricacies at least a\n8:07 little bit so that can actually be pretty good\n8:12 however again circling back to what we said at the start if the idea or your\n8:18 application is just a very simple you know you need to generate some text based on some basic input maybe you\n8:24 should just use an API that's completely valid as well now we just said okay okay\n8:30 a lot of people coming to L chain might not be from an AI background so another question for a lot\n8:36 of these Engineers might be okay if I want to learn about you know rag agents\n8:41 all these things should I skip line chain and just try and build it from scratch myself well line chain can help\n8:49 a lot with that Learning Journey so you can start very abstract and as you gradually begin to\n8:57 understand the framework better you can strip away more and more of those abstractions and get more into the\n9:03 details and in my opinion this gradual shift towards more explicit code with\n9:11 less abstraction is a really nice feature and it's also what we focus on\n9:17 right throughout this course that's what we're going to be doing going sing abstract stripping away the abstractions\n9:22 and getting more explicit with what we're building so for example building an agent in L chain there's in very\n9:30 simple and Incredibly abstract crate tools agent method that we can use and\n9:37 like it creates a tool agent for you it's it doesn't tell you anything so you can you can use that right and we will\n9:46 use that initially in the course but then you can actually go from that to\n9:51 defining your full agent execution logic which is basically a tools call to open\n9:58 AI you going to be getting that tool information back but then You' got to figure out okay how am I going to\n10:03 execute that how am I going to Sol this information and then how am I going to iterate through this so we're going to\n10:08 be seeing that stripping way abstractions as we work through as we build agents as we do as we bu like our\n10:16 streaming use case among many other things even chat memory we'll see there as well so line chain can act as the\n10:24 onramp to your AI learning experience then what you might find and I do think\n10:31 this is quite true for most people is that if you if you're really serious\n10:36 about AI engineering and that's what you want to do like that's your focus right which isn't for everyone for certain a\n10:43 lot of people just want to understand a bit of AI and they want to continue doing what they're doing and just integrate AI here and there and maybe\n10:50 those you know if that's your focus you might sick with ly chain you know there's not necessarily a reason to move\n10:55 on but in the other scenario where you're thinking okay I want to get\n11:00 really good at this I want to just learn as much as I can and I'm going to\n11:06 dedicate basically my you know my short-term future of my career on becoming AI\n11:13 engineer then line chain might be the on-ramp it might be your initial learning curve but then after you've\n11:20 become competent with line chain you might actually find that you want to move on to other Frameworks and that\n11:25 doesn't necessarily mean that you're going to have wasted your time with L chain because one L chain is a thing helping\n11:32 you learn and two one of the main Frameworks that I recommend a lot of people to move on to is actually line\n11:38 graph which is still within the L chain ecosystem and it still uses a lot of L chain objects and\n11:45 methods and of course Concepts as well so even if you do move on from line\n11:50 chain you may move on to something like L graph which you can no line chain for\n11:56 anyway and let's say you do move on to another framework in set said in that scenario the concepts that you learn\n12:02 from Lang chain are still pretty important so to just finish up this chapter I just want to summarize on that\n12:09 question of should you be using Lang chain what's important to remember is that Lang chain does abstract a lot now\n12:16 this abstraction of L chain is both a strength and a weakness with more\n12:23 experience those abstractions can feel like a limitation and\n12:29 that is why we sort of go with the idea that L chain is really good to get\n12:35 started with but as a project grows in complexity or the engineers get more experience they might move on something\n12:41 like Lang graph which in any case is going to be using Lang chain to some degree so in either one of those\n12:48 scenarios L chain is going to be a core tool in an AI engineered toolkit so\n12:56 it's worth learning in our opinion but of course it comes with its you know it comes with its weaknesses and it's just\n13:03 good to be aware of that it's not a perfect framework but for the most part you will learn a lot from it and you\n13:09 will be able to build a lot with it so with all of that we'll move on to our\n13:16 first of Hands-On chapter with Lang chain where we'll just introduce Lang\n13:21 chain some of the essential Concepts I'm not going to Dag too much into the syntax but we're just going to understand a little bit of what we can\n13:27 do with it okay so moving on to our next next chapter getting started with a line chain in this chapter we're going to be\n13:33 introducing a line Chain by building a simple LM powered assistant that will do\n13:39 various things for us it will multimodal generating some text generating images\n13:45 generate some stret shed outputs it will do a few things now to get started we will go over to the course repo all of\n13:53 the code all the chapters are in here there are two ways of running this either locally or in Google collab we\n14:00 would recommend running in Google collab because it's just a lot simpler with environments but you can also run it\n14:06 locally and actually for the cap Zone we will be running it locally there's no\n14:12 way of us doing that in collab so if you would like to run everything locally\n14:17 I'll show you how quickly now if you would like to run in collab which I would recommend at least for the the\n14:23 first notebook chapters just skip ahead there will be chapter points\n14:29 in the timeline of the video so for running running it locally we just come\n14:34 down to here so this actually tells you everything that you need so you will\n14:40 need to install uvie all right so this is the package manager that we recommended by the python and package\n14:47 management Library you don't need to use uvie it's it's up to you uvie is is very\n14:53 simple it works really well so I would recommend that so you would install it with this command here this is on Mac so\n15:01 it will be different otherwise if you are on Windows or otherwise you can uh\n15:07 look at the installation guide there and it'll tell you what to do and so before we actually do this what I will do is go\n15:13 ahead and just clone this REO so we'll come into here I'm going to\n15:19 create like a temp directory for me because I already have the line chain course in there and what I'm going to do\n15:26 is just get loan line chain course okay so you will also need to install git if\n15:31 you don't have that okay so we have that then what we'll do is copy this okay so\n15:38 this will install python 3.2.7 for us with this command then this will create\n15:45 a new VM within that or using python 3.2.7 that we've installed and then UV\n15:53 sync we actually be looking at the Pi Project at TL file that's like the uh\n15:59 the package installation for the repo and using that to install everything\n16:04 that we need now we should actually make sure that we are within the line chain course directory and then yes we can run\n16:11 those three and there we go so everything should install with that now if you are\n16:20 in cursor you can just do cursor dot or we can run code do if mvs code I'll just\n16:28 be running this this and then I've opened up the course now within that course you have your notebooks and then\n16:35 you just run through these making sure you select your kernel pth environment and making sure you're using the correct\n16:41 VN from here so that should pop up already as this VM bin Python and you'll\n16:47 click that and then you can run through when you are running locally don't run these you don't need to you've already\n16:54 installed everything so you don't this specifically is for collab so that is\n16:59 running things locally now let's have a look at running things in collab so for\n17:06 running everything in collab we have our notebooks in here we click through and then we have each of the chapters\n17:12 through here so starting with the first chapter the introduction which is where we are\n17:18 now so what you can do to open this in collab is either just click this collab\n17:24 button here or if you really want to for example Maybe this it is not loading for\n17:31 you what you can do is you can copy the URL at the top here you can go over to\n17:36 collab you can go to open GitHub and then just paste that in\n17:43 there and press enter and there we go we have our\n17:48 notebook okay so we're in now uh what we will do first is just install the\n17:53 prerequisites so we have line chain just a load of line chain packages here line\n17:59 chain core line chain open a because we're using open Ai and line chain Community which is needed for running\n18:06 what we're running okay so that has installed everything for us so we can\n18:12 move on to our first step which is initializing our LM so we're going to be\n18:19 using GT40 mini which is side of small but fast but also cheaper model uh that\n18:26 is also very good for open AI so what we need to do here is get an API key okay\n18:33 so for getting that API key we're going to go to open's website and you can see\n18:39 here that we're opening platform. open.com and then we're going to go into settings organization API\n18:45 keys so you can copy that I'll just click it from here okay so I'm going to\n18:50 go ahead and create a new secret key actually just in case you're kind of\n18:56 looking for where this is It's settings organization API Keys again okay create\n19:01 a new API key I'm going to call it Line train\n19:06 course I'll just put it on the semantic router that's just my organization you you put it wherever you want it to be\n19:14 and then you would copy your API key you can see mine here I'm obviously going to\n19:19 reval that before you see this but you can try and use it if you really like so I'm going to copy that and I'm going to\n19:25 place it into this little box here you could also just and place it put your uh\n19:31 full API key in here it's up to you but this little box just makes things easier\n19:36 now that what we've basically done there is just passed in our API key we're setting our opening model GT40 mini and\n19:45 what we're going to be doing now is essentially just connecting and setting up our llm parameters with L chain so we\n19:53 run that we say okay we're using a GT4 mini and we're also setting ourselves up\n19:59 to use two different LMS here or two of the same LM with slightly different\n20:05 settings so the first of those is an LM with a temperature setting of zero the\n20:10 temperature setting basically controls almost the randomness of the output of\n20:17 your llm and the way that it works is when an LM is predicting the sort of\n20:24 next token or next word in sequence know provide a probability actually for all\n20:29 of the tokens within the lm's knowledge base or what the LM has been trained on\n20:35 so what we do when we set temperature of zero is we say you are going to give us\n20:41 the token with highest probability according to you okay whereas when we\n20:47 set a temperature of 0.9 what we're saying is okay there's actually an increased probability of you giving us a\n20:55 token that according to your generated output is not the token with the highest\n21:01 probability according to the lmm but what that tends to do is give us more sort of creative outputs so that's what\n21:08 the temperature does so we are creating a normal llm and then a more creative\n21:13 llm with this so what are we going to be building we're going to be taking a\n21:19 article draft so like a draft article uh from the aelio learning page and we're\n21:27 going to be using line chain to generate various sces that we might um find helpful as we're you know we have this\n21:34 article draft and we're editing it and just kind of like finalizing it so what are those going to be you can see them\n21:40 here we have the title for the article the description and SEO friendly\n21:45 description specifically third one we're going to be getting the LM to Providers\n21:50 advice on existing paragraph and essentially writing a new paragraph for us from the existing paragraph and what\n21:57 it's going to do this is the structured output part is going to write a new version of that paragraph for us and\n22:03 it's going to give us advice on where we can improve our writing then we're going to generate a thumbnail hero image for\n22:10 our article so nice image that you would put at the top so here we're just going\n22:16 to input our article you can you can put something else in here if you like essentially this is just a big article\n22:24 that's written a little while back on agents and and now we can go ahead and\n22:29 start preparing out our prompts which are essentially the instructions for our llm so line chain comes with a lot of\n22:38 different uh like utilities for prompts and we're going to dive into them in a lot more detail but I do want to just\n22:44 give you uh the Essentials now just so you can understand what we're looking at at least conceptually so prompts for\n22:51 chat agents are at a minimum broken up into three components those are the\n22:57 system pront this provides instructions to our LM on how it should behave what its objective is and how it should go\n23:03 about achieving that objective generally system prompts are going to be a bit\n23:08 longer than what we have here depending on the use case then we have our user prompts so these are user written\n23:15 messages usually sometimes we might want to pre-populate those if we want to encourage a particular type of um\n23:22 conversational patterns from our agent but for the most part yes these are going to be using generated then we have\n23:29 our AI promps so these are of course AI generated and again in some cases we\n23:36 might want to generate those ourselves beforehand or within a conversation if we have a particular reason for doing so\n23:43 but for the most part you can assume that these are actually user and AI generated now the line chain provides us\n23:51 with templates for each one of these prompt types let's go ahead and have a\n23:57 look at what these look like within line chain so to begin we are looking at this\n24:03 one so we have our system message prom template and human message which the the\n24:09 user that we saw before so we have these two system prom keeping it quite simple here you are AI system that helps\n24:16 generate article titles right so so our first component where we want to generate is article title so we're\n24:22 telling the AI that's what we want it to do and then here right so here we're\n24:29 actually providing kind of like a template for a user input\n24:36 so yes as I mentioned user input can be\n24:41 um it can be fully generated by user it might be kind of not generated by user\n24:47 it might be setting up a conversation beforehand which a user would later use or in this scenario we're actually\n24:54 creating a template and the what the user will providers will actually just be inserted\n25:00 here inside article and that's why we have this import variables so what this\n25:07 is going to do is okay we have all of these instructions around here they're all going to be provided to openai as if\n25:14 it is the user saying this but it will actually just be this here that user\n25:20 will be providing okay and we might want to also format this a little nicer it kind of depends this will work as it is\n25:27 but we can also put you know something like this to make it a little bit clearer to the llm okay what is the\n25:34 article where the prompts so we have that and you can see in this scenario\n25:41 there's not that much difference between what the system prompt and user prom is doing and this is It's a particular scenario it varies when you get into the\n25:49 more conversational stuff as we will do later uh you'll see that the user prompt\n25:54 is generally more fully user generated or mostly user generated and much of\n26:01 these types of instructions we might actually be putting into the system prompt it varies and we'll see\n26:07 throughout of course many different ways of using these different types of PRS in various different\n26:13 places then you'll see here so I just want to show you how this is working we\n26:19 can use this format method on our user prompt here to actually insert something\n26:25 within the uh article input here so we're going to go us prompt format and\n26:31 then we pass in something for article okay and we can also maybe format this a little nicer but I'll just show you this\n26:37 for now so we have our human message and then inside the content this is the the text that we had right you can see that\n26:43 we have all this right and this is what we wrote before we wrote all this except from this part we didn't write this\n26:50 instead of this we had article right so let's format this a little nicer so we\n26:57 can see okay so this is exactly what we wrote up here exactly the same except\n27:02 from now we have test string instead of article so later when we insert our\n27:07 article it's going to go inside there allly doing it's like it's an it's an F string in Python okay and this is again\n27:15 this is one of those things where people might complain about Lang chain you know this sort of thing can be you it seems\n27:21 excessive because you could just do this with an nring but there are as we'll see later particularly when you're streaming\n27:27 just really helpful features that come with using line chains kind of built-in\n27:34 uh prompt templates or at least uh message objects that we'll see so\n27:41 you we need to uh keep that in mind again as soon as you get more complicated line chain can be a bit more\n27:47 useful so chat prom template uh this is basically just going to take what we\n27:53 have here our system promt user prompts you could also include some AI prompts in there and what it's going to do is\n27:59 merge both of those and then when we do format what it's going to do is put both\n28:06 of those together into a chat history okay so let's see what that looks like first uh in a more messy way okay so you\n28:15 can see we have just the content right so it doesn't include the whole you know\n28:21 before we had human message we're not include we're not seeing anything like that here instead we're just seeing the\n28:26 string so now let's switch back to print and we can see that what we have\n28:33 is our system message here it's just prefixed with this system and then we have human and it's prefixed by human\n28:39 and then it continues right so that's that's all it's doing it's just kind of merging those in some sort of chat lug we could also put in like AI messages\n28:46 and they would appear in there as well okay so we have that now that is our\n28:51 prompt template let's put that together with an LM to create what would be in\n28:57 past line ch be called an llm chain uh now we wouldn't necessarily call it an llm chain because we're not using the\n29:03 llm chain abstraction it's not super important if that doesn't make sense we we'll go into it in more detail later\n29:10 particularly in the in the ELO chapter so what this chain will do you\n29:17 think L chain is just chains where're chaining together these multiple components it will perform the STS\n29:24 prompt formatting so that's what I just showed you LM generation so sending our\n29:31 prom to open AI getting a response and getting that output so you can also add\n29:37 another set here if you want to format that in a particular way we're going to be outputting that in a particular\n29:42 format so that we can feed it into the next set more easily but there are also things called output passes which pass\n29:49 your output in a more dynamic or complicated way depending on what you're doing so this is our first look at Elsa\n29:58 don't want us to focus too much on the syntax here because we will be doing that later but I do want you to just\n30:04 understand what is actually happening here and logically what are we writing\n30:11 so all we really need to know right now is we Define our inputs with the first\n30:17 dictionary segment here right so this is a you know our inputs which we have\n30:23 defined already okay so if we come up to our\n30:29 user prompt here we said the input variable is our article right and we might have also added input variables to\n30:34 the system prompt here as well in that case you know let's say we had your AI assistant\n30:42 called name right that helps generate article\n30:47 titles in this scenario we might have an input variables name here right and then\n30:55 what we would have to do down here is we would also have to pass that\n31:01 in right so also we would have article but we would also have name so basically\n31:09 we just need to make sure that in here we're including the variables that we have Define as input variables for our\n31:16 our first prompts okay so we can actually go ahead and let's add that uh so we can see it's in action so we'll\n31:23 run this again and just include that or or reinitialize is our first prompt so\n31:29 we see that and if we just have a look at what that means for this format\n31:35 function here it means we'll also need to pass in a name okay and call it Joe\n31:40 okay so Joe the AI right so you are an AI system called Joe now okay so we have\n31:46 Joe our AI that is going to be fed in through these input variables then we have this pipe operator the pipe\n31:53 operator is basically saying whatever is to the left of the pipe operator which\n31:58 in this case would be this is going to go into whatever is on the right of the pipe operator it's that's simple again\n32:05 we'll we'll dive into this and kind of break it apart in the Elso chapter but for now that's all we need to know so\n32:11 this is going to go into our first prompt that is going to format everything it's going to add the name\n32:17 and the article that we provided into our first prompt then it's going to Output that right going to Output that\n32:23 we have our P operate here so the output of this is going to go into the input of our Next Step it's our creative\n32:30 LM then that is going to generate some tokens it's going to generate our output\n32:36 that output is going to be an AI message and as you saw before if I take this bit\n32:44 out within those message objects we have this content field okay so we are\n32:50 actually going to extract the content field out from our AI message to just\n32:56 get the content and that is what we do here so we get the AI message out from ilm and then we're extracting the\n33:02 content from that AI message object and we're going to passing it into a dictionary that just contains article\n33:07 title like so okay we don't need to do that we can just get the AI message directly I just want to show you how we\n33:15 are using this sort of chain in Elso so once we have set up our chain we then\n33:23 call it or execute it using the invoke method into that we will need to pass in\n33:28 those variables so we have our article already but we also gave our AI a name now so let's add that and we'll ruin\n33:36 this okay so Joe has generated us a article title unlocking the future the\n33:43 rise of neuros symbolic AI agents cool much better name than what I gave the article which was AI agents are neuros\n33:52 symbolic systems no I don't think I did too bad okay so we have that\n33:58 now let's continue and what we're going to be doing is building more of these\n34:03 types of LM chain pipelines where we're feeding in some prompts we're generating\n34:10 something getting something and and doing something with it so as mentioned we have the title we're\n34:16 now moving on to the description so to generate description so we have our human message prompt template so this is\n34:22 actually going to go into a similar format as before we also\n34:28 want to redefine this because I think I'm using the same system message there\n34:33 so let's let's go ahead and do modify that or what we could also do is let's\n34:40 just remove the name now because I've showing that so what we could do is\n34:46 you're an AI system that helps build good articles right build good\n34:53 articles and we could just use this as our you know generic system prompt now\n34:58 so let's say that's our new system prompt now we have our user prompt your task creating description for the\n35:04 article the article is here fure examine article here is the article title okay so we need the article title now as well\n35:10 in our input variables and then we're going to Output an AO friendly article description and we're just saying you\n35:17 just to be certain here do not output anything other than the description so you know sometimes an LM might say hey\n35:23 look this is what I've generated for you the reason I think this is good is because so on and so on so on right if you're programmatically taking some\n35:31 output from an LM you don't want all of that fluff around what the LM is generated you just want exactly what\n35:37 you've asked it for okay because otherwise you need to pass out with code and it can get messy and also just far\n35:44 less reliable so we're just saying do iput anything else then we're putting all these together so system prompt and\n35:50 the second user prompt this one here putting those together into a new chat\n35:55 prompt template and then we're going to to feed all that in to another LOL chain\n36:00 as we have here to well to generate our our description so let's go ahead we\n36:06 invoke that as before we're just make sure we add in the article title that we got from before and let's see what we\n36:13 get okay so we have this explore the transformative potential of neuros symbolic Ai ageny and a little bit long\n36:21 to be honest but yeah you can see what it's doing here right and of course we could then go in we see this is kind of\n36:26 too long right a yeah SEO friendly description not not really so we can\n36:33 modify this output the SEO friendly description um make sure we don't exceed\n36:41 let me put that on a new line make sure we don't exceed say 200 characters or\n36:46 maybe it's even less to se I don't I don't have a clue I'll just say 120 characters I do not outly anything other\n36:53 than the description right so we could just you know go back modify our prompting see what that generates again\n36:59 okay so much shorter probably too short now but that's fine cool so we have that we have a summary process that and\n37:06 that's now you know in this dictionary form that we have here cool now the\n37:12 third step we want to consume that first article variable with our full article\n37:18 and we're going to generate a few different output Fields so for this\n37:24 we're going to be using the structured output feature so let's scroll down\n37:29 we'll see what that is what that looks like so structured output is essentially\n37:36 we're forcing their lmic like it has to Output a dictionary with these you know\n37:41 particular Fields okay and we can modify this quite a bit but in this scenario\n37:47 what I want to do is I want there to be an original paragraph right so I just want it to regenerate the original\n37:53 paragraph cuz I'm lazy and I don't want to extract it out then I want to get the new edited paragraph This is the LM\n38:00 generated improved paragraph and then we want to get some feedback because we we don't want to just automate ourselves we\n38:07 want to augment ourselves and get better with AI rather than just being like I\n38:13 you do you do this so that's what we do here and you can see that here we're using this pantic object and what pantic\n38:21 allows us to do is Define these particular fields and it also allows us to assign these descriptions to a field\n38:27 and and line chain is actually going to go ahead read all of this right even reads so for example we could put\n38:33 integer here and we could actually get a numeric score for our paragraph right we\n38:39 can try that right so let's uh let's let's just try that quickly I'll show you so numeric numeric score in fact\n38:48 let's even just ignore let's not put anything here so I'm going to put constructive feedback on the original\n38:53 paragraph but I just put into here so let's see what happens okay so we have that and what I'm going to do is I'm\n38:59 going to get our creative llm I'm going to use this with structured output method and that's actually going to\n39:05 modify that llm class create a new llm class that forces that llm to use this\n39:10 structure for the output right so passing in paragraph into here using this we're creating this new structure\n39:17 LM so let's run that and see what happens okay so we're going to modify\n39:23 our chain accordingly maybe what I can do let's also just remove this bit for now\n39:30 so we can just see what the strictured llm outputs directly and let's\n39:36 see okay so now you can see that we actually have that paragraph object\n39:42 right the one we defined up here which is kind of cool and then in there we have the original paragraph right so\n39:48 this is where this is coming from I definitely remember writing something\n39:53 that looks a lot like that so I think that is correct we have the edited par so this is okay what thinks it's better\n40:00 and then interestingly the feedback is three which is weird right because uh\n40:06 here we said the constructive feedback on the original paragraph but what we're doing when we use this with structured\n40:12 output but what Lang chain is doing is is essentially performing a tool core to open Ai and what a tool core can do is\n40:20 force a particular structure in the output of an LM so when we say feedback\n40:26 has to be an integer no matter what we put here it's going to give us an integer because how do you provide\n40:31 constructive feedback with an integer it doesn't really make sense but because we've set that limitation that\n40:38 restriction here that is what it does it just gives us the uh a numeric value so\n40:44 I'm going to shift that to string and then let's rerun this see what we get okay we should now see that we actually\n40:50 do get constructive feedback all right so yeah you can see it's quite quite long so the original paragraph\n40:57 effectively communicates the limitations of neuro AI systems in performing certain tests however it could benefit\n41:03 from slightly improved Clarity and conciseness for example the phrase was becoming clear can be made more direct\n41:09 by changing it to became evident yeah true thank you very much so yeah now we\n41:17 actually get that that feedback which is pretty nice now let's add in this final\n41:22 setep to our chain okay and it's just going to pull out our paragraph object here and\n41:29 extracting into a dictionary we don't necessarily need to do this honestly I actually kind of prefer it within this paragraph object but just so we can see\n41:38 how we would pass things on the other side of the chain okay so now we can see\n41:44 we've extracted that out cool so we have all of that interesting feedback again\n41:52 but let's leave it there for the text part of this now let's have a look at at\n41:57 the sort of multimodal features that we can work with so this is you know maybe one of those things that kind of seems a\n42:04 bit more abstracted a little bit complicated where it maybe could be improved but you know we're not going to\n42:10 really be focusing too much on the M time modal stuff sub be focusing on language but I did want to just show you\n42:16 very quickly so we want this article to look better okay we want to\n42:22 generate a prompt based on the article it's self that we can then pass to DAR\n42:30 the the image generation model from open AI that will then generate an image like like a thumbnail image for us okay so\n42:39 the first step of that is we're actually going to get an LM to generate that right so we have our prompt that we're\n42:44 going to use for that so I'm say generate a prompt with less than 500 characters to uh generate an image based\n42:52 on the following article okay so that's our prompt yeah super simple uh using\n42:57 the generic prompt template here you can use that you can use user uh prompt template it's up to you this is just\n43:03 like the generic prom template then what we're going to be doing is based on what\n43:10 this outputs we're then going to feed that in to this generate and display image function via the image prompt\n43:17 parameter that is going to use the darly API rapper from line chain it's going to\n43:23 run that image prompt and we're going to get a a eurl out from that essentially and then we're going to read that using\n43:29 SK image here right so we're just going to read that image URL going to get the image data and then we're just going to\n43:35 display it okay so pretty straightforward now again this is a lell\n43:43 thing here that we're doing we have this runable Lambda thing when we're running\n43:49 functions within lell we need to wrap them within this runable Lambda I you\n43:54 know I don't want to go too much into what this is doing here because we do cover in the L cell chapter but it's\n44:01 just you know all you really need to know is we have a custom function wrap in runable Lambda and then what we get\n44:07 from that we can use within this here right the the L Sal syntax so what are\n44:14 we doing here let's figure this out we are taking our original that image prom that we defined just up here right input\n44:21 variable to that is article okay we have our article d being input here feeding\n44:28 that into our prompt from there we get our message that we then feed into our\n44:33 llm from the LM it's going to generate us a like an image prompt like a prompt\n44:39 for generating our image for this article we can even Let's uh let's print that out so that we can see what it\n44:46 generates because I'm also kind of curious okay so we'll just run that and\n44:52 then let's see it will feed in that content into our room reable which is\n44:58 basically this function here and we'll see what it generates okay don't expect\n45:03 anything amazing from darly it's not it's not the best to be honest but we at\n45:08 least we see how to use it okay so we can see the prom that was used here\n45:14 create an image that visually represents the concept of neuros symbolic agents depict a futuristic interface where\n45:20 large D interacts with traditional code symbolizing integration of oh my gosh uh\n45:27 something computation include elements like a brain to represent neur networks gears or circuits or symbolic logic and\n45:35 web of connections illustrating vast use cases of AI agents oh my gosh look at\n45:41 all that big prompt then we get this so you know dar's interesting I would say we\n45:48 could even take this let's just see what that comes up with in something like mid\n45:53 Journey you can see these way cooler images that we get from just another image generation model far better but\n46:00 pretty cool honestly so in terms of Generation image the phrasing the The\n46:05 Prompt itself is actually pretty good the image you know could be better but\n46:11 that's it right so with all of that we've seen a little introduction to what\n46:16 we might build in with lighing chain so that's it for our introduction chapter as I mentioned we don't want to go too\n46:22 much into what each of these things is doing just really want to focus on okay\n46:29 this is kind of how we're building something with line chain this is the overall flow uh but we don't really want\n46:37 to be focusing too much on okay what exactly lell is doing or what exactly uh\n46:42 you know this prompt thing is that we're setting up we're going to be focusing\n46:48 much more on all of those things and much more in the upcoming chapters so\n46:53 for now we've just seen a little bit of what we can build before diving in in more detail okay so now we're going to\n47:00 take a look at AI observability using lsmith now lsmith is another piece of\n47:08 the broader Lang chain ecosystem its focus is on allowing us to see what our\n47:15 llms agents Etc are actually doing and it's something that we would definitely recommend using if you are going to be\n47:22 using line chain Lang graph now let's take a look at how we would set L Smith up which is incredibly simple so I'm\n47:29 going to open this in collab and I'm just going to install the prerequisites here you'll see these are all the same\n47:36 as before but we now have the Lin Smith Library here as well now we are going to be using Lin Smith throughout the course\n47:43 so in all the following chapters we're going to be importing limith and that will be tracking everything we're doing\n47:49 but you don't need Lin Smith to go through the course it's an an optional dependency but as mentioned I would\n47:55 recommend it so we'll come down to here and first thing that we will need is the line chain API key now we do need an API\n48:03 key but that does come with a reasonable free tier so we can see here they have\n48:09 each of the plans and this is the one that we are by default on so it's free\n48:16 for one user up to 5,000 traces per month if you're building out an\n48:21 application I think it's fairly easy to go beyond that but it really depends on what you're building so it's a good\n48:28 place to start with and then of course you can upgrade as required so we would go to smith. L\n48:36 chain.com and you can see here that this will log me in automatically I have all\n48:41 of these tracing projects these are all from me running the various chapters of the course yours if you do use l Smith\n48:48 throughout course your L Smith dashboard will end up looking something like this now what we need is an API key so we go\n48:56 over to settings we have API keys and we're just going to create an API key because we're\n49:03 just going through some personal learning right now I would go with personal access token we can give a name or description if you want okay and\n49:10 we'll just copy that and then we come over to our notebook and we enter our API key there and that is all we\n49:16 actually need to do that's absolutely everything supposed the one thing to be aware of is that you should set your L\n49:22 chain project to whatever project you're working within so of course Within within the course we have individual\n49:28 project names for each chapter but for your own projects of course you should make sure this is something that you\n49:34 recognize and is useful to you so L Smith actually does a lot without needing to do anything so we can\n49:40 actually go through let's just initialize our LM and start invoking it and seeing what L Smith returns to us so\n49:48 we'll need our open API key enter it here and then let's just invoke\n49:55 hello okay so nothing has changed on this end right so us running the code there's nothing different here however\n50:02 now if we go to Lang Smith I'm going to go back to my dashboard okay and you can\n50:08 see that the the order of these projects just changed a little bit and that's because the most recently used project I\n50:15 this one at the top Lang chain course Lang Smith openai which is the current chapter we're in that was just triggered\n50:21 so I can go into here and I can see oh look at this so we actually have something in the Lang Smith UI and we\n50:28 didn't all we did was enter our L train apid that's all we did and we set some environment variables and that's it so\n50:34 we can actually click through to this and it will give us more information so you can see what was the\n50:39 input what was the output and some other metadata here you see you know there's not that much in\n50:46 here however when we do the same for agents we'll get a lot more information\n50:52 so I can even show you a quick example from the future chapters if we come through to agents\n51:00 intro here for example and we just take a look at one of\n51:05 these okay so we have this input and output but then on the left here we get all this information and the reason we\n51:12 get all this information because agents are they're performing multiple LM calls etc etc so there's a lot more going on\n51:20 so we can see okay what was the first LM call and then we get these tool use traces we get another LM another rmm\n51:27 call another tool use and another LM call so you can see all this information which is incredibly useful and\n51:33 Incredibly easy to do because all I did when setting this up in that agent chapter was simply set the API key and\n51:40 the environment variables as we have done just now so you get a lot out of\n51:46 very little effort with Lang Smith which is great so let's return to our Lang Smith project here and let's invoke some\n51:53 more now I've already shown you you know we're going to see a lot of things just by default but we can also add other\n52:00 things that Lang Smith wouldn't typically Trace so to do that we will\n52:05 just import a traceable decorator from Lang Smith and then let's make these\n52:12 just random functions traceable within limith okay so we'll run those we have\n52:19 three here so we're going to generate a random number we're going to modify how\n52:24 long a function takes and also generate a random number and then in this one\n52:30 we're going to either return this no error or we're going to raise an error\n52:36 so we're going to see how limith handles these different scenarios so let's just\n52:41 iterate through and run those a few times so we're just going to run each one of those 10\n52:47 times okay so let's see what happens so they're running let's go over to our Lin\n52:54 sth UI and see what is happening over here so we can see that everything is updating we adding that information\n52:59 through and we can see if we go into a couple of these we can see a little more information so have the input and the\n53:05 output took three seconds see random error here in this\n53:12 scenario random error passed without any issues let me just refresh the page\n53:18 quickly okay so now we have the rest of that information and we can see that occasionally if there is an error from\n53:24 our random error function it is is signified with this and we can see the\n53:30 traceback as well that was returned there which is useful okay so we can see if an error has been raised we have to\n53:35 see what that error is we can see the various latencies of these functions so\n53:42 you can see that varying throughout here we see all the inputs to each one\n53:48 of our functions and then of course the outputs so we can see a lot in there\n53:53 which is pretty good now another thing that we can do do is we can actually filter so if we come to here we can add\n54:01 a filter let's filter for errors that would be value error and\n54:06 then we just get all of the cases where one of our functions has returned or\n54:12 raise an error or value error specifically okay so that's useful and then yeah there's there's various other\n54:19 filters that we can add there so we could add a name for example if we want\n54:24 to look for the generate string delay function only we could also do\n54:30 that okay and then we can see the varying latencies of that function as well cool so we have\n54:37 that now one final thing that we might want to do is maybe we want to make\n54:43 those function names a bit more descriptive or easy to search for for example and we can do that by saying the\n54:50 name of the traceable decorator like so so let's run that we'll run this a few\n54:56 times and then let's jump over to limith again going to limith project okay and you can\n55:02 see those coming through as well so then we could also search for those based on that new name so what was it chitchat\n55:09 maker like so and then we can see all that information being streamed through\n55:16 to limith so that is our introduction to limith there is really not all that much\n55:23 to go through here it's very easy to sell up and as we scen it gives us a lot of observability into what we are building\n55:31 and we will be using this throughout the course we don't rely on it too much it's a completely optional dependency so you\n55:37 don't want to use l space you don't need to but it's there and I would recommend doing so so that's it for this chapter\n55:43 we'll move on to the next one now we're going to move on to the chapter on\n55:48 prompts in Lang chain now prompts they seem like a simple concept and they are\n55:53 a simple concept but there's actually quite a lot to them when you start diving into them and they truly have\n56:00 been a very fundamental part of what has propelled us forwards from pre llm times\n56:07 to the current llm times you have to think until llms became widespread the\n56:14 way to fine-tune a AI model or ml model\n56:19 back then was to get loads of data for your particular use case spend a load of\n56:26 training your specific Transformer or part of the Transformer to essentially\n56:31 adapt it for that particular task that could take a long time depending on the\n56:38 the task it could take you you know months or in some times if it was a\n56:43 simpler task it might take probably days potentially weeks now the interesting\n56:48 thing with L LMS is that rather than needing to go through this whole\n56:54 fine-tuning process to to modify a model for one task over\n57:00 another task rather than doing that we just prompt it differently we literally tell the model hey I want you to do this\n57:07 in this particular way and that is a you know that's a paradigm shift in what you're doing it's so much faster it's\n57:14 going to take you you know a couple of minutes rather than days weeks or months and LMS are incredibly powerful when it\n57:22 comes to just generalizing to you know across these many different tasks so prompts which control those instructions\n57:31 are a fundamental part of that now line chain naturally has many functionalities\n57:37 around prompts and we can build very Dynamic prompting pipelines that modify\n57:43 the structure and content of what we're actually feeding into our llm depending on different variables different inputs\n57:49 and we'll see that in this chapter so we're going to work through prompting\n57:54 within the scope of of a rag example so let's start by just dissecting the\n58:01 various parts of a prompt that we might expect to see for a use case like rag so\n58:07 our typical prompt for rag or retrieval augmented generation will include rules\n58:14 for the LM and this is this you will see in most prompts if not all this part of\n58:22 the promt sets up the behavior of the llm that is how it should be responding\n58:28 to user queries what sort of Personality it should be taking on what it should be\n58:33 focusing on when it is responding any particular rules or boundaries that we want to set and really what we're trying\n58:41 to do here is just to Simply provide as much information as possible to the llm\n58:48 about well what we're doing we just want to give the llm context as to the the\n58:56 place that it finds itself in because an LM has no idea where it is it's just it's a it takes in some information and\n59:04 spits out information if the only information it receives is from the user you know user query it has you know\n59:10 doesn't know the context what is the application that it is within what is its objective what is its aim what are\n59:17 the boundaries all of this we need to just assume the llm has absolutely no\n59:24 idea about because it it truly does not so as much context as we can provide but\n59:32 it's important that we don't overdo it it's uh we see this all the time people will over prompt an llm you want to be\n59:39 concise you don't want fluff and in general every single part of your prompt\n59:45 the more concise and less fluffy you can make it the better now those rules or\n59:50 instructions are typically in the system prompt of your llm now the second one is\n59:55 context which is rag specific the context refers to some sort of external\n1:00:00 information that you are feeding into your llm we may have received this information from like web search\n1:00:08 database query or quite often in this case of rag it's a vector database this\n1:00:15 external information that we provide is essentially the r retrieval augmentation\n1:00:22 of rag we are augmenting the knowledge of our llm which the the knowledge of\n1:00:29 our LM is contained within the llm model weights we're augmenting that knowledge with some external knowledge that's what\n1:00:36 we're doing here now for chat LMS this context is typically placed within a\n1:00:44 conversational context within the user or assistant messages uh and with\n1:00:52 more recent models it can also be placed within uh tool messages as well then we\n1:00:58 have the question this pretty straightforward this is the query from the user this is or is this usually a\n1:01:06 user message of course there might be some additional formatting around this you might add a\n1:01:12 little bit of extra context or you might add some additional instructions if you\n1:01:17 find that you L them sometimes VAR off the rules that you've set within the system prompt you might you know append\n1:01:24 or prefix something something here but for the most part it's probably just going to be the user's input and finally\n1:01:30 uh so these are all the inputs for our prompt here is going to be the output\n1:01:36 that we get so the answer from the assistant again I mean that's not even specific to rag it's just what you would\n1:01:43 expect in a in a chat llm or any LM and of course that would be an assistant\n1:01:49 message so putting all of that together in an actual prompt you can see everything we have here so we have the\n1:01:55 uh rules for our prompt here the instructions we're just saying okay answer the question based on the context\n1:02:01 below if you cannot answer the question using the information answer with I don't know then we have some context\n1:02:08 here okay in this scenario that context that we're feeding in here because it's\n1:02:14 the first message we might putting that into the system prompt but that may also be turned around okay if you if you for\n1:02:21 example have an agent you might have your question up here before the context\n1:02:27 and then that would be coming from a user message and then this context would follow the question and be recognized as\n1:02:34 a tool message it would be fed in that way as well kind of depends on on what\n1:02:40 sort of structure you're going for there but you can do either you can feed it into the system message if it's less\n1:02:45 conversational whereas if it's more conversational you might feed it in as a tool message okay and then we have a\n1:02:51 user query which is here and then we'd have the AI answer okay and obviously\n1:02:56 that would be generated here okay so let's switch across to the code we're in\n1:03:01 the L chain course repo notebooks 03 prompts and I'm just going to open this in collab okay let scroll down and we'll\n1:03:09 start just by installing the prerequisites okay so we just have the various libraries again as I mentioned\n1:03:16 before Lang Smith is optional you don't need to install it but if you would like to see your tracers and everything in\n1:03:22 Lang Smith then I would recommend doing that and if you are using L Smith you will need to enter your API key here\n1:03:28 again if you're not using Lang Smith you don't need to enter anything here you just skip that cell okay cool and let's\n1:03:36 jump into the basic prompting then so we're going to start with this prompt\n1:03:42 answer used query based on the question below so we're just structuring what we just saw uh in\n1:03:47 code and we're going to be using the chat problem template because generally\n1:03:53 speaking we're using chat llms in most most cases nowadays so we have our chat\n1:04:00 promp template and that is going to contain a list of messages system message to begin with which is just\n1:04:06 going to contain this and we're feeding in the the context within that there and\n1:04:12 we have our user query here okay so we'll run this and if we take a look\n1:04:21 uh here we haven't specified what our input variables are okay but we can see\n1:04:28 that we have query and we have context up here right so we can see that okay\n1:04:33 these are the input variables we just haven't explicitly defined them here so\n1:04:39 let's just confirm with this that line chain did pick those up and we can see that it did\n1:04:45 so it has context and query as our input variables for the prompt template that\n1:04:50 we just defined okay so we can also see the structure of our temp plates let's\n1:04:57 have a look okay so we can see that within messages here we have a system message\n1:05:03 prompt template the way that we Define this you can see here that we have from messages and this will consume various\n1:05:10 uh different structures so you can see here that it has a from messages it is a\n1:05:18 sequence of message like representation so we could pass in a system prompt\n1:05:24 template object and then a user prompt template object or we can just use a\n1:05:31 tupol like this and this actually defines okay this system this is a user and you could also do assistant or tool\n1:05:39 messages and stuff here as well using the same structure and then we can look in here\n1:05:44 and of course that is being translated into the system message prom template and human message prompt template okay\n1:05:53 we have our input variables in there and there and we have the template too okay\n1:05:59 now let's uh continue we'll see here what I what I just said so we're\n1:06:05 importing our system message prompt template and human message prompt template and you can see we're using the\n1:06:10 same from messages method here right and you can see it's so sequence of message\n1:06:16 like representation it's just you know what that actually means it can vary right so\n1:06:22 here we have system message prom template from template here from template query you know there's various\n1:06:28 ways that you might want to do this it just depends on how explicit you want to\n1:06:33 be generally speaking I think for myself I would prefer that we\n1:06:40 stick with the objects themselves and be explicit but it is definitely a little\n1:06:45 harder to pass when you're when you're reading this so I understand why you might also prefer this it's definitely\n1:06:52 cleaner and it is it does look simpler so it just depends I suppose on\n1:07:00 preference okay so we can see again that this is exactly the same okay with chat\n1:07:06 prom template and it contains this and this okay you probably want to see the\n1:07:12 exact output so it was messages okay exactly the same as what I\n1:07:19 output before cool so we have all that let's see how we would invoke our l L with\n1:07:26 these we're going to be using 40 mini again we do need our open API key so\n1:07:32 enter that and we'll just initialize our LM we\n1:07:38 are going with a low temperature here so less Randomness or less\n1:07:43 creativity and you in in many cases this is actually what I would be doing the\n1:07:48 reason in this scenario that we're going with a low temperature is we're doing\n1:07:54 Rag and if you you remember before if we scroll up a little bit here our template says answer the user's query based on\n1:08:00 the context below if you cannot answer the question using the provided answer information answer with I don't know\n1:08:08 right so just from reading that we know that we want our llm to be as truthful\n1:08:15 and accurate as possible so a more creative llm is going to struggle with\n1:08:20 that and is more likely to hallucinate whereas a low creativity or\n1:08:26 low temperature llm will probably stick with the rules a little better so again it depends on your use case you know if\n1:08:33 you're creative writing you might want to go with a higher temperature there but for things like rag where the\n1:08:40 information being output should be accurate and truthful it's important I\n1:08:46 think that we keep temperature low okay I talk about that a little bit here so\n1:08:52 um of course lower temperature of zero makes the LM output more deterministic which in theory should lead to less\n1:08:59 hallucination okay so we're going to go with L cell again here this is for those\n1:09:04 of you that use LINE chain pass this is equivalent to an llm chain object so our\n1:09:09 prompt template is being fed into our LM okay and from now we have this pipeline\n1:09:18 now let's see how we would use that pipeline so going to get some uh create\n1:09:23 some context here so so this just some Contex around orelio\n1:09:30 AI mention that we built sematic routers SM junkers there AI\n1:09:37 platform and development services we mentioned I think we\n1:09:42 specifically outline this later on in the example so the Align chain experts little piece of information now most LMS\n1:09:49 would have not been trained on the recent internet so the fact that this\n1:09:54 came in September M 2024 is relatively recent so a lot of LMS out of the box\n1:10:00 you wouldn't expect them to know that so that is a good little bit of information\n1:10:05 to ask about so we invoke we have our query so what do we do and we have that\n1:10:11 context okay so we're feeding that into that pipeline that we defined here all right so when we invoke that that is\n1:10:18 automatically going to take query and context and actually feed it into our prompt template okay\n1:10:26 if we want to we can also be a little more explicit so you you will probably\n1:10:32 see me doing this uh throughout the course because I do like to be explicit\n1:10:37 with everything to be honest and you'll probably see me doing\n1:10:47 this okay and this is doing the same thing or you'll see it will in a moment\n1:10:54 this is doing the exact same thing again this is just a outo thing\n1:10:59 so all I'm doing in this scenario is I'm saying okay take from the dictionary\n1:11:08 query and then also take from that input dictionary the context\n1:11:16 key okay so this is doing the exact same thing uh the reason that we might want\n1:11:22 to write this is mainly for clarity to be honest just to explicit say okay\n1:11:27 these are the inputs because otherwise we don't really have them in the code other than within our original prompts\n1:11:34 up here which is not super clear so I think it's usually a good\n1:11:40 idea to just be more expc with these things and of course if you decide you're going to modify things a little\n1:11:45 bit let's say you modify this to input down the line you can still feed in the same input here you're just you know\n1:11:52 mapping it between different Keys essentially or if you would like to just modify that I don't know you need to\n1:11:58 locase it on the way in or something you can do so you have that I'll just redefine\n1:12:08 actually and we'll invoke again okay we see that it does the exact\n1:12:14 same thing okay so R AI so this is the AI message just generated by the llm\n1:12:21 okay expertise in building AI agents several open source framework router AI\n1:12:29 platform okay right so they have everything there other than the line\n1:12:34 train experts thing it didn't mention that but we will yeah we'll test it later on that okay so on to Future\n1:12:41 prompting this is a specific prompting technique now many sort of State ofthe art or also to LMS are very good at\n1:12:49 instruction following so you'll find that fuch shop prompting is less common\n1:12:54 now than it used to be at least for the sort of bigger more safy art models but\n1:13:01 when you start using smaller models not really what we can use here but let's say you're using a open source model\n1:13:08 like llama 3 or llama 2 which is much smaller you will probably need to\n1:13:15 consider things like f shot prompting although that being said with the open AI models you're at least the current\n1:13:23 open AI models this is not so important nonetheless it can be useful so the idea\n1:13:29 behind fuchia prompting is that you are providing a few examples to your llm of\n1:13:35 how it should behave before you are actually going\n1:13:41 into the main part of the conversation so let's see how that would look so we\n1:13:46 create an example prom so we have our human in AI so human input AI response\n1:13:52 so we're basically saying up okay this with this type of input you should provide this type of output that's what\n1:13:58 we're doing here and we're just going to provide some examples okay so we have our input here's query one here is the\n1:14:06 answer one right this is just I just want to show you how it works this is not what we'd actually feed into our LM\n1:14:13 then with both these examples and our example prompt we'd feed both of these\n1:14:18 into uh line chains few shot chat message prompt template okay and\n1:14:25 well you'll see what we get out of it okay so we basically get it formats everything and structures everything for\n1:14:31 us okay and using this of course it depends\n1:14:38 on let's say you see that your user is talking about a particular topic and you\n1:14:44 would like to guide your llm to talk about that particular topic and a particular way right so you could\n1:14:51 identify that the user is talking about that topic either like a keyword match or a semantic similarity match and based\n1:14:58 on that you might want to modify these examples that you feed into your few sh\n1:15:03 chat message promp template and then obviously for that could be what you do for topic a for topic B you might have\n1:15:10 another set of examples that you feed into this all all this time your example prompt is remaining the same but you're\n1:15:16 you're just modifying the examples that are going in so that they're more relevant to whatever it is your user is\n1:15:21 actually talking about so that can be useful now let's see an example of that so when we are using a tiny LM It's\n1:15:29 ability would be limited although I think we are we're probably fine here we're going to say answer the US query\n1:15:36 based on the context below always answering mark down format you know being very specific the self system\n1:15:42 prompt okay that's nice but what we've kind of said here is okay always\n1:15:48 answering mod down for I did do that but when doing so please provide headers\n1:15:54 short summary and follow bullet points then conclude okay so you see this here\n1:16:01 okay so we get this overview of already you have this and this it's actually quite good but if we come down here what\n1:16:08 I specifically want is to always follow this structure right so we have the\n1:16:14 double header for the topic summary header a couple of bullet points and\n1:16:21 then I always want to follow this pattern where it's like to conclude always it's always bold you know I want to be very specific on\n1:16:27 what I want and to be you know fully honest with GT40 mini you can actually\n1:16:34 just prompt most of this in but for the sake of the example we're going to provide a few shot um examples in our\n1:16:42 few shot prompt examples instead to get this so we're going to provide one example here second example here and\n1:16:50 you'll see we're just following that same pattern we're just setting up the pattern that the llm should use so we're\n1:16:57 going to set that up here we have our main header a little summary some\n1:17:03 subheaders bullet points subheader bullet points subheader bullet points to conclude so on and so on same with this\n1:17:09 one here okay and let's see what we\n1:17:15 got okay so this is the structure of our new F shop prompt template you can see\n1:17:24 what all this looks like let's come down and we're going to do we're basically going to insert that directly into our\n1:17:31 chat prompt template so we have for messages system\n1:17:36 prompt user prompt and then we have in there these so let me actually show you\n1:17:44 very quickly right so we just have um this few shot chat to message prompt template\n1:17:50 which will be fed into the middle here run that and then feed all this back into our pipeline okay and this will you\n1:17:57 know modify the structure so that we have that bold to conclude at the end here okay we can see nicely here so we\n1:18:04 get a bit more of that exact structure that we were getting again with GT40\n1:18:10 models and many other opening air models you don't really need to do this but you will see it in other examples we do have\n1:18:17 an example of this where we're using a llama and we're using I think llama 2 if\n1:18:23 I'm not wrong and you can see that adding this fuse shot promp template is\n1:18:29 actually a very good way of getting those smaller less capable models to follow your instructions so this is RAR\n1:18:37 when you're working those smaller lenss this can be super useful but even for so models like\n1:18:42 gp40 if you do find that you're struggling with the prompting it's just not quite following exactly what you\n1:18:48 want it to do this is a very good technique for actually getting it to\n1:18:53 follow a very straight structure or behavior okay so moving on we have Chain of Thought prompting so this is a more\n1:19:02 common prompting technique that encourages the LM to think through its\n1:19:08 reasoning or its thoughts step by step so it's Chain of Thought the idea behind\n1:19:13 this is that okay in math class when you're a kid the teachers would always\n1:19:19 push you to put down your your working out right and there was a more reasons\n1:19:25 for that one of them is to get you to think because they they know in a lot of cases actually you know you're a kid and you're in Aran you don't really care\n1:19:31 about this test and the you know they're just trying to get you to slow down a\n1:19:37 little bit and actually put down your reasoning and that kind of forc you to think oh actually I'm skipping a little\n1:19:43 bit in my head because I'm trying to just do everything up here if I write it down all of a sudden it's like oh\n1:19:48 actually I yeah I need to actually do that slightly differently you you realize okay you're probably rushing now\n1:19:55 I'm not saying an LM is rushing but it's a similar effect by an LM writing everything down they tend to actually\n1:20:01 get things right more frequently and at the same time also similar to when\n1:20:07 you're a child and a teacher is reviewing your exam work by having the LM write down its reasoning you as a as\n1:20:15 a human or engineer you can see where the llm went wrong if it did go wrong\n1:20:21 which can be very useful when you're trying to diagnose problems so with train of thought we should see uh less\n1:20:27 hallucinations and generally better performance now to implement train of thought in line chain there's no\n1:20:32 specific like line chain objects that do that instead it's it's just prompting okay so let's go down and just see how\n1:20:39 we might do that okay so be helpful assistant and answer users question you\n1:20:44 must answer the question directly without any other text or explanation okay so that's our no Chain of Thought\n1:20:51 system problems I will just note here especially with open AI again this is one of those things where you'll see it\n1:20:57 more with the smaller models most LMS are actually trained to use train thought prompting by default so we're\n1:21:03 actually specifically telling it here you must answer the question directly without any other text or explanation\n1:21:09 okay so we're actually kind of reverse prompting it to not use train of thought otherwise by default it actually will\n1:21:16 try and do that because it's been trained to that's how that's how relevant Chain of Thought is okay so I'm\n1:21:22 going to say how many key strokes I need to type in type the numbers from 1 to 500 okay we set up our like llm chain\n1:21:31 Pipeline and we're going to just invoke our query and we'll see what we get\n1:21:36 total number of key strokes needed to type the numbers from one to 500 is\n1:21:43 1,511 uh the actual an as I've written here is 1,392 without chain thought is\n1:21:50 hallucinating okay now let's go ahead and see okay with Chain of Thought apprting what does it do so be helpful\n1:21:57 assistant and answer user question to answer the question you must list systematically and in precise detail all\n1:22:05 sub problems that are needed to be solved to answer the question solve each sub problem individually you have to\n1:22:11 shout at the LM sometimes to get them to listen and in sequence finally use\n1:22:17 everything you've worked through to provide the final answer okay so we're getting it we're forcing it to kind of\n1:22:22 go through the full problem there can remove that not sure why that's there so\n1:22:27 run that again I don't know why we have context there I remove that and let's\n1:22:35 see you can see straight away that's taking a lot longer to generate output\n1:22:40 that's because it's generating so many more tokens so that's just one one drawback of this but let's see what we\n1:22:46 have so to determine how many keystrokes to tie those numbers we is breaking down\n1:22:52 several sub problems so count number of digits from 1 to 9 10 to 99 so so on and\n1:22:58 count the digits in number 500 okay interesting so that's how it's breaking it up some more digits count in the\n1:23:05 previous steps so we go through total digits and we see that's okay nine\n1:23:12 digits for those for here 180 for here\n1:23:17 1,200 and then of course three here so it gets all those sums those digits and\n1:23:25 actually comes to the right answer okay so that that is you that's the difference with with Chain of Thought\n1:23:30 versus without so without it we just get the wrong answer basically guessing with\n1:23:37 chain of thought we get the right answer just by the llm writing down its reasoning and breaking the problem down\n1:23:44 into multiple Parts which is I found that super interesting that it it does that so that's pretty cool now I will\n1:23:53 just see so as I as we mentioned before most llms nowadays are actually training\n1:23:58 to use train of thought prompting by default so let's just see if we don't mention anything right be a helpful\n1:24:03 assistant and answer the users question so we're not telling it not to think through it's reasoning and we're not\n1:24:09 telling it to think through its reasoning let's just see what it does okay so you can see again it's\n1:24:16 actually doing the exact same reasoning\n1:24:21 okay it doesn't it doesn't give us like the sub problems that the start but it is going through and it's breaking\n1:24:27 everything apart okay which is quite interesting and we get the same correct answer so the formatting here is\n1:24:33 slightly different it's probably a little cleaner actually although I think\n1:24:38 uh I don't know I here we get a lot more information so both are fine and in this\n1:24:46 scenario we actually do get the the right answer as well so you can see that that Chain of Thought prompting has\n1:24:52 actually been quite literally trained into the model and you'll see that with\n1:24:58 most well I think all save the-art lenss Okay cool so that is our our chapter on\n1:25:07 prompting again we're focusing very much on a lot of the fundamentals of\n1:25:13 prompting there and of course tying that back to the actual objects and methods\n1:25:19 within langing but for now that's it for prompting and we'll move on to the next ch chapter in this chapter we're going\n1:25:26 to be taking a look at conversational memory in line chain we're going to be\n1:25:31 taking a look at the core like chat memory components that have really been\n1:25:37 in line chain since the start but are essentially no longer in the library and\n1:25:43 we'll be seeing how we actually Implement those historic conversational\n1:25:49 memory Utilities in the new versions of Lang chain so 0.3 now as a pre-warning\n1:25:57 this chapter is fairly long but that is because conversational memory is just\n1:26:02 such a critical part of chatbots and agents conversational memory is what\n1:26:08 allows them to remember previous interactions and without it our chat\n1:26:13 boox and agents would just be responding to the most recent message without any\n1:26:18 understanding of previous interactions within a conversations so they would just not be coners ational and depending\n1:26:26 on the type of conversation we might want to go with various approaches to\n1:26:32 how we remember those interactions within a conversation now throughout\n1:26:38 this chapter we're going to be focusing on these for memory types we'll be\n1:26:43 referring to these and I'll be showing you actually how each one of these works but what we're really focusing on is\n1:26:49 rewriting these for the latest version of Lang chain using the what it's called\n1:26:54 the runable with\n1:27:00 message history so we're going to be essentially taking a look at the original implementations for each of\n1:27:07 these four original memory types and then we'll be rewriting them with the the runnable memory history class so\n1:27:14 just taking a look at each of these four very quickly conversational buffer\n1:27:19 memory is I think the simplest and most intuitive of these meor types it is\n1:27:26 literally just you have your messages they come into this object they are sold\n1:27:33 in this object as essentially a list and when you need them again it will return\n1:27:38 them to you there's nothing and nothing else to it's super simple the conversation Buffet window memory okay\n1:27:44 so new word in the middle of the window this works in pretty much the same way\n1:27:51 but those messages that it has stored is not going to return all of them for you instead it's just going to return the\n1:27:57 most recent let's say the most recent three for example okay and that is defined by a parameter K conation of\n1:28:05 summary memory rather than keeping track of the entire uh interaction memory\n1:28:11 directly what it's doing is as those interactions come in it's actually going to take them and it's going to compress\n1:28:18 them into a smaller little summary of what has been within that conversation\n1:28:24 and as every new interaction is coming in it's going to do that going to keep iterating on that summary and then that\n1:28:31 is going to be return to us when we need it and finally we have the conversational summary buffer memory so\n1:28:37 this is it's taking so the buffer part of this is actually referring to very\n1:28:43 similar thing to the buffer window memory but rather than it being a you know most K messages it's looking at the\n1:28:50 number of tokens within your memory and it's returning the most recent K\n1:28:57 tokens that's what the buffer part is there and then it's also merging that\n1:29:02 with the summary memory here so essentially what you're getting is almost like a list of the most recent\n1:29:09 messages based on the token length rather than the number of interactions plus a summary which would you know come\n1:29:16 at the the top here so you get kind of both the idea is that obviously this\n1:29:21 summary here would maintain all of your interactions in a very\n1:29:27 compressed form so you're you're losing less information and you're still maintaining you know maybe the very\n1:29:33 first interaction the user might have introduced themselves giving you their name hopefully that would be maintained\n1:29:40 within the summary and it would not be lost and then you have almost like higher resolution on the most recent um\n1:29:47 K or k tokens from your memory okay so let's jump over to the code we're going\n1:29:53 into the 04 chat memory notebook open that in collab okay now here we are let's go ahead and install the\n1:30:01 prerequisites run all we again can or cannot use align\n1:30:06 Smith it is up to you enter that and let's come down and start so first just\n1:30:13 initialize our LM using 40 mini in this example again low\n1:30:20 temperature and we're going to start with conversation buffer memory right so this is the original version of this uh\n1:30:29 memory type so let me where are we we're here so memory conversation both of\n1:30:36 memory and we're returning messages that needs to be set to true so the reason that we set return messages true it it\n1:30:44 mentions up here is if you do not do this it's going to returning your chat\n1:30:50 history as a string to an llm whereas well chat lm's nowadays would expect\n1:30:58 message objects so yeah you just want to be returning these as messages rather\n1:31:03 than as strings okay otherwise yeah you're going to get some kind of strange Behavior out from your llms if you\n1:31:10 return them strings so you do want to make sure that it's true I think by default it might not be true but this is\n1:31:16 coming this is deprecated right it does tell you here as de creation warning\n1:31:21 this is coming from older BL chain but it's a good place to start just to understand this and then we're going to rewrite this with the runnables which is\n1:31:28 the recommended way of doing so nowadays okay so adding messages to our memory\n1:31:34 we're going to write this okay so it's just a just a conversation user AI user\n1:31:39 AI so on and so on random chat main things to not here is I do provide my\n1:31:45 name we have the the model's name right towards the start of those interactions okay so I'm just going to add all of\n1:31:52 those with do it like this okay then we can just see we can load our history\n1:32:01 like so so let's just see what we have there okay so we have human message AI message human message right this is\n1:32:08 exactly what we I showed you just here it's just in that message format from line chain okay so we can do that\n1:32:16 alternatively we can actually do this so we can get our memory we initialize the conation buffer memory as we did before\n1:32:24 and we can actually add it directly the message into our memory like that so we can use this add us message add AI\n1:32:30 message so on and so on load again and it's going to give us the exact same thing again there's multiple ways to do\n1:32:37 the same thing cool so we have that to pass all of this into our LM again this\n1:32:42 is all deprecated so we're going to learn how to properly in a moment but this is how L chain was doing in the\n1:32:49 past so to pass all of this into our LM we' be using this conversation chain\n1:32:55 right again this is deprecated nowadays we would be using lell for this so I I\n1:33:02 just want to show you okay how this would all go together and then we would invoke okay what is my name again let's\n1:33:08 run that and we'll see what we get it's remembering everything remember so this\n1:33:13 conversation buffer memory it doesn't drop messages it just remembers everything right and honestly with the\n1:33:20 sort of high context Windows of many LMS might be what you do it depends on how\n1:33:25 long you expect the conversation to go on for but you could you probably in most cases would get away with this okay\n1:33:32 so what let's see what we get um I say what is my name again okay let's see\n1:33:38 what it gives me says your name is chains great thank you that works now as\n1:33:44 I mentioned all of this that I just showed you is actually deprecated that's the old way doing things let's see how\n1:33:49 we actually do this in modern or up toate L chain so we're going to be using this runable with message history to\n1:33:57 implement that we will need to use LL and for that we will need to just Define\n1:34:02 prompt templates our LM as we usually would okay so we're going to set up our system prompt which is just a helpful\n1:34:09 assist called Zeta okay we're going to put in this messages\n1:34:15 placeholder okay so that's important essentially that is where our messages\n1:34:20 are coming from our conversation Buffet for memory is going to be inserted right\n1:34:26 so it's going to be that chat history is going to be inserted after our system prompt but before our most recent query\n1:34:33 which is going to be inserted last here okay so messages placeholder item that's\n1:34:39 important and we use that throughout the course as well so we use it both for chat history and we'll see later on we\n1:34:45 also use it for the intermediate thoughts that a agent would go through as well so important to remember that\n1:34:52 little thing well link our prompt template to our LM again if we would\n1:34:58 like we could also add in the I think we only have the query here oh we would\n1:35:04 probably also want our history as well but I'm not going to do that right now\n1:35:10 okay so we have our Pipeline and we can go ahead and actually Define our runnable with message history now this\n1:35:17 class or object when we are initializing it does require a few items we can see them here okay so we' see that we have\n1:35:24 our Pipeline with history so it's basically going to be uh you can you can see here right we have that history\n1:35:30 messages key right this here has to align with what we provided as a meses\n1:35:36 placeholder in our pipeline right so we have our pipeline prompt template here\n1:35:44 and here right so that's where it's coming from it's coming from messes placeholder variable name is history\n1:35:49 right that's important that links to this then for the input messages key\n1:35:55 here we have query that again links to this okay so both important to have\n1:36:03 that the other thing that is important is obviously we're passing in that pipeline from before but then we also\n1:36:09 have this get session history basically what this is doing is it's saying okay I need to get uh the list of messages that\n1:36:16 make up my chat history that are going to be inserted into this variable so that is a function that we Define okay\n1:36:22 and with within this function what we're trying to do here is actually replicate\n1:36:28 what we have with the previous conversation buffer memory okay\n1:36:34 so that's what we're doing here so it's very simple right so we have this in\n1:36:40 memory chat message history okay so that's just the object that we're going to be returning what this will do is it\n1:36:47 will set up a session ID the session ID is essentially like a unique identifier so that eachers ation or interaction\n1:36:55 within a single conversation is being mapped to a specific conversation so you don't have overlapping let say have\n1:37:00 multiple users using the same system you want to have a unique session ID for each one of those okay and what it's\n1:37:06 doing is saying okay if session ID is not in the chat map which is this empty dictionary we defined here we are going\n1:37:13 to initialize that session with an inmemory chat message history okay\n1:37:21 that's it and we return okay and all that's going to do is it's going to basically append our messages they will\n1:37:28 be appended within this chat map session ID and they're going to get returned\n1:37:33 there's nothing R there's nothing else to it to be honest so we invoke our\n1:37:39 rable let's see what we get I need to ruin\n1:37:45 this okay note that we do have this config so we have a session ID that's to\n1:37:50 again as I mentioned keep different conversations separate Okay so we've run that now let's run a few more so what is\n1:37:57 my name again let's see if it remembers your name is James how can I help you today James okay so it's what we've just\n1:38:06 done there is literally conversation buter memory but for upto-date L chain\n1:38:14 with L cell with Runner BS so you the recommended way of doing it nowadays so\n1:38:20 that's a very simple example okay really and not that much to it it gets a little\n1:38:27 more complicated as we start thinking about the different types of memory although that being said it's not\n1:38:33 massively complicated we're only rarely going to be changing the way that we're getting our interactions so let's uh\n1:38:41 let's dive into that and see how we will do something similar with the conation buff for window memory but first let's\n1:38:47 actually just understand okay what is Con station buffer window memory so as I mentioned near the start it's going to\n1:38:53 keep track of the last K messages so there's a few things to keep in mind\n1:38:58 here more messages does mean more tokens send with each request and if we have\n1:39:03 more tokens in each request it means that we're increasing the latency of our responses and also the cost so with the\n1:39:10 previous memory type we're just sending everything and because we're sending everything that is going to be\n1:39:15 increasing our cost it's going to be increasing our latency for every message especially as a conversation gets longer\n1:39:21 and longer and we don't we might not necessarily want to do that so with this conversation buffer window memory we're\n1:39:28 going to just say okay just return me the most recent messages okay so let's\n1:39:35 well let's see how that would work here we're going to return the most recent four messages okay we are again make\n1:39:41 sure we've turned messages is set to True again this is deprecated this is just the old way of doing it in a moment\n1:39:48 we'll see the updated way of doing this we'll add all of our\n1:39:54 messages okay so we have this and just see here right so we've added in all\n1:40:01 these messages there's more than four messages here and we can actually see that here so we have human message AI\n1:40:07 human AI human AI human AI right so we've got four pairs of human AI\n1:40:14 interactions there but here we don't have there's more than four pairs so four pairs will take us back all the way\n1:40:21 to here I'm researching different types of conversational uh memory okay and if\n1:40:28 we take a look here the most the first message we have is I'm researching different types of conversational memory\n1:40:33 so it's cut off these two here which will be a bit problematic when we ask you what our name is okay so let's just\n1:40:40 see going to be using conversation chain object again again just remember that is\n1:40:45 deprecated and I want to say what is my name again let's see let's see what it\n1:40:51 says uh I'm sorry I but I don't have access to your name or any personal information if you like you can tell me\n1:40:56 your name right so it doesn't actually remember uh so that's kind of like a negative of the conversation Buffet\n1:41:04 window memory of course the uh to fix that in this scenario we might just want to increase K maybe we say remember the\n1:41:12 previous eight interaction Pairs and it will actually remember so what is my\n1:41:17 name again your name is James so now it remembers we've just modified how much it is remembering but of course you know\n1:41:24 pros and cons to this it really depends on what you're trying to build so let's take a look at how we would actually\n1:41:30 implement this with the runable with message history okay so you getting a little\n1:41:38 more complicated here although it it's it's not it's not complicated but well\n1:41:44 we'll see okay so we have buffer window message history we're creating a class here this class is going to inherit from\n1:41:51 the base chat message history object from line chain okay and in all of our\n1:41:57 other message history objects can do the same thing before with the inmemory\n1:42:02 message object that was basically replicating the buffer memory so we\n1:42:08 didn't actually need to do anything we didn't need to Define our own class here\n1:42:13 so in this case we do so we follow the same pattern that line chain follows\n1:42:19 with this base chat message history and you can see a few of the functions here that are important so add messages and\n1:42:26 clear are the ones that we're going to be focusing on we also need to have messages which this object attribute\n1:42:31 here okay so we're just implementing the synchronous methods here if we want this\n1:42:38 to be async if we want to support async we would have to add a add messages um a\n1:42:44 get messages and a clay as well so let's go ahead and do that we have messages we\n1:42:49 have K again we're looking at remembering the top K messages or most recent K messages only so it's important\n1:42:56 that we have that variable we are adding messages through this class this is\n1:43:01 going to be used by line chain within our runnable so we need to make sure that we do have this method and all\n1:43:06 we're going to be doing is extending the self messages uh list here and then we're actually just going to be trimming\n1:43:13 that down so that we're not remembering anything beyond those you know most recent K\n1:43:19 messages that we have set from here and then we also have the clear method\n1:43:25 as well so we need to include that that's just going to clear the history okay so it's not this isn't complicated\n1:43:31 right it just gives us this nice default standard interface for message history\n1:43:37 and we just need to make sure we're following that pattern okay I've included the uh this print here just so we can see what's happening okay so we\n1:43:45 have that and now for that get chat history function that we defined earlier\n1:43:51 rather than using the buin method we're going to be using our own object which is a buffer window message history which\n1:43:58 will be defined just here okay so if session ID is not in the chat map as we\n1:44:05 did before we're going to be initializing our buffer window message history we're setting K up here with a\n1:44:10 default value of four and then we just return it okay and and that is it so let's run this we have our runable with\n1:44:18 message history we have all of these variables which are exactly the same as before four but then we also have these\n1:44:25 variables here with it's history Factory config and this is where if we have um\n1:44:33 new variables that we've added to our message history in this case k that we\n1:44:38 have down here we need to provide that to line train and sell it this is a new configurable field okay and we've also\n1:44:46 added it for the session ID here as well so we're just being explicit and have everything in that so we have that\n1:44:53 and we run okay now let's go ahead and invoke and see what we get okay so\n1:45:00 important here this history Factory config that is kind of being fed through\n1:45:06 into our invoke so that we can actually modify those variables from here okay so we have config configurable session ID\n1:45:14 okay we just put whatever we want in here and then we also have the number K okay so remember the previous four\n1:45:22 interaction I think in this one we're doing something slightly different I think we're remembering the four\n1:45:27 interactions rather than the previous four interaction pairs okay so my name is James uh we're going to go through\n1:45:34 I'm just going to actually clear this and now I'm going to start again and we're going to use the exact same ad\n1:45:40 user message ad AI message that we used before we're just manually inserting all that into our history so that we can\n1:45:46 then just see okay what is the result and you can see that k equal 4 is actually unlike before where we were\n1:45:53 having the uh saving the top four interaction pairs we now saving the most\n1:46:01 recent four interactions not pairs just interactions and honestly I just think\n1:46:06 that's clearer I think it's weird that the number four for K would actually save the most recent eight messages\n1:46:14 right I I think that's odd so I'm just not replicating that weirdness we could\n1:46:19 if we wanted to I just don't like it so I'm not doing that and anyway we can see\n1:46:26 from messages that we're returning just the most four recent messages okay which\n1:46:31 should be these four Okay cool so we've just using the runable we've replicated\n1:46:38 the old way of having a window memory and okay I'm going to say what is my\n1:46:43 name again as before it's not going to remember so we can come to here I'm sorry about I don't have access to\n1:46:48 personal information so on and so on if you like to tell me your name doesn't know now let's try a new one where we\n1:46:56 initialize a new session okay so we're going with ID K4 so that's going to\n1:47:01 create a new conversation there and we're going to say we're going to set K to\n1:47:07 14 okay great I'm going to manually insert the other uh messages as we did\n1:47:13 before okay and we can see all of those and see at the top here we are still maintaining that hi my name is James\n1:47:19 message now let's see if it remembers my name your name is James okay there we go\n1:47:27 cool so that is working we can also see so we just added this what is my name again let's just see if did that get\n1:47:34 added to our list of messages right what is my name again nice and then we also\n1:47:39 have the response your name is James so just by invoking this because we're using the the runable with message\n1:47:46 history it's just automatically adding all of that into our message history\n1:47:52 which is nice cool all right so that is the buffer window memory now we are going to take a\n1:47:59 look at how we might do something a little more complicated which is the the summaries okay so when you think about\n1:48:06 the summary you know what are we doing we're actually taking the messages we're using that LM call to summarize them to\n1:48:14 compress them and then we're storing them within messages so let's see how we would actually uh do that so to start\n1:48:22 with let's just see how it was done in Old Line chain so we have conversation\n1:48:28 summary memory go through that and let's just see what we get so\n1:48:35 again same interactions right I'm just invoking invoking invoking I'm not adding these\n1:48:41 directly to the messages because it actually needs to go through a um like that summarization process and if we\n1:48:49 have a look we can see it happening okay current conversation so sorry current\n1:48:55 conversation hello there my name is James AI is generating current conversation the human introduces\n1:49:01 himself as James AI greets James warmly and expresses its Readiness to chat and assist inquiring about how his day is\n1:49:08 going right so it's summarizing the the previous interactions and then we have\n1:49:14 you know after that summary we have the most recent human message and then the AI is going to generate its response\n1:49:21 okay and that continues your own Contin is going and you see that the the final summary here is going to be a lot longer\n1:49:27 okay it's different that first summary of course asking about his Day Men researching different types of\n1:49:32 conversational memory the AI responds enthusiastically explaining that conversational memory includes\n1:49:37 short-term memory longterm memory contextual memory personalized memory and then inquires if James is focused on a specific type of memory Okay cool so\n1:49:46 we get essentially the summary is just getting uh longer and longer as we go but at some point the idea is that it's\n1:49:53 not going to keep growing and it should actually be shorter than if you were saving every single interaction whilst\n1:49:59 maintaining as much all the information as possible but of course you're not\n1:50:04 going to maintain all of the information that you would with for example the the\n1:50:09 buffer memory right with the summary you are going to lose information but\n1:50:15 hopefully less information than if you're just cutting interactions so\n1:50:20 you're trying to reduce your token count whilst maintaining as much information as\n1:50:25 possible now let's go and ask what is my name again it should be able to answer\n1:50:30 because we can see in the summary here that I introduced myself as\n1:50:36 James okay respondents your name is James how is your research going okay so\n1:50:41 has that cool let's see how we'd Implement that so again as before we're\n1:50:46 going to go with that conversation summary message history we're going to\n1:50:52 be importing this system message uh we're going to be using that not for the LM that we're chatting with but for the LM that will be generating our summary\n1:51:01 so actually that is not quite correct there is create a summary not that it\n1:51:06 matters it's just the doct string so we have our messages and we also have the LM so different different attribute here\n1:51:12 to what we had before when we initialize a conversation summary message history we need to passing in our LM we have the\n1:51:19 same methods as before we have ADD messages and clear and what we're doing is as messages coming we extend with our\n1:51:27 current messages but then we're modifying those okay so we construct our like\n1:51:34 instructions to make a summary okay so that is here we have the system front uh\n1:51:40 giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring to\n1:51:45 maintain as much relevant information as possible okay then we have a human message here through that we're passing\n1:51:52 the existing summary okay and then we're passing in the new\n1:51:57 messages Okay cool so we format those invoke the\n1:52:05 llm here and then what we're doing is in the messages we're actually replacing\n1:52:11 the existing history that we had before with a new history which is just a single system summary message okay let's\n1:52:20 see what we get as before we have that get chat history exactly the same as before the only real difference is that\n1:52:26 we're passing in the llm parameter here and of course as we're passing in the LM parameter in here it does also mean that\n1:52:33 we're going to have to include that in the configurable field spec and that we're going to need to include that when\n1:52:40 we're invoking our pipeline okay so we run that pass in the\n1:52:48 LM now of course one side effect of generating summaries for everything is that way actually you know we're\n1:52:54 generating more so you are actually using quite a lot of tokens whether or\n1:52:59 not you are saving tokens or not actually depends on the length of a conversation as the conversation gets\n1:53:05 longer if you're storing everything after a little while that the token usage is actually going to increase so\n1:53:12 if in your use case you expect to have shorter conversations you would be\n1:53:17 saving money and tokens by just using this standard buffer memory\n1:53:23 whereas if you're expecting very long conversations you would be saving tokens and money by using the summary history\n1:53:31 okay so let's see what we got from there we have a summary of the conversation James introduced himself by saying hi\n1:53:37 name James a I responded War asking hi James Interac include details about token\n1:53:42 usage okay so we actually included everything here which we probably should\n1:53:48 not have done why did we do that as so in here we're including all\n1:53:56 of the out in here so we using or including\n1:54:02 all of the content from the messages so I think maybe we just do X content for X in messages that\n1:54:12 should resolve that okay there we go so we quickly fli\n1:54:20 that so yeah before we pass them in the entire mage object which obviously includes all of this information whereas\n1:54:26 actually we just want to be passing into the content so we modified that and now\n1:54:31 we're getting what we would expect okay cool and then we can keep going right so as we as we keep going\n1:54:38 the summary should get more like abstract like as we just saw here is\n1:54:43 literally just giving us the messages directly almost okay so we're getting a bit of summary there and we can keep\n1:54:50 going we're going to add just more messages to that we'll see the you as we'll get send those we'll get a\n1:54:57 response send it again get a response and we just adding all of that invoking all of that and that will be of course\n1:55:03 adding everything into our message history Okay cool so we've run that\n1:55:09 let's see what the latest summary is okay and then we have this so this is\n1:55:15 a summary that we have inside of our our chat history okay cool now finally let's see\n1:55:24 what is my name again we can just double check you know it has my name in there so it should be able to tell\n1:55:33 us okay cool so your name is James pretty interesting so let's have a quick\n1:55:39 look over at limith so the reason I want to do this is just to point out okay the\n1:55:45 different essentially token usage that we're getting with each one of these okay so we can see that we have these Runner mess history which probably uh\n1:55:53 improved in naming there but we can see okay how long is each one of these taken\n1:55:59 how many tokens are they also using come back to here we have this runable\n1:56:04 message history this is we'll go through a few of these maybe to here I think we\n1:56:11 can see here this is that first interaction where we're using the buffer memory and we can see how many tokens we\n1:56:17 used here so 112 tokens when we're asking what is my name again okay then\n1:56:23 we modified this to include I think it was like 14 interactions or something on\n1:56:29 those lines obviously increases the number of tokens that we're using right so we can could see that actually happening all in Lang which is quite\n1:56:36 nice and we can compare okay how many tokens is each one of these using now this is looking at the buffer window and\n1:56:44 then if we come down to here and look at this one so this is using our summary\n1:56:49 okay so our summary with what is my name again actually use more tokens in this scenario right which is interesting\n1:56:55 because we're trying to compress information the reason there more is because there's not there hasn't been that many interactions as the\n1:57:02 conversation length increases with the summary this total\n1:57:08 number of tokens especially if we prompt it correctly to keep that low that should remain relatively small\n1:57:15 whereas with the buffer memory that will just keep increasing and increasing as the as the conversation gets longer\n1:57:23 so useful little way of using Lang Smith there to just kind of figure out okay in\n1:57:29 terms of tokens and costs of what we're looking at for each of these memory types okay so our final memory type acts\n1:57:37 as a mix of the summary memory and the buffer memory so what it's going to do\n1:57:43 is keep the buffer up until an N number of tokens and then once a message\n1:57:50 exceeds the N number of tokens limit for the buffer it is actually going to be\n1:57:55 added into our summary so this memory has the benefit of remembering in detail\n1:58:03 the most recent interactions whilst also not having the limitation of using too\n1:58:10 many tokens as a conversation gets longer and even potentially exceeding context Windows if you try super hard so\n1:58:18 this is a very interesting approach now as before let's try the original way of\n1:58:24 implementing this then we will go ahead and use our update method for\n1:58:30 implementing this so we come down to here and we're going to do L chain memory import conversation summary\n1:58:37 buffer memory okay a few things here LM for summary we have the N number of\n1:58:44 tokens that we can keep before they get added to the summary and then return messages of course okay you can see\n1:58:51 again this is dicated we use the conversation chain and then we just passing our memory there and\n1:58:57 then we can chat okay so super straightforward first message we'll add\n1:59:03 a few more here and we have to invoke because how\n1:59:08 memory type here is using NM to create those summaries as it goes and let's see\n1:59:14 what they look like okay so we can see for the first message here we have human message and then an AI message\n1:59:22 then we come a little bit lower down again it's same thing human message is the first thing in our history here then\n1:59:29 it's a system message so this is at the point where we've exceeded that 300 token limit and the memory type here is\n1:59:36 generating those summaries so that summary comes in as a system message and we can see okay the human named James\n1:59:43 introduces himself and mentions he's researching different types of conversational memory and so on and so on right okay cool so we have that then\n1:59:52 let's come down a little bit further we can see okay so the summary there okay\n1:59:59 so that's what we that's what we have that is the implementation for the old\n2:00:05 version of this memory again we can see it's deprecated so how do we implement this for our more recent versions of\n2:00:14 Lang chain and specifically 0.3 well again we're using that runable message\n2:00:20 history and it looks a little more complicated than we were getting before but it's actually just you know it's\n2:00:27 nothing too complex we're just creating a summary as we did with the previous\n2:00:33 memory type but the decision for adding to that summary is based on in this case\n2:00:39 actually the number of messages so I didn't go with the the Lang chain version where it's a number of tokens I\n2:00:46 don't like that I prefer to go with messages so what I'm doing is saying okay let K messages\n2:00:52 okay once we exceed K messages the messages beyond that are going to be\n2:00:58 added to the memory Okay cool so let's see we first initialize our conversation\n2:01:06 summary buffer message history class with llm and K okay so these two here so\n2:01:14 LM of course to create summaries and K is just the the limit of the number of messages that we want to keep before\n2:01:19 adding them to the summary or dropping them from now messages and adding them to the summary okay so we will begin with okay\n2:01:29 do we have an existing summary so the reason we set this in none is we can't\n2:01:36 extract the summary the existing summary unless it already exists and the only\n2:01:41 way we can do that is by checking okay do we have any messages if yes we want to check if within those messages we\n2:01:48 have a system message because we're we're doing the same structure is what we have up here where the system message\n2:01:54 that first system message is actually our summary so that's what we're doing here we're checking if there is a\n2:02:00 summary message already stored within our messages okay so we're checking for that if we\n2:02:08 find it we'll just do we have this little print statement so we can see that we found something and then we just\n2:02:14 make our existing summary I should actually move this to the first instance\n2:02:21 here yeah okay so that existing summary will be set to the first\n2:02:30 message okay and this would be a system message rather than a\n2:02:35 string cool so we have that then we want to add any new messages to our history\n2:02:43 okay so we're extending the history there and then we're saying okay if the length of our history is exceeds the K\n2:02:50 value that we set we're going say okay we found that many messages we're going to be dropping the latest it's going to be the latest two\n2:02:57 messages this I will say here one thing or one problem with this is that we're\n2:03:04 not going to be saving that many tokens if we're summarizing every two messages so what I would probably do is in in an\n2:03:12 actual like production setting I would probably say let's go up to 20 messages\n2:03:19 and once we hit 20 messages let's take the previous 10 we're going to summarize them and put them into our summary\n2:03:26 alongside any you know previous summary that already existed but in in you know this is also fine as well okay so we say\n2:03:36 we found those mes we're going to drop the latest two messages okay so we pull\n2:03:41 the the oldest messages out I should say not the latest it's the\n2:03:48 oldest not the latest I want to keep the latest drop the oldest so we pull out\n2:03:54 the oldest messages and keep only the most recent messages okay then I'm\n2:04:01 saying okay if we if we don't have any old messages to summarize we don't do\n2:04:06 anything we just return okay so this in the case that this has not been triggered we would hit this but in the\n2:04:14 case this has been triggered and we do have old messages we're going to come to\n2:04:20 here okay okay so this is we can see have a system message prompt template\n2:04:26 saying giving the existing conversation summary and the new messages generate a new summary of the conversation ensuring\n2:04:32 to maintain as much relevant information as possible so if you want to be more conservative with tokens we could modify\n2:04:39 this prompt here to say keep the summary to within the length of a single\n2:04:44 paragraph for example and then we have our human M prom template which is going to say okay here's the existing\n2:04:50 conversation summary in here on new messages now new messages here is actually the old messages but the way\n2:04:57 that we're framing it to the llm here is that we want to summarize the whole conversation right it doesn't need to\n2:05:03 have the most recent messages that we're storing within our buffer it doesn't need to know about those that's\n2:05:09 irrelevant to the summary so we just tell it that we have these Zoom mes and as far as this LM is concerned this is\n2:05:15 like the full set of interactions okay so then we would format those and invoke\n2:05:21 our LM and then we'll print out our new summary so we can see what's going on there and we would prend that new\n2:05:30 summary to our conversation history okay and and this will work so we can just\n2:05:37 prend it like this because we've already\n2:05:42 popped where was it up here if we have an existing\n2:05:47 summary we already pop that from the list it's already been pulled out of that list so it's okay for us to just we\n2:05:53 don't need to say like we don't need to do this because we've already dropped that initial system message if it\n2:06:00 existed okay and then we have the clear method as before so that's all of the\n2:06:05 logic for our conversational summary buffer memory we redefine our get chat\n2:06:14 history function with the LM and K parameters there and then we'll also\n2:06:20 want to set the configurable Fields again so that is just going to be of course session ID LM and\n2:06:28 K okay so now we can invoke the K value to begin with is going to be\n2:06:35 four okay so we can see no old messages to update summary with it's good let's\n2:06:42 invoke this a few times and let's see what we get okay so now M to summary with\n2:06:51 found six messages dropping the aest 2 and then we have new summary in the conversation James Inu himself and first\n2:06:58 is interestes in researching different types of conversational memory right so you can see there's quite a lot in here\n2:07:03 at the moment so we would definitely want to prompt the LM the summary LM to keep\n2:07:10 that short otherwise we're just getting a ton of stuff right but we can see that that is\n2:07:17 you know it's it's working it's functional so let's go back and see if we can prompt it to be a little more\n2:07:23 concise so we come to here ensuring to maintain as much relevant information as\n2:07:28 possible however we need to\n2:07:33 keep our summary concise the\n2:07:39 limit is a single short paragraph okay\n2:07:45 something like this let's try and let's see what we get with\n2:07:50 that okay so message one again nothing to update see this so new summary you can\n2:07:56 see it's a bit shorter it doesn't have all those bullet\n2:08:01 points okay so that seems better let's see so you can see the first summary is\n2:08:09 a bit shorter but then as soon as we get to the second and third summaries the\n2:08:14 second summary is actually slightly longer than the third one okay so we're going to be we're going to be losing a\n2:08:20 bit of information in this case more than we were before but we're saving a ton of tokens so that's of course a good\n2:08:28 thing and of course we could keep going and adding many interactions here and we should see that this conversation\n2:08:34 summary will be it should maintain that sort of length of around one short\n2:08:40 paragraph So that is it for this chapter on conation memory we've seen a few\n2:08:47 different memory types we've implemented their old deprecated version so we can see what they were like and then we've\n2:08:55 reimplemented them for the latest versions of Lang chain and to be honest using logic where we are getting much\n2:09:02 more into the weees and that is in some ways okay it complicates things that is\n2:09:07 true but in other ways it gives us a ton of control so we can modify those memory\n2:09:13 types as we did with that final summary buffer memory type we can modify those\n2:09:18 to our liking which is incredibly useful when you're actually building\n2:09:23 applications for the real world so that is it for this chapter we'll move on to the next one in this chapter we are\n2:09:30 going to introduce agents now agents I think are one of the most important\n2:09:37 components in the world of AI and I don't see that going away anytime soon I\n2:09:43 think the majority of AI applications the intelligent part of\n2:09:49 those will be was always an implementation of an AI agent or mle AI\n2:09:55 agents so in this chapter we are just going to introduce agents within the\n2:10:00 context of line chain we're going to keep it relatively simple we're going to go into much more depth in agents in the\n2:10:09 next chapter where we'll do a bit of a deep dive but we'll focus on just introducing the Core Concepts and of\n2:10:16 course agents within line chain here so jumping thing straight into our notebook\n2:10:24 let's run our prerequisites you'll see that we do have an additional prerequisite here which is\n2:10:30 Google search results that's because we're going to be using the sub API to allow our llm as an agent to search a\n2:10:38 web which is one of the great things about agents is that they can do all of\n2:10:44 these additional things and LM by itself obviously cannot so we come down to here\n2:10:49 we have our lsmith parameters again of course so you enter your Lang chain API\n2:10:54 if you have one and now we're going to take a look at tools which is a very\n2:11:00 essential part of Agents so tools are a way for us to augment our llms with\n2:11:07 essentially anything that we can write in code so we mentioned that that we're going to have a Google Search tool that\n2:11:14 Google Search tool it's some code that gets executed by our llm in order to\n2:11:19 search Google and get some results so a tool can be thought of as any code logic\n2:11:26 or any function in the C in the case of python any function that has been\n2:11:32 formatted in a way so that our LM can understand how to use it and then\n2:11:38 actually use it although the the LM itself is not using the tool it's more\n2:11:43 our agent execution logic which uses the tool for the llm so we're going to go\n2:11:49 ahead and actually create a few simple tools we're going to be using what is called the tool decorator from Lang\n2:11:55 chain and there are a few things to keep in mind when we're building tools so for\n2:12:02 Optimal Performance our tool needs to be just very readable and what I mean by readable is we need three main things\n2:12:10 one is a DOT string that is written in natural language and it is going to be used to explain to the Alm when and why\n2:12:18 and how it should use this tool we should also have clear parameter names\n2:12:23 those parameter names should tell the llm okay what each one of these\n2:12:28 parameters are they should be self-explanatory if they are not self-explanatory we should be including\n2:12:36 an explanation for those parameters within the doc string then finally we should have type annotations for both\n2:12:43 our parameters and also what we're returning from the tool so let's jump in\n2:12:48 and see how we would Implement all of that so we come down to here and we have line chain core tools import tool okay so\n2:12:56 these are just four incredibly simple tools we have the addition or add tool\n2:13:03 multiply the exponentiate and the subtract tools okay so a few calculator\n2:13:09 S tools now when we add this tool decorator it is turning each of these\n2:13:16 tools into what we call a structured tool object so we can see that\n2:13:21 here we can see we have this structured tool we have a name description okay and\n2:13:28 then we have this Al schema we'll see this in a moment and a function right so this function is literally just the\n2:13:35 original function it's it's a mapping to the original function so in this case it it's the add function now the\n2:13:41 description we can see is coming from our doc string and of course the name as well is just coming from the function\n2:13:47 name okay and then we can also see let's just print the name and\n2:13:53 description but then we can also see the ARs schema right we can so this thing\n2:13:58 here that we can't read at the moment to read it we're just going to look at the\n2:14:03 model Json schema method and then we can see what that contains which is all of this information so this actually\n2:14:10 contains everything includes properties so we have the X it C or title for that\n2:14:16 and it also specifies the type okay so the type that we Define is float float\n2:14:23 for open AI gets mapped to number rather than just being float and then we also\n2:14:28 see that we have this required field so this is telling how LM which parameters\n2:14:33 are required which ones are optional so we yeah in some cases you would we can\n2:14:39 even do that here let's do Z that is going to be float or none okay and we\n2:14:48 just going to say it is 0.3 all right well I'm going to remove this\n2:14:53 in a minute because it's kind of weird but let's just see what that looks like so you see that we now have X Y and\n2:15:02 Z but then in Z we have some additional information okay so it can be any of it\n2:15:08 can be a number or it can just be nothing the default value for that is 0.3 okay and then if we look here we can\n2:15:16 see that the required field does not include Z so it's just X and Y so it's\n2:15:21 describing the full function schema for us but let's remove\n2:15:27 that okay and we can see that again with our exponentiate tool similar thing okay\n2:15:33 so how how are we going to invoke our tool so the llm the underlying LM is\n2:15:41 actually going to generate a string okay so we'll look something like this this is going to be our llm output so it is\n2:15:50 it's a string that is some Json and of course to load a string into a\n2:15:56 dictionary format we just use Json loes okay so let's see that so this could be\n2:16:03 the output Fromm we load it into a dictionary and then we get an actual dictionary and then what we would do is\n2:16:11 we can take our exponentiate uh tool we access the underlying function and then\n2:16:17 we pass it the keyword arguments from our diction here\n2:16:24 okay and that will execute our tool that is the tool execution log you that line chain implements and then later on in\n2:16:31 the next chapter we'll be implementing ourselves cool so let's move on to creating an agent now we're going to be\n2:16:38 constructing a simple tool calling agent we're going to be using Lang chain expression language to do this now we\n2:16:45 will be covering Lang chain expression language or also more in a upcoming\n2:16:50 chapter but for now all we need to know is that our agent will be constructed\n2:16:57 using syntax and components that like this so we would start with our input\n2:17:03 parameters that is going to include our user query and of course the chat history because we need our agent to be\n2:17:09 conversational and remember previous interactions within the conversation these input parameters will also include\n2:17:15 a placeholder for what we call the agent scratch Pad now the agent scratch Pad is essentially where we are storing the\n2:17:22 internal thoughts or the internal dialogue of the agent as it is using tools getting observations from those\n2:17:28 tools and working through those multiple internal steps so in the case that we\n2:17:34 will see it will be using for example the addition tool getting the result using the multiply tool getting the\n2:17:40 result and then providing a final answer to us as a user so let's jump in and see\n2:17:46 what it looks like okay so we'll just start with defining our prompt so our prompt is going to include the system\n2:17:52 message there nothing we're not putting anything special in there we're going to\n2:17:57 include the chat history which is a messages placeholder then we include our\n2:18:03 human message and then we include a placeholder for the agent scratch Pad\n2:18:08 now the way that we implement this later is going to be slightly different for the scratch Pad we actually use this\n2:18:13 message's placeholder but this is how we use it with the built-in create tool agent from BL chain next we sign our LM\n2:18:21 we do need our Opening Our API key for that so we'll enter that here like so\n2:18:28 okay so come down Okay so we're going to be creating this agent we need conversation memory and we are going to\n2:18:34 use the older conversation buffer memory class rather than the newer renable with message history class that's just\n2:18:40 because we're also using this older create tool calling agent and this is\n2:18:46 this is the older way of doing things in the next chapter we are going to be using the more recent basically what we\n2:18:54 already learned on chat history we're going to be using all of that to implement our chat history but for now\n2:19:00 we're going to be using the older method uh which is deprecated just as a pre-warning but again as I mentioned at\n2:19:07 the very solid of course we're starting abstract and then we're getting into the details so we're going to initialize our\n2:19:15 agent for that we need these four things LM as we defined tools as we have\n2:19:20 defined prompt as we have defined and then the memory which is our old conversation\n2:19:26 buffer memory so with all of that we are going to go ahead and we create a tool\n2:19:32 calling agent and then we just provide it with everything okay there we go now H you'll see here I didn't pass\n2:19:40 in the the memory I'm passing it in down here instead so we're going to start with this question which is what is 10.7\n2:19:47 MTI 7.68 eight okay so given the Precision of\n2:19:55 these numbers our l a normal LM would not be able to answer that or almost\n2:20:01 definitely will not be able to answer that correctly we need a external tool to answer that accurately and we'll see\n2:20:08 that that is exactly what it's going to do so we can see that the tool agent\n2:20:15 action message here we see that it decided okay I'm going to use the multiply tool and here at parameters\n2:20:21 that I want to use for that tool okay we can see X is 10.7 and Y is\n2:20:26 7.68 you can see here that this is already a dictionary and that is because\n2:20:32 Lang chain has taken the string from our llm C and already converted it into a\n2:20:38 dictionary for us okay so that's just it's happening behind the scenes there and you can actually see if we go into\n2:20:44 the details a little bit we can see that we have these arguments and this is the original string that was coming fromn\n2:20:50 okay which has already been of of course processed by line chain so we have that\n2:20:56 now the one thing missing here is that okay we've got that the LM\n2:21:03 wants us to use multiply and we've got what the LM wants us to put into modly but where's the answer right there is no\n2:21:11 answer because the tool itself has not been executed because it can't be executed by the llm but then okay didn't\n2:21:19 we already Define our agent here yes redefined the part of our agent that is\n2:21:26 how llm has our tools and it is going to generate which tool to use but it actually doesn't include the\n2:21:33 agent execution part which is okay the agent executor is a broader thing it's\n2:21:41 it's broader logic like just code logic which acts as a scaffolding within which\n2:21:47 we have the iteration through multiple steps of our llm calls followed by the\n2:21:54 llm outputting what tools use followed by us actually executing that for the llm and then providing the output back\n2:22:02 into the llm for another decision or another step so the agent itself here is\n2:22:08 not the full agentic flow that we might expect instead for that we need to\n2:22:15 implement this agent executor class this agent executor includes our agent from\n2:22:21 before then it also includes the tools and one thing here is okay we we already\n2:22:26 passed the tools to our agent why do we need to pass them again well the tools being passed to our agent up\n2:22:32 here that is being used so that is essentially extracting out those\n2:22:38 function schemers and passing it to our LM so that our LM knows how to use the tools then we're down here we're passing\n2:22:44 the tools again to our agent executor and this is rather than looking at how to use those tools this is just\n2:22:50 looking at okay I want the functions for those tools so that I can actually execute them for the llm or for the\n2:22:57 agent okay so that's why it's happening there now we can also pass in our memory\n2:23:02 directly so you see if we scroll up a little bit here I actually had to pass\n2:23:08 in the memory like this with our agent that's just because we weren't using the agent executor now we have the agent\n2:23:14 executor it's going to handle that for us and another thing that's going to handle for us is it intermediate steps\n2:23:21 so you'll see in a moment that when we invoke the agent executor we don't include the intermediate steps and\n2:23:27 that's because it that is already handled by the agent executor now so we'll come down we'll set the both equal\n2:23:34 to true so we can see what is happening and then we can see here there's no\n2:23:39 intermediate steps anymore and we we do still pass in the chat history like this\n2:23:46 but then the addition of those new interactions to our memory is going to be handled by the\n2:23:52 executor so let me actually show that very quickly before we jump in okay so\n2:23:59 that's cently empty we're going to execute this okay we entered that new Asian\n2:24:05 execute chain let's just have a quick look at our messages again and now you can see that the agent\n2:24:11 executor automatically handled the addition of our human message and then the responding AI message for us okay\n2:24:19 which is useful now what happened so we can see that the multiply tool was\n2:24:25 invoked with these parameters and then this pink text here that we got that is\n2:24:30 the observation from the tool assist what the tool output back to us okay then this final message here it's not\n2:24:37 formatted very nicely well this final message here is coming from our llm so the green is our llm output the pink is\n2:24:45 our tool output okay so the LM after seeing this output says 10.7 MTI by 7.68\n2:24:56 is approximately 82.8 okay cool use and then we can also\n2:25:03 see the the chat history which we we already just saw great so that has been\n2:25:08 used correctly we can just also confirm that that is correct okay 82\n2:25:15 1759 recurring which is exactly what we get here okay and we the reason for all\n2:25:20 that is obviously how multiply tool is just doing this exact operation cool so let's try this with a\n2:25:29 bit of memory so I'm going to ask or I'm going to sayate to the agent hello my\n2:25:34 name is James we'll leave that as the it's not actually the first interaction because\n2:25:40 we already have these but it's an early interaction with my name in there then\n2:25:48 we're going to try and perform more tool calls within a single execution Loop and what you'll see with when it is calling\n2:25:54 these tools is that it can actually use multiple Tools in parallel so for sure I think two or three of these were used in\n2:26:00 parallel and then the final subtract had to wait for those previous results so it would have been executed afterwards and\n2:26:08 we should actually be able to see this in Langs Smith so if we go here yeah we\n2:26:13 can see that we have this initial cord and then we have add and multiply and exponentially we all use in parallel\n2:26:20 then we have another call which use subtract and then we get the response okay which is pretty cool and\n2:26:27 then the final result there is11 now when you look at whether the\n2:26:33 answer is accurate I think the order here of calculations is not quite\n2:26:39 correct so if we put the actual computation here it gets it right but\n2:26:45 otherwise if I use natural language it's like I'm doing maybe I'm phrasing it in a in a poor way\n2:26:52 okay so I suppose that is pretty important so okay if we put the computation in here we get\n2:26:59 the13 so it's something to be careful with and probably requires a little bit of prompting to promting and maybe\n2:27:07 examples in order to get that smooth so that it does do things in the way that\n2:27:12 we might expect or maybe we as humans are just bad and misus the systems one\n2:27:18 or the other okay so now we've gone through that a few times let's go and see if our agent can still recall our\n2:27:25 name okay and it remembers my name is James good so it still has that memory in there as well that's good let's move\n2:27:32 on to another quick example where we're just going to use Google search so we're going to be using the Ser\n2:27:39 API you can okay you can get the API key that you need from here so Ser ai.com\n2:27:46 usersign in and just enter that in here so you will get it up to 100 stes per\n2:27:54 month for free so just be aware of that if you overuse it I don't think they\n2:28:00 charge you cuz I don't think you enter your card details straight away but yeah just be aware of that\n2:28:07 limit now there are certain tools that line chain have already built for us so\n2:28:12 they're pre-built tools and we can just load them using the load tools function so we do that like so we have our load\n2:28:19 tools and we just pass in the Ser API tool only we could pass in more there if we want to and then we also pass in our\n2:28:26 LM now I'm going to one use that tool but I'm also going to Define my own tool\n2:28:32 which is to get the current location based on the IP address now this is we're in collab at the moment so it's\n2:28:38 actually going to get the IP address for the collab instance that I'm currently on and we'll find out where that is so\n2:28:45 that is going to get the IP address and then it's going to provide the data back to our LM this format here so we're\n2:28:51 going to latitude longitude City and Country okay we're also going to get the current day and time so now we're going\n2:28:59 to redefine our prompt I'm not going to include chat history here I just want this to be like a one shot\n2:29:06 thing I'm going to redefine our agent and agent executor using our new tools which is our set API plus to get current\n2:29:14 date time and get location from IP then I'm going to invoke our agent executor\n2:29:20 with I have a few questions what is the date and time right now how is the weather where I am and please give me\n2:29:27 degrees in celce so when it gives me that weather okay and let's see what we\n2:29:33 get okay so apparently we're in Council Bluffs in the\n2:29:39 US it is 13 fah which I think is absolutely freezing oh my gosh it is yes\n2:29:46 minus 10 so it's super cold over there and you can see that okay it did give us\n2:29:53 Fahrenheits that is because the tool that we were using provided us with Fahrenheit which is fine but it did\n2:30:00 translate that over into a estimate of Celsius fours which is pretty cool so let's actually output that so we get\n2:30:08 this which I is correct we do us approximately this and\n2:30:14 we also get an description of the conditions as well as partly cloudy with z % precipitation lucky for\n2:30:21 them and humidity of 66% okay well pretty cool so that is it\n2:30:27 for this introduction to Lang chain agents as I mentioned next chapter we're going to dive much deeper into agents\n2:30:34 and also Implement that for Lang chain version 0.3 so we'll leave this chapter here and jump into the next one in this\n2:30:41 chapter we're going to be taking a deep dive into agents with the Lang chain and\n2:30:48 we're going to be covering what an agent is we're going to talk a\n2:30:53 little bit conceptually about agents the react agent and the type of agent that\n2:30:59 we're going to be building and based on that knowledge we are actually going to build out our own agent execution logic\n2:31:07 which we refer to as the agent executor so in comparison to the previous video\n2:31:14 on agents in line chain which is more of an introduction this is far far more\n2:31:20 detailed we'll be getting into the weeds a lot more with both what agents are and\n2:31:25 also agents within Lang chain now when we talk about agents a significant part\n2:31:31 of the agent is actually relatively simple code\n2:31:37 logic that iteratively runs llm calls\n2:31:42 and processes their outputs potentially running or executing tools the exact\n2:31:49 logic for each approach to building an agent will actually vary pretty\n2:31:55 significantly but we'll focus on one of those which is the react agent now react\n2:32:02 is it's a very common pattern and although being relatively old now most\n2:32:08 of the tool agents that we see used by openai and essentially every LM company\n2:32:15 they all use a very similar pattern now the reactor agent follows a patter and like this okay so we would have our user\n2:32:23 input up here okay so our input here is a question right aside from the Apple\n2:32:29 remote what other device you can control the program Apple remote was originally designed to interact with now probably\n2:32:35 most LMS would actually be able to answer this directly now this is from the paper which was a few years back now\n2:32:42 in this scenario assuming our LM didn't already know the answer there are most steps\n2:32:48 that an llm or an agent might take in order to find out the answer okay so the\n2:32:54 first of those is we say our question here is what other device can control the program Apple remote was originally\n2:33:01 designed to interact with so the first thing is okay what was the program that the Apple remote was originally designed\n2:33:07 to interact with that's the first question we have here so what we do is I\n2:33:13 need to search Apple remote and find a program it was use for this is a reasoning step so the llm is reasoning\n2:33:19 about what it needs to do I need to search for that and find a program useful so we are taking an action this\n2:33:26 is a tool call here okay so we're going to use the search tool and our query will be apple remote and the observation\n2:33:33 is the response we get from executing that tool okay so the response here would be the Apple remote it's designed\n2:33:39 to control the front row mediate Center so now we know the programmer for was\n2:33:45 originally designed to interact with now we're going to go through another it\n2:33:50 okay so this is one iteration of our reasoning action and\n2:33:56 observation so when we're talking about react here although again this sort of\n2:34:02 pattern is very common across many agents when we're talking about react\n2:34:07 the name actually is reasoning or the first two characters of re reasoning\n2:34:13 followed by action okay so that's where the react comes from so this is one of\n2:34:19 our react agent Loops or iterations we're going to go and do another one so\n2:34:24 next step we have this information the LM is now provided with this information now we want to do a search for front row\n2:34:32 okay so we do that this is the reasoning step we per the action search front row\n2:34:38 okay tool search query front row observation this is the response front\n2:34:43 row is controlled by an apple remote or keyboard function keys all right cool so\n2:34:50 we know keyboard function keys are the other device that we were asking about up here so now we have all the\n2:34:58 information we need we can provide an answer to our user so we go through\n2:35:04 another iteration here reasoning and action our reasoning is I can now\n2:35:09 provide the answer of keyboard function keys to the user okay great so then we\n2:35:16 use the answer tool like Final Answer In more common tool agent use and the\n2:35:25 answer would be keyboard function keys which we then output to our user okay so\n2:35:32 that is the react Loop okay so looking at this how where are we actually calling\n2:35:40 an llm and what and in what way are we actually calling llm\n2:35:46 so we have our reasoning step our LM is generating the text here right so LM is\n2:35:52 generating okay what should I do then our LM is going to generate input\n2:35:59 parameters to our action step here that will th those input parameters and and\n2:36:05 the tool being used will be taken by our code logic our agent executor logic and\n2:36:10 they'll be used to execute some code in which we will get an output that output\n2:36:15 might be taken directly to our observation or our llm might take that output and then generate an observation\n2:36:22 based on that it depends on how you've implemented everything so our LM could\n2:36:29 potentially being be being used at every single step there and of course that\n2:36:35 will repeat through every iteration so we have further iterations down here so\n2:36:41 you're potentially using LM multiple times throughout this whole process which of course in terms of latency and\n2:36:47 token cost it does mean that you're going to be paying more for an agent\n2:36:53 than you are with just a sun LM but that that is of course expected because you have all of these different things going\n2:36:59 on but the idea is that what you can get out of an agent is of course much better\n2:37:05 than what you can get out of an LM alone so when we're looking at all of this all\n2:37:11 of this iterative Chain of Thought and Tool use all this needs to be controlled\n2:37:17 by what we call the agent executor okay which is our code logic which is hitting our llm processing its outputs and\n2:37:25 repeating that process until we get to our answer so breaking that part down what does it actually look like it looks\n2:37:32 kind of like this so we have our user input goes into our llm okay and then we move on to the\n2:37:39 reasoning and action steps is the action the answer if it is the answer so as we\n2:37:47 saw here where is the answer if the action is the answer so true we\n2:37:53 would just go straight to our outputs otherwise we're going to use our select tool agent executor is going to handle\n2:37:59 all this it's going to execute our tool and then from that we get our you know\n2:38:04 three reasoning action observation inputs and outputs and then we're feeding all that information back into\n2:38:11 our llm okay in which case we go back through that Loop so we could be looping\n2:38:16 for a little while until we get to that final but okay so let's go across to the\n2:38:22 code when be going into the agent executor notebook we'll open that up in\n2:38:27 coab and we'll go ahead and just install our prerequisites nothing different here is\n2:38:34 just L chain L Smith optionally as before again optionally line chain API\n2:38:40 key if you do want to use l Smith okay and then we'll come down to our first\n2:38:47 section where it's going to define a few quick tools I'm not necessarily going to\n2:38:52 go through these because we've already covered them in the agent introduction\n2:38:58 but very quickly Lang chain core tool is we're just importing this tool decorator which transforms each of our functions\n2:39:05 here into what we would call a structured tool object this thing here\n2:39:12 okay which we can see just having a quick look here and then if we want to we can extract all of the sort of key\n2:39:19 information from that structure tool using these parameters here or attributes so name description AR\n2:39:25 schemer model Json streer which give us essentially how the llm should use our\n2:39:32 function okay so I'm going to keep pushing through that now very quickly\n2:39:40 again we did cover this in the intro video so I don't want to necessar go over again into much detail but our\n2:39:48 agent EX future logic is going to need this part so we're going to be getting a\n2:39:53 string from our llm we're going to be loading that into to a dictionary object and we're going to be using that to\n2:40:00 actually execute our tool as we do here using keyword\n2:40:05 arguments okay like that okay so with the tools out of the way let's take a\n2:40:11 look at how we create our agent so when I say agent here I'm specifically\n2:40:16 talking about the part that is generating our reasoning St then generating which\n2:40:24 tool and what the input parameters to that tool will be then the rest of that\n2:40:29 is not actually covered by the agent okay the rest of that would be covered by the agent execution logic which would\n2:40:35 be taking the tool to be used the parameters executing the tool getting\n2:40:41 the response aka the observation and then iterating through that until the llm is satisfied and we have enough\n2:40:47 information to answer a question so looking at that our agent we look\n2:40:53 something like this it's pretty simple so we have our input parameters including the chat history user query we\n2:41:00 have our input parameters including the chat history us query and actually would also have any intermediate STS that have\n2:41:07 happened in here as well we have our prompt template and then we have our llm binded with tools so let's see how all\n2:41:15 this would look starting with we'll Define our promp template searching look\n2:41:20 like this we have our system message your helpful assistant when answering these question you should use on to\n2:41:27 provide after using a tool tool outp will provide in the scratch Pad below okay which we naming here if you have an\n2:41:34 answer in scratch Pad you should not use any more tools and set answer directly to the user okay so we have that as our\n2:41:41 system message we could obviously modify that based on what we're actually doing\n2:41:46 then following our system message we're going to have our chat history so any previous interactions between the user\n2:41:52 and the AI then we have our current message from the user okay we should be\n2:41:57 fed into the input field there and then following this we have our agent stretch\n2:42:03 pad or the intermediate thoughts so this is where things like the llm deciding\n2:42:09 okay this is what I need to do this is how I'm going to do it AKA The Tool call and this is the observation that's where\n2:42:15 all of that information will be going right so each of those to pass in as a\n2:42:21 message okay and the way that we look is that any tool call generation from the\n2:42:27 llm so when the llm is saying use this tool please that will be a assistant\n2:42:32 message and then the responses from our tool so the\n2:42:37 observations they will be returned as tool messages great so we'll run that to\n2:42:44 Define our prompt template we're going to Define our LM we're going to be using\n2:42:49 J2 40 mini with a temperature of zero because we want less creativity here\n2:42:55 particularly when we're doing tour calling there's just no need for us to use a high temperature here so we need\n2:43:01 to enter our open ey API key which we would get from platform open ey.com we enter this then we're going to continue\n2:43:09 and we're just going to add tools to our LM here\n2:43:15 okay these and we're going to bind them here then we have tool Choice any so\n2:43:22 tool Choice any we we'll see in a moment I'll go through this a little bit more in a second but that's going to\n2:43:28 essentially force a tool call you can also put required which is actually a bit more uh it's bit clearer but I'm\n2:43:35 using any here so I'll stick with it so these are our tools we're going through we have our inputs into the agent\n2:43:42 runable we have our prom template and then that will get fed into our llm so\n2:43:48 let's run that now we would invoke the agent part of everything here with this okay so\n2:43:55 let's see what it outputs this is important so I'm asking what is 10+ 10 obviously that should use the addition\n2:44:01 tool and we can actually see that happening so the agent message content is actually empty here this is where\n2:44:08 you'd usually get an answer but if we go and have a look we have additional keyword dos in there we have tool calls\n2:44:16 and then we have function arguments Okay so we're calling a function Arguments for that function are this okay so we\n2:44:24 can see this is string again the way that we would pass that as we do Json loads and that becomes a dictionary and\n2:44:30 then we can see which function is being called and it is the add function and that is all we need in order to actually\n2:44:37 execute our function or our our tool okay we can see it's a little more\n2:44:43 detail here now what do we do from here we're going to map the to name to the\n2:44:49 tool function and then we're just going to execute the tool function with the generated ARS I\n2:44:55 those I'll also just point out quickly that here we are getting the dictionary directly which I think is coming from\n2:45:02 somewhere else in this which is prob which is here okay so even that step\n2:45:08 here where we're passing this out we don't necessarily need to do that because I think on the L chain side\n2:45:14 they're doing it for us so we're already getting that so Json loads we don't\n2:45:20 necessarily need here okay so we're just creating this tool name to function\n2:45:25 mapping dictionary here so we're taking the well the tool names and we're just mapping those back to our tool functions\n2:45:32 and this is coming from our tools list so that tools list that we defined here\n2:45:37 okay or can even just see quickly that that will include everything or each of\n2:45:43 the tools you define there okay that's all it is now we're going to execute\n2:45:49 using our name to Tool mapping okay so this here will get us the function so it will get us this\n2:45:56 function and then to that function we're going to pass the arguments that we\n2:46:02 generated okay let's see what it looks like all right so the response so the observation\n2:46:10 is 20 now we are going to feed that back\n2:46:15 into our llm using the tool message and we're actually going to put a little bit of text around this to make it a little\n2:46:22 bit nice so we don't necessarily need to do this to be completely honest we could\n2:46:27 just return the answer directly uh I don't understand I don't even think\n2:46:33 there would really be any difference so we we could do either in some cases that\n2:46:38 could be very useful in other cases like here it doesn't really make too much difference particularly because we have\n2:46:44 this tool call ID and what this tool call ID is doing is it's being used by AI is being read by the LM so that the\n2:46:52 LM knows that the response we got here is actually mapped back to the the tool\n2:47:01 execution that it's identified here because you see that we have this ID right we have an ID here the LM is going\n2:47:08 to see the ID it's going see the ID that we pass back in here and it's going to\n2:47:13 see those two are connected so see okay this is the tool I called and this is the response I got from\n2:47:19 because of that you don't necessarily need to say which tool you used here you can it it depends on what you're\n2:47:27 doing okay so what do we get here we have okay just running everything again\n2:47:34 we've added our tool call so that's the original AI message that includes okay user add tool and then we have the tool\n2:47:40 execution tool message which is the observation we map those to the agent\n2:47:46 scratch pad and then what do we get we have an AI message but the content is empty again which is interesting because\n2:47:53 we we said to our llm up here if you have an answer to the in the scratchpad\n2:47:59 you should not use any more tools and said answer directly to the user so why why is our\n2:48:06 llm not answering well the reason for that is down here we specify tool Choice\n2:48:15 equals any which again it's the same tool Choice required which is telling\n2:48:22 the L land that it cannot actually answer directly it has to use a tool and\n2:48:28 I usually do this right I would usually put tool Choice equals any or required and for the LM to use a tool every\n2:48:36 single time so then the question is if it has to use a tool every time how does\n2:48:41 it answer our user well we'll see in a moment first I just want to show you\n2:48:49 the two options essentially that we have the second is what I would usually use but let's let's start with the first so\n2:48:56 the first option is that we set tool Choice equal to Auto and this tells the Ln that it can either use a tool or it\n2:49:03 can answer the user directly using the the final answer or using that content\n2:49:09 field so if we run that like we're specifying to choices Auto we run that\n2:49:15 let's invoke okay initially you see ah wait there's still no content that's because\n2:49:21 we didn't add anything into the agent scratch Pad here there's no information right it's all\n2:49:27 empty um actually it's empty because sorry so here you have the chat history that's empty we didn't specify the agent\n2:49:36 scratch Cad and the reason that we can do that is because we're using if you look here we're using get so essentially\n2:49:42 it's saying try and get agent scratch pad from this dictionary but if it hasn't been provided we're just going to\n2:49:48 give an empty list so that's what that's why we don't need to specify it here but\n2:49:54 that means that oh okay the the agent doesn't actually know anything here it hasn't used a tool yet so we're going to\n2:50:01 just go through our iteration again right so we're going to get our tool output we're going to use that to create\n2:50:07 the tool message and then we're going to add our tool call from the AI and the\n2:50:14 observation we're going to pass those to the agent scratch pad and this time we see we run that okay now we get the\n2:50:22 content okay so now it's not calling you see here there's no to call or anything going\n2:50:28 on we just get content so that is this is the standard\n2:50:34 way of doing or building a tool calling agent the other option which I mentioned\n2:50:40 this is what I would usually go with so number two here I would usually create a\n2:50:45 final answer tool so why would we even do that why would we\n2:50:53 create a final answer tool rather than just you know this method is actually perfectly you know it works so why would\n2:50:59 we not just use this there are a few reasons the main ones are that with\n2:51:05 option two where we're forcing tool calling this removes possibility of an\n2:51:11 agent using that content field directly and the reason at least the reason I\n2:51:17 found this good when building agents in the past is that occasionally when you do want to use a tool it's actually\n2:51:22 going to go with the content field and it can get quite annoying and and use the content field quite frequently when\n2:51:29 you actually do want it to be using one of the tools and this is particularly\n2:51:36 noticeable with smaller models with bigger models it's not as common\n2:51:41 although does so happen now the second thing that I quite like about using a\n2:51:47 tool as your final answer is that you can enforce a\n2:51:52 structured output in your answer so this is something we're stting I think the first yes the first line chain example\n2:52:01 where we were using the structured output tool of Lang chain and what that\n2:52:06 actually is the structured outputs feature of Lang chain it's actually just a tool call right so it's forcing a tool\n2:52:13 call from your LM it's just abstracted away so you don't realize that that's what it's doing but that is what it's doing\n2:52:20 so I find that structured outputs are very useful particularly when you have a\n2:52:26 lot of code around your agent so when that output needs to go Downstream into\n2:52:32 some logic that can be very useful because you can you have a reliable\n2:52:39 output format that you know is going to be output and it's also incredibly useful if you have multiple outputs or\n2:52:47 multiple fields that you need to generate for so those can be very useful\n2:52:53 now to implement this so to implement option two we need to create a final answer tool we as with our other tools\n2:53:02 we're actually going to description and you can or you cannot do this so you can\n2:53:08 you can also just return non and actually just use the generated\n2:53:14 action as the essentially what you're going to send out of your agent\n2:53:19 execution logic or you can actually just execute the tool and just pass that\n2:53:24 information directly through perhaps in some cases you might have some additional postprocessing for your final\n2:53:31 answer maybe you do some checks to make sure it hasn't said anything weird you could add that in this tool\n2:53:37 here but yeah in in this case we're just trying to pass those through directly\n2:53:43 so let's run this we've added where are we\n2:53:48 Final Answer we've added the final answer tool to our named tool mapping so our agent can now use it we redefine our\n2:53:56 agent setting tool choice to any because we're forcing the tool Choice here and let's go with what is 10 + 10 see what\n2:54:04 happens okay we get this right we can also one thing nice thing here is that\n2:54:10 we don't need to check is out up in the content field or is it in the tool course field we know it's going to be in\n2:54:15 the tool course field because we're forcing that tool use quite nice so okay we know we're using the ad tool and\n2:54:22 these are the arguments great we go or go through our process again we're going\n2:54:27 to create our tool message and then we're going to add those messages into our scratch pad or intermediate sets and\n2:54:34 then we can see again ah okay content field is empty that is expected we we're\n2:54:41 forcing tool users no way that this can be this can be or have anything inside\n2:54:46 it but then if we come down here to our to calls nice final answer arbs answer\n2:54:53 10 + 10 = 20 all right we also have this tools used where's tools used coming\n2:55:00 from okay while I mentioned before that you can add additional things or or\n2:55:06 outputs when you're using this tool use for your final answer so if you just\n2:55:11 come up here to here you can see that I asked the llm to use that Tool's use\n2:55:18 field which I defined here it's a list of strings use this to tell me what tools you used in your answer right so\n2:55:25 I'm getting the normal answer but I'm also getting this information as well which is kind of nice so that's where\n2:55:30 that is coming from see that okay so we have our actual answer here and then we\n2:55:36 just have some additional information okay and we've also defined a type here it's just a list of strings which is\n2:55:41 really nice it's giving us a lot of control over what we're outputting which is perfect that's you know when you're\n2:55:47 building with agents the biggest problem in most cases\n2:55:52 is control of your llm so here we're getting a honestly pretty unbelievable\n2:56:01 amount of control over what our LM is going to be doing which is perfect for\n2:56:06 when you're building in the real world so this is everything we need this\n2:56:13 is our answer and we would of course be passing that Downstream into whatever log\n2:56:19 our AI application would be using okay so maybe that goes directly to a front\n2:56:25 end and we're displaying this as our answer and we're maybe providing some information about okay where did this\n2:56:31 answer come from or maybe there's some additional steps Downstream where we're\n2:56:36 actually doing some more processing or Transformations but yeah we have that that's great now everything we've just\n2:56:44 done here we've been executing everything one by one and that's to help us understand what process we go through\n2:56:53 when we're building an agent executor but we're not going to want to\n2:56:59 do that all the time are we most of the time we probably want to abstract all this away and that's what we're going to\n2:57:05 do now so we're going to build essentially everything we've just taken\n2:57:11 we're going to abstra take that and Abstract it away into a custom agent\n2:57:16 executor class so let's have a quick look at what we're doing here although it's it's literally just what we we just\n2:57:23 did okay so custom maor executor we initialize it we set this m\n2:57:29 Max iterations I'll talk about this in a moment we initialize it that is going to set out chat history to just being empty\n2:57:38 okay it's a new agent there should be no chat history in this case then we actually Define our agent right so that\n2:57:44 poted logic that is going to be taking out inputs and generating what to do next AKA what tool call to do okay and\n2:57:52 we set everything as attributes of our class and then we're going to Define an\n2:57:58 invoke method this invoke method is going to take an input which just a\n2:58:03 string so it's going to be our message from the user and what it's going to do is it's\n2:58:09 going to iterate through essentially everything we just did okay until we hit\n2:58:15 the The Final Answer tool Okay so well what does that mean we have our\n2:58:21 tool call right which is we're just invoking our agent right so it's going to generate what tool to use and what\n2:58:28 parameters should go into that okay and that's a that's an AI message so we would append that to our\n2:58:36 agent stretch pad and then we're going to use the information from our tool call so the name of the tool and the ARs\n2:58:42 and also the ID we're going to use all of that information to execute our tool\n2:58:49 and then provide the observation back to our llm okay so we execute our tool here\n2:58:55 we then format the tool output into a tool message see here that I'm just\n2:59:01 using the the output directly I'm not adding that additional information there\n2:59:06 we need do need to always pass in the tool call ID so that our LM knows which\n2:59:12 output is mapped to which tool I didn't mention this before in in this video at\n2:59:17 least but that is that's important when we have multiple toour calls happening in parallel because that can happen when\n2:59:23 we have multiple toour calls happening in parallel let's say we have 10 tool calls all those responses might come\n2:59:28 back at different times so then the order of those can get messed up so we\n2:59:34 wouldn't necessarily always see that it's a AI message beginning a tool call followed\n2:59:41 by the answer to that tool call instead it might be AI message followed by like 10 different tool call responses so you\n2:59:49 need to have those IDs in there okay so then we pass our tool output back to our\n2:59:57 agent scratch pad or intermediate steps I'm sing a print in here so that we can see what's happening whilst everything\n3:00:03 is running then we increment this count number we'll talk about that in a moment\n3:00:08 so com past that we say okay if the tool name here is final answer that means we\n3:00:15 should stop okay so so once we get the final answer that means we can actually\n3:00:21 extract our final answer from the the final tool call okay and in this case\n3:00:26 I'm going to say that we're going to extract the answer from the tool call or\n3:00:33 the the observation we're going to extract the answer that was generated we're going to pass that into our chat\n3:00:40 history so we're going to have our user message is the one the user came up with followed by our answer which is just the\n3:00:47 the natur answer field and that's going to be an AI message but then we're actually going to be including all of\n3:00:53 the information so this is the the answer natural language answer and also\n3:00:59 the tools used output we're going to be feeding all of that out to some\n3:01:04 Downstream process as preferred so we have that now one thing that can happen if\n3:01:12 we're not careful is that our agent executor might may run many many times\n3:01:19 and particularly if we've done something wrong in our logic as we're building these things it can happen that maybe\n3:01:26 we've not connected the observation back up into our agent executor logic and in\n3:01:33 that case what we might see is our agent executor runs again and again and again and I mean that's fine we're going to\n3:01:38 stop it but if we don't realize straight away and we're doing a lot of llm cords\n3:01:44 that can get quite expensive quite quickly so what we can do is we can set a limit right so that's\n3:01:50 what we've done up here with this Max iterations we said okay if we go past three max iterations by default I'm\n3:01:56 going to say stop all right so that's that's why we have the count here while\n3:02:01 count is less than the max iterations we're going to keep going once we hit the number of Max iterations we stop\n3:02:08 okay so the while loop will will just stop looping okay so it just protects Us\n3:02:14 in case of that and it also potentially maybe it's Point your agent might be\n3:02:19 doing too much to answer a question so this will force it to stop and just provide an answer although if that does\n3:02:26 happen I just realize there's a bit of a fault in the logic here if that does happen we wouldn't necessarily have the\n3:02:33 answer here right so we would probably want to handle that nicely but in this\n3:02:39 scenario a very simple use case we're not going to see that happening so we\n3:02:44 initialize our custom agent executor and then we invoke\n3:02:50 it okay and let's see what happens all right there we go so that just wrapped\n3:02:56 everything into a single single invoke so everything is handled for us uh we\n3:03:03 could say okay what is 10 you know we can modify that and say 7.4 for example\n3:03:12 and that we'll go through we'll use the multiply tool instead and then we'll come back to the final answer again okay\n3:03:18 so we can see that with this custom agent executor we've built an agent and\n3:03:24 we have a lot more control over everything that is going on in here one thing that we would probably need to add\n3:03:34 in this scenario is right now I'm assuming that only one tool call will happen at once it's also why I'm asking\n3:03:39 here I'm not asking a complicated question because I don't want it to go and try and execute multiple tool Calles\n3:03:46 at once uh which which can happen so let's just try\n3:03:51 this okay so this is actually completely fine so this did just execute it one after the other so you can see that when\n3:03:59 asking this more complicated question it first did the exponentiate tool followed\n3:04:05 by the ad tool and then they actually gave us our final answer which is cool also told us we use both of those tools\n3:04:12 which it did but one thing that we should just be aware of is that from\n3:04:18 open AI open AI can actually execute multiple tool calls in parallel so by\n3:04:24 specifying that we're just using this zero here we're actually assuming that we're only ever going to be calling one\n3:04:31 tool at any one time which is not always going to be the case so you would probably need to add a little bit of exual logic there in case of scenarios\n3:04:38 if you're building an an agent that is likely to be running parallel to calls\n3:04:44 but yeah you can see here actually it's completely fine so it's running one after the other okay so with that we\n3:04:50 built our agent executor I know there's a lot to that and of course you can just\n3:04:56 use the very abstract agent executor in L chain but I think it's very good to\n3:05:01 understand what is actually going on to build our own agent executor in this case and it sets you up nicely for\n3:05:08 building more complicated or use case specific agent logic as\n3:05:14 well so that is it for this chapter in this chapter we're going to\n3:05:20 be taking a look at line chains expression language we'll be looking at the runnables the serializable and\n3:05:27 parallel of those the runable pass through and essentially how we use l\n3:05:33 cell in its full capacity now to do that well what I want to do is actually start\n3:05:40 by looking at the traditional approach to building chains in L chain so to do\n3:05:47 do that we're going to go over to the ELO chapter and open that Cur up okay so\n3:05:54 let's come down we'll do the prerequisites as before nothing measure in here the one thing that is new is Doc\n3:06:01 array because later on as you see we're going to be using this as an example of\n3:06:08 the parallel capabilities in L cell if you want to use Lim Smith you just need\n3:06:14 to add in your lime train API key okay and then let's okay so now let's dive into the\n3:06:20 traditional approach to chains in line chain so the LM chain I think is\n3:06:28 probably one of the first things introduced in line chain if I'm not wrong this take it to prompt and feeds\n3:06:33 into an l and that that's it it you can also you can add like output passing to\n3:06:40 that as well but that's optional and I don't think we're going to cover here so\n3:06:47 what that might look like is we have for example this promp template here give me a small report on topic okay so that\n3:06:54 would be our prompt template we set up as we usually do with the prom templates\n3:07:01 as we've seen before we then Define our LM need our\n3:07:07 open a key for this which as usual we would get from platform.\n3:07:13 open.com then we go ahead I'm just just showing you that you can Ino the LM there then we go ahead\n3:07:20 actually Define a output POS so we do do this I wasn't sure we did but we would\n3:07:26 then Define our LM chain like this okay so LM chain we adding our prompt adding\n3:07:32 our LM adding our alasa okay this is the traditional\n3:07:39 approach so I would then say Okay retrieve Org the generation and what's going to do it's going to give me a\n3:07:45 little report back on on rag okay t a moment but you can see that\n3:07:51 that's what we get here we can format out nicely as we\n3:07:56 usually do and we get okay look we get a nice little report however the LM chain\n3:08:02 is one it's quite restrictive right we have to have like particular parameters that have been predefined as being\n3:08:09 usable which is you know restrictive and it's also been deprecated so you know\n3:08:16 this isn't the standard way of doing this anymore but we can still use it however the preferred method to building\n3:08:23 this and building anything else really or chains in general in L chain is using El cell right and it's super simple\n3:08:30 right so we just actually take the prompt lemon Apple P that we had before and then we just chain them together\n3:08:36 with these pipe operators so the pipe operator here is saying take what is output from here and input it into here\n3:08:43 take wi's output from here and input it into here it's all it does super simple\n3:08:48 so put those together and we invoke it in the same way and we'll get the same\n3:08:54 output okay and that's what we get there is actually a slight difference on what\n3:09:00 we're getting out from there you can see here we got actually a dictionary but\n3:09:06 that is pretty much the same okay so we get that and as before we can display\n3:09:12 that in markdown with this okay so we saw just now that we have this pipe\n3:09:18 operator here it's not really\n3:09:23 standard P python syntax to use this or at least it's definitely not common it's\n3:09:29 it's it's an aberration of the intended use of python I think but anyway it\n3:09:36 does it looks cool and when you understand it I kind of get why they do\n3:09:42 because it make it does make things quite simple in comparison to what it could be otherwise so I kind get it it's\n3:09:48 a little bit weird but it's what they're doing and I'm teaching that so that's\n3:09:53 what we're going to learn so what is that pipe operator\n3:09:59 actually doing well it's as I mentioned it's\n3:10:04 taking the output from this putting it as input into into what is ever under right but how does that actually work\n3:10:12 well let's actually implement it ourselves without line chain so we're going to create this class called\n3:10:18 runnable this class when we initialize it it's going to take a function okay so this is literally a python function it's\n3:10:24 going to take that and it's going to essentially turn it into what we would\n3:10:30 call a runnable in line chain and what does that actually mean well it doesn't really mean anything it just means that\n3:10:38 when you use run the invoke method on it it's going to call that function in the\n3:10:43 way that you would have done otherwise all right so using just function you know brackets open parameters brackets\n3:10:50 closed it's going to do that but it's also going to add this method this all method now this all method in typical\n3:10:59 python syntax now this all method is essentially going to take your runnable\n3:11:06 function the one that you initialize with and it's also going to take an other function okay this other function\n3:11:13 is actually going to be a runnable I believe yes it's going to be runnable just like this and what it's going to do\n3:11:20 is it's going to run this runnable based on the output of your current runable\n3:11:28 okay that's what this or is going to do seems a bit weird maybe but I'll explain\n3:11:34 in a moment we'll see why that works so I'm going to chain a few functions\n3:11:39 together using this or method so first we're just going to turn\n3:11:45 them all into runnables Okay so these are normal functions as you can see normal python functions we then turn\n3:11:51 them into this runnable using our runnable class then look what we can do right so\n3:11:58 we we're going to create a chain that is going to be our runnable\n3:12:04 chained with another runnable chained with another runnable okay let's see what happens so we're going to invoke\n3:12:11 that chain of runnables with three so what is this going to do\n3:12:16 okay we start with five we're going to add five to three so we'll get eight\n3:12:22 then we're going to subtract five from8 to give us three again and then we're\n3:12:29 going to multiply three by five to give us 15 and we can inval that and we get\n3:12:38 15 okay pretty cool so that is interesting how does that relate to the\n3:12:44 pipe operator well that pipe operator in Python is actually a\n3:12:51 shortcut for the or method so what we just implemented is the pipe operator so\n3:12:56 we can actually run that now with the pipe operator here and we'll get the same get 15 right so that's that's what\n3:13:03 line chain is doing like under the hood that is what that pipe operator is it's\n3:13:08 just chaining together these multiple runnables as we'd call them using their own internal or operator okay which is\n3:13:17 cool I I I will give them that it's kind of a cool way of doing this creative I wouldn't have thought about it\n3:13:24 myself so yeah that is a pipe operator then we have these runnable things okay\n3:13:31 so this is a this is different to the runable I just defined here this is we Define this ourselves it's not a lang\n3:13:37 chain thing we didn't get this from Lang chain Instead This runnable Lambda\n3:13:44 object here that is actually exactly the same as what we just defined\n3:13:49 all right so what we did here this runnable this runnable Lambda is the same thing but in Lang\n3:13:58 chain okay so if we use that okay we use that to now Define three runnables from\n3:14:05 the functions that we defined earlier we can actually pair those together now using the the pipe operator you could\n3:14:12 also pair them together if you want with the or operator right\n3:14:18 so we could do what we did earlier we can invoke that okay or as we were doing\n3:14:25 originally we use pipe operator exactly the same so this runnable Lambda from line chain is just\n3:14:31 what we what we just built with the runable cool so we have that now let's\n3:14:37 try and do something a little more interesting we're going to generate a report and we're going to try and edit that report using this this\n3:14:43 functionality okay so give me a small report about topic okay we'll Z through\n3:14:48 here we're going to get our report on\n3:14:54 AI okay so we have this you can see that AI is mentioned many times in\n3:15:00 here then we're going to take a very simple function right so I'm extract\n3:15:07 fact this is basically going to take uh what is it see taking the\n3:15:14 first okay so we're actually trying to remove the introduction here I'm not sure if this actually will work as\n3:15:20 expected but it's it's fine try it anyway but then more importantly we're\n3:15:28 going to replace this word okay so we're going to replace an old word with a new word our old word going to be Ai and the\n3:15:34 word is going to be Skynet okay so we can wrap both of these functions as\n3:15:40 runable lambas okay we can add those as additional steps inside our entire chain\n3:15:46 all right so we're going to extract try and remove the introduction although I think it needs a bit more processing\n3:15:53 than just splitting here and then we're going to replace the word we need that actually to be AI run that run\n3:16:01 this okay so now we get artificial intelligence Skynet refers to the\n3:16:07 simulation of human intelligent process by machines uh we have narrow Skynet weak\n3:16:12 Skynet and strong Skynet applications of Skynet Skynet Technologies is being applied in\n3:16:18 numerous Fields including all these things scary despite potential sky that poses\n3:16:24 several challenges systems can perpetrate exist and biases it ra significant privacy\n3:16:31 concerns it can be exploited for malicious purposes okay so we have all\n3:16:37 these you know it's just a silly little example we can see also the introduction didn't work here the reason for that is\n3:16:43 because our introduction includes multiple new lines here so I would actually if I want to remove the\n3:16:50 introduction we should remove it from here I think and this is a I I would\n3:16:56 never actually recommend you do that uh because it's not it's not very flexible\n3:17:01 it's not very robust but just so I show you that that is actually working so\n3:17:07 this extract fact runnable right so now we're essentially just removing the\n3:17:14 introduction right why what do we want to do that I don't know but it's there just so you can see that we can have\n3:17:20 multiple of these runnable operations running and they can be whatever you want them to be okay it is worth knowing\n3:17:28 that the inputs to our functions here were all single arguments okay if you\n3:17:35 have function that is accepting multiple arguments you can do that the way that I would probably do it or you can do it in\n3:17:42 multiple ways one of the ways that you can do that is actually write your function to except for arguments but\n3:17:49 actually do them through a single argument so just like a single like X which would be like a dictionary or something and then just unpack them\n3:17:56 within the function and and use them as needed that's just yeah that's one way you can do it now we also have these\n3:18:02 different uh runnable objects that we can use so here we have runnable parallel and runnable pass through kind\n3:18:10 of self-explanatory to some degree so let me let just go through those so runable parallel allow you to run\n3:18:17 multiple runnable instances in parallel runnable pass through May was less\n3:18:24 self-explanatory allows us to pass a variable through to the next runnable without modifying it okay so let's see\n3:18:31 how they would work so we're going to come down here and we're going to set these two dock arrays obiously these two\n3:18:38 sources of information and we're going to need our\n3:18:43 LM to pull information from both of these sources of information in parallel which is going to look like this so we\n3:18:49 have these two sources of information Vector store a vector store B this is\n3:18:55 our dock array a and dock array B these are both going to be fed in as\n3:19:00 context into our prompt then our LM is going to use all of that to answer the\n3:19:06 question okay so to actually Implement that we have our we need an embedding\n3:19:12 model so he open our embeddings we have our vetur a a vector B they're not you\n3:19:18 know real vectors they're not full-on vectors SS here we're just passing in a very small amount of information to both\n3:19:26 so we're saying okay we're going to create an inmemory vect S using these\n3:19:31 two bits of information so when say half the information is here this would be an irrelevant piece of information then we\n3:19:37 had the relevant information which is deep seek re3 was released in December 2024 okay then we're going to have some\n3:19:45 other information in our other Vector sore again irrelevant piece here and\n3:19:50 relevant piece here okay the Deep seek V3 LM is a mixure of experts model with\n3:19:56 671 billion parameters at its largest okay so based on that we're also going\n3:20:03 to build this prompt string so we're going to pass in both of those contexts into our prompt then I'm going to ask a\n3:20:10 question we don't actually need we don't need that bit and actually we don't even\n3:20:15 need that bit what am I doing so we just need this so we have the both the contexts there and we would run them\n3:20:22 through our prompt template okay so we have our system promp template which is\n3:20:27 this and then we're just going to have okay our question is going to go into here as a user message cool so we have that and then\n3:20:36 let me make this easier to read we're going to convert both those\n3:20:41 STS to retrievers which just means we can retrieve stuff from them and we're going to use this runnable parallel to\n3:20:49 run both of these in parallel right so these are being both being run in\n3:20:55 parallel but then we're also running our question in parallel because this needs to be essentially passed through this\n3:21:02 component without us modifying anything so when we look at this here it's almost\n3:21:07 like okay the this section here would be our runable parallel and these are being\n3:21:14 running parallel but also our query is being passed through so it's almost like\n3:21:19 there's another line there which is our runable pass through okay so that's what we're doing here these running in\n3:21:25 parallel one of them is a pass through I need\n3:21:30 to run here I just realized here we're using\n3:21:35 the uh deprecated embeddings just switch it to this so L chain open\n3:21:42 AI we run that run this run that and now\n3:21:47 this is set up okay so we then put our initial so this\n3:21:56 using our runable parallel and runnable pass through that is our initial step we\n3:22:01 then have our prompt LM now pass which would being chained together with usual\n3:22:07 you know the usual type operator okay and now we're going to invoke question what architecture does\n3:22:13 the mod deep seek release in December use okay okay so for the elm to answer\n3:22:19 this question it's here to need to tell us what it needs the information about the Deep seek model that was released in\n3:22:24 December which we have specified in one half uh here and then it also needs to\n3:22:31 know what architecture that model uses which is defined in the other half over\n3:22:37 here okay so let's run this okay there we go deep SE V3 model\n3:22:44 released in December 2024 is a mix experts model with 671 billion\n3:22:49 parameters okay so mixture of experts and this many parameters pretty cool so\n3:22:55 we've put together our pipeline using elol using the pipe operator the\n3:23:01 runnables specifically we've looked at the runable parallel runable pass through and also the runable lampas so\n3:23:08 that's it for this chapter on lell and we'll move on to the next one in this\n3:23:14 chapter we're going to cover streaming and async in Lang chain now both using\n3:23:19 async code and using streaming are incredibly important components of I\n3:23:27 think almost any conversational chat interface or at least any good\n3:23:32 conversational chat interface for async if your application is not async and\n3:23:39 you're spending a load of time in your API or whatever else waiting for llm\n3:23:45 calls because a lot of those are behind apis you are waiting and your\n3:23:50 application is doing nothing because you've written synchronous code and that\n3:23:55 well there are many problems with that mainly it doesn't scale so asyn code\n3:24:00 generally performs much better and especially for AI where a lot of the\n3:24:05 time we're kind of waiting for API calls so asyn is incredibly important for that\n3:24:10 for streaming now streaming is slightly different thing so let's say I want to\n3:24:16 to tell me a story okay I'm using gbt 4 here it's a\n3:24:22 bit slower so we can achieve string we can see that token by token this text is being produced and sent to us now this\n3:24:29 is not just a visual thing this is the LM when it is generating tokens or words\n3:24:38 it is generating them one by one and and that's because these llms literally\n3:24:43 generate tokens one by one so they're looking at all of the previous tokens in order to generate the next one and then\n3:24:48 generate next one generate next one that's how they work so when we are\n3:24:54 implementing streaming we're getting that feed of tokens directly from the LM\n3:24:59 through to our you know our back end or our front end that is what we see when when we see that token by token\n3:25:06 interface right so that's one thing what one other thing that I can do that let\n3:25:12 me switch across to 40 is I can say okay we just got this story I'm going to\n3:25:18 ask are there any standard\n3:25:25 storytelling techniques to follow used above please\n3:25:32 use search okay so look we we get this very\n3:25:41 briefly there we saw that it was searching the web and the way it's not because we told it okay we told the llm\n3:25:48 to use the search tool but then the llm output some tokens to say use the search\n3:25:55 tool that is going to use a Search tool and it also would have output the token saying what that search query would have\n3:26:02 been although we didn't see it there but what the chat GPT interface is\n3:26:08 doing there so it received those tokens saying hey I'm going to use a Search tool it didn't just send us those tokens\n3:26:15 like it does with the tokens here instead it used those tokens to show us\n3:26:21 that searching the web little text box so streaming is not just the streaming\n3:26:28 of these direct tokens it's also the streaming of these intermediate steps\n3:26:34 that the llm may be thinking through which is particularly important when it\n3:26:40 comes to agents and agentic interfaces so it's also a feature thing right\n3:26:45 streaming does doesn't just look nice is also a feature then finally of course\n3:26:51 when we're looking at this okay let's say we go back to\n3:26:57 GT4 and I say okay use all of this\n3:27:04 information to generate a long story for\n3:27:10 me right and okay we are getting the first token now we know something is\n3:27:16 happening and we need start reading now imagine if we were not streaming anything here and we're just waiting\n3:27:23 right we're still waiting now we're still waiting and we wouldn't see anything we're just like oh it's just\n3:27:28 blank or maybe there's a little loading spinner so we'd still be waiting and even now we're still\n3:27:39 waiting right this is an extreme example but can you imagine just waiting\n3:27:45 for so long and not seeing anything as a user right now just now we would have\n3:27:51 got our answer if we were not streaming I mean that that would be painful as a\n3:27:56 user you you not want to wait especially in a chat interface you don't want to wait that\n3:28:01 long it's okay with okay for example deep research takes a long time to process but you know it's going to take\n3:28:08 a long time to process and it's a different user case right you're getting a report this is a chat interface and\n3:28:15 yes most messages are not going to take that long to generate we're also\n3:28:21 probably not going to be using GPT 4 depending on I don't know maybe some people still do but in some scenarios\n3:28:29 it's painful to need to wait that long okay and it's also the same for agents it's nice when you're using agents again\n3:28:36 update on okay we're using this tool it's using this tool this is how it's using them perplexity for example have a\n3:28:42 very nice example of this so okay what what's this open I founder joins morati\n3:28:49 sub let's see right so we see this is really nice it's we're using Pro search it's searching for news sharing with the\n3:28:55 results like we're getting all this information as we're waiting which is really cool and it helps to understand\n3:29:02 what is actually happening right it's not needed in all use cases but it's super nice to have those intermediate\n3:29:09 steps right so then we're not waiting and then I think this bit probably also streamed but it was just super fast so I\n3:29:15 I didn't see it but that's pretty cool so streaming is pretty important let's\n3:29:22 dive into our example okay we'll open that in cab enough we out so starting\n3:29:27 with the prerequisites same as always Lang chain optionally L Smith we'll also\n3:29:34 enter our L chain API key if you'd like to use l Smith we'll also enter our openi API key so that is platform.\n3:29:42 open.com and then as usual we can just invoke our l m right so we have that\n3:29:48 it's working now let's see how we would stream with a stream okay so whenever a\n3:29:56 method so stream is actually a method as well we could use that but it's not acing right so whenever we see a method\n3:30:02 in line chain that has a prefixed onto what would be another method that's like\n3:30:08 the async version of this so we can actually stream using async\n3:30:16 super easily using just LM a stream okay\n3:30:21 now this is just a an example in to be completely honest you probably will not\n3:30:27 be able to use this in an actual application but it's just an example and we're going to see how we would use this\n3:30:34 or how we would stream asynchronously in an application further down in this\n3:30:40 notebook so starting with this you can see here that we're getting these tokens\n3:30:45 right we're just appending it to token here we don't actually need to do that I don't think we're using this but maybe\n3:30:51 we yeah we do it here it's fine so we're just pending the tokens as they come\n3:30:56 back from our LM pending it to this we'll see what that is in a moment and\n3:31:02 then I'm just printing the token content right so the content of the\n3:31:07 token so in this case that would be l in this case it would be LP it would be Sans for so on and so on so you can see\n3:31:14 for the most part it's it's tends to be Word level but it can also be subword level as you see scent iment is one word\n3:31:23 of course so you know they get broken up in in various ways then adding this pipe character\n3:31:30 onto the end here so we can see okay where are our individual tokens then we\n3:31:36 also have flush so flush uh you can actually turn this off and it's still going to stream you're still going to\n3:31:41 see everything which going to be a bit more you can see it's kind of a it's like bit by bit when we use flush it\n3:31:49 forces the console to update what is being shown to us immediately all right\n3:31:54 so we get a much smoother um when we're looking at this it's much smoother versus when flush is\n3:32:02 not set true so yeah when you're printing that is good to do just so you can see you don't necessarily need to\n3:32:08 okay now we added all those tokens to the tokens list so we can have a look at each individual object that was returned\n3:32:15 turn to us right and this is interesting so we see that we have the AI message chunk right that's an object and then\n3:32:22 you have the content the first one's actually empty second one has that n for NLP and\n3:32:30 yeah I mean that's all we rarely need to know they're very simple objects but they're actually quite\n3:32:35 useful because uh just look at this right so we can add each one of our AI\n3:32:40 message chunks right let's see what that does it doesn't create a list it creates this right so we still just have one AI\n3:32:48 message chunk uh but it's combined the content within those AI message chunks\n3:32:55 which is kind of cool right so for example like we could remove\n3:33:01 these right and then we just see NLP so that's kind of nice little feature there\n3:33:06 I do I actually quite like that but uh you do need to just be a little bit careful because obviously you can do\n3:33:13 that the wrong way and you're going to get like a I don't know all that is some weird token salad so yeah you need to\n3:33:21 just make sure you are going to be merging those into correct order unless you I don't know unless you're doing\n3:33:28 something weird Okay cool so streaming that that was streaming from a LM let's\n3:33:34 have look at streaming with agents so we it gets a bit more complicated to be\n3:33:41 completely honest but we also need to things are going to get a bit more\n3:33:46 complicated so that we can implement this in for example an API right so is\n3:33:52 it's kind of like a necessary thing in any case so to just very quickly we're\n3:33:58 going to construct our agent executor like we did in the agent execution chapter and for that for the agent\n3:34:06 executor we're going to need tools chat prompt template llm agent and the agent H itself okay very quickly I'm not going\n3:34:13 to go through these uh in detail we just def find our tools have ADD multiply exponentiate subtract and Final Answer\n3:34:20 tool merge those into a single list of tools then we have our prompt template\n3:34:26 again same as before we just have system message we have chat history we have you query and then we have the agent scratch\n3:34:34 pad for those intermediate sets then we Define our agent using L cell L cell\n3:34:40 works quite well with both streaming and async by the way it supports both out of\n3:34:46 the box which is nice so we Define our agent then coming down here we're going\n3:34:54 to create the agan ice fter this is the same as before right so there's nothing\n3:34:59 new in here I don't think so just initialize our agent things there then\n3:35:05 it's you know We're looping through looping through yeah nothing\n3:35:11 nothing new there so we're just executing invoking our agent seeing if\n3:35:17 there's a tool call uh this is slightly we could shift this to before or after it doesn't actually matter that\n3:35:23 much so we're checking if it's final answer if not we continue X to our tools\n3:35:29 and so on Okay cool so then we can invoke\n3:35:35 that okay we go what is 10 + 10 there we go right so we have our\n3:35:42 agent executor it is working now now when we are running our agent executor\n3:35:51 with every new query if we're putting this into an API we're probably going to need to\n3:35:57 provide it with a a fresh callback Handler okay so this is the corat Handler that's going to handle taking\n3:36:04 the tokens that are being generated by a LMO agent and giving them to some other\n3:36:10 piece of code like for example the the streaming response for a API\n3:36:16 and our Corbat Handler is going to put those tokens in a queue in our case and\n3:36:22 then our for example the streaming object is going to pick them up from the queue and put them wherever they need to\n3:36:28 be so to allow us to do that with every\n3:36:33 new query or is needing to initialize everything when we actually initialize\n3:36:39 our agent we can add a configural field to our llm okay so we set the configural\n3:36:45 Fields here oh also one thing is that way we set streaming equal to true that's very manting but just so you see\n3:36:52 that there we do do that so we add some configurable fields to our LM which means we can basically pass an object in\n3:37:00 for these on every new invocation so we set our configurable\n3:37:05 field it's going to be called callbacks and we we just add a description right there's nothing more to it so this will\n3:37:12 now allow us to provide that field when we're invoking our agent okay now we\n3:37:21 need to Define our callback Handler and as I mentioned what is basically going\n3:37:26 to be happening is this callback Handler is going to be passing tokens into our a\n3:37:32 sync IO Q object and then we're going to be picking them up from the que\n3:37:37 elsewhere okay so we can call it a q callback Handler okay and that is\n3:37:42 inhering from the async Callback Handler cuz we want all this to be done asynchronously because we're we're\n3:37:47 thinking here about okay how do we Implement all this stuff within apis and actual real world code and we we do want\n3:37:55 to be doing all this in aing so let me execute that and I'll just explain a little bit of what we're looking at so\n3:38:01 we have the initialization right it's nothing nothing specific here we just\n3:38:07 what we really want to be doing is we want to be setting our Q object assigning that to the class attributes\n3:38:14 and then there's also this Final Answer scene which we're setting to fults so\n3:38:19 what we're going to be using that for is we our llm will be streaming tokens\n3:38:26 towards whilst it's using its tool calling and we might not want to display those immediately or we remember to\n3:38:32 display them in a different way so by setting this Final Answer scene to\n3:38:37 false whilst our LM is outputting those tool tokens we can handle them in a\n3:38:44 different way and then as soon as we see that it's done with the tool calls and it's on to the final answer which is\n3:38:49 actually another tool call but once we see that it's on to the final answer tool call we can set this true and then\n3:38:56 we can start processing our tokens in a you know different way essentially okay so we have that then we have this AER\n3:39:04 method this is required for any async generator object\n3:39:11 so what that is going to be doing is going to iterating through right it's a generator it's going to be going\n3:39:16 iterating through and it's going saying okay if our queue is empty right this is the que that we set up here if it's\n3:39:22 empty wait a moment right we use the Sleep Method here and this is an async\n3:39:28 Sleep Method this is super important we're using we are waiting for an asynchronous sleep all right so whilst\n3:39:35 we're whilst we're waiting for that 0.1 seconds our our code can be doing other things\n3:39:42 right that that is important if we if we use I think the standard is time dos sleep that is not asynchronous and so it\n3:39:50 will actually block the thread for that 0. one seconds so we don't want that to happen generally our Q should probably\n3:39:57 not be empty that frequently given how quickly uh tokens are going to be added to the queue so the only way that this\n3:40:05 would potentially be empty is maybe our LM stops maybe there's like a connection\n3:40:11 Interruption for it you know a brief second or something and no tokens are added so in that case we don't actually\n3:40:17 do anything we don't keep the checking the queue we just wait a moment okay and then we check again now if it was empty\n3:40:26 we wait and then we continue onto the next iteration otherwise it probably won't be empty we get whatever is from\n3:40:33 our inside our queue we get that out pull it out then we say Okay if that\n3:40:40 token is a done token we're going to return so we're going to stop this\n3:40:45 generator right we're finished otherwise if it's something else we're going to yield that token\n3:40:52 which means we're we're returning that token but then we're continuing through that loop again\n3:40:57 right so that is our generator logic then we have some other methods here\n3:41:04 these are L these are line chain specific okay we have on LM new token\n3:41:10 and we have on LM end starting with on LM new token this is basically when an\n3:41:15 LM returns a token to us line chain is going to run or execute this method okay\n3:41:22 this is the method that will be called what this is going to do is it's going to go into the keyword arguments and\n3:41:28 it's going to get the chunk object so this is coming Fromm if there is something in that chunk it's going to\n3:41:36 check for a final answer tool call First okay so we get our tool calls and we say\n3:41:43 if the name within our ch chunk right probably this will be emptying most of\n3:41:48 the tokens we return right so you remember before when we're looking at the chunks here this is what we're\n3:41:54 looking at right the content for us is actually always going to be empty and instead we're actually going to get the\n3:41:59 additional keyword objects here and inside there we're going to have our tool calling our tool calls as we s in\n3:42:06 the the previous videos right so that's what we're extracting we're extracting that information that's why we're going\n3:42:12 additional keyword ARS right and get those tool the tool call information\n3:42:18 right or it will be nonone right so if if it is nonone I don't think it ever\n3:42:24 would be none to be honest it would be strange if it's none I think that means something would be wrong okay so here\n3:42:29 we're using the wars operator so the wars operator what it's doing here is whil we're checking the if logic here\n3:42:38 whilst we do that it's also assigning whatever is inside this it's assigning\n3:42:43 over to Tool Calles and then with the if we're checking whether tool cause is\n3:42:49 something or nonone right because we're using get here so if if this get\n3:42:54 operation fails and there is no tool calls this object here will be equal to\n3:42:59 none which gets assigned to Tool calls here and then this this if none will\n3:43:05 return false and this logic will not run okay and it will just continue if this\n3:43:11 is true so if there is something returned here we're going to check if that's something returned is using the\n3:43:16 function name or tool name final answer if it is we're going to set that final answer see equal to True otherwise we're\n3:43:24 just going to add our chunk into Q okay we use put no weight here because we're\n3:43:30 we're using async otherwise if you were not using async I think you might just put weight or maybe even put put no okay\n3:43:39 you you use put if it's a synchronous code but I I don't think I've ever implemented a synchronous so it would\n3:43:46 actually just be put no weight for Asing okay and then return so we have\n3:43:52 that then we have on llm end okay so this is when line chain sees that the\n3:43:59 llm has returned or indicated that it is finished with the response line chain\n3:44:06 will call this so you you have to be aware that this\n3:44:12 will happen multiple times during an agent execution because if you think within our agent executor we're hitting\n3:44:21 the LM multiple times we have that first step where it's deciding oh I'm going to use the add tool or the multiply tool\n3:44:28 and then that response gets back towards we execute that tool and then we pass\n3:44:33 the output from that tool and all the original user query in the chat history pass that back to our LM again all right\n3:44:39 so that's another call to our LM that's going to come back it's going to finish it's going to give us something else\n3:44:45 right so there's multiple llm cods happening throughout our agent execution\n3:44:50 logic so this on LM call will actually get called at the end of every single one of those llm calls now if we get to\n3:44:59 the end of our llm call and it was just a it was a tool invocation so we had the you know it\n3:45:05 called the ad tool we don't want to put the done token into our Cube because\n3:45:13 when the done token is added to our Cube we're going to stop iterating\n3:45:19 okay instead if it was just a tool call we're going to say step end right and\n3:45:24 we'll actually get this token back so this is useful on for example the front\n3:45:29 end you could have okay I've I've used the ad tool the these are the parameters\n3:45:35 and it's the end of the step so you could have that your tool callers being used on some front end and then as soon\n3:45:42 as it sees step end it knows okay we're done with here was a response right and and it can just show you that and we're\n3:45:49 going to use that we'll see that soon but let's say we get to the final an tool we're on the final answer tool and\n3:45:55 then we get this signal that the llm has finished then we need to stop iterating\n3:46:02 otherwise our our streaming generator is just going to keep going forever right nothing's going to stop it or maybe it\n3:46:09 will time out I don't think it will though so at that point we need to send okay\n3:46:15 stop right we need to say we're done and then that will that will come back to here to our a iterator and to our asnc\n3:46:23 iterator and it will return and stop the generator okay so that's the core logic\n3:46:31 that we have inside there I know there's a lot going on there it's but we need all of this so it's important to be\n3:46:38 aware of it okay so now let's see how we might actually call our agent with all\n3:46:45 of this streaming uh in this way so we're going to initialize our queue\n3:46:52 we're going to use that to initialize a streamer okay using the the custom streamer that we just sell custom\n3:46:57 callback Handler whatever you want to call it okay then I'm going to define a function so this is an asynchronous\n3:47:04 function it has to be if if we're using async and what it's going to do is it's going to call our agent with a config\n3:47:12 here and we're going to pass it that call the the Callback which is the streamer right note here I'm not calling\n3:47:19 the agent executor I'm just calling the agent right so the uh if we come back up\n3:47:24 here we're calling this all right so that's not going to include all tool execution logic and\n3:47:30 importantly we're calling the agent with the config that uses callbacks right so\n3:47:37 this this configurable field here from our LM is actually being fed through it propagates through to our agent object\n3:47:43 as well to the runable so realizable all right so that's what we're executing here we see agent with\n3:47:49 config and we're passing in those callbacks which is just one actually okay so that sets up our agent and then\n3:47:57 we invoke it with a stram okay like we did before and we're just going to return everything so let's uh run that\n3:48:05 okay and we see all the token or the chunk objects that are being returned and this is useful to understand what\n3:48:12 we're actually doing up here right so when we're doing this chunk\n3:48:17 message additional C keyword arguments right we can see that in here so this would be the chunk message object we get\n3:48:24 the additional keyword objects go into tool calls and we get the information here so we have the ID for that tool\n3:48:30 call as we saw in the previous chapters then we have our function right so the\n3:48:37 function includes the name right so we know what tool we're calling from this first chunk but we don't know the\n3:48:43 arguments right those arguments are going to be streamed to us so we can see them begin to come through in the next\n3:48:50 chunk so the next chunk is just it's just a first token for for the ad\n3:48:55 function right and we can see these all come together over multiple steps and we\n3:49:01 actually get all of our arguments okay that's pretty cool\n3:49:07 so actually one thing I would like to show you here as well so if we just do token\n3:49:13 equals token sorry and we\n3:49:20 do tokens. pen\n3:49:26 token okay we have all of our tokens in here now right you see that they're all\n3:49:31 AI message chunks so we can actually add those together right so let's we'll go with\n3:49:37 these here and based on these we're going to get all the arguments okay so this is kind of interesting so it's one\n3:49:45 until I think like the second to last\n3:49:51 maybe right so we have these and actually we just want to add those\n3:49:56 together so I'm going to go with tokens one I'm just going to go\n3:50:04 four uh four token in we're going to go from\n3:50:10 the second onwards I'm going to TK plus\n3:50:16 token right and let's see what TK looks like at the end here\n3:50:23 TK okay so now you see I kind of merged with all those um arguments here sorry\n3:50:30 plus equal okay so run that and you can see here that it's merged those arguments it\n3:50:36 didn't get all of them so I kind of missed some at the end there but it's merging them right so we can see that that logic where it's you know before it\n3:50:43 was adding the content from various trunks it also does the same for the\n3:50:49 other parameters within your trunk object which is is I I think it's pretty cool you can see here the name wasn't\n3:50:55 included that's because we started on token one or on token zero where the name was so if we actually started from\n3:51:02 token zero and let's just let's just pull them in there right so from one\n3:51:10 onwards we're going to get a complete AI message chunk which includes the name\n3:51:16 here and all of those arguments and you you'll see also here right populate everything which is pretty\n3:51:23 cool okay so we have that now based on this we're going to want to modify our\n3:51:29 custom agent executor because we're streaming everything right so we want to add\n3:51:35 streaming inside our agent executor which we're doing here right so this is\n3:51:40 async death stream and we're sharing async for token in the a string okay so\n3:51:47 this is like the very first instance if output is none we're just going to be adding our token so the the chunk sorry\n3:51:56 to our output like the first token becomes our output otherwise we're just\n3:52:02 appending our tokens to the output okay if the token content is empty which it\n3:52:08 should be right because we're using tool cores all the time we're just going to print content okay I just added these as\n3:52:15 so we see like print everything I just want to want to be able to see that I wouldn't expect this to run because\n3:52:22 we're saying it has to use tool calling okay so within our agent if we come up\n3:52:28 to here we said tool Choice any so it's been forced to use tool calling so it should never really be returning\n3:52:34 anything inside the content field but just in case it's there right so we'll we'll see if that is actually true then\n3:52:40 we're just getting out our tool CES information okay from our trunk and we're going to say okay if there's\n3:52:46 something in there we're going to print what is in there okay and then we're going to extract our tool name if there\n3:52:51 is some if there is a tool name I'm going to show you the tool name then we're going to go to ORS and\n3:52:57 if the ORS are not empty we're going to see what we get in there okay and then\n3:53:03 from all of this we're actually going to we merge all of it into our AI message right because we're merging everything\n3:53:09 as we're going through we're merging everything into outputs as I showed you before okay cool and then we just\n3:53:14 awaiting our stream that will like kick it off okay and then we do the the standard agent execut stuff again here\n3:53:21 right so we're just pulling out tool name Tool logs tool call ID and then we're using all that to execute our tool\n3:53:27 here and then we're creating a new tool message and passing that back in and then also here I move the break for The\n3:53:35 Final Answer into the final step so that is our custom Asian executor with streaming and let's see what let's see\n3:53:42 what it does okay St for b equal true so we see all those print\n3:53:49 statements okay so you can kind of see it's a little bit messy but you can see\n3:53:55 we have tool calls that had some stuff inside it had add here and what we're\n3:54:01 printing out here is we're printing out the full AI message chunk with tool calls and then I'm just printing out\n3:54:06 okay what are we actually pulling out from from that so these are actually coming from the same thing okay and then\n3:54:12 the same here right so we're looking at full message and then we're looking okay we're getting this argument out from it\n3:54:19 okay so we can see everything that is being pulled out you know chunk by chunk\n3:54:25 or token by to token and that's it okay so we could just get everything like that however right so I'm I'm printing\n3:54:32 everything so we can see that it's streaming what if I don't print okay so we're setting the bo or by default the\n3:54:39 both is equal to false here so what happens if we invoke now plus\n3:54:47 C okay cool we got\n3:54:54 nothing so the reason we got nothing is because we're not\n3:54:59 printing but we don't if you are if you're building an an API for example\n3:55:06 you're you're pulling your tokens through you can't print them to your\n3:55:14 like like a front end or or print them as to the output of your API printing\n3:55:20 goes to your terminal right your console window it doesn't go anywhere else instead what we want to do is we\n3:55:28 actually want to get those tokens out right but if but how do we do that all\n3:55:33 right so we we printed them but another place that those tokens are is in our que all right because we set them up to\n3:55:41 go to the que so we can actually pull them out of\n3:55:47 our queue whilst our agent executor is running and then we can do whatever we\n3:55:52 want with them because our code is async so it can be doing multiple things at the same time so whilst our code is\n3:55:58 running the agent executor whilst that is happening our code can also be pulling out from our\n3:56:05 queue tokens that are in there and sending them to like an API for example\n3:56:12 right or whatever Downstream you you have so let's see what that looks like\n3:56:17 we start by just initializing our que initializing our streamer with that que then we create a task so this is\n3:56:24 basically saying okay I I want to run this but don't run it right now I'm not ready yet the reason that I say I'm not\n3:56:31 ready yet is because I also want to Define here my async Loop which is going\n3:56:37 to be printing those tokens right but this is async right so we we set this up\n3:56:43 this is like get ready to run this because it is async this is running right this is just running like it there\n3:56:50 it's already running so we get this we continue we continue this none of this is actually executed\n3:56:57 yet right only here when we await the task that we set up here only then does\n3:57:04 our agent executor run and our async object here\n3:57:11 begin getting tokens right and here again printing but I don't need to print\n3:57:16 I could I could have like a let's say where this is within an API or\n3:57:22 something let's say I'm I'm saying okay send token to\n3:57:29 XYZ token right that's sending up token somewhere or if we're maybe we're yielding this to our some sort streamer\n3:57:37 object within our API right we can do whatever we want with those tokens okay I'm just printing them because I want to\n3:57:44 see them okay but just important here is that we're not printing them within our\n3:57:50 agent executor we're printing them outside the agent executor we've got them out and we can put them wherever we\n3:57:56 want which is perfect when you're building an actual sort real world use KS we using an API or something else\n3:58:02 okay so let's run that let's see what we get look at that we get all of the\n3:58:07 information we could need and a little bit more right because now we're using the agent executor and now we can also\n3:58:15 see oh we have this step end right so I know all I I know just from looking at this right this is my first tool use so\n3:58:24 what tool is it let's have a look it's the add tool and then we have these arguments I can then pass them right\n3:58:31 Downstream then we have the next tool use which is here down here so we can\n3:58:38 then pass them in the way that we like so that's pretty cool\n3:58:45 let's I mean let's see right so we're getting those fers out can we can we do\n3:58:50 something with them before I before I print them and show them yes let's see\n3:58:55 okay so we're now modifying our our Loop here same stuff right we're still\n3:59:01 initializing our queue initializing our streamer initializing our task okay and we're still doing this aing for token\n3:59:08 streamer okay but then we're doing stuff with our tokens so I'm saying okay if if we're on\n3:59:15 stream end I'm not actually going to print stream end I'm going to print new line okay otherwise if we're getting a tool\n3:59:23 call here we're going to say if that tool call is the tool name I am going to\n3:59:29 Sprint calling tool name okay if it's the arguments I'm going to print the\n3:59:35 tool argument and I'm going to end up with nothing so that we don't go to a new line so we're actually going to be\n3:59:41 streaming everything okay so let's just see what this looks\n3:59:47 like oh my bad I just added\n3:59:52 that okay you see that so it go it goes very\n3:59:58 fast so it's kind of hard to see it I'm going to slow it down so you can see so you can see that we as soon as we get\n4:00:04 the toour name we stream that we're calling the add tool then we stream token by token the actual Arguments for\n4:00:11 that tool then for the next one again we do the same we're calling this tool name then we're streaming token by token\n4:00:18 again we're processing everything Downstream from outside of the agent\n4:00:24 executor and this is an essential thing to be able to do when we're actually implementing streaming and acing and\n4:00:32 everything else in an actual application so I know that's a lot but it's\n4:00:39 important so that is it for our chapter on streaming and Asing\n4:00:44 I hope this all been useful thanks now we're on to the final Capstone chapter\n4:00:50 we're going to be taking everything that we've learned so far and using it to build a actual chat application now the\n4:00:59 chat application is what you can see right now and we can go into this and ask some pretty interesting questions\n4:01:05 and because it's an agent because as iess is tools it will be able to answer them for us so we'll see inside our\n4:01:12 application that we can ask questions s that require tool use such as this and\n4:01:17 because of the streaming that we've implemented we can see all this information real time so we can see that sub API tool is being used these are the\n4:01:24 queries we saw all that was in parallel as well so each one of those tools were\n4:01:29 being used in parallel we modified the code a little bit to enable that and we\n4:01:35 see that we have the answer we can also see the structured output being used here so we can see our answer followed\n4:01:41 by the tools used here and then we could ask followup questions as well because this is conversational so we say how is\n4:01:48 the weather in each of those\n4:02:00 cities okay that's pretty cool so this is what we're going to be building we\n4:02:05 are of course going to be focusing on the API the back end I'm not front end engineer so I can't take you through\n4:02:11 that but the code is there so for those of you that do want to go through the front end code you can of course go and\n4:02:17 do that but we'll be focusing on how we build the API that powers all of this\n4:02:23 using of course everything that we've learned so far so let's jump into it the first thing we going to want to do is\n4:02:29 clone this repo so we'll copy this URL this is repo orelio Labs line chain\n4:02:36 course and you'll just clone your repo like so I've already done this so I'm\n4:02:42 not going to do it again instead I'll just navigate to the line chain course repo now there's a few\n4:02:50 setup things that you do need to do all of those can be found in the read me so\n4:02:56 we just open a new tab here and I'll open the read me okay so\n4:03:02 this explains everything we need we have if you were running this locally already you will have seen this or you will have\n4:03:09 already done all of this but for those of you that haven't we go through quickly now so you will need to install\n4:03:17 the UV Library so this is how we manage our pyth environment our packages we use\n4:03:24 UV on Mac you would install it like so if you're on Windows or Linux just\n4:03:31 double check how you'd install over here once you have installed this you then go\n4:03:37 to install python so UV python install then we want to to create our VM our\n4:03:45 virtual environment using that version of python so the VM\n4:03:52 here then as you can see here we need to activate that virtual environment which\n4:03:57 I did miss from here so let me quickly add that so you just run that for me I'm\n4:04:03 using fish so I just add fish onto the end there but if you're using bash or zsh I think you can you can just run\n4:04:09 that directly and then finally we need to sync I install all of our packages using UV\n4:04:17 sync and you see that will install everything for you great so we have that\n4:04:24 and we can go ahead and actually open cursor or vs code and then we should\n4:04:31 find ourselves within cursor or vs code so in here you'll find a few things that\n4:04:39 we will need so first is environment variables so we can come over to here\n4:04:45 and we have open AI API key larning chain API key and ser API API key create\n4:04:50 a copy of this and you would make this your EMV file or if you want to run it\n4:04:57 with Source you can well I like to use mac. EnV when I'm on Mac and I just add\n4:05:03 export onto the start there and then enter my API Keys now I actually already\n4:05:09 have these in this local. EMV file which over in my terminal I'll just activate with Source\n4:05:17 again like that now we'll need that when we are running our API and application\n4:05:23 later but for now let's just focus on understanding what the API actually\n4:05:29 looks like so navigating into the 09 Capstone chapter we'll find a few things\n4:05:36 what we're going to focus on is the API here and we have a couple of notebooks that help us just understand okay what\n4:05:44 are we actually doing here so let me give you a quick overview of the API\n4:05:49 first so the API we're using fast API for this we have a few functions in here\n4:05:54 the one we'll start with is this okay so this is our post Endo for invoke and\n4:06:01 this essentially sends something to our llm and begins a streaming response so\n4:06:07 we can go ahead and actually start the API and we can just see what this looks like so we'll go into chapter 09 caps\n4:06:14 there and API after setting our environment variables here and we just want to do UV\n4:06:20 run uicorn main colon app reload we don't need to reload but if we're\n4:06:26 modifying the code that can be useful okay and we can see that our API is now running on Local Host Port\n4:06:34 8000 and if we go to our browser we can actually open the dots for our API so we\n4:06:42 go to 8,000 slash dos okay we just see that we have that single invoke method\n4:06:48 it stripes the content and it gives us a small amount of information there now we could try\n4:06:55 out here so if we say say hello we can\n4:07:00 run that and we'll see that we get a response we get this okay now the thing\n4:07:09 that we're missing here is that this is actually being streamed back to us okay so this is not a just a direct response\n4:07:16 this is a stream to see that we're going to navigate over to here to this streaming test notebook and we'll run\n4:07:25 this so we are using request here we are not just doing a you know the standard\n4:07:32 post request because we want to stream the output and then print the output as we are receiving them okay so that's why\n4:07:40 this looks a little more complicated than just a typical request. post or request. getet so what we're doing here\n4:07:48 is we're starting our session which is our our post request and then we're just\n4:07:53 iterating through the content as we receive it from that request when we\n4:07:59 receive a token because sometimes this might be non we print that okay and we\n4:08:04 have that flush equals TRS we have used in the past so let's define that and\n4:08:10 then let's just ask a simple question what is 5 + 5\n4:08:16 okay and we we saw that that was it was pretty quick so it generated this response first and then it went ahead\n4:08:23 and actually continued streaming with all of this okay and we can see that there\n4:08:29 these special tokens are being provided this is to help the front end basically\n4:08:35 decide okay what should go where so here where we're showing these multiple steps\n4:08:43 of tool use and the parameters the way the front end is deciding how to display\n4:08:48 those is it's just it's being provided the single stream but it has the SE tokens has a SE has se name then it has\n4:08:57 the parameters followed by the sort of ending of the step token and it's looking at each one of these and then\n4:09:03 the one step name that it treats differently is where it will see The Final Answer step name when it sees the\n4:09:10 final answer step name rather than displaying this tool interface it instead begins streaming the tokens\n4:09:16 directly at like typical chat interface and if we look at what we actually get\n4:09:22 in our final answer it's not just the answer itself right so we have the answer here this is streamed into that\n4:09:31 typical chat output but then we also have tools used and then this is added\n4:09:36 into the little boxes that we have below the chat here so there's quite a lot\n4:09:42 going on just within this little stream now we can try with some other questions\n4:09:47 here so we can say okay tell me about the latest news in the world you can see that there's a little bit of a wait here\n4:09:53 whilst it's waiting to get the response and then yeah it's streaming a lot of stuff quite quickly okay so there's a\n4:10:00 lot coming through here okay and then we can ask other questions like okay this\n4:10:05 one here how called is in Osa right now is five mtip by five right so these two\n4:10:10 are going to be executed in parallel and then it will after it has the answers for those the agent will use the another\n4:10:18 multiply tool to multiply those two values together and all of that will get streamed okay and then as we saw earlier\n4:10:26 we have the what is the current Daye and time in these places same thing so three questions three questions here what is\n4:10:33 the current date and time in Dubai what is the current date and time in Tokyo and what is the current date and time in Berlin those three questions get\n4:10:41 executed in parallel against St I search at all and then all answers get returned\n4:10:47 within that final answer okay so that is how our API is working now let's dive a\n4:10:55 little bit into the code and understand how it is working so there are a lot of\n4:11:01 important things here there's some complexity but at the same time we've tried to make this as simple as possible\n4:11:07 as well so let's just fast API syntax here with the app post invoke so our\n4:11:13 invoke endpoint we consume some some content which just a string and then if you remember from the agent execut a\n4:11:21 deep dive which is what we've implemented here or a modified version of that we have to initialize our asyn q\n4:11:30 and our streamer which is the Q coreback Handler which I believe is exactly the same as what we defined in that earlier\n4:11:37 chapter there's no differences there so we Define that and then we return and\n4:11:43 this streaming response object right again this is a fast API thing this is so that you are streaming a response\n4:11:50 that streaming response has a few attributes here which again are fast API\n4:11:55 things or just generic API things so some headers giving instructions to the\n4:12:01 API and then the media type here which is text event stream you can also use I\n4:12:06 think it's text plane possibly as well but I believe this standard here would\n4:12:11 be to use event screen and then the more important part for us is this token\n4:12:17 generator okay so what is this token generator well it is this function that\n4:12:23 we defined up here now if you again if you remember that earlier chapter at the\n4:12:28 end of the chapter we set up a a for Loop where we were printing out\n4:12:34 different tokens in various formats so we kind of pro postprocessing them before deciding how to display them\n4:12:42 that's exactly what doing here so in this block here We're looping through\n4:12:50 every token that we're receiving from our streamer We're looping through and we're just saying okay if this is the\n4:12:57 end of a step we're going to yield this end of Step token which we we saw here\n4:13:03 okay so it's this end of end of St token there otherwise if this is a tool call\n4:13:10 so again we've got that W operator here so what we're doing is saying okay get the tool calls out from our current\n4:13:18 message if there is something there so if this is not nonone we're going to execute what inside here and what is\n4:13:25 being executed inside here is we're checking for the tool name if we have the tool name we return this okay so we\n4:13:32 have the start step token the start of Step name token the tool name or set\n4:13:39 name whichever those you want to call it and then the end of the set name token\n4:13:45 okay and then this of course comes through to the front end like that okay that's what we have there\n4:13:52 otherwise we should only be seeing the tool name returned as part of first token for every step after that it\n4:13:59 should just be tool arguments so in this case we say okay if we have those tool\n4:14:05 or function arguments we're going to just return them directly so then that is the part that would stream all of\n4:14:11 this here okay like these would be individual tokens right for example\n4:14:16 right so we might have the open curly brackets followed by query could be a token latest could be a token world\n4:14:24 could be a token news could be a token Etc okay so that is what is happening there this should not get executed but\n4:14:31 we have a we just handle that just in case so we have any issues with tokens\n4:14:37 being returned there we're just going to print as error and we're going to continue with the streaming but that\n4:14:43 should not really be happening cool so that is our like token streaming Loop\n4:14:51 now the way that we are picking up tokens from our stream object here is of\n4:14:56 course through our agent execution logic which is happening in parallel okay so\n4:15:02 all of this is asynchronous we have this async definition here so all of this is happening asynchronously so what has\n4:15:09 happened here is here we have created a task which is the agent ex you to invoke\n4:15:16 and we passing our content we passing that streamer which we're going to be pulling tokens from and we also set\n4:15:21 Theos to true we can actually remove that but that would just allow us to see\n4:15:27 additional output in our terminal window if we want it I don't think there's\n4:15:33 anything particularly interesting to look at in there but particularly if you are debugging that can be useful so we\n4:15:41 create our task here but this does not begin the task right this is a asyn iio\n4:15:47 create task but this does not begin until we await it down here so what is\n4:15:53 happening here is essentially this code here is still being run and like a we're\n4:16:00 in an asynchronous Loop here but then we await this task as soon as we await this task tokens will start being placed\n4:16:07 within our que which then get picked up by the streamer object here so then this\n4:16:13 begins receiving tokens I know asyn code is always a little bit more confusing\n4:16:21 given the strange order of things but that is essentially what is happening\n4:16:26 you can imagine all this is essentially being executed all at the same time so\n4:16:31 we have that is there anything else to go through here I don't think so it's all sort of boiler plates stuff for fast\n4:16:37 API rather than the actual AI code itself so we have that that's our\n4:16:43 streaming function now let's have a look at the agent code itself okay so agent code where would\n4:16:50 that be so we're using this agent executor invoke and we're importing this\n4:16:56 from the agent file so we can have a look in here for this now you can see\n4:17:02 straight away we're pulling in our API Keys here just yeah make sure that you\n4:17:07 do have those now all of our C okay this is what we've seen before\n4:17:13 in that agent execut to Deep dive chapter this is all practically the same\n4:17:20 so we have our LM we've set those configurable fields as we did in the\n4:17:26 earlier chapters that configurable field is for our callbacks we have our prompt this has been modified a little bit so\n4:17:35 essentially just telling it okay make sure you use the tools provided we say You must use the final\n4:17:41 answer tool to provide a final answer to the user and one thing that I added that I notice every now and again so I have explicitly\n4:17:48 said Ed to answer to users current question not pre-used questions so I\n4:17:54 found with this setup it will occasionally if I just have a little bit of small talk with the agent and\n4:18:01 beforehand I was asking questions about okay like what was the weather in this place or that place the agent will kind\n4:18:07 of hang on to those previous questions and try and use a tool again to answer and that is just something that you can\n4:18:13 more or less prompt out of it okay so we have that this is all exactly the same\n4:18:19 as before okay so we have our chat history to make this conversational we have our human message and then our\n4:18:25 agent scratchpad so that agent can think through multiple tool use messages great\n4:18:31 so we also have the article class so this is to process results from Sur\n4:18:39 API we have our Ser API function here I will talk about that a little more in a\n4:18:44 moment because this is also a little bit different to what we covered before what we covered before with C API if you\n4:18:51 remember was synchronous because we're using the Ser API client directly or the\n4:18:57 Ser API tool directly from line chain and because we want everything to be\n4:19:02 asynchronous we have had to recreate that tool in a asynchronous fashion\n4:19:09 which we'll talk about a little bit later but for now let's move on from that we see our final answer being used\n4:19:18 here so this is I think we defined the exact same thing before probably in that\n4:19:23 deep dive chapter again where we have just the answer and the tools that have been used great so we have that one thing\n4:19:31 that is a little different here is when we are defining our name to Tool\n4:19:38 function so this takes a tool name and it Maps it to a tool to function when we\n4:19:45 have synchronous tools we actually use tool Funk here okay so rather than tool\n4:19:52 cartin it would be tool Funk however we are using a synchronous\n4:19:58 tools and so this is actually tool co-routine and this is why this is why\n4:20:04 if you if you come up here I've made every single tool asynchronous now that is not really NE\n4:20:12 for a tool like final answer because there is no there's no API calls happening an API call is a very typical\n4:20:19 scenario where you do want to use async because if you make an API call with a synchronous function your code is just\n4:20:26 going to be waiting for the response from the API while the API is processing\n4:20:32 and doing whatever it's doing so that is an ideal scenario where you would want to use async because rather\n4:20:39 than your code just waiting for the response from the API it can instead go\n4:20:44 and do something else whilst it's waiting right so that's an ideal scenario where you'd use async which is\n4:20:50 why we would use it for example with a Ser API tool here but for final answer\n4:20:55 and for all of these calculator tools that we built there's actually no need\n4:21:02 to have these as async because our code is just running through its executing\n4:21:07 this code there's no waiting involved so it doesn't necessarily make sense have these a synchronous however by making\n4:21:15 them asynchronous it means that I can do tool care routine for all of them rather than saying oh if this tool is\n4:21:22 synchronous use tool. Funk whereas if this one is async use tool. cartin so\n4:21:28 just simplifies the code for us a lot more but yeah not directly necessary but\n4:21:34 it does help us write cleaner code here this is also true later on because we actually have\n4:21:40 to await our tool code which we can see over here right so we\n4:21:47 have to await those tool calls that would get Messier if we were using the\n4:21:53 like some sync tools some async tools so we have that we have our Q callback\n4:21:59 Handler this is again that's the same as before so I'm not going to go through\n4:22:04 I'm not going to go through that we covered that in the earlier Deep dive chapter we have our execute tool\n4:22:09 function here again that is a synchronous this just helps us you know clean up code a little bit this would I\n4:22:16 think in the Deep dive chapter we had this directly placed within our agent\n4:22:21 executor function and you can do that it's fine it's just a bit cleaner to kind of pull this out and we can also\n4:22:28 add more type annotations here which I like so execute tool expects us to\n4:22:33 provide an AI message which includes a tool call within it and it will return\n4:22:38 as a tool message okay agent exor this\n4:22:43 is all the same as before and we're actually not even using verose here so we could fully remove it but I I will\n4:22:49 leave it of course if you would like to use that you can just add a ifos and then log or print some stuff where you\n4:22:57 need it okay so what do we have in here we have our streaming function so this\n4:23:03 is what actually calls our agent right so we have a query this will\n4:23:10 call our agent just here and we could even make this a little clearer so for example this could be\n4:23:17 configured agent because this is this is not the response this is a configured\n4:23:23 agent so I think this is may be a lot clearer so we are configuring our agent with our callbacks okay which is just\n4:23:30 our streamer then we're iterating through the tokens are returned by our\n4:23:35 agent using a stream here okay and as we are iterating through this because we\n4:23:42 pass our streamer to the Callback here what that is going to do is every single\n4:23:48 token that our agent returns is going to get processed through our Q callback\n4:23:56 Handler here okay so this on LM new token on LMN these are going to get\n4:24:03 executed and then all of those tokens you can see here I'll pass to our Q okay\n4:24:09 then we come up here and we have this a it so that this aor method here is used\n4:24:15 by our generator over in our API is used by this token\n4:24:21 generator to pick up from the queue the tokens that have been put in the queue\n4:24:28 by these other methods here okay so it's putting tokens into the queue and\n4:24:34 pulling them out with this okay so that is just happening in\n4:24:39 parallel as well as this code is running here now the reason that we extract the\n4:24:45 tokens out here is that we want to pull out our tokens and we append them all to\n4:24:50 our outputs now those outputs that becomes a list of AI messages which are\n4:24:57 essentially the AI telling as what tool to use and what parameters to pass to each one of those tools this is very\n4:25:05 similar to what we covered in that deep dive chapter but the one thing that I have modified here is I've enabled us to\n4:25:12 use parallel tool calls so that is what we see here with this these four lines\n4:25:20 of code we're saying okay if our tool call includes an ID that means we have a\n4:25:25 new tool call or a new AI message so what we do is we append that AI message\n4:25:32 which is the AI message chunk to our outputs and then following that if we\n4:25:37 don't get an ID that means we're getting the tool arguments so following that we're just\n4:25:42 adding our AI message chunk to the most recent AI message Chunk from our outputs\n4:25:49 okay so what that will do is it it will create that list of AI messages would be\n4:25:56 like AI message one and then this will just append everything to that AI\n4:26:02 message one then we'll get our next AI message chunk this will then just append\n4:26:08 everything to that until we get a complete AI message and so on and so on\n4:26:15 okay so what we do here is here we've collected all our AI message chunk\n4:26:21 objects then finally what we do is just transform all those AI message chunk objects into actual AI message objects\n4:26:28 and then return them from our function which we then receive over here so into the tool cuse variable okay now this is\n4:26:37 very similar to The Deep dive chapter again we're going through that that count that Loop where we have a Max\n4:26:44 iterations at which point we will just stop but until then we continue\n4:26:49 iterating through and making more tool calls executing those tool calls and so on so what what is going on here let's\n4:26:57 see so we got our tool calls there's going to be a list of AI message objects then what we do with those AI\n4:27:04 message objects is we pass them to this ex cuute tool function if you remember what is that that is this function here\n4:27:13 so we pass each AI message individually to this function and that will execute\n4:27:19 the tool force and then return us that observation from the tool okay so that is what you see\n4:27:28 happening here but this is an async method so typically what you'd have to\n4:27:33 do is you'd have to do await X you tool and we could do that so we could do a\n4:27:39 okay let me let me make this a little bigger for us okay and so what we could do for example\n4:27:45 which might be a bit clearer is you could do tool OBS equals an empty list\n4:27:51 and what you can do is you can say for Tool call oops in tool calls the\n4:27:58 tool observation is we're going to append execute tool call which would\n4:28:04 have to be in a weit so we' actually put your weight in there and what this would do is actually the exact same thing as\n4:28:10 what we're doing here the difference being that we're doing this tool by Tool\n4:28:16 okay so we are we're executing async here but we're doing them sequentially\n4:28:23 whereas what we can do which is better is we can use asyn I gather so what this\n4:28:28 does is gathers all those Co routines and then we await them all at the same\n4:28:33 time to run them all asynchronously they all begin at the same time or almost\n4:28:38 exactly at the same time and we get those responses kind of in parallel but of course it's\n4:28:44 saying so it's not fully in parallel but practically in parallel cool so we have\n4:28:51 that and then that okay we get all of our tool observations from that so that's all of our tool messages and then\n4:28:57 one interesting thing here is if we let's say we have all of our AI messages\n4:29:04 of all of our tool cores and we just append all of those to our agent scratch Pad right so let's say here we're just\n4:29:11 like oh okay scratch Pad extend and then we would just have\n4:29:17 okay we' have our tool calls and then we do agent stretch PCT\n4:29:23 send tool OBS all right so what what is happening here is this would essentially\n4:29:28 give us something that looks like this so we have our AI message say I'm just\n4:29:37 going to put okay we'll just put tool call IDs in here to simplify a little bit\n4:29:42 this would be tool call ID a then we would have ai message tool call ID B\n4:29:50 then we'd have tool message let's just remove this content\n4:29:55 field I don't want that and Tool message tool call ID B right so it would look\n4:30:02 something like this so the the order is the tool message is not following the AI\n4:30:07 message which you would think okay we have this tool qual ID that's probably fine actually when we're running this if\n4:30:14 you add these to your agent scratch pad in this order what you'll see is your response\n4:30:20 just hangs like nothing nothing happens when you come through to your second uh\n4:30:25 iteration of your agent call so actually what you need to do is these need to be\n4:30:30 sorted so that they are actually in order and it doesn't actually doesn't\n4:30:36 necessarily matter which order in terms of like a or b or c or whatever you use so you could have this order we have ai\n4:30:42 message tool message AI message tool message just as long as you have your tool call IDs are both together or you\n4:30:49 could know invert this for example right so you could have this right and that that will work\n4:30:55 as well it's essentially just as long as you have your AI message followed by your tool message and both of those are\n4:31:02 sharing that tool call ID you need to make sure you have that order okay so\n4:31:08 that of course would not happen if we do this and instead what we need to do is\n4:31:14 something like this okay so I made this a lot easier to read okay so we're\n4:31:20 taking the tool call ID we are pointing it to the tool observation and we're\n4:31:26 doing that for every tool call and to Observation within like a zip of those\n4:31:31 okay then what we're saying is for each tool call within our tool calls we are\n4:31:37 extending our agent scratch pad with that tool call followed by by the tool\n4:31:43 observation message which is the tool message so this would be our this is the AI message and that is\n4:31:50 the tool messages down there okay so that is what it's happening and that is how we get this correct order which will\n4:31:58 run otherwise things will not run so that's important to be aware of okay now\n4:32:05 we're we're almost done I know there's we just been through quite a lot so we continue we incre increment our count as\n4:32:11 we were doing before then we need to check for the final answer tool okay and because we're running these tools in parallel okay\n4:32:18 because we're allowing multiple tool calls in one step we can't just look at the most recent tool and look if it is\n4:32:25 it has the name Final Answer instead we need to iterate through all of our tool calls and check if any of them have the\n4:32:30 name final answer if they do we say okay we extract that final answer call we\n4:32:35 extract the final answer as well so this is the direct text content and we say\n4:32:40 okay we have found found the final answer so this will we set to True okay which should happen every time but let's\n4:32:47 say if our agent gets stuck in a loop of calling multiple tools this might not\n4:32:53 happen before we break based on the max iterations here so we might end up\n4:32:59 breaking based on Max iterations rather than we found a final answer okay so\n4:33:04 that can happen so anyway if we find that final answer we break out of this\n4:33:09 for Loop here and then of course we do need to break out of our wow Loop which is here so we say if we found the final\n4:33:16 answer break okay cool so we have that\n4:33:22 finally after all of that so this is our you know we've executed our tool our\n4:33:27 agent steps and iterations has process we've been through those finally we come\n4:33:34 down to here where we say okay we're going to add that final output to our chat history so this is just going to be\n4:33:41 the text content right so this here get direct answer but then what we do is we\n4:33:49 return the full final answer call the full final answer call is basically this here right so this answer and tools used\n4:33:57 but of course populated so we're saying here that if we have a final answer okay\n4:34:03 if we have that we're going to return the final answer call which was generated by our llm otherwise we're\n4:34:09 going to return this one so this is in the scenario that maybe the agent got caught in a loop and just kept iterating\n4:34:16 if that happens we'll say it will come back with okay no answer found and it will just return okay we didn't use any\n4:34:23 tools which is not technically true but it's this is like a exception handling\n4:34:29 event so it ideally it shouldn't happen but it's not really a big deal if we're\n4:34:35 saying okay there were no tools use in my opinion anyway cool so we have all of\n4:34:41 that and yeah we just we initialize our agent executor and then I mean that that is\n4:34:48 our agent execution code the one last thing we want to go through is the Ser API tool which we will do in a moment\n4:34:56 okay so Ser API let's see what let's see\n4:35:01 how we build our Ser API tool okay so we'll start with the synchronous Ser API\n4:35:10 now the reason we're starting with this is that it's actually it's just a bit simpler so I'll show you this quickly\n4:35:16 before we move on to the async implementation which is what we're using within our app so we want to get our set\n4:35:23 API API key so I'll run that and we just enter it at the top\n4:35:29 there and this will R so we're going to use the sub API SDK first we're\n4:35:36 importing Google search and these are the input prameters so we have our API key we're using we say want use Google\n4:35:42 we our question is so query so Q for query we're searching for the latest\n4:35:47 news in the world it will return quite a lot of stuff you can see there's a ton of stuff in there right\n4:35:56 now what we want is contained within this organic results key so we can run\n4:36:02 that and we'll see K is talking about you various things pretty recent stuff\n4:36:09 at the moment so we can tell okay that is that is in fact working now this is\n4:36:14 quite messy so what I would like to do first is just clean that up a little bit so we Define this article base model\n4:36:21 which is pantic and we're saying okay from a set of results okay so we're\n4:36:27 going to iterate through each of these we're going to extract the title source\n4:36:32 link and the snippet so you can see title source link and snippet here\n4:36:42 okay so that's all usedful we'll run that and what we do is we go through\n4:36:48 each of the results in organic results and we just load them into our article using this class method here and then we\n4:36:55 can see okay let's have a look at what those look like it's much nicer okay we\n4:37:02 get this nicely formatted object here cool that's great now all of this what\n4:37:10 we just did here so this is using sub apis SDK which is great super easy to\n4:37:15 use the problem is that they don't offer a async SDK which is a shame but it's\n4:37:22 not that hard for us to set up ourselves so typically with a synchronous requests\n4:37:29 what we can use is the aiio HTTP Library it's well it's you can see what we're\n4:37:35 doing here so this is equivalent to requests Dot get okay that's essentially\n4:37:43 what we're doing here and the equivalent is literally this okay so this is the\n4:37:50 equivalent using requests that we are running here but we're using asyn Code\n4:37:55 so we're using AI Hep client session and then session. getet okay with this async\n4:38:03 width here and then we just await our response so this is all yeah this is what we do rather than this to make our\n4:38:11 code async so it's really simple and then the output that we get is exactly the same\n4:38:16 right so we still get this exact same output so that means of course that we can use that articles method like this\n4:38:25 in the exact same way and we get we get the same result there's no need to make this article from sub API result asnc\n4:38:34 because again like this this bit of code here is fully local it's just our python\n4:38:39 running everything so this does not need to be async okay and we can see that we\n4:38:45 get literally the exact same result there so with that we have everything\n4:38:51 that we would need to build a fully asynchronous Sur API tool which is exactly what we do here for Lang chain\n4:38:58 so we import those tools and I mean there's nothing is there anything different here no this is exactly what\n4:39:04 we we just said but I will run this because I would like to show you very quickly this okay so this is how we were\n4:39:12 initially calling our Tools in previous chapters because we we were okay mostly\n4:39:18 with using the the synchronous tools however you can see that the funk here\n4:39:26 is just empty right so if I do type just a non-type that is\n4:39:31 because well this is an async function okay it's an async tool sorry so it was\n4:39:40 defined with async here what happens when you do that is you get\n4:39:45 this Co routine object so rather than Funk which is it isn't here you get that\n4:39:52 cartine if we then modified this which would be kind of okay let's just remove\n4:39:58 all the ayns here and the await if we modify that like so and then\n4:40:06 we look at the set API structure tool we go across we see that we now get that\n4:40:12 funk okay so that is that is just the difference between an async structured\n4:40:17 tool versus a sync structured tool we of course on\n4:40:22 async okay now we have K again so important to be aware of that and of\n4:40:29 course we we run using the sub API care\n4:40:34 routine so that is that's how we build the sub API tool\n4:40:41 uh there's nothing I mean that is exactly what we did here so I don't need to I don't think we need to go through that any further so yeah I think that is\n4:40:50 basically all of our code behind this API with all of that we can then go\n4:40:55 ahead so we have our API running already let's go ahead and actually run also our\n4:41:02 front end so we're going to go to documents orelo line chain course and\n4:41:07 then we want to go to Chapters 09 Capstone app and you will need to have npm\n4:41:14 installed so to do that what do we do we can take a look at this answer for example this is probably what I would\n4:41:21 recommend okay so I would run Brew install node followed by Brew install mpm if you're on Mac of course it's\n4:41:28 different if you're on Linux or Windows once you have those you can do npm install and this will just install all\n4:41:34 of the oop sorry mpm install and this would just install all of the node\n4:41:40 packages that we need and then we can just run npm run Dev okay and now we\n4:41:48 have our app running on locost 3000 so we can come over to here open\n4:41:53 that up and we have our application can ignore this so in here we can begin just\n4:42:00 asking questions okay so we can start with quick question what is 5 +\n4:42:07 5 and you see so we have our streaming happening here it said the agent wants to use ad tool and these are the input\n4:42:14 parameters to the ad tool and then we get the streamed response so this is the\n4:42:20 final answer tool where we're outputting that answer key and value and then here we're outputting that tools used key and\n4:42:27 value which is just an array of the tools being used which just functions add so we have that then let's ask\n4:42:35 another question this time we'll trigger Ser API with tell me about the latest news in the world\n4:42:42 okay so we can see that's using C API and a query is latest world\n4:42:48 news and then it comes down here and we actually get some citations here which is kind of cool so you can also come\n4:42:55 through to here okay and it teses through to here so that's pretty cool unfortunately I\n4:43:01 just lost my chat so fine let me I can ask that\n4:43:07 question again\n4:43:16 okay we can see that to us set API there now let's continue with the next question from our notebook which is how\n4:43:23 cold is in I like right now what is five M by five what do you get when multiplying those two numbers together\n4:43:30 I'm just going to modify that to say in Celsius so that I can understand thank\n4:43:36 you okay so for this one we can see what did we get so we got current temperature\n4:43:41 in ow we got multiply 5 by five which our second question and then we also got\n4:43:49 subtract interesting that I I don't know why it did that it's kind of weird so it\n4:43:54 it decided to use oh ah okay so this is\n4:44:00 okay so then here it was okay that kind of makes sense does\n4:44:05 that make sense roughly okay so I think the the conversion for Fahrenheits Celsius is\n4:44:11 say like subtract 32 okay yes so to go from Fahrenheit to\n4:44:18 Celsius you are doing basically Fahrenheit minus 32 and then you're\n4:44:23 multiplying by this number here which the iume the AI did\n4:44:28 not oh it roughly did okay so subtracting 36 like 32 would have given us four and it gave us approximately two\n4:44:36 so if you think okay multiply by this it's practically multiplying by 0.5 five\n4:44:41 so halfing the value and that would give us roughly 2 so that's what this was\n4:44:47 doing here kind of interesting Okay cool so we've gone through we have seen how\n4:44:54 to build a fully fledged chat application using what\n4:44:59 we've learned throughout a course and we've built quite a lot if you think about this application you're getting\n4:45:07 the real time updates on what tools are being used the parameters being input to those tools and then that is all being\n4:45:13 returned in a streamed output and even in a structured output for your final\n4:45:18 answer including the answer and the tools that we use so of course you know what we built here is fairly limited but\n4:45:26 it's super easy to extend this like you could maybe something that you might want to go and do is take what we've\n4:45:33 built here like Fork this application and just go and add different tools to it and see what happens because this is\n4:45:40 very extensible you can do a lot with it but yeah that is the end of the course\n4:45:46 of course this is just the beginning of whatever it is you're wanting to learn\n4:45:52 or build with AI treat this as the beginning and just go out and find all\n4:45:58 the other cool interesting stuff that you can go and build so I hope this course has been useful\n4:46:06 informative and gives you an advantage in whatever it is is you're going out of this build so thank you very much for\n4:46:13 watching and taking the course and sticking through right to the end I know it's pretty long so I appreciate it a\n4:46:21 lot and I hope you get a lot out of it thanks bye"

### Ran Playwright code
await page.evaluate('() => {\n  const segments = document.querySelectorAll(\'ytd-transcript-segment-renderer\');\n  let transcript = [];\n  segments.forEach(segment => {\n    const timestamp = segment.querySelector(\'.segment-timestamp\')?.textContent?.trim();\n    const text = segment.querySelector(\'.segment-text\')?.textContent?.trim();\n    if (timestamp && text) {\n      transcript.push(`${timestamp} ${text}`);\n    }\n  });\n  return transcript.join(\'\\n\');\n}');

### New console messages
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204 was preloaded using li...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204?conn2 was preloaded us...
- [WARNING] The resource https://i.ytimg.com/generate_204 was preloaded using link preload but not use...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204 was preloaded using li...
- [WARNING] The resource https://rr1---sn-5ualdnls.googlevideo.com/generate_204?conn2 was preloaded us...
- [WARNING] The resource https://i.ytimg.com/generate_204 was preloaded using link preload but not use...

### Page state
- Page URL: https://www.youtube.com/watch?v=Cyv-dgv80kE
- Page Title: LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3] - YouTube
- Page Snapshot:
```yaml
- <changed> generic [ref=e27]:
  - ref=e28 [unchanged]
  - ref=e29 [unchanged]
  - button "1 Minutes 53 Seconds of 4 Hours 46 Minutes 46 Seconds" [ref=e16785] [cursor=pointer]: 1:53 / 4:46:46
  - ref=e288 [unchanged]
```
